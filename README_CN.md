# Latest Adversarial Attack Papers
**update at 2022-06-01 06:31:33**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. GAN-based Medical Image Small Region Forgery Detection via a Two-Stage Cascade Framework**

基于两级级联框架的GaN医学图像小区域篡改检测 eess.IV

**SubmitDate**: 2022-05-30    [paper-pdf](http://arxiv.org/pdf/2205.15170v1)

**Authors**: Jianyi Zhang, Xuanxi Huang, Yaqi Liu, Yuyang Han, Zixiao Xiang

**Abstracts**: Using generative adversarial network (GAN)\cite{RN90} for data enhancement of medical images is significantly helpful for many computer-aided diagnosis (CAD) tasks. A new attack called CT-GAN has emerged. It can inject or remove lung cancer lesions to CT scans. Because the tampering region may even account for less than 1\% of the original image, even state-of-the-art methods are challenging to detect the traces of such tampering.   This paper proposes a cascade framework to detect GAN-based medical image small region forgery like CT-GAN. In the local detection stage, we train the detector network with small sub-images so that interference information in authentic regions will not affect the detector. We use depthwise separable convolution and residual to prevent the detector from over-fitting and enhance the ability to find forged regions through the attention mechanism. The detection results of all sub-images in the same image will be combined into a heatmap. In the global classification stage, using gray level co-occurrence matrix (GLCM) can better extract features of the heatmap. Because the shape and size of the tampered area are uncertain, we train PCA and SVM methods for classification. Our method can classify whether a CT image has been tampered and locate the tampered position. Sufficient experiments show that our method can achieve excellent performance.

摘要: 利用产生式对抗性网络(GAN)对医学图像进行数据增强，对许多计算机辅助诊断(CAD)任务有重要的帮助。一种名为CT-GAN的新攻击已经出现。它可以将肺癌病变注入或移除到CT扫描中。由于篡改区域甚至可能只占原始图像的不到1%，即使是最先进的方法也很难检测到这种篡改的痕迹。针对CT-GaN等基于GaN的医学图像小区域伪造问题，提出了一种级联检测框架。在局部检测阶段，我们用较小的子图像来训练检测器网络，使得真实区域中的干扰信息不会影响检测器。我们使用深度可分离的卷积和残差来防止检测器过拟合，并通过注意机制增强发现伪造区域的能力。将同一图像中所有子图像的检测结果合并成热图。在全局分类阶段，使用灰度共生矩阵(GLCM)可以更好地提取热图的特征。由于篡改区域的形状和大小是不确定的，我们训练了主成分分析和支持向量机方法进行分类。我们的方法可以对CT图像是否被篡改进行分类，并定位被篡改的位置。充分的实验表明，我们的方法可以达到很好的性能。



## **2. Why Adversarial Training of ReLU Networks Is Difficult?**

为什么RELU网络的对抗性训练很难？ cs.LG

**SubmitDate**: 2022-05-30    [paper-pdf](http://arxiv.org/pdf/2205.15130v1)

**Authors**: Xu Cheng, Hao Zhang, Yue Xin, Wen Shen, Jie Ren, Quanshi Zhang

**Abstracts**: This paper mathematically derives an analytic solution of the adversarial perturbation on a ReLU network, and theoretically explains the difficulty of adversarial training. Specifically, we formulate the dynamics of the adversarial perturbation generated by the multi-step attack, which shows that the adversarial perturbation tends to strengthen eigenvectors corresponding to a few top-ranked eigenvalues of the Hessian matrix of the loss w.r.t. the input. We also prove that adversarial training tends to strengthen the influence of unconfident input samples with large gradient norms in an exponential manner. Besides, we find that adversarial training strengthens the influence of the Hessian matrix of the loss w.r.t. network parameters, which makes the adversarial training more likely to oscillate along directions of a few samples, and boosts the difficulty of adversarial training. Crucially, our proofs provide a unified explanation for previous findings in understanding adversarial training.

摘要: 本文从数学上推导了RELU网络上对抗性扰动的解析解，并从理论上解释了对抗性训练的困难。具体地说，我们描述了由多步攻击产生的对抗扰动的动力学，这表明对抗扰动倾向于增强对应于损失的Hessian矩阵的几个顶层特征值的特征向量。输入。我们还证明了对抗性训练倾向于以指数的方式增强具有大梯度范数的不自信输入样本的影响。此外，我们还发现，对抗性训练增强了损失的黑森矩阵的影响。网络参数，使得对抗性训练更容易沿着少数样本的方向振荡，增加了对抗性训练的难度。至关重要的是，我们的证据为理解对抗性训练提供了一个统一的解释。



## **3. Domain Constraints in Feature Space: Strengthening Robustness of Android Malware Detection against Realizable Adversarial Examples**

特征空间中的域约束：增强Android恶意软件检测对可实现的恶意示例的健壮性 cs.LG

**SubmitDate**: 2022-05-30    [paper-pdf](http://arxiv.org/pdf/2205.15128v1)

**Authors**: Hamid Bostani, Zhuoran Liu, Zhengyu Zhao, Veelasha Moonsamy

**Abstracts**: Strengthening the robustness of machine learning-based malware detectors against realistic evasion attacks remains one of the major obstacles for Android malware detection. To that end, existing work has focused on interpreting domain constraints of Android malware in the problem space, where problem-space realizable adversarial examples are generated. In this paper, we provide another promising way to achieve the same goal but based on interpreting the domain constraints in the feature space, where feature-space realizable adversarial examples are generated. Specifically, we present a novel approach to extracting feature-space domain constraints by learning meaningful feature dependencies from data, and applying them based on a novel robust feature space. Experimental results successfully demonstrate the effectiveness of our novel robust feature space in providing adversarial robustness for DREBIN, a state-of-the-art Android malware detector. For example, it can decrease the evasion rate of a realistic gradient-based attack by $96.4\%$ in a limited-knowledge (transfer) setting and by $13.8\%$ in a more challenging, perfect-knowledge setting. In addition, we show that directly using our learned domain constraints in the adversarial retraining framework leads to about $84\%$ improvement in a limited-knowledge setting, with up to $377\times$ faster implementation than using problem-space adversarial examples.

摘要: 增强基于机器学习的恶意软件检测器对现实规避攻击的健壮性仍然是Android恶意软件检测的主要障碍之一。为此，现有的工作集中于在问题空间中解释Android恶意软件的域约束，在问题空间中生成可实现的敌意示例。在本文中，我们提供了另一种有希望的方法来实现相同的目标，但基于对特征空间中的域约束的解释，在特征空间中生成可实现的对抗性实例。具体地说，我们提出了一种新的方法，通过从数据中学习有意义的特征依赖关系，并基于新的稳健特征空间来应用它们来提取特征空间域约束。实验结果成功地证明了新的稳健特征空间在为最先进的Android恶意软件检测器Drebin提供对抗鲁棒性方面的有效性。例如，它可以使基于现实梯度的攻击的逃避率在有限知识(迁移)环境下降低96.4美元，在更具挑战性的完美知识环境下降低13.8美元。此外，我们还表明，在对抗性再训练框架中直接使用我们学习到的领域约束可以在知识有限的情况下带来约84美元的改进，与使用问题空间对抗性示例相比，执行速度最高可快377倍。



## **4. Guided Diffusion Model for Adversarial Purification**

对抗性净化中的引导扩散模型 cs.CV

**SubmitDate**: 2022-05-30    [paper-pdf](http://arxiv.org/pdf/2205.14969v1)

**Authors**: Jinyi Wang, Zhaoyang Lyu, Dahua Lin, Bo Dai, Hongfei Fu

**Abstracts**: With wider application of deep neural networks (DNNs) in various algorithms and frameworks, security threats have become one of the concerns. Adversarial attacks disturb DNN-based image classifiers, in which attackers can intentionally add imperceptible adversarial perturbations on input images to fool the classifiers. In this paper, we propose a novel purification approach, referred to as guided diffusion model for purification (GDMP), to help protect classifiers from adversarial attacks. The core of our approach is to embed purification into the diffusion denoising process of a Denoised Diffusion Probabilistic Model (DDPM), so that its diffusion process could submerge the adversarial perturbations with gradually added Gaussian noises, and both of these noises can be simultaneously removed following a guided denoising process. On our comprehensive experiments across various datasets, the proposed GDMP is shown to reduce the perturbations raised by adversarial attacks to a shallow range, thereby significantly improving the correctness of classification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under PGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on the challenging ImageNet dataset.

摘要: 随着深度神经网络(DNN)在各种算法和框架中的广泛应用，安全威胁已成为人们关注的问题之一。对抗性攻击干扰了基于DNN的图像分类器，攻击者可以故意在输入图像上添加不可察觉的对抗性扰动来愚弄分类器。在本文中，我们提出了一种新的净化方法，称为引导扩散净化模型(GDMP)，以帮助保护分类器免受对手攻击。该方法的核心是将净化嵌入到去噪扩散概率模型(DDPM)的扩散去噪过程中，使其扩散过程能够淹没带有逐渐增加的高斯噪声的对抗性扰动，并在引导去噪过程后同时去除这两种噪声。在不同数据集上的综合实验表明，所提出的GDMP将对抗性攻击引起的扰动减少到较小的范围，从而显著提高了分类的正确性。GDMP在CIFAR10数据集上的稳健准确率提高了5%，在PGD攻击下达到了90.1%。此外，GDMP在具有挑战性的ImageNet数据集上获得了70.94%的健壮性。



## **5. CalFAT: Calibrated Federated Adversarial Training with Label Skewness**

卡尔法特：带有标签偏斜度的校准联合对抗性训练 cs.LG

**SubmitDate**: 2022-05-30    [paper-pdf](http://arxiv.org/pdf/2205.14926v1)

**Authors**: Chen Chen, Yuchen Liu, Xingjun Ma, Lingjuan Lyu

**Abstracts**: Recent studies have shown that, like traditional machine learning, federated learning (FL) is also vulnerable to adversarial attacks. To improve the adversarial robustness of FL, few federated adversarial training (FAT) methods have been proposed to apply adversarial training locally before global aggregation. Although these methods demonstrate promising results on independent identically distributed (IID) data, they suffer from training instability issues on non-IID data with label skewness, resulting in much degraded natural accuracy. This tends to hinder the application of FAT in real-world applications where the label distribution across the clients is often skewed. In this paper, we study the problem of FAT under label skewness, and firstly reveal one root cause of the training instability and natural accuracy degradation issues: skewed labels lead to non-identical class probabilities and heterogeneous local models. We then propose a Calibrated FAT (CalFAT) approach to tackle the instability issue by calibrating the logits adaptively to balance the classes. We show both theoretically and empirically that the optimization of CalFAT leads to homogeneous local models across the clients and much improved convergence rate and final performance.

摘要: 最近的研究表明，与传统的机器学习一样，联邦学习(FL)也容易受到对手攻击。为了提高FL的对抗健壮性，已有几种联邦对抗训练(FAT)方法被提出在全局聚集之前局部应用对抗训练。虽然这些方法在独立同分布(IID)数据上显示了良好的结果，但它们在具有标签偏斜的非IID数据上存在训练不稳定性问题，导致自然精度大大降低。这往往会阻碍FAT在实际应用中的应用，在现实应用中，跨客户端的标签分布通常是不对称的。本文研究了标签倾斜下的FAT问题，首次揭示了训练不稳定性和自然准确率下降问题的一个根本原因：标签倾斜会导致类别概率不一致和局部模型的异构性。然后，我们提出了一种校准FAT(CALFAT)方法来解决不稳定性问题，方法是自适应地校准逻辑以平衡类别。我们从理论和实验两个方面证明了CALFAT算法的优化可以得到跨客户的同质局部模型，并且大大提高了收敛速度和最终性能。



## **6. CausalAdv: Adversarial Robustness through the Lens of Causality**

CausalAdv：通过因果关系的透镜进行对抗的健壮性 cs.LG

ICLR2022, 20 pages, 3 figures

**SubmitDate**: 2022-05-30    [paper-pdf](http://arxiv.org/pdf/2106.06196v2)

**Authors**: Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Schölkopf, Kun Zhang

**Abstracts**: The adversarial vulnerability of deep neural networks has attracted significant attention in machine learning. As causal reasoning has an instinct for modelling distribution change, it is essential to incorporate causality into analyzing this specific type of distribution change induced by adversarial attacks. However, causal formulations of the intuition of adversarial attacks and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of adversarial attacks. From the causal perspective, we study the distinction between the natural and adversarial distribution and conclude that the origin of adversarial vulnerability is the focus of models on spurious correlations. Inspired by the causal understanding, we propose the Causal inspired Adversarial distribution alignment method, CausalAdv, to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method. Our work is the first attempt towards using causality to understand and mitigate the adversarial vulnerability.

摘要: 深度神经网络的对抗性脆弱性在机器学习中引起了广泛的关注。由于因果推理具有模拟分布变化的本能，因此将因果关系纳入到分析由对抗性攻击引起的这种特定类型的分布变化中是至关重要的。然而，对抗性攻击的直觉和健壮DNN的发展的因果公式在文献中仍然缺乏。为了弥补这一差距，我们构建了一个因果图来建模对抗性实例的生成过程，并定义了对抗性分布来形式化对抗性攻击的直觉。从因果关系的角度，我们研究了自然分布和对抗性分布之间的区别，并得出结论：对抗性脆弱性的来源是伪相关性模型的重点。受因果理解的启发，我们提出了因果启发的对抗性分布对齐方法CausalAdv，通过考虑伪相关性来消除自然分布和对抗性分布之间的差异。大量实验证明了该方法的有效性。我们的工作是首次尝试使用因果关系来理解和缓解对抗性脆弱性。



## **7. Exposing Fine-grained Adversarial Vulnerability of Face Anti-spoofing Models**

暴露Face反欺骗模型的细粒度攻击漏洞 cs.CV

**SubmitDate**: 2022-05-30    [paper-pdf](http://arxiv.org/pdf/2205.14851v1)

**Authors**: Songlin Yang, Wei Wang, Chenye Xu, Bo Peng, Jing Dong

**Abstracts**: Adversarial attacks seriously threaten the high accuracy of face anti-spoofing models. Little adversarial noise can perturb their classification of live and spoofing. The existing adversarial attacks fail to figure out which part of the target face anti-spoofing model is vulnerable, making adversarial analysis tricky. So we propose fine-grained attacks for exposing adversarial vulnerability of face anti-spoofing models. Firstly, we propose Semantic Feature Augmentation (SFA) module, which makes adversarial noise semantic-aware to live and spoofing features. SFA considers the contrastive classes of data and texture bias of models in the context of face anti-spoofing, increasing the attack success rate by nearly 40% on average. Secondly, we generate fine-grained adversarial examples based on SFA and the multitask network with auxiliary information. We evaluate three annotations (facial attributes, spoofing types and illumination) and two geometric maps (depth and reflection), on four backbone networks (VGG, Resnet, Densenet and Swin Transformer). We find that facial attributes annotation and state-of-art networks fail to guarantee that models are robust to adversarial attacks. Such adversarial attacks can be generalized to more auxiliary information and backbone networks, to help our community handle the trade-off between accuracy and adversarial robustness.

摘要: 对抗性攻击严重威胁着人脸反欺骗模型的高精度。一点敌意的噪音就会扰乱他们对现场直播和恶搞的分类。现有的对抗性攻击无法计算出目标人脸的哪一部分是易受攻击的反欺骗模型，这使得对抗性分析变得棘手。因此，我们提出了细粒度攻击，以暴露人脸反欺骗模型的攻击漏洞。首先，我们提出了语义特征增强(SFA)模块，使得对抗噪声能够感知活特征和欺骗特征。SFA在人脸反欺骗的背景下考虑了模型的对比类数据和纹理偏向，使攻击成功率平均提高了近40%。其次，基于SFA和带辅助信息的多任务网络生成细粒度的对抗性实例。我们在四个骨干网络(VGG、RESNET、Densenet和Swin Transformer)上评估了三种标注(面部属性、欺骗类型和光照)和两种几何映射(深度和反射)。我们发现，人脸属性标注和最新的网络无法保证模型对对手攻击是健壮的。这种对抗性攻击可以推广到更多的辅助信息和主干网络，以帮助我们的社区处理准确性和对抗性健壮性之间的权衡。



## **8. Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning**

基于在线深度强化学习的高效奖赏中毒攻击 cs.LG

**SubmitDate**: 2022-05-30    [paper-pdf](http://arxiv.org/pdf/2205.14842v1)

**Authors**: Yinglun Xu, Qi Zeng, Gagandeep Singh

**Abstracts**: We study data poisoning attacks on online deep reinforcement learning (DRL) where the attacker is oblivious to the learning algorithm used by the agent and does not necessarily have full knowledge of the environment. We demonstrate the intrinsic vulnerability of state-of-the-art DRL algorithms by designing a general reward poisoning framework called adversarial MDP attacks. We instantiate our framework to construct several new attacks which only corrupt the rewards for a small fraction of the total training timesteps and make the agent learn a low-performing policy. Our key insight is that the state-of-the-art DRL algorithms strategically explore the environment to find a high-performing policy. Our attacks leverage this insight to construct a corrupted environment for misleading the agent towards learning low-performing policies with a limited attack budget. We provide a theoretical analysis of the efficiency of our attack and perform an extensive evaluation. Our results show that our attacks efficiently poison agents learning with a variety of state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc. under several popular classical control and MuJoCo environments.

摘要: 我们研究了针对在线深度强化学习(DRL)的数据中毒攻击，其中攻击者不知道代理使用的学习算法，并且不一定完全了解环境。我们通过设计一个称为对抗性MDP攻击的通用奖励中毒框架来展示最新的DRL算法的内在脆弱性。我们实例化了我们的框架来构造几个新的攻击，这些攻击只破坏了总训练时间步骤的一小部分回报，并使代理学习一个低性能的策略。我们的关键见解是，最先进的DRL算法从战略上探索环境，以找到高性能的策略。我们的攻击利用这一洞察力来构建一个腐败的环境，以误导代理在有限的攻击预算下学习低性能策略。我们对我们的攻击效率进行了理论分析，并进行了广泛的评估。我们的结果表明，在几种流行的经典控制和MuJoCo环境下，我们的攻击有效地毒化了使用各种先进的DRL算法学习的代理，如DQN、PPO、SAC等。



## **9. Unfooling Perturbation-Based Post Hoc Explainers**

基于非愚弄扰动的帖子随机解说器 cs.AI

10 pages (not including references and supplemental)

**SubmitDate**: 2022-05-29    [paper-pdf](http://arxiv.org/pdf/2205.14772v1)

**Authors**: Zachariah Carmichael, Walter J Scheirer

**Abstracts**: Monumental advancements in artificial intelligence (AI) have lured the interest of doctors, lenders, judges, and other professionals. While these high-stakes decision-makers are optimistic about the technology, those familiar with AI systems are wary about the lack of transparency of its decision-making processes. Perturbation-based post hoc explainers offer a model agnostic means of interpreting these systems while only requiring query-level access. However, recent work demonstrates that these explainers can be fooled adversarially. This discovery has adverse implications for auditors, regulators, and other sentinels. With this in mind, several natural questions arise - how can we audit these black box systems? And how can we ascertain that the auditee is complying with the audit in good faith? In this work, we rigorously formalize this problem and devise a defense against adversarial attacks on perturbation-based explainers. We propose algorithms for the detection (CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our approach successfully detects whether a black box system adversarially conceals its decision-making process and mitigates the adversarial attack on real-world data for the prevalent explainers, LIME and SHAP.

摘要: 人工智能(AI)的巨大进步吸引了医生、贷款人、法官和其他专业人士的兴趣。尽管这些事关重大的决策者对这项技术持乐观态度，但那些熟悉人工智能系统的人对其决策过程缺乏透明度持谨慎态度。基于扰动的后自组织解释器提供了一种模型不可知的方法来解释这些系统，而只需要查询级别的访问。然而，最近的研究表明，这些解释程序可能会被相反的人愚弄。这一发现对审计师、监管者和其他哨兵产生了不利影响。考虑到这一点，几个自然的问题就产生了--我们如何审计这些黑匣子系统？我们如何确定被审计人是真诚地遵守审计的？在这项工作中，我们严格地形式化了这个问题，并设计了一个防御对基于扰动的解释器的敌意攻击。在新的条件异常检测方法KNN-CAD的辅助下，我们提出了针对这些攻击的检测(CAD-检测)和防御(CAD-防御)算法。我们证明，我们的方法成功地检测到黑盒系统是否恶意地隐藏了其决策过程，并缓解了流行的解释程序LIME和Shap对真实数据的恶意攻击。



## **10. On the Robustness of Safe Reinforcement Learning under Observational Perturbations**

安全强化学习在观测摄动下的稳健性 cs.LG

27 pages, 3 figures, 3 tables

**SubmitDate**: 2022-05-29    [paper-pdf](http://arxiv.org/pdf/2205.14691v1)

**Authors**: Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo Li, Ding Zhao

**Abstracts**: Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective state adversarial attackers in the safe RL setting. We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and proposed two new approaches - one maximizes the cost and the other maximizes the reward. One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a more effective adversarial training framework for safe RL and evaluate it via comprehensive experiments. This work sheds light on the inherited connection between observational robustness and safety in RL and provides a pioneer work for future safe RL studies.

摘要: 安全强化学习(RL)训练一种策略，在满足安全约束的同时最大化任务奖励。虽然以前的工作主要集中在性能最优性上，但我们发现许多安全RL问题的最优解对于精心设计的观测扰动并不是健壮的和安全的。我们形式化地分析了在安全RL环境下设计有效的状态对抗攻击者的独特性质。我们证明了标准RL任务的基线对抗性攻击技术对于安全RL并不总是有效的，并提出了两种新的方法-一种最大化成本，另一种最大化回报。一个有趣和违反直觉的发现是，最大奖励攻击是强大的，因为它既可以诱导不安全的行为，又可以通过保持奖励来使攻击隐形。我们进一步提出了一种更有效的安全RL对抗训练框架，并通过综合实验对其进行了评估。这项工作揭示了RL中观测稳健性和安全性之间的遗传联系，并为未来的安全RL研究提供了开创性的工作。



## **11. Superclass Adversarial Attack**

超类对抗性攻击 cs.CV

**SubmitDate**: 2022-05-29    [paper-pdf](http://arxiv.org/pdf/2205.14629v1)

**Authors**: Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki

**Abstracts**: Adversarial attacks have only focused on changing the predictions of the classifier, but their danger greatly depends on how the class is mistaken. For example, when an automatic driving system mistakes a Persian cat for a Siamese cat, it is hardly a problem. However, if it mistakes a cat for a 120km/h minimum speed sign, serious problems can arise. As a stepping stone to more threatening adversarial attacks, we consider the superclass adversarial attack, which causes misclassification of not only fine classes, but also superclasses. We conducted the first comprehensive analysis of superclass adversarial attacks (an existing and 19 new methods) in terms of accuracy, speed, and stability, and identified several strategies to achieve better performance. Although this study is aimed at superclass misclassification, the findings can be applied to other problem settings involving multiple classes, such as top-k and multi-label classification attacks.

摘要: 对抗性攻击只专注于改变分类器的预测，但它们的危险在很大程度上取决于类的错误程度。例如，当自动驾驶系统将波斯猫误认为暹罗猫时，这几乎不是问题。然而，如果它把猫错当成120公里/小时的最低速度标志，可能会出现严重的问题。作为更具威胁性的对抗性攻击的垫脚石，我们认为超类对抗性攻击不仅会导致细类的错误分类，而且会导致超类的错误分类。我们首次对超类对抗性攻击(现有方法和19种新方法)在准确性、速度和稳定性方面进行了全面分析，并确定了几种实现更好性能的策略。虽然这项研究是针对超类错误分类的，但研究结果也适用于其他涉及多类的问题，如top-k和多标签分类攻击。



## **12. Graph Structure Based Data Augmentation Method**

一种基于图结构的数据增强方法 cs.LG

**SubmitDate**: 2022-05-29    [paper-pdf](http://arxiv.org/pdf/2205.14619v1)

**Authors**: Kyung Geun Kim, Byeong Tak Lee

**Abstracts**: In this paper, we propose a novel graph-based data augmentation method that can generally be applied to medical waveform data with graph structures. In the process of recording medical waveform data, such as electrocardiogram (ECG) or electroencephalogram (EEG), angular perturbations between the measurement leads exist due to discrepancies in lead positions. The data samples with large angular perturbations often cause inaccuracy in algorithmic prediction tasks. We design a graph-based data augmentation technique that exploits the inherent graph structures within the medical waveform data to improve both performance and robustness. In addition, we show that the performance gain from graph augmentation results from robustness by testing against adversarial attacks. Since the bases of performance gain are orthogonal, the graph augmentation can be used in conjunction with existing data augmentation techniques to further improve the final performance. We believe that our graph augmentation method opens up new possibilities to explore in data augmentation.

摘要: 在本文中，我们提出了一种新的基于图的数据增强方法，该方法普遍适用于具有图结构的医学波形数据。在记录心电或脑电等医学波形数据的过程中，由于导联位置的差异，测量导联之间存在角度摄动。在算法预测任务中，具有较大角度摄动的数据样本经常导致不准确。我们设计了一种基于图形的数据增强技术，该技术利用医学波形数据中固有的图形结构来提高性能和稳健性。此外，我们还通过对敌意攻击的测试，证明了图增强带来的性能提升来自于健壮性。由于性能增益的基础是正交性的，图增强可以与现有的数据增强技术相结合来进一步提高最终的性能。我们相信，我们的图形增强方法为探索数据增强开辟了新的可能性。



## **13. Problem-Space Evasion Attacks in the Android OS: a Survey**

Android操作系统中的问题空间逃避攻击：综述 cs.CR

**SubmitDate**: 2022-05-29    [paper-pdf](http://arxiv.org/pdf/2205.14576v1)

**Authors**: Harel Berger, Dr. Chen Hajaj, Dr. Amit Dvir

**Abstracts**: Android is the most popular OS worldwide. Therefore, it is a target for various kinds of malware. As a countermeasure, the security community works day and night to develop appropriate Android malware detection systems, with ML-based or DL-based systems considered as some of the most common types. Against these detection systems, intelligent adversaries develop a wide set of evasion attacks, in which an attacker slightly modifies a malware sample to evade its target detection system. In this survey, we address problem-space evasion attacks in the Android OS, where attackers manipulate actual APKs, rather than their extracted feature vector. We aim to explore this kind of attacks, frequently overlooked by the research community due to a lack of knowledge of the Android domain, or due to focusing on general mathematical evasion attacks - i.e., feature-space evasion attacks. We discuss the different aspects of problem-space evasion attacks, using a new taxonomy, which focuses on key ingredients of each problem-space attack, such as the attacker model, the attacker's mode of operation, and the functional assessment of post-attack applications.

摘要: 安卓是全球最受欢迎的操作系统。因此，它是各种恶意软件的目标。作为对策，安全社区夜以继日地开发合适的Android恶意软件检测系统，基于ML或基于DL的系统被认为是一些最常见的类型。针对这些检测系统，智能攻击者开发了一系列广泛的逃避攻击，攻击者略微修改恶意软件样本以逃避其目标检测系统。在这篇调查中，我们讨论了Android操作系统中的问题空间逃避攻击，即攻击者操纵实际的APK，而不是他们提取的特征向量。我们的目标是探索这类攻击，由于缺乏Android领域的知识，或者由于专注于一般的数学逃避攻击，即特征空间逃避攻击，经常被研究界忽视。我们讨论了问题空间逃避攻击的不同方面，使用了一种新的分类方法，重点讨论了每种问题空间攻击的关键要素，如攻击者的模型、攻击者的操作模式以及攻击后应用程序的功能评估。



## **14. BadDet: Backdoor Attacks on Object Detection**

BadDet：针对对象检测的后门攻击 cs.CV

**SubmitDate**: 2022-05-28    [paper-pdf](http://arxiv.org/pdf/2205.14497v1)

**Authors**: Shih-Han Chan, Yinpeng Dong, Jun Zhu, Xiaolu Zhang, Jun Zhou

**Abstracts**: Deep learning models have been deployed in numerous real-world applications such as autonomous driving and surveillance. However, these models are vulnerable in adversarial environments. Backdoor attack is emerging as a severe security threat which injects a backdoor trigger into a small portion of training data such that the trained model behaves normally on benign inputs but gives incorrect predictions when the specific trigger appears. While most research in backdoor attacks focuses on image classification, backdoor attacks on object detection have not been explored but are of equal importance. Object detection has been adopted as an important module in various security-sensitive applications such as autonomous driving. Therefore, backdoor attacks on object detection could pose severe threats to human lives and properties. We propose four kinds of backdoor attacks for object detection task: 1) Object Generation Attack: a trigger can falsely generate an object of the target class; 2) Regional Misclassification Attack: a trigger can change the prediction of a surrounding object to the target class; 3) Global Misclassification Attack: a single trigger can change the predictions of all objects in an image to the target class; and 4) Object Disappearance Attack: a trigger can make the detector fail to detect the object of the target class. We develop appropriate metrics to evaluate the four backdoor attacks on object detection. We perform experiments using two typical object detection models -- Faster-RCNN and YOLOv3 on different datasets. More crucially, we demonstrate that even fine-tuning on another benign dataset cannot remove the backdoor hidden in the object detection model. To defend against these backdoor attacks, we propose Detector Cleanse, an entropy-based run-time detection framework to identify poisoned testing samples for any deployed object detector.

摘要: 深度学习模型已被部署在许多现实世界的应用中，如自动驾驶和监控。然而，这些模型在对抗性环境中很容易受到攻击。后门攻击正在成为一种严重的安全威胁，它向一小部分训练数据注入后门触发器，使训练后的模型在良性输入下正常运行，但在特定触发器出现时给出错误的预测。虽然大多数关于后门攻击的研究都集中在图像分类上，但对目标检测的后门攻击还没有被探索过，但同样重要。目标检测已经成为自动驾驶等各种安全敏感应用中的一个重要模块。因此，对目标检测的后门攻击可能会对人类的生命和财产构成严重威胁。针对目标检测任务，我们提出了四种后门攻击：1)对象生成攻击：1)对象生成攻击：1)对象生成攻击；2)区域误分类攻击：1)局部误分类攻击：1)全局误分类攻击；3)全局误分类攻击：单个触发器可以将图像中所有对象的预测更改为目标类；4)对象消失攻击：1)触发器可以使检测器无法检测到目标类对象。我们开发了适当的度量来评估对象检测中的四种后门攻击。我们使用两个典型的目标检测模型--FASTER-RCNN和YOLOv3在不同的数据集上进行了实验。更关键的是，我们证明了即使对另一个良性数据集进行微调也不能消除隐藏在目标检测模型中的后门。为了防御这些后门攻击，我们提出了检测器Cleanse，这是一个基于熵的运行时检测框架，可以为任何部署的对象检测器识别有毒测试样本。



## **15. Policy Smoothing for Provably Robust Reinforcement Learning**

可证明稳健强化学习的策略平滑 cs.LG

Published as a conference paper at ICLR 2022

**SubmitDate**: 2022-05-28    [paper-pdf](http://arxiv.org/pdf/2106.11420v3)

**Authors**: Aounon Kumar, Alexander Levine, Soheil Feizi

**Abstracts**: The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on static supervised learning tasks such as image classification. However, DNNs have been used extensively in real-world adaptive tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in provable robustness in RL seek to certify the behaviour of the victim policy at every time-step against a non-adaptive adversary using methods developed for the static setting. But in the real world, an RL adversary can infer the defense strategy used by the victim agent by observing the states, actions, etc., from previous time-steps and adapt itself to produce stronger attacks in future steps. We present an efficient procedure, designed specifically to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step. Our main theoretical contribution is to prove an adaptive version of the Neyman-Pearson Lemma -- a key lemma for smoothing-based certificates -- where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Building on this result, we propose policy smoothing where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Our robustness certificates guarantee that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack. Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice.

摘要: 深度神经网络(DNN)的可证对抗鲁棒性研究主要集中在图像分类等静态监督学习任务上。然而，DNN已被广泛应用于现实世界中的自适应任务，如强化学习(RL)，这使得此类系统也容易受到对手攻击。RL中的可证明健壮性方面的先前工作试图使用为静态设置开发的方法来证明受害者策略在针对非自适应对手的每个时间步的行为。但在现实世界中，RL对手可以通过观察以前时间步长的状态、动作等来推断受害者代理所使用的防御策略，并在未来的步骤中进行调整以产生更强的攻击。我们提出了一个有效的程序，专门设计来防御自适应的RL对手，它可以直接证明总的奖励，而不需要策略在每个时间步都是健壮的。我们的主要理论贡献是证明了Neyman-Pearson引理的一个自适应版本--这是一个用于平滑基于证书的关键引理--其中在特定时间的对抗性扰动可以是当前和先前的观察和状态以及先前操作的随机函数。基于这一结果，我们提出了策略平滑，即在通过策略函数之前，代理在每个时间步向其观测结果添加高斯噪声。我们的稳健性证书保证了策略平滑获得的最终总回报保持在一定的阈值以上，即使中间时间步长的动作在攻击下可能发生变化。我们在Cartpoll、Pong、Freeway和Mountain Car等环境下的实验表明，该方法在实践中可以产生有意义的健壮性保证。



## **16. Certifying Model Accuracy under Distribution Shifts**

分布漂移下的模型精度验证 cs.LG

**SubmitDate**: 2022-05-28    [paper-pdf](http://arxiv.org/pdf/2201.12440v2)

**Authors**: Aounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi

**Abstracts**: Certified robustness in machine learning has primarily focused on adversarial perturbations of the input with a fixed attack budget for each point in the data distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under the transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any (natural or adversarial) shift of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to: (i) certify robustness against natural (non-adversarial) transformations of images such as color shifts, hue shifts and changes in brightness and saturation, (ii) certify robustness against adversarial shifts of the input distribution, and (iii) show provable lower bounds (hardness results) on the performance of models trained on so-called "unlearnable" datasets that have been poisoned to interfere with model training.

摘要: 机器学习中已证明的稳健性主要集中在输入的对抗性扰动上，对数据分布中的每个点都有固定的攻击预算。在这项工作中，我们给出了在数据分布的有界Wasserstein位移下模型精度的可证明的稳健性保证。我们证明了在变换空间内随机化模型输入的简单过程对变换下的分布位移是被证明是健壮的。我们的框架允许特定于基准的扰动大小在输入分布的不同点上变化，并且足够普遍以包括固定大小的扰动。我们的证书为输入分布在Wasserstein球中围绕原始分布的任何(自然或对抗性)移动产生了模型性能的保证下限。我们将我们的技术应用于：(I)证明对图像的自然(非对抗性)变换的稳健性，例如颜色漂移、色调漂移以及亮度和饱和度的变化，(Ii)验证对输入分布的对抗性漂移的稳健性，以及(Iii)显示在已被毒害到干扰模型训练的所谓的“不可学习”数据集上训练的模型的性能的可证明的下界(困难结果)。



## **17. SHORTSTACK: Distributed, Fault-tolerant, Oblivious Data Access**

ShortStack：分布式、容错、不经意的数据访问 cs.CR

Full version of USENIX OSDI'22 paper

**SubmitDate**: 2022-05-28    [paper-pdf](http://arxiv.org/pdf/2205.14281v1)

**Authors**: Midhul Vuppalapati, Kushal Babel, Anurag Khandelwal, Rachit Agarwal

**Abstracts**: Many applications that benefit from data offload to cloud services operate on private data. A now-long line of work has shown that, even when data is offloaded in an encrypted form, an adversary can learn sensitive information by analyzing data access patterns. Existing techniques for oblivious data access--that protect against access pattern attacks--require a centralized, stateful and trusted, proxy to orchestrate data accesses from applications to cloud services. We show that, in failure-prone deployments, such a centralized and stateful proxy results in violation of oblivious data access security guarantees and/or system unavailability. Thus, we initiate the study of distributed, fault-tolerant, oblivious data access.   We present SHORTSTACK, a distributed proxy architecture for oblivious data access in failure-prone deployments. SHORTSTACK achieves the classical obliviousness guarantee--access patterns observed by the adversary being independent of the input--even under a powerful passive persistent adversary that can force failure of arbitrary (bounded-sized) subset of proxy servers at arbitrary times. We also introduce a security model that enables studying oblivious data access with distributed, failure-prone, servers. We provide a formal proof that SHORTSTACK enables oblivious data access under this model, and show empirically that SHORTSTACK performance scales near-linearly with number of distributed proxy servers.

摘要: 许多受益于数据分流到云服务的应用程序都在私有数据上运行。目前的一系列工作表明，即使以加密的形式卸载数据，对手也可以通过分析数据访问模式来获取敏感信息。现有的不经意数据访问技术--防止访问模式攻击--需要一个集中的、有状态的、可信的代理来协调从应用程序到云服务的数据访问。我们表明，在容易出现故障的部署中，这种集中式和有状态的代理会导致违反不经意的数据访问安全保证和/或系统不可用。因此，我们开始了对分布式、容错、不经意的数据访问的研究。我们提出了ShortStack，这是一种分布式代理体系结构，用于在容易出现故障的部署中进行不经意的数据访问。ShortStack实现了经典的遗忘保证--攻击者观察到的访问模式独立于输入--即使在强大的被动持久对手下也是如此，该对手可以在任意时间强制任意(有限大小的)代理服务器子集发生故障。我们还介绍了一个安全模型，该模型能够研究使用分布式的、容易发生故障的服务器的不经意的数据访问。我们给出了一个形式化的证明，证明了在该模型下，ShortStack能够实现不经意的数据访问，并通过实验证明了ShortStack的性能随着分布式代理服务器的数量近似线性地扩展。



## **18. Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction**

用于弹道预测的半监督语义制导对抗性训练 cs.LG

11 pages, adversarial training for trajectory prediction

**SubmitDate**: 2022-05-27    [paper-pdf](http://arxiv.org/pdf/2205.14230v1)

**Authors**: Ruochen Jiao, Xiangguo Liu, Takami Sato, Qi Alfred Chen, Qi Zhu

**Abstracts**: Predicting the trajectories of surrounding objects is a critical task in self-driving and many other autonomous systems. Recent works demonstrate that adversarial attacks on trajectory prediction, where small crafted perturbations are introduced to history trajectories, may significantly mislead the prediction of future trajectories and ultimately induce unsafe planning. However, few works have addressed enhancing the robustness of this important safety-critical task. In this paper, we present the first adversarial training method for trajectory prediction. Compared with typical adversarial training on image tasks, our work is challenged by more random inputs with rich context, and a lack of class labels. To address these challenges, we propose a method based on a semi-supervised adversarial autoencoder that models disentangled semantic features with domain knowledge and provides additional latent labels for the adversarial training. Extensive experiments with different types of attacks demonstrate that our semi-supervised semantics-guided adversarial training method can effectively mitigate the impact of adversarial attacks and generally improve the system's adversarial robustness to a variety of attacks, including unseen ones. We believe that such semantics-guided architecture and advancement in robust generalization is an important step for developing robust prediction models and enabling safe decision making.

摘要: 在自动驾驶和许多其他自主系统中，预测周围物体的轨迹是一项关键任务。最近的工作表明，对轨迹预测的敌意攻击，即在历史轨迹中引入微小的精心设计的扰动，可能会严重误导对未来轨迹的预测，并最终导致不安全的规划。然而，很少有工作涉及增强这一重要的安全关键任务的健壮性。在本文中，我们提出了第一种用于轨迹预测的对抗性训练方法。与典型的对抗性图像训练相比，我们的工作面临着更多的随机输入和丰富的上下文，以及缺乏类别标签的挑战。为了应对这些挑战，我们提出了一种基于半监督对抗性自动编码器的方法，该方法利用领域知识对解开的语义特征进行建模，并为对抗性训练提供额外的潜在标签。对不同类型的攻击进行的大量实验表明，本文提出的半监督语义制导的对抗性训练方法能够有效地缓解对抗性攻击的影响，并总体上提高了系统对包括不可见攻击在内的各种攻击的鲁棒性。我们认为，这种语义引导的体系结构和在健壮泛化方面的进步是开发健壮预测模型和实现安全决策的重要一步。



## **19. A Single-Adversary-Single-Detector Zero-Sum Game in Networked Control Systems**

网络控制系统中的单对手-单检测器零和博弈 math.OC

6 pages, 6 figures, 1 table, accepted to the 9th IFAC Conference on  Networked Systems, Zurich, July 2022

**SubmitDate**: 2022-05-27    [paper-pdf](http://arxiv.org/pdf/2205.14001v1)

**Authors**: Anh Tung Nguyen, André M. H. Teixeira, Alexander Medvedev

**Abstracts**: This paper proposes a game-theoretic approach to address the problem of optimal sensor placement for detecting cyber-attacks in networked control systems. The problem is formulated as a zero-sum game with two players, namely a malicious adversary and a detector. Given a protected target vertex, the detector places a sensor at a single vertex to monitor the system and detect the presence of the adversary. On the other hand, the adversary selects a single vertex through which to conduct a cyber-attack that maximally disrupts the target vertex while remaining undetected by the detector. As our first contribution, for a given pair of attack and monitor vertices and a known target vertex, the game payoff function is defined as the output-to-output gain of the respective system. Then, the paper characterizes the set of feasible actions by the detector that ensures bounded values of the game payoff. Finally, an algebraic sufficient condition is proposed to examine whether a given vertex belongs to the set of feasible monitor vertices. The optimal sensor placement is then determined by computing the mixed-strategy Nash equilibrium of the zero-sum game through linear programming. The approach is illustrated via a numerical example of a 10-vertex networked control system with a given target vertex.

摘要: 本文提出了一种基于博弈论的方法来解决网络控制系统中检测网络攻击的传感器最优配置问题。该问题被描述为一个有两个参与者的零和博弈，即一个恶意对手和一个检测器。在给定一个受保护的目标顶点的情况下，检测器在单个顶点放置一个传感器来监视系统并检测对手的存在。另一方面，敌手选择单个顶点进行网络攻击，最大限度地破坏目标顶点，同时保持不被检测器检测。作为我们的第一个贡献，对于给定的攻击和监视顶点对和已知的目标顶点，博弈收益函数被定义为各自系统的输出到输出的增益。然后，利用检测器刻画了保证博弈收益有界值的可行行为集。最后，给出了一个判定给定顶点是否属于可行监视顶点集的代数充分条件。然后通过线性规划计算零和博弈的混合策略纳什均衡来确定传感器的最优配置。给出了一个具有给定目标节点的10点网络控制系统的算例。



## **20. Standalone Neural ODEs with Sensitivity Analysis**

带敏感度分析的独立神经网络模型 cs.LG

25 pages, 15 figures

**SubmitDate**: 2022-05-27    [paper-pdf](http://arxiv.org/pdf/2205.13933v1)

**Authors**: Rym Jaroudi, Lukáš Malý, Gabriel Eilertsen, Tomas B. Johansson, Jonas Unger, George Baravdish

**Abstracts**: This paper presents the Standalone Neural ODE (sNODE), a continuous-depth neural ODE model capable of describing a full deep neural network. This uses a novel nonlinear conjugate gradient (NCG) descent optimization scheme for training, where the Sobolev gradient can be incorporated to improve smoothness of model weights. We also present a general formulation of the neural sensitivity problem and show how it is used in the NCG training. The sensitivity analysis provides a reliable measure of uncertainty propagation throughout a network, and can be used to study model robustness and to generate adversarial attacks. Our evaluations demonstrate that our novel formulations lead to increased robustness and performance as compared to ResNet models, and that it opens up for new opportunities for designing and developing machine learning with improved explainability.

摘要: 提出了一种能够描述完整深度神经网络的连续深度神经网络模型--独立神经网络模型(SNODE)。该算法采用一种新的非线性共轭梯度(NCG)下降优化方案进行训练，其中可以引入Soblev梯度来改善模型权重的平滑程度。我们还给出了神经敏感度问题的一般公式，并展示了如何将其用于NCG训练。敏感度分析为不确定性在整个网络中的传播提供了可靠的度量，并可用于研究模型的健壮性和生成对抗性攻击。我们的评估表明，与ResNet模型相比，我们的新公式导致了更高的稳健性和性能，并为设计和开发具有更好解释性的机器学习开辟了新的机会。



## **21. Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment**

多智能体城市驾驶环境下自主对抗性策略的深度强化学习稳健性评价 cs.AI

**SubmitDate**: 2022-05-27    [paper-pdf](http://arxiv.org/pdf/2112.11947v2)

**Authors**: Aizaz Sharif, Dusica Marijan

**Abstracts**: Deep reinforcement learning is actively used for training autonomous and adversarial car policies in a simulated driving environment. Due to the large availability of various reinforcement learning algorithms and the lack of their systematic comparison across different driving scenarios, we are unsure of which ones are more effective for training and testing autonomous car software in single-agent as well as multi-agent driving environments. A benchmarking framework for the comparison of deep reinforcement learning in a vision-based autonomous driving will open up the possibilities for training better autonomous car driving policies. Furthermore, autonomous cars trained on deep reinforcement learning-based algorithms are known for being vulnerable to adversarial attacks. To guard against adversarial attacks, we can train autonomous cars on adversarial driving policies. However, we lack the knowledge of which deep reinforcement learning algorithms would act as good adversarial agents able to effectively test autonomous cars. To address these challenges, we provide an open and reusable benchmarking framework for systematic evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous and adversarial driving in a single- and multi-agent environment. Using the framework, we perform a comparative study of five discrete and two continuous action space deep reinforcement learning algorithms. We run the experiments in a vision-only high fidelity urban driving simulated environments. The results indicate that only some of the deep reinforcement learning algorithms perform consistently better across single and multi-agent scenarios when trained in a multi-agent-only setting.

摘要: 深度强化学习被用于在模拟驾驶环境中训练自主的和对抗性的汽车策略。由于各种强化学习算法的可用性很高，而且缺乏对不同驾驶场景的系统比较，我们不确定哪种算法在单代理和多代理驾驶环境下训练和测试自动驾驶汽车软件更有效。在基于视觉的自动驾驶中比较深度强化学习的基准框架将为培训更好的自动汽车驾驶政策打开可能性。此外，经过深度强化学习算法训练的自动驾驶汽车，众所周知容易受到对手的攻击。为了防范对抗性攻击，我们可以对自动驾驶汽车进行对抗性驾驶策略培训。然而，我们缺乏关于哪些深度强化学习算法将作为能够有效测试自动驾驶汽车的良好对手代理的知识。为了应对这些挑战，我们提供了一个开放和可重用的基准测试框架，用于在单代理和多代理环境中对自主和对抗性驾驶的深度强化学习算法进行系统评估和比较分析。利用该框架，我们对五种离散动作空间和两种连续动作空间深度强化学习算法进行了比较研究。我们在视觉高保真的城市驾驶模拟环境中进行了实验。结果表明，只有一些深度强化学习算法在仅有多智能体的情况下训练时，在单智能体和多智能体场景中的表现一致较好。



## **22. Adversarial Deep Reinforcement Learning for Improving the Robustness of Multi-agent Autonomous Driving Policies**

对抗性深度强化学习提高多智能体自主驾驶策略的稳健性 cs.AI

**SubmitDate**: 2022-05-27    [paper-pdf](http://arxiv.org/pdf/2112.11937v2)

**Authors**: Aizaz Sharif, Dusica Marijan

**Abstracts**: Autonomous cars are well known for being vulnerable to adversarial attacks that can compromise the safety of the car and pose danger to other road users. To effectively defend against adversaries, it is required to not only test autonomous cars for finding driving errors, but to improve the robustness of the cars to these errors. To this end, in this paper, we propose a two-step methodology for autonomous cars that consists of (i) finding failure states in autonomous cars by training the adversarial driving agent, and (ii) improving the robustness of autonomous cars by retraining them with effective adversarial inputs. Our methodology supports testing ACs in a multi-agent environment, where we train and compare adversarial car policy on two custom reward functions to test the driving control decision of autonomous cars. We run experiments in a vision-based high fidelity urban driving simulated environment. Our results show that adversarial testing can be used for finding erroneous autonomous driving behavior, followed by adversarial training for improving the robustness of deep reinforcement learning based autonomous driving policies. We demonstrate that the autonomous cars retrained using the effective adversarial inputs noticeably increase the performance of their driving policies in terms of reduced collision and offroad steering errors.

摘要: 众所周知，自动驾驶汽车容易受到对抗性攻击，这些攻击可能会危及汽车的安全，并对其他道路使用者构成危险。为了有效地防御对手，不仅需要测试自动驾驶汽车是否发现驾驶错误，还需要提高汽车对这些错误的稳健性。为此，在本文中，我们提出了一种自动驾驶汽车的两步方法，包括(I)通过训练对抗性驾驶主体来发现自动驾驶汽车中的故障状态，(Ii)通过对自动驾驶汽车进行有效的对抗性输入来重新训练它们来提高自动驾驶汽车的稳健性。我们的方法支持在多智能体环境中测试自动驾驶控制系统，在这个环境中，我们训练并比较两个定制奖励函数上的对抗性汽车策略，以测试自动驾驶汽车的驾驶控制决策。我们在基于视觉的高保真城市驾驶模拟环境中进行了实验。结果表明，对抗性测试可以用来发现错误的自主驾驶行为，然后通过对抗性训练来提高基于深度强化学习的自主驾驶策略的稳健性。我们证明，使用有效的对抗性输入进行再培训的自动驾驶汽车在减少碰撞和越野转向错误方面显著提高了其驾驶策略的性能。



## **23. fakeWeather: Adversarial Attacks for Deep Neural Networks Emulating Weather Conditions on the Camera Lens of Autonomous Systems**

虚假天气：对自主系统摄像机镜头上模拟天气条件的深度神经网络的敌意攻击 cs.LG

To appear at the 2022 International Joint Conference on Neural  Networks (IJCNN), at the 2022 IEEE World Congress on Computational  Intelligence (WCCI), July 2022, Padua, Italy

**SubmitDate**: 2022-05-27    [paper-pdf](http://arxiv.org/pdf/2205.13807v1)

**Authors**: Alberto Marchisio, Giovanni Caramia, Maurizio Martina, Muhammad Shafique

**Abstracts**: Recently, Deep Neural Networks (DNNs) have achieved remarkable performances in many applications, while several studies have enhanced their vulnerabilities to malicious attacks. In this paper, we emulate the effects of natural weather conditions to introduce plausible perturbations that mislead the DNNs. By observing the effects of such atmospheric perturbations on the camera lenses, we model the patterns to create different masks that fake the effects of rain, snow, and hail. Even though the perturbations introduced by our attacks are visible, their presence remains unnoticed due to their association with natural events, which can be especially catastrophic for fully-autonomous and unmanned vehicles. We test our proposed fakeWeather attacks on multiple Convolutional Neural Network and Capsule Network models, and report noticeable accuracy drops in the presence of such adversarial perturbations. Our work introduces a new security threat for DNNs, which is especially severe for safety-critical applications and autonomous systems.

摘要: 近年来，深度神经网络(DNN)在许多应用中取得了令人瞩目的性能，同时一些研究也增强了它们对恶意攻击的脆弱性。在本文中，我们模拟自然天气条件的影响来引入看似合理的扰动来误导DNN。通过观察这种大气扰动对相机镜头的影响，我们对图案进行建模，以创建不同的面具来模拟雨、雪和冰雹的影响。尽管我们的攻击带来的干扰是可见的，但由于它们与自然事件有关，它们的存在仍然没有被注意到，这对全自动驾驶和无人驾驶车辆来说可能是特别灾难性的。我们在多个卷积神经网络和胶囊网络模型上测试了我们提出的虚假天气攻击，并且报告了在存在这种对抗性扰动的情况下显著的准确率下降。我们的工作给DNN带来了新的安全威胁，这对安全关键型应用和自治系统尤其严重。



## **24. Face Morphing: Fooling a Face Recognition System Is Simple!**

人脸变形：愚弄人脸识别系统很简单！ cs.CV

**SubmitDate**: 2022-05-27    [paper-pdf](http://arxiv.org/pdf/2205.13796v1)

**Authors**: Stefan Hörmann, Tianlin Kong, Torben Teepe, Fabian Herzog, Martin Knoche, Gerhard Rigoll

**Abstracts**: State-of-the-art face recognition (FR) approaches have shown remarkable results in predicting whether two faces belong to the same identity, yielding accuracies between 92% and 100% depending on the difficulty of the protocol. However, the accuracy drops substantially when exposed to morphed faces, specifically generated to look similar to two identities. To generate morphed faces, we integrate a simple pretrained FR model into a generative adversarial network (GAN) and modify several loss functions for face morphing. In contrast to previous works, our approach and analyses are not limited to pairs of frontal faces with the same ethnicity and gender. Our qualitative and quantitative results affirm that our approach achieves a seamless change between two faces even in unconstrained scenarios. Despite using features from a simpler FR model for face morphing, we demonstrate that even recent FR systems struggle to distinguish the morphed face from both identities obtaining an accuracy of only 55-70%. Besides, we provide further insights into how knowing the FR system makes it particularly vulnerable to face morphing attacks.

摘要: 最新的人脸识别(FR)方法在预测两个人脸是否属于同一身份方面表现出了显著的效果，根据协议的难度，准确率在92%到100%之间。然而，当暴露在变形的面部时，准确度会大幅下降，这些变形的面部是专门生成的，看起来类似于两个身份。为了生成变形人脸，我们将一个简单的预先训练的FR模型集成到一个生成性对抗网络(GAN)中，并修改了几个用于人脸变形的损失函数。与前人的工作不同，我们的方法和分析并不局限于相同种族和性别的正面脸对。我们的定性和定量结果证实了我们的方法实现了两个人脸之间的无缝变化，即使在不受约束的场景中也是如此。尽管使用了更简单的FR模型的特征进行人脸变形，但我们证明了即使是最新的FR系统也很难区分变形后的人脸和两种身份，获得的准确率仅为55-70%。此外，我们还提供了关于了解FR系统如何使其特别容易受到变形攻击的进一步见解。



## **25. Adversarial attacks and defenses in Speaker Recognition Systems: A survey**

说话人识别系统中的对抗性攻击与防御 cs.CR

38pages, 2 figures, 2 tables. Journal of Systems Architecture,2022

**SubmitDate**: 2022-05-27    [paper-pdf](http://arxiv.org/pdf/2205.13685v1)

**Authors**: Jiahe Lan, Rui Zhang, Zheng Yan, Jie Wang, Yu Chen, Ronghui Hou

**Abstracts**: Speaker recognition has become very popular in many application scenarios, such as smart homes and smart assistants, due to ease of use for remote control and economic-friendly features. The rapid development of SRSs is inseparable from the advancement of machine learning, especially neural networks. However, previous work has shown that machine learning models are vulnerable to adversarial attacks in the image domain, which inspired researchers to explore adversarial attacks and defenses in Speaker Recognition Systems (SRS). Unfortunately, existing literature lacks a thorough review of this topic. In this paper, we fill this gap by performing a comprehensive survey on adversarial attacks and defenses in SRSs. We first introduce the basics of SRSs and concepts related to adversarial attacks. Then, we propose two sets of criteria to evaluate the performance of attack methods and defense methods in SRSs, respectively. After that, we provide taxonomies of existing attack methods and defense methods, and further review them by employing our proposed criteria. Finally, based on our review, we find some open issues and further specify a number of future directions to motivate the research of SRSs security.

摘要: 说话人识别由于易于远程控制和经济友好的特点，在智能家居和智能助理等许多应用场景中变得非常流行。支持向量机的快速发展离不开机器学习特别是神经网络的发展。然而，以往的工作表明，机器学习模型在图像领域容易受到对抗性攻击，这启发了研究人员探索说话人识别系统中的对抗性攻击和防御。遗憾的是，现有文献缺乏对这一主题的全面回顾。在本文中，我们通过对SRSS中的对抗性攻击和防御进行全面的调查来填补这一空白。我们首先介绍了SRSS的基本知识和与对抗性攻击相关的概念。然后，我们提出了两套标准来分别评价SRSS中攻击方法和防御方法的性能。之后，我们提供了现有的攻击方法和防御方法的分类，并使用我们提出的标准对它们进行了进一步的审查。最后，基于我们的回顾，我们发现了一些尚待解决的问题，并进一步指出了一些未来的方向，以推动SRSS安全的研究。



## **26. Sequential Nature of Recommender Systems Disrupts the Evaluation Process**

推荐系统的顺序性扰乱了评价过程 cs.IR

To Appear in Third International Workshop on Algorithmic Bias in  Search and Recommendation (Bias 2022)

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13681v1)

**Authors**: Ali Shirali

**Abstracts**: Datasets are often generated in a sequential manner, where the previous samples and intermediate decisions or interventions affect subsequent samples. This is especially prominent in cases where there are significant human-AI interactions, such as in recommender systems. To characterize the importance of this relationship across samples, we propose to use adversarial attacks on popular evaluation processes. We present sequence-aware boosting attacks and provide a lower bound on the amount of extra information that can be exploited from a confidential test set solely based on the order of the observed data. We use real and synthetic data to test our methods and show that the evaluation process on the MovieLense-100k dataset can be affected by $\sim1\%$ which is important when considering the close competition. Codes are publicly available.

摘要: 数据集通常是以顺序方式生成的，其中先前样本和中间决策或干预会影响后续样本。这在存在重大人类与人工智能交互的情况下尤其突出，例如在推荐系统中。为了表征这种关系在样本中的重要性，我们建议在流行的评估过程中使用对抗性攻击。我们提出了顺序感知的Boost攻击，并提供了仅基于观察到的数据的顺序可以从机密测试集中利用的额外信息量的下限。我们使用真实数据和合成数据来测试我们的方法，并表明在MovieLense-100k数据集上的评估过程会受到$Sim1$的影响，这在考虑到激烈的竞争时是很重要的。代码是公开提供的。



## **27. Membership Inference Attack Using Self Influence Functions**

基于自影响函数的隶属度推理攻击 cs.LG

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13680v1)

**Authors**: Gilad Cohen, Raja Giryes

**Abstracts**: Member inference (MI) attacks aim to determine if a specific data sample was used to train a machine learning model. Thus, MI is a major privacy threat to models trained on private sensitive data, such as medical records. In MI attacks one may consider the black-box settings, where the model's parameters and activations are hidden from the adversary, or the white-box case where they are available to the attacker. In this work, we focus on the latter and present a novel MI attack for it that employs influence functions, or more specifically the samples' self-influence scores, to perform the MI prediction. We evaluate our attack on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets, using versatile architectures such as AlexNet, ResNet, and DenseNet. Our attack method achieves new state-of-the-art results for both training with and without data augmentations. Code is available at https://github.com/giladcohen/sif_mi_attack.

摘要: 成员推理(MI)攻击旨在确定是否使用特定数据样本来训练机器学习模型。因此，MI是对针对私人敏感数据(如医疗记录)进行培训的模型的主要隐私威胁。在MI攻击中，可以考虑黑盒设置，在黑盒设置中，模型的参数和激活对对手隐藏，或者在白盒情况下，攻击者可以使用它们。在这项工作中，我们关注后者，并提出了一种新的MI攻击，该攻击使用影响函数，或者更具体地说，样本的自我影响分数来执行MI预测。我们使用多种架构，如AlexNet、ResNet和DenseNet，评估我们对CIFAR-10、CIFAR-100和微小ImageNet数据集的攻击。我们的攻击方法在有数据增强和没有数据增强的训练中都获得了新的最先进的结果。代码可在https://github.com/giladcohen/sif_mi_attack.上找到



## **28. On the Anonymity of Peer-To-Peer Network Anonymity Schemes Used by Cryptocurrencies**

加密货币使用的对等网络匿名方案的匿名性研究 cs.CR

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2201.11860v2)

**Authors**: Piyush Kumar Sharma, Devashish Gosain, Claudia Diaz

**Abstracts**: Cryptocurrency systems can be subject to deanonimization attacks by exploiting the network-level communication on their peer-to-peer network. Adversaries who control a set of colluding node(s) within the peer-to-peer network can observe transactions being exchanged and infer the parties involved. Thus, various network anonymity schemes have been proposed to mitigate this problem, with some solutions providing theoretical anonymity guarantees.   In this work, we model such peer-to-peer network anonymity solutions and evaluate their anonymity guarantees. To do so, we propose a novel framework that uses Bayesian inference to obtain the probability distributions linking transactions to their possible originators. We characterize transaction anonymity with those distributions, using entropy as metric of adversarial uncertainty on the originator's identity. In particular, we model Dandelion, Dandelion++ and Lightning Network. We study different configurations and demonstrate that none of them offers acceptable anonymity to their users. For instance, our analysis reveals that in the widely deployed Lightning Network, with 1% strategically chosen colluding nodes the adversary can uniquely determine the originator for about 50% of the total transactions in the network. In Dandelion, an adversary that controls 15% of the nodes has on average uncertainty among only 8 possible originators. Moreover, we observe that due to the way Dandelion and Dandelion++ are designed, increasing the network size does not correspond to an increase in the anonymity set of potential originators. Alarmingly, our longitudinal analysis of Lightning Network reveals rather an inverse trend -- with the growth of the network the overall anonymity decreases.

摘要: 通过利用其对等网络上的网络级通信，加密货币系统可能会受到反匿名化攻击。在对等网络中控制一组串通节点的敌手可以观察正在交换的交易并推断所涉及的各方。因此，各种网络匿名方案被提出来缓解这一问题，一些解决方案提供了理论上的匿名性保证。在这项工作中，我们对这种对等网络匿名解决方案进行建模，并评估它们的匿名性保证。为此，我们提出了一个新的框架，它使用贝叶斯推理来获得将事务链接到可能的发起者的概率分布。我们使用这些分布来表征交易匿名性，使用熵作为对发起者身份的敌意不确定性的度量。特别是，我们对蒲公英、蒲公英++和闪电网络进行了建模。我们研究了不同的配置，并证明它们都不能为用户提供可接受的匿名性。例如，我们的分析表明，在广泛部署的闪电网络中，通过1%的策略选择合谋节点，对手可以唯一地确定网络中约50%的总交易的发起者。在蒲公英中，一个控制了15%节点的对手平均只有8个可能的发起者中存在不确定性。此外，我们观察到，由于蒲公英和蒲公英++的设计方式，增加网络规模并不对应于潜在发起者匿名性集合的增加。令人担忧的是，我们对Lightning Network的纵向分析揭示了一个相反的趋势--随着网络的增长，总体匿名性下降。



## **29. Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks**

走向实用化部署--深度神经网络的阶段后门攻击 cs.CR

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2111.12965v2)

**Authors**: Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu

**Abstracts**: One major goal of the AI security community is to securely and reliably produce and deploy deep learning models for real-world applications. To this end, data poisoning based backdoor attacks on deep neural networks (DNNs) in the production stage (or training stage) and corresponding defenses are extensively explored in recent years. Ironically, backdoor attacks in the deployment stage, which can often happen in unprofessional users' devices and are thus arguably far more threatening in real-world scenarios, draw much less attention of the community. We attribute this imbalance of vigilance to the weak practicality of existing deployment-stage backdoor attack algorithms and the insufficiency of real-world attack demonstrations. To fill the blank, in this work, we study the realistic threat of deployment-stage backdoor attacks on DNNs. We base our study on a commonly used deployment-stage attack paradigm -- adversarial weight attack, where adversaries selectively modify model weights to embed backdoor into deployed DNNs. To approach realistic practicality, we propose the first gray-box and physically realizable weights attack algorithm for backdoor injection, namely subnet replacement attack (SRA), which only requires architecture information of the victim model and can support physical triggers in the real world. Extensive experimental simulations and system-level real-world attack demonstrations are conducted. Our results not only suggest the effectiveness and practicality of the proposed attack algorithm, but also reveal the practical risk of a novel type of computer virus that may widely spread and stealthily inject backdoor into DNN models in user devices. By our study, we call for more attention to the vulnerability of DNNs in the deployment stage.

摘要: AI安全社区的一大目标是安全可靠地为现实世界的应用程序生成和部署深度学习模型。为此，基于数据中毒的深度神经网络(DNN)在生产阶段(或训练阶段)的后门攻击以及相应的防御措施近年来得到了广泛的研究。具有讽刺意味的是，部署阶段的后门攻击通常会发生在非专业用户的设备上，因此在现实世界中可以说威胁要大得多，但社区对此的关注要少得多。我们将这种不平衡的警觉性归因于现有部署阶段后门攻击算法的实用性较弱，以及现实世界攻击演示的不足。为了填补这一空白，在这项工作中，我们研究了部署阶段后门攻击对DNN的现实威胁。我们的研究基于一种常用的部署阶段攻击范式--对抗性权重攻击，在这种攻击中，攻击者选择性地修改模型权重，将后门嵌入到部署的DNN中。为了更接近实际应用，我们提出了第一种灰盒和物理可实现权重的后门注入攻击算法，即子网替换攻击算法(SRA)，该算法只需要受害者模型的体系结构信息，并且能够支持现实世界中的物理触发。进行了广泛的实验模拟和系统级真实世界攻击演示。我们的结果不仅表明了所提出的攻击算法的有效性和实用性，还揭示了一种新型计算机病毒的实际风险，这种病毒可能会广泛传播并悄悄地向用户设备中的DNN模型注入后门。通过我们的研究，我们呼吁更多地关注DNN在部署阶段的脆弱性。



## **30. Denial-of-Service Attack on Object Detection Model Using Universal Adversarial Perturbation**

基于通用对抗摄动的拒绝服务攻击目标检测模型 cs.CV

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13618v1)

**Authors**: Avishag Shapira, Alon Zolfi, Luca Demetrio, Battista Biggio, Asaf Shabtai

**Abstracts**: Adversarial attacks against deep learning-based object detectors have been studied extensively in the past few years. The proposed attacks aimed solely at compromising the models' integrity (i.e., trustworthiness of the model's prediction), while adversarial attacks targeting the models' availability, a critical aspect in safety-critical domains such as autonomous driving, have not been explored by the machine learning research community. In this paper, we propose NMS-Sponge, a novel approach that negatively affects the decision latency of YOLO, a state-of-the-art object detector, and compromises the model's availability by applying a universal adversarial perturbation (UAP). In our experiments, we demonstrate that the proposed UAP is able to increase the processing time of individual frames by adding "phantom" objects while preserving the detection of the original objects.

摘要: 针对基于深度学习的目标检测器的对抗性攻击在过去的几年中得到了广泛的研究。拟议的攻击仅旨在损害模型的完整性(即，模型预测的可信度)，而针对模型可用性的对抗性攻击，这是自动驾驶等安全关键领域的关键方面，尚未被机器学习研究社区探索。在本文中，我们提出了一种新的方法NMS-Sponge，它对最新的目标检测器YOLO的决策延迟产生了负面影响，并通过应用通用对抗性扰动(UAP)来折衷模型的可用性。在我们的实验中，我们证明了所提出的UAP能够在保持对原始对象的检测的同时，通过增加“幻影”对象来增加单帧的处理时间。



## **31. Circumventing Backdoor Defenses That Are Based on Latent Separability**

绕过基于潜在可分离性的后门防御 cs.LG

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13613v1)

**Authors**: Xiangyu Qi, Tinghao Xie, Saeed Mahloujifar, Prateek Mittal

**Abstracts**: Deep learning models are vulnerable to backdoor poisoning attacks. In particular, adversaries can embed hidden backdoors into a model by only modifying a very small portion of its training data. On the other hand, it has also been commonly observed that backdoor poisoning attacks tend to leave a tangible signature in the latent space of the backdoored model i.e. poison samples and clean samples form two separable clusters in the latent space. These observations give rise to the popularity of latent separability assumption, which states that the backdoored DNN models will learn separable latent representations for poison and clean populations. A number of popular defenses (e.g. Spectral Signature, Activation Clustering, SCAn, etc.) are exactly built upon this assumption. However, in this paper, we show that the latent separation can be significantly suppressed via designing adaptive backdoor poisoning attacks with more sophisticated poison strategies, which consequently render state-of-the-art defenses based on this assumption less effective (and often completely fail). More interestingly, we find that our adaptive attacks can even evade some other typical backdoor defenses that do not explicitly build on this separability assumption. Our results show that adaptive backdoor poisoning attacks that can breach the latent separability assumption should be seriously considered for evaluating existing and future defenses.

摘要: 深度学习模型很容易受到后门中毒攻击。特别是，攻击者只需修改模型的一小部分训练数据，就可以将隐藏的后门嵌入到模型中。另一方面，人们也普遍观察到，后门中毒攻击往往会在后置模型的潜在空间中留下有形的特征，即有毒样本和干净样本在潜在空间中形成两个可分离的簇。这些观察结果导致了潜在可分离性假设的流行，该假设表明，倒退的DNN模型将学习毒物和清洁种群的可分离潜在表示。许多流行的防御措施(例如，频谱签名、激活集群、扫描等)正是建立在这个假设之上的。然而，在本文中，我们表明，通过设计具有更复杂中毒策略的自适应后门中毒攻击，可以显著抑制潜在分离，从而使基于这一假设的最新防御措施不那么有效(并且往往完全失败)。更有趣的是，我们发现我们的适应性攻击甚至可以避开其他一些典型的后门防御，这些后门防御并不明确地建立在可分离性假设的基础上。我们的结果表明，在评估现有和未来的防御措施时，应该认真考虑可能违反潜在可分离性假设的适应性后门中毒攻击。



## **32. PerDoor: Persistent Non-Uniform Backdoors in Federated Learning using Adversarial Perturbations**

PerDoor：使用对抗性扰动的联合学习中持久的非一致后门 cs.CR

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13523v1)

**Authors**: Manaar Alam, Esha Sarkar, Michail Maniatakos

**Abstracts**: Federated Learning (FL) enables numerous participants to train deep learning models collaboratively without exposing their personal, potentially sensitive data, making it a promising solution for data privacy in collaborative training. The distributed nature of FL and unvetted data, however, makes it inherently vulnerable to backdoor attacks: In this scenario, an adversary injects backdoor functionality into the centralized model during training, which can be triggered to cause the desired misclassification for a specific adversary-chosen input. A range of prior work establishes successful backdoor injection in an FL system; however, these backdoors are not demonstrated to be long-lasting. The backdoor functionality does not remain in the system if the adversary is removed from the training process since the centralized model parameters continuously mutate during successive FL training rounds. Therefore, in this work, we propose PerDoor, a persistent-by-construction backdoor injection technique for FL, driven by adversarial perturbation and targeting parameters of the centralized model that deviate less in successive FL rounds and contribute the least to the main task accuracy. An exhaustive evaluation considering an image classification scenario portrays on average $10.5\times$ persistence over multiple FL rounds compared to traditional backdoor attacks. Through experiments, we further exhibit the potency of PerDoor in the presence of state-of-the-art backdoor prevention techniques in an FL system. Additionally, the operation of adversarial perturbation also assists PerDoor in developing non-uniform trigger patterns for backdoor inputs compared to uniform triggers (with fixed patterns and locations) of existing backdoor techniques, which are prone to be easily mitigated.

摘要: 联合学习(FL)使众多参与者能够协作地训练深度学习模型，而不会暴露他们的个人、潜在敏感数据，使其成为协作培训中数据隐私的一种有前途的解决方案。然而，FL和未经审查的数据的分布式性质使其天生就容易受到后门攻击：在这种情况下，对手在训练期间向集中式模型注入后门功能，这可能会被触发，导致对特定对手选择的输入造成所需的错误分类。先前的一系列工作在FL系统中建立了成功的后门注入；然而，这些后门并没有被证明是持久的。如果将对手从训练过程中移除，则后门功能不会保留在系统中，因为集中式模型参数在连续的FL训练轮期间不断变化。因此，在这项工作中，我们提出了PerDoor，这是一种持久的构造后门注入技术，受对手扰动和集中式模型的目标参数的驱动，这些参数在连续的FL轮中偏离较小，对主任务精度的贡献最小。与传统的后门攻击相比，考虑图像分类场景的详尽评估描绘了在多个FL轮上平均花费10.5\x$持久性。通过实验，我们进一步展示了PerDoor在FL系统中存在最先进的后门预防技术时的有效性。此外，对抗性扰动的操作还有助于PerDoor为后门输入开发非统一的触发模式，而不是现有后门技术的统一触发(具有固定的模式和位置)，后者容易被缓解。



## **33. An Analytic Framework for Robust Training of Artificial Neural Networks**

一种神经网络稳健训练的分析框架 cs.LG

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13502v1)

**Authors**: Ramin Barati, Reza Safabakhsh, Mohammad Rahmati

**Abstracts**: The reliability of a learning model is key to the successful deployment of machine learning in various industries. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. Consequently, many studies investigate the phenomenon by proposing a simplified model of how adversarial examples occur and validate it by predicting some aspect of the phenomenon. While these studies cover many different characteristics of the adversarial examples, they have not reached a holistic approach to the geometric and analytic modeling of the phenomenon. This paper propose a formal framework to study the phenomenon in learning theory and make use of complex analysis and holomorphicity to offer a robust learning rule for artificial neural networks. With the help of complex analysis, we can effortlessly move between geometric and analytic perspectives of the phenomenon and offer further insights on the phenomenon by revealing its connection with harmonic functions. Using our model, we can explain some of the most intriguing characteristics of adversarial examples, including transferability of adversarial examples, and pave the way for novel approaches to mitigate the effects of the phenomenon.

摘要: 学习模型的可靠性是机器学习在各个行业成功部署的关键。创建一个健壮的模型，特别是一个不受对抗性攻击影响的模型，需要对对抗性例子现象有一个全面的了解。然而，由于机器学习中问题的复杂性，这一现象很难描述。因此，许多研究通过提出对抗性例子如何发生的简化模型来研究这一现象，并通过预测该现象的某些方面来验证该模型。虽然这些研究涵盖了对抗性例子的许多不同特征，但它们还没有达成对这一现象的几何和解析建模的整体方法。本文提出了一种学习理论中研究这一现象的形式化框架，并利用复分析和全纯理论为人工神经网络提供了一种稳健的学习规则。在复杂分析的帮助下，我们可以毫不费力地在现象的几何和解析视角之间切换，并通过揭示它与调和函数的联系来提供对该现象的进一步见解。使用我们的模型，我们可以解释对抗性例子的一些最有趣的特征，包括对抗性例子的可转移性，并为缓解这一现象的影响的新方法铺平道路。



## **34. Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection**

关于理解和利用图像变换在对抗检测中的作用 cs.CV

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2201.01080v3)

**Authors**: Hui Liu, Bo Zhao, Yuefeng Peng, Weidong Li, Peng Liu

**Abstracts**: Deep neural networks (DNNs) are threatened by adversarial examples. Adversarial detection, which distinguishes adversarial images from benign images, is fundamental for robust DNN-based services. Image transformation is one of the most effective approaches to detect adversarial examples. During the last few years, a variety of image transformations have been studied and discussed to design reliable adversarial detectors. In this paper, we systematically synthesize the recent progress on adversarial detection via image transformations with a novel classification method. Then, we conduct extensive experiments to test the detection performance of image transformations against state-of-the-art adversarial attacks. Furthermore, we reveal that each individual transformation is not capable of detecting adversarial examples in a robust way, and propose a DNN-based approach referred to as \emph{AdvJudge}, which combines scores of 9 image transformations. Without knowing which individual scores are misleading or not misleading, AdvJudge can make the right judgment, and achieve a significant improvement in detection rate. Finally, we utilize an explainable AI tool to show the contribution of each image transformation to adversarial detection. Experimental results show that the contribution of image transformations to adversarial detection is significantly different, the combination of them can significantly improve the generic detection ability against state-of-the-art adversarial attacks.

摘要: 深度神经网络(DNN)受到敌意例子的威胁。敌意检测是基于DNN的稳健服务的基础，它区分敌意图像和良性图像。图像变换是检测敌意例子最有效的方法之一。在过去的几年里，人们已经研究和讨论了各种图像变换来设计可靠的对抗性检测器。本文采用一种新的分类方法，系统地综述了基于图像变换的对抗性检测的最新研究进展。然后，我们进行了大量的实验来测试图像变换对最新的敌意攻击的检测性能。此外，我们揭示了每个个体变换并不能稳健地检测敌意示例，并提出了一种基于DNN的方法，该方法结合了9个图像变换的分数。在不知道哪些分数具有误导性或哪些不具有误导性的情况下，AdvJustice能够做出正确的判断，并实现了检测率的显著提高。最后，我们利用一个可解释的人工智能工具来展示每个图像变换对对抗检测的贡献。实验结果表明，图像变换对敌意检测的贡献是不同的，它们的结合可以显著提高对最新敌意攻击的一般检测能力。



## **35. A Physical-World Adversarial Attack Against 3D Face Recognition**

一种针对3D人脸识别的物理世界对抗性攻击 cs.CV

10 pages, 5 figures, Submit to NeurIPS 2022

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13412v1)

**Authors**: Yanjie Li, Yiquan Li, Bin Xiao

**Abstracts**: 3D face recognition systems have been widely employed in intelligent terminals, among which structured light imaging is a common method to measure the 3D shape. However, this method could be easily attacked, leading to inaccurate 3D face recognition. In this paper, we propose a novel, physically-achievable attack on the fringe structured light system, named structured light attack. The attack utilizes a projector to project optical adversarial fringes on faces to generate point clouds with well-designed noises. We firstly propose a 3D transform-invariant loss function to enhance the robustness of 3D adversarial examples in the physical-world attack. Then we reverse the 3D adversarial examples to the projector's input to place noises on phase-shift images, which models the process of structured light imaging. A real-world structured light system is constructed for the attack and several state-of-the-art 3D face recognition neural networks are tested. Experiments show that our method can attack the physical system successfully and only needs minor modifications of projected images.

摘要: 三维人脸识别系统在智能终端中得到了广泛的应用，其中结构光成像是一种常用的三维形状测量方法。然而，这种方法容易受到攻击，导致3D人脸识别不准确。在本文中，我们提出了一种新颖的、物理上可实现的对条纹结构光系统的攻击，称为结构光攻击。该攻击利用投影仪将光学对抗性条纹投射到人脸上，以生成具有精心设计的噪声的点云。本文首次提出了一种3D变换不变损失函数，以增强3D对抗实例在物理世界攻击中的稳健性。然后，我们将3D对抗性的例子反转到投影仪的输入上，在相移图像上放置噪声，这是对结构光成像过程的模拟。针对攻击构建了一个真实世界的结构光系统，并测试了几种最先进的3D人脸识别神经网络。实验表明，该方法可以成功地攻击物理系统，并且只需要对投影图像进行很小的修改。



## **36. BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning**

BppAttack：基于图像量化和对比性学习的隐蔽高效木马攻击深度神经网络 cs.CV

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13383v1)

**Authors**: Zhenting Wang, Juan Zhai, Shiqing Ma

**Abstracts**: Deep neural networks are vulnerable to Trojan attacks. Existing attacks use visible patterns (e.g., a patch or image transformations) as triggers, which are vulnerable to human inspection. In this paper, we propose stealthy and efficient Trojan attacks, BppAttack. Based on existing biology literature on human visual systems, we propose to use image quantization and dithering as the Trojan trigger, making imperceptible changes. It is a stealthy and efficient attack without training auxiliary models. Due to the small changes made to images, it is hard to inject such triggers during training. To alleviate this problem, we propose a contrastive learning based approach that leverages adversarial attacks to generate negative sample pairs so that the learned trigger is precise and accurate. The proposed method achieves high attack success rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. It also effectively bypasses existing Trojan defenses and human inspection. Our code can be found in https://github.com/RU-System-Software-and-Security/BppAttack.

摘要: 深度神经网络容易受到特洛伊木马的攻击。现有攻击使用可见模式(例如，补丁或图像转换)作为触发器，这容易受到人工检查。本文提出了一种隐蔽高效的特洛伊木马攻击BppAttack。基于现有关于人类视觉系统的生物学文献，我们提出使用图像量化和抖动作为特洛伊木马的触发器，进行潜移默化的改变。这是一种不需要训练辅助模型的隐身而有效的攻击。由于图像的微小变化，在训练中很难注入这样的触发因素。为了缓解这一问题，我们提出了一种基于对比学习的方法，该方法利用对抗性攻击来生成负样本对，从而学习到的触发器是精确和准确的。该方法在MNIST、CIFAR-10、GTSRB和CelebA四个基准数据集上取得了较高的攻击成功率。它还有效地绕过了现有的特洛伊木马防御和人工检查。我们的代码可以在https://github.com/RU-System-Software-and-Security/BppAttack.中找到



## **37. Random Walks for Adversarial Meshes**

对抗性网格的随机游动 cs.CV

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2202.07453v2)

**Authors**: Amir Belder, Gal Yefet, Ran Ben Izhak, Ayellet Tal

**Abstracts**: A polygonal mesh is the most-commonly used representation of surfaces in computer graphics. Therefore, it is not surprising that a number of mesh classification networks have recently been proposed. However, while adversarial attacks are wildly researched in 2D, the field of adversarial meshes is under explored. This paper proposes a novel, unified, and general adversarial attack, which leads to misclassification of several state-of-the-art mesh classification neural networks. Our attack approach is black-box, i.e. it has access only to the network's predictions, but not to the network's full architecture or gradients. The key idea is to train a network to imitate a given classification network. This is done by utilizing random walks along the mesh surface, which gather geometric information. These walks provide insight onto the regions of the mesh that are important for the correct prediction of the given classification network. These mesh regions are then modified more than other regions in order to attack the network in a manner that is barely visible to the naked eye.

摘要: 多边形网格是计算机图形学中最常用的曲面表示形式。因此，最近提出了一些网格分类网络也就不足为奇了。然而，尽管对抗性攻击在2D领域得到了广泛的研究，但对抗性网络领域的研究还很少。本文提出了一种新颖的、统一的、通用的对抗性攻击，该攻击导致了几种最新的网格分类神经网络的错误分类。我们的攻击方法是黑匣子，即它只能访问网络的预测，而不能访问网络的完整架构或梯度。其核心思想是训练一个网络来模仿给定的分类网络。这是通过利用沿网格曲面的随机漫游来完成的，该随机漫游收集几何信息。这些遍历提供了对网格区域的洞察，这些区域对于给定分类网络的正确预测非常重要。然后，这些网格区域被修改得比其他区域更多，以便以肉眼几乎看不到的方式攻击网络。



## **38. Denial-of-Service Attacks on Learned Image Compression**

针对学习图像压缩的拒绝服务攻击 cs.CV

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13253v1)

**Authors**: Kang Liu, Di Wu, Yiru Wang, Dan Feng, Benjamin Tan, Siddharth Garg

**Abstracts**: Deep learning techniques have shown promising results in image compression, with competitive bitrate and image reconstruction quality from compressed latent. However, while image compression has progressed towards higher peak signal-to-noise ratio (PSNR) and fewer bits per pixel (bpp), their robustness to corner-case images has never received deliberation. In this work, we, for the first time, investigate the robustness of image compression systems where imperceptible perturbation of input images can precipitate a significant increase in the bitrate of their compressed latent. To characterize the robustness of state-of-the-art learned image compression, we mount white and black-box attacks. Our results on several image compression models with various bitrate qualities show that they are surprisingly fragile, where the white-box attack achieves up to 56.326x and black-box 1.947x bpp change. To improve robustness, we propose a novel model which incorporates attention modules and a basic factorized entropy model, resulting in a promising trade-off between the PSNR/bpp ratio and robustness to adversarial attacks that surpasses existing learned image compressors.

摘要: 深度学习技术在图像压缩方面取得了很好的效果，压缩后的潜伏期具有较高的比特率和图像重建质量。然而，虽然图像压缩已经朝着更高的峰值信噪比(PSNR)和更少的每像素位(BPP)发展，但它们对角点图像的稳健性从未得到深思熟虑。在这项工作中，我们首次研究了图像压缩系统的稳健性，在这种情况下，输入图像的不可察觉的扰动可以导致其压缩潜伏期的比特率显著增加。为了表征最先进的学习图像压缩的稳健性，我们安装了白盒和黑盒攻击。我们在几种不同码率质量的图像压缩模型上的结果表明，它们令人惊讶地脆弱，其中白盒攻击达到了56.326倍，黑盒攻击达到了1.947倍的BPP变化。为了提高鲁棒性，我们提出了一种新的模型，它结合了注意力模块和基本的因式分解熵模型，在PSNR/BPP比和对抗攻击的稳健性之间取得了很好的权衡，其性能超过了现有的学习图像压缩器。



## **39. Certified Robustness Against Natural Language Attacks by Causal Intervention**

通过因果干预验证对自然语言攻击的健壮性 cs.LG

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.12331v2)

**Authors**: Haiteng Zhao, Chang Ma*, Xinshuai Dong, Anh Tuan Luu, Zhi-Hong Deng, Hanwang Zhang

**Abstracts**: Deep learning models have achieved great success in many fields, yet they are vulnerable to adversarial examples. This paper follows a causal perspective to look into the adversarial vulnerability and proposes Causal Intervention by Semantic Smoothing (CISS), a novel framework towards robustness against natural language attacks. Instead of merely fitting observational data, CISS learns causal effects p(y|do(x)) by smoothing in the latent semantic space to make robust predictions, which scales to deep architectures and avoids tedious construction of noise customized for specific attacks. CISS is provably robust against word substitution attacks, as well as empirically robust even when perturbations are strengthened by unknown attack algorithms. For example, on YELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness against word substitutions, and achieves 79.4% empirical robustness when syntactic attacks are integrated.

摘要: 深度学习模型在许多领域都取得了很大的成功，但它们很容易受到对手例子的影响。本文从因果关系的角度分析了敌意攻击的脆弱性，提出了通过语义平滑进行因果干预的方法，这是一种新的针对自然语言攻击的健壮性框架。与其仅仅对观测数据进行拟合，CIS通过在潜在语义空间中进行平滑以做出稳健的预测来学习因果效应p(y|do(X))，该预测可扩展到深层体系结构，并避免针对特定攻击定制的乏味的噪声构造。可以证明，该系统对单词替换攻击具有较强的健壮性，即使在未知攻击算法加强了扰动的情况下，也具有较强的经验性。例如，在Yelp上，在对单词替换的验证健壮性方面，CISS超过亚军6.7%，并且在整合句法攻击时获得了79.4%的经验健壮性。



## **40. Transferable Adversarial Attack based on Integrated Gradients**

基于集成梯度的可转移敌意攻击 cs.LG

**SubmitDate**: 2022-05-26    [paper-pdf](http://arxiv.org/pdf/2205.13152v1)

**Authors**: Yi Huang, Adams Wai-Kin Kong

**Abstracts**: The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention maps, and smoothing decision surfaces, are commonly used to craft adversarial examples. By tightly integrating the three approaches, we propose a new and simple algorithm named Transferable Attack based on Integrated Gradients (TAIG) in this paper, which can find highly transferable adversarial examples for black-box attacks. Unlike previous methods using multiple computational terms or combining with other methods, TAIG integrates the three approaches into one single term. Two versions of TAIG that compute their integrated gradients on a straight-line path and a random piecewise linear path are studied. Both versions offer strong transferability and can seamlessly work together with the previous methods. Experimental results demonstrate that TAIG outperforms the state-of-the-art methods. The code will available at https://github.com/yihuang2016/TAIG

摘要: 深度神经网络对敌意例子的脆弱性已经引起了社会各界的极大关注。优化标准目标函数、利用注意图和平滑决策面这三种方法通常被用来制作对抗性例子。通过将这三种方法紧密地结合在一起，本文提出了一种新的简单的基于集成梯度的可转移攻击算法(TIAG)，该算法可以发现高度可转移的黑盒攻击对手实例。与以往使用多个计算项或与其他方法相结合的方法不同，TAIG将这三种方法集成到一个项中。研究了在直线路径和随机分段线性路径上计算其积分梯度的两个版本的TAIG。这两个版本都提供了很强的可移植性，并可以与之前的方法无缝配合使用。实验结果表明，TAIG的性能优于目前最先进的方法。该代码将在https://github.com/yihuang2016/TAIG上提供



## **41. Textual Backdoor Attacks with Iterative Trigger Injection**

使用迭代触发器注入的文本后门攻击 cs.CL

**SubmitDate**: 2022-05-25    [paper-pdf](http://arxiv.org/pdf/2205.12700v1)

**Authors**: Jun Yan, Vansh Gupta, Xiang Ren

**Abstracts**: The backdoor attack has become an emerging threat for Natural Language Processing (NLP) systems. A victim model trained on poisoned data can be embedded with a "backdoor", making it predict the adversary-specified output (e.g., the positive sentiment label) on inputs satisfying the trigger pattern (e.g., containing a certain keyword). In this paper, we demonstrate that it's possible to design an effective and stealthy backdoor attack by iteratively injecting "triggers" into a small set of training data. While all triggers are common words that fit into the context, our poisoning process strongly associates them with the target label, forming the model backdoor. Experiments on sentiment analysis and hate speech detection show that our proposed attack is both stealthy and effective, raising alarm on the usage of untrusted training data. We further propose a defense method to combat this threat.

摘要: 后门攻击已经成为自然语言处理(NLP)系统的新威胁。在有毒数据上训练的受害者模型可以被嵌入有“后门”，使其在满足触发模式(例如，包含特定关键字)的输入上预测对手指定的输出(例如，积极情绪标签)。在这篇文章中，我们证明了通过迭代地向一小组训练数据注入“触发器”来设计有效和隐蔽的后门攻击是可能的。虽然所有触发器都是符合上下文的常见单词，但我们的中毒过程将它们与目标标签紧密关联，形成了模型后门。情感分析和仇恨语音检测实验表明，本文提出的攻击具有隐蔽性和有效性，对不可信训练数据的使用提出了警告。我们进一步提出了应对这种威胁的防御方法。



## **42. Deniable Steganography**

可否认隐写术 cs.CR

**SubmitDate**: 2022-05-25    [paper-pdf](http://arxiv.org/pdf/2205.12587v1)

**Authors**: Yong Xu, Zhihua Xia, Zichi Wang, Xinpeng Zhang, Jian Weng

**Abstracts**: Steganography conceals the secret message into the cover media, generating a stego media which can be transmitted on public channels without drawing suspicion. As its countermeasure, steganalysis mainly aims to detect whether the secret message is hidden in a given media. Although the steganography techniques are improving constantly, the sophisticated steganalysis can always break a known steganographic method to some extent. With a stego media discovered, the adversary could find out the sender or receiver and coerce them to disclose the secret message, which we name as coercive attack in this paper. Inspired by the idea of deniable encryption, we build up the concepts of deniable steganography for the first time and discuss the feasible constructions for it. As an example, we propose a receiver-deniable steganographic scheme to deal with the receiver-side coercive attack using deep neural networks (DNN). Specifically, besides the real secret message, a piece of fake message is also embedded into the cover. On the receiver side, the real message can be extracted with an extraction module; while once the receiver has to surrender a piece of secret message under coercive attack, he can extract the fake message to deceive the adversary with another extraction module. Experiments demonstrate the scalability and sensitivity of the DNN-based receiver-deniable steganographic scheme.

摘要: 隐写术将秘密信息隐藏在掩护媒体中，产生了一种可以在公共渠道上传输而不会引起怀疑的隐写媒体。作为其对策，隐写分析的主要目的是检测秘密信息是否隐藏在给定的媒体中。虽然隐写技术在不断改进，但复杂的隐写分析总能在一定程度上破解已知的隐写方法。发现隐写媒体后，攻击者可以找到发送者或接收者，并强迫他们泄露秘密信息，本文称之为强制攻击。受可否认加密思想的启发，我们首次提出了可否认隐写的概念，并讨论了它的可行构造。作为一个例子，我们提出了一种接收方可否认的隐写方案，利用深度神经网络(DNN)来应对接收方的强制攻击。具体地说，除了真实的秘密信息外，封面还嵌入了一条虚假信息。在接收方，可以使用提取模块来提取真实消息；而一旦接收方在强制攻击下不得不交出一条秘密消息，他可以使用另一个提取模块来提取虚假消息来欺骗对手。实验证明了基于DNN的接收方可否认隐写方案的可扩展性和敏感度。



## **43. Misleading Deep-Fake Detection with GAN Fingerprints**

基于GaN指纹的误导性深伪检测 cs.CV

In IEEE Deep Learning and Security Workshop (DLS) 2022

**SubmitDate**: 2022-05-25    [paper-pdf](http://arxiv.org/pdf/2205.12543v1)

**Authors**: Vera Wesselkamp, Konrad Rieck, Daniel Arp, Erwin Quiring

**Abstracts**: Generative adversarial networks (GANs) have made remarkable progress in synthesizing realistic-looking images that effectively outsmart even humans. Although several detection methods can recognize these deep fakes by checking for image artifacts from the generation process, multiple counterattacks have demonstrated their limitations. These attacks, however, still require certain conditions to hold, such as interacting with the detection method or adjusting the GAN directly. In this paper, we introduce a novel class of simple counterattacks that overcomes these limitations. In particular, we show that an adversary can remove indicative artifacts, the GAN fingerprint, directly from the frequency spectrum of a generated image. We explore different realizations of this removal, ranging from filtering high frequencies to more nuanced frequency-peak cleansing. We evaluate the performance of our attack with different detection methods, GAN architectures, and datasets. Our results show that an adversary can often remove GAN fingerprints and thus evade the detection of generated images.

摘要: 生成性对抗网络(GAN)在合成看起来逼真的图像方面取得了显着的进步，这些图像甚至有效地超过了人类。虽然几种检测方法可以通过检查生成过程中的图像伪影来识别这些深度伪像，但多次反击已经证明了它们的局限性。然而，这些攻击仍然需要一定的条件才能成立，例如与检测方法交互或直接调整GAN。在本文中，我们介绍了一类新的简单的反击，它克服了这些限制。具体地说，我们证明了攻击者可以直接从生成的图像的频谱中移除指示性伪像，即GaN指纹。我们探索了这种去除的不同实现，从过滤高频到更细微的频率峰值净化。我们使用不同的检测方法、GAN架构和数据集来评估我们的攻击的性能。我们的结果表明，攻击者经常可以移除GaN指纹，从而逃避生成图像的检测。



## **44. A Survey of Graph-Theoretic Approaches for Analyzing the Resilience of Networked Control Systems**

网络控制系统弹性分析的图论方法综述 eess.SY

**SubmitDate**: 2022-05-25    [paper-pdf](http://arxiv.org/pdf/2205.12498v1)

**Authors**: Mohammad Pirani, Aritra Mitra, Shreyas Sundaram

**Abstracts**: As the scale of networked control systems increases and interactions between different subsystems become more sophisticated, questions of the resilience of such networks increase in importance. The need to redefine classical system and control-theoretic notions using the language of graphs has recently started to gain attention as a fertile and important area of research. This paper presents an overview of graph-theoretic methods for analyzing the resilience of networked control systems. We discuss various distributed algorithms operating on networked systems and investigate their resilience against adversarial actions by looking at the structural properties of their underlying networks. We present graph-theoretic methods to quantify the attack impact, and reinterpret some system-theoretic notions of robustness from a graph-theoretic standpoint to mitigate the impact of the attacks. Moreover, we discuss miscellaneous problems in the security of networked control systems which use graph-theory as a tool in their analyses. We conclude by introducing some avenues for further research in this field.

摘要: 随着网络控制系统规模的增加和不同子系统之间的交互变得更加复杂，这种网络的弹性问题变得越来越重要。使用图形语言重新定义经典系统和控制论概念的需要最近开始作为一个肥沃而重要的研究领域得到关注。本文综述了网络控制系统弹性分析的图论方法。我们讨论了运行在网络系统上的各种分布式算法，并通过观察其底层网络的结构属性来研究它们对恶意行为的恢复能力。我们提出了图论方法来量化攻击的影响，并从图论的角度重新解释了一些系统理论中的健壮性概念，以减轻攻击的影响。此外，我们还讨论了以图论为分析工具的网络控制系统的各种安全问题。最后，我们介绍了这一领域进一步研究的一些途径。



## **45. Label Leakage and Protection from Forward Embedding in Vertical Federated Learning**

垂直联合学习中的标签泄漏及前向嵌入保护 cs.LG

**SubmitDate**: 2022-05-25    [paper-pdf](http://arxiv.org/pdf/2203.01451v3)

**Authors**: Jiankai Sun, Xin Yang, Yuanshun Yao, Chong Wang

**Abstracts**: Vertical federated learning (vFL) has gained much attention and been deployed to solve machine learning problems with data privacy concerns in recent years. However, some recent work demonstrated that vFL is vulnerable to privacy leakage even though only the forward intermediate embedding (rather than raw features) and backpropagated gradients (rather than raw labels) are communicated between the involved participants. As the raw labels often contain highly sensitive information, some recent work has been proposed to prevent the label leakage from the backpropagated gradients effectively in vFL. However, these work only identified and defended the threat of label leakage from the backpropagated gradients. None of these work has paid attention to the problem of label leakage from the intermediate embedding. In this paper, we propose a practical label inference method which can steal private labels effectively from the shared intermediate embedding even though some existing protection methods such as label differential privacy and gradients perturbation are applied. The effectiveness of the label attack is inseparable from the correlation between the intermediate embedding and corresponding private labels. To mitigate the issue of label leakage from the forward embedding, we add an additional optimization goal at the label party to limit the label stealing ability of the adversary by minimizing the distance correlation between the intermediate embedding and corresponding private labels. We conducted massive experiments to demonstrate the effectiveness of our proposed protection methods.

摘要: 垂直联合学习(VFL)近年来得到了广泛的关注，并被用来解决数据隐私问题。然而，最近的一些工作表明，即使参与者之间只传递前向中间嵌入(而不是原始特征)和反向传播的梯度(而不是原始标签)，VFL也容易受到隐私泄漏的影响。由于原始标签往往包含高度敏感的信息，最近的一些工作被提出以有效地防止VFL中反向传播梯度的标签泄漏。然而，这些工作只是识别和防御了反向传播梯度带来的标签泄漏威胁。这些工作都没有注意到中间嵌入带来的标签泄漏问题。本文提出了一种实用的标签推理方法，即使使用了标签差分隐私和梯度扰动等现有保护方法，也能有效地从共享中间嵌入中窃取私有标签。标签攻击的有效性离不开中间嵌入与对应的私有标签之间的关联。为了缓解前向嵌入带来的标签泄漏问题，我们在标签方增加了一个额外的优化目标，通过最小化中间嵌入与对应的私有标签之间的距离相关性来限制对手的标签窃取能力。我们进行了大量的实验来证明我们提出的保护方法的有效性。



## **46. Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks**

Recipe2Vec：基于图神经网络的多模式配方表示学习 cs.LG

Accepted by IJCAI 2022

**SubmitDate**: 2022-05-24    [paper-pdf](http://arxiv.org/pdf/2205.12396v1)

**Authors**: Yijun Tian, Chuxu Zhang, Zhichun Guo, Yihong Ma, Ronald Metoyer, Nitesh V. Chawla

**Abstracts**: Learning effective recipe representations is essential in food studies. Unlike what has been developed for image-based recipe retrieval or learning structural text embeddings, the combined effect of multi-modal information (i.e., recipe images, text, and relation data) receives less attention. In this paper, we formalize the problem of multi-modal recipe representation learning to integrate the visual, textual, and relational information into recipe embeddings. In particular, we first present Large-RG, a new recipe graph data with over half a million nodes, making it the largest recipe graph to date. We then propose Recipe2Vec, a novel graph neural network based recipe embedding model to capture multi-modal information. Additionally, we introduce an adversarial attack strategy to ensure stable learning and improve performance. Finally, we design a joint objective function of node classification and adversarial learning to optimize the model. Extensive experiments demonstrate that Recipe2Vec outperforms state-of-the-art baselines on two classic food study tasks, i.e., cuisine category classification and region prediction. Dataset and codes are available at https://github.com/meettyj/Recipe2Vec.

摘要: 学习有效的食谱表示法在食品研究中是必不可少的。与已开发的基于图像的配方检索或学习结构化文本嵌入不同，多模式信息(即，配方图像、文本和关系数据)的组合效果受到的关注较少。在本文中，我们将多通道配方表示学习问题形式化，以便将视觉、文本和关系信息集成到配方嵌入中。特别是，我们首先提出了Large-RG，这是一个具有超过50万个节点的新配方图数据，使其成为迄今为止最大的配方图。在此基础上，提出了一种新的基于图神经网络的食谱嵌入模型Recipe2Vec，用于获取多模式信息。此外，我们引入了对抗性攻击策略，以确保稳定的学习和提高性能。最后，我们设计了节点分类和对抗性学习的联合目标函数来优化模型。广泛的实验表明，Recipe2Vec在两个经典的食物研究任务上表现优于最先进的基线，即烹饪类别分类和区域预测。数据集和代码可在https://github.com/meettyj/Recipe2Vec.上获得



## **47. Label Leakage and Protection in Two-party Split Learning**

两方分裂学习中的标签泄漏及防护 cs.LG

Accepted to ICLR 2022 (https://openreview.net/forum?id=cOtBRgsf2fO)

**SubmitDate**: 2022-05-24    [paper-pdf](http://arxiv.org/pdf/2102.08504v3)

**Authors**: Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith, Chong Wang

**Abstracts**: Two-party split learning is a popular technique for learning a model across feature-partitioned data. In this work, we explore whether it is possible for one party to steal the private label information from the other party during split training, and whether there are methods that can protect against such attacks. Specifically, we first formulate a realistic threat model and propose a privacy loss metric to quantify label leakage in split learning. We then show that there exist two simple yet effective methods within the threat model that can allow one party to accurately recover private ground-truth labels owned by the other party. To combat these attacks, we propose several random perturbation techniques, including $\texttt{Marvell}$, an approach that strategically finds the structure of the noise perturbation by minimizing the amount of label leakage (measured through our quantification metric) of a worst-case adversary. We empirically demonstrate the effectiveness of our protection techniques against the identified attacks, and show that $\texttt{Marvell}$ in particular has improved privacy-utility tradeoffs relative to baseline approaches.

摘要: 两方分裂学习是一种流行的跨特征分区数据学习模型的技术。在这项工作中，我们探索了在分裂训练过程中，一方是否有可能从另一方窃取私有标签信息，以及是否有方法可以防止此类攻击。具体地说，我们首先建立了一个现实的威胁模型，并提出了一种隐私损失度量来量化分裂学习中的标签泄漏。然后，我们证明了在威胁模型中存在两种简单而有效的方法，它们可以允许一方准确地恢复另一方拥有的私有地面事实标签。为了对抗这些攻击，我们提出了几种随机扰动技术，包括$\exttt{Marvell}$，一种通过最小化最坏情况对手的标签泄漏量(通过我们的量化度量来衡量)来战略性地发现噪声扰动的结构的方法。我们经验性地证明了我们的保护技术对识别的攻击的有效性，并表明与基准方法相比，$\exttt{Marvell}$尤其改进了隐私效用的权衡。



## **48. PORTFILER: Port-Level Network Profiling for Self-Propagating Malware Detection**

PORTFILER：用于自传播恶意软件检测的端口级网络分析 cs.CR

An earlier version is accepted to be published in IEEE Conference on  Communications and Network Security (CNS) 2021

**SubmitDate**: 2022-05-24    [paper-pdf](http://arxiv.org/pdf/2112.13798v2)

**Authors**: Talha Ongun, Oliver Spohngellert, Benjamin Miller, Simona Boboila, Alina Oprea, Tina Eliassi-Rad, Jason Hiser, Alastair Nottingham, Jack Davidson, Malathi Veeraraghavan

**Abstracts**: Recent self-propagating malware (SPM) campaigns compromised hundred of thousands of victim machines on the Internet. It is challenging to detect these attacks in their early stages, as adversaries utilize common network services, use novel techniques, and can evade existing detection mechanisms. We propose PORTFILER (PORT-Level Network Traffic ProFILER), a new machine learning system applied to network traffic for detecting SPM attacks. PORTFILER extracts port-level features from the Zeek connection logs collected at a border of a monitored network, applies anomaly detection techniques to identify suspicious events, and ranks the alerts across ports for investigation by the Security Operations Center (SOC). We propose a novel ensemble methodology for aggregating individual models in PORTFILER that increases resilience against several evasion strategies compared to standard ML baselines. We extensively evaluate PORTFILER on traffic collected from two university networks, and show that it can detect SPM attacks with different patterns, such as WannaCry and Mirai, and performs well under evasion. Ranking across ports achieves precision over 0.94 with low false positive rates in the top ranked alerts. When deployed on the university networks, PORTFILER detected anomalous SPM-like activity on one of the campus networks, confirmed by the university SOC as malicious. PORTFILER also detected a Mirai attack recreated on the two university networks with higher precision and recall than deep-learning-based autoencoder methods.

摘要: 最近的自我传播恶意软件(SPM)活动危害了互联网上数十万受攻击的计算机。在攻击的早期阶段检测这些攻击是具有挑战性的，因为攻击者利用常见的网络服务，使用新的技术，并且可以逃避现有的检测机制。提出了一种应用于网络流量检测的机器学习系统PORTFILER(PORTFILER)，用于检测SPM攻击。PORTFILER从在受监控网络边界收集的Zeek连接日志中提取端口级特征，应用异常检测技术来识别可疑事件，并跨端口对警报进行排序，以供安全运营中心(SOC)调查。我们提出了一种新的集成方法，用于在PORTFILER中聚合单个模型，与标准的ML基线相比，该方法提高了对几种规避策略的弹性。我们对PORTFILER在两个大学网络上收集的流量进行了广泛的测试，结果表明，它可以检测出不同模式的SPM攻击，如WannaCry和Mirai，并且在规避情况下表现良好。在排名靠前的警报中，跨端口排名可实现0.94以上的精确度和较低的误警率。当部署在大学网络上时，PORTFILER在其中一个校园网络上检测到异常的类似SPM的活动，并被大学SOC确认为恶意活动。PORTFILER还检测到在两个大学网络上重新创建的Mirai攻击，与基于深度学习的自动编码器方法相比，具有更高的精确度和召回率。



## **49. Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks**

抗单词替换攻击的对抗性扰动自监督对比学习 cs.CL

In Findings of NAACL 2022

**SubmitDate**: 2022-05-24    [paper-pdf](http://arxiv.org/pdf/2107.07610v3)

**Authors**: Zhao Meng, Yihan Dong, Mrinmaya Sachan, Roger Wattenhofer

**Abstracts**: In this paper, we present an approach to improve the robustness of BERT language models against word substitution-based adversarial attacks by leveraging adversarial perturbations for self-supervised contrastive learning. We create a word-level adversarial attack generating hard positives on-the-fly as adversarial examples during contrastive learning. In contrast to previous works, our method improves model robustness without using any labeled data. Experimental results show that our method improves robustness of BERT against four different word substitution-based adversarial attacks, and combining our method with adversarial training gives higher robustness than adversarial training alone. As our method improves the robustness of BERT purely with unlabeled data, it opens up the possibility of using large text datasets to train robust language models against word substitution-based adversarial attacks.

摘要: 在本文中，我们提出了一种方法，通过利用对抗性扰动进行自我监督的对比学习来提高ERT语言模型对基于单词替换的对抗性攻击的健壮性。我们创建了一个词级对抗性攻击，在对比学习期间动态生成硬积极词作为对抗性例子。与以前的工作相比，我们的方法在不使用任何标记数据的情况下提高了模型的稳健性。实验结果表明，我们的方法提高了ERT对四种不同的基于单词替换的对抗性攻击的健壮性，并且将我们的方法与对抗性训练相结合比单独对抗性训练具有更高的健壮性。由于我们的方法提高了纯粹使用未标记数据的ERT的稳健性，因此它打开了使用大型文本数据集来训练稳健的语言模型以抵御基于单词替换的对手攻击的可能性。



## **50. Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box Score-Based Query Attacks**

对攻击者的对抗性攻击：减轻基于黑盒分数的查询攻击的后处理 cs.LG

**SubmitDate**: 2022-05-24    [paper-pdf](http://arxiv.org/pdf/2205.12134v1)

**Authors**: Sizhe Chen, Zhehao Huang, Qinghua Tao, Yingwen Wu, Cihang Xie, Xiaolin Huang

**Abstracts**: The score-based query attacks (SQAs) pose practical threats to deep neural networks by crafting adversarial perturbations within dozens of queries, only using the model's output scores. Nonetheless, we note that if the loss trend of the outputs is slightly perturbed, SQAs could be easily misled and thereby become much less effective. Following this idea, we propose a novel defense, namely Adversarial Attack on Attackers (AAA), to confound SQAs towards incorrect attack directions by slightly modifying the output logits. In this way, (1) SQAs are prevented regardless of the model's worst-case robustness; (2) the original model predictions are hardly changed, i.e., no degradation on clean accuracy; (3) the calibration of confidence scores can be improved simultaneously. Extensive experiments are provided to verify the above advantages. For example, by setting $\ell_\infty=8/255$ on CIFAR-10, our proposed AAA helps WideResNet-28 secure $80.59\%$ accuracy under Square attack ($2500$ queries), while the best prior defense (i.e., adversarial training) only attains $67.44\%$. Since AAA attacks SQA's general greedy strategy, such advantages of AAA over 8 defenses can be consistently observed on 8 CIFAR-10/ImageNet models under 6 SQAs, using different attack targets and bounds. Moreover, AAA calibrates better without hurting the accuracy. Our code would be released.

摘要: 基于分数的查询攻击(SQA)通过在数十个查询中精心设计敌意扰动，仅使用模型的输出分数，对深度神经网络构成实际威胁。尽管如此，我们注意到，如果产出的损失趋势受到轻微干扰，质量保证人员很容易受到误导，从而变得不那么有效。根据这一思想，我们提出了一种新的防御方法，即对攻击者的对抗性攻击(AAA)，通过略微修改输出日志来迷惑SQA对错误的攻击方向。这样，(1)无论模型在最坏情况下的稳健性如何，都可以防止SQA；(2)原始模型预测几乎不会改变，即不会降低干净的精度；(3)置信度得分的校准可以同时得到改善。通过大量的实验验证了上述优点。例如，通过在CIFAR-10上设置$\ell_\infty=8/255$，我们提出的AAA帮助WideResNet-28在Square攻击($2500$查询)下获得$80.59\$准确性，而最好的先前防御(即对抗性训练)仅达到$67.44\$。由于AAA攻击SQA的一般贪婪策略，因此在使用不同的攻击目标和边界的6个SQA下的8个CIFAR-10/ImageNet模型上，可以一致地观察到AAA相对于8个防御的优势。此外，AAA在不影响精度的情况下校准得更好。我们的代码就会被发布。



