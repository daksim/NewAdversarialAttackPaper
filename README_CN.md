# Latest Adversarial Attack Papers
**update at 2023-06-21 09:42:01**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers**

虚幻攻击：对顺序决策者的对抗性攻击中的可探测性问题 cs.AI

**SubmitDate**: 2023-06-20    [abs](http://arxiv.org/abs/2207.10170v3) [paper-pdf](http://arxiv.org/pdf/2207.10170v3)

**Authors**: Tim Franzmeyer, Stephen McAleer, João F. Henriques, Jakob N. Foerster, Philip H. S. Torr, Adel Bibi, Christian Schroeder de Witt

**Abstract**: Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of temporal consistency makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce perfect illusory attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and provably statistically undetectable. We then propose the more versatile R-attacks, which result in observation transitions that are consistent with the state-transition function of the adversary-free environment and can be learned end-to-end. Compared to existing attacks, we empirically find R-attacks to be significantly harder to detect with automated methods, and a small study with human subjects suggests they are similarly harder to detect for humans. We propose that undetectability should be a central concern in the study of adversarial attacks on mixed-autonomy settings.

摘要: 部署在现实世界中的自主代理需要强大地抵御对感觉输入的敌意攻击。将代理策略规模化需要预测可能最强的攻击。我们证明了现有的对强化学习代理的观察空间攻击有一个共同的弱点：虽然有效，但它们缺乏时间一致性，使得它们可以使用自动手段或人工检查来检测。对于对手来说，可探测性是不可取的，因为它可能会引发安全升级。我们引入了完全虚幻攻击，这是一种针对序列决策者的新型对抗性攻击，既有效，又在统计上可证明是不可检测的。然后，我们提出了更通用的R-攻击，它产生的观察转移与无对手环境的状态转移函数一致，并且可以端到端地学习。与现有的攻击相比，我们经验上发现，使用自动化方法检测R攻击要困难得多，一项针对人类受试者的小型研究表明，人类同样更难检测到R攻击。我们建议在混合自主环境下的对抗性攻击研究中，不可检测性应该是一个核心问题。



## **2. Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning**

基于可解释深度强化学习的无人机制导规划鲁棒对抗攻击检测 cs.LG

13 pages, 16 figures

**SubmitDate**: 2023-06-20    [abs](http://arxiv.org/abs/2206.02670v4) [paper-pdf](http://arxiv.org/pdf/2206.02670v4)

**Authors**: Thomas Hickling, Nabil Aouf, Phillippa Spencer

**Abstract**: The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents operating in public are increasing. Adopting AI-based techniques and, more specifically, Deep Learning (DL) approaches to control and guide these UAVs can be beneficial in terms of performance but can add concerns regarding the safety of those techniques and their vulnerability against adversarial attacks. Confusion in the agent's decision-making process caused by these attacks can seriously affect the safety of the UAV. This paper proposes an innovative approach based on the explainability of DL methods to build an efficient detector that will protect these DL schemes and the UAVs adopting them from attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for guidance and planning. The agent is trained with a Deep Deterministic Policy Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that utilises Artificial Potential Field (APF) to improve training times and obstacle avoidance performance. A simulated environment for UAV explainable DRL-based planning and guidance, including obstacles and adversarial attacks, is built. The adversarial attacks are generated by the Basic Iterative Method (BIM) algorithm and reduced obstacle course completion rates from 97\% to 35\%. Two adversarial attack detectors are proposed to counter this reduction. The first one is a Convolutional Neural Network Adversarial Detector (CNN-AD), which achieves accuracy in the detection of 80\%. The second detector utilises a Long Short Term Memory (LSTM) network. It achieves an accuracy of 91\% with faster computing times compared to the CNN-AD, allowing for real-time adversarial detection.

摘要: 对在公共场合工作的无人驾驶飞行器(UAV)特工进行对抗性攻击的危险正在增加。采用基于人工智能的技术，更具体地说，深度学习(DL)方法来控制和引导这些无人机在性能方面可能是有益的，但可能会增加对这些技术的安全性及其抵御对手攻击的脆弱性的担忧。这些攻击导致的代理决策过程中的混乱会严重影响无人机的安全。本文提出了一种基于DL方法的可解释性的创新方法，以构建一个有效的检测器来保护这些DL方案以及采用这些方案的无人机免受攻击。代理采用深度强化学习(DRL)方案进行指导和规划。代理使用深度确定性策略梯度(DDPG)和优先体验重播(PER)DRL方案进行训练，该方案利用人工势场(APF)来改进训练时间和避障性能。建立了无人机基于DRL的可解释规划和制导的仿真环境，包括障碍物和对抗性攻击。对抗性攻击由基本迭代法(BIM)算法生成，障碍路径完成率由97降至35。提出了两个对抗性攻击检测器来对抗这种减少。第一种是卷积神经网络敌手检测器(CNN-AD)，它可以达到80%的检测精度。第二检测器利用长短期记忆(LSTM)网络。与CNN-AD相比，它具有91%的准确率和更快的计算时间，允许实时检测对手。



## **3. FDInet: Protecting against DNN Model Extraction via Feature Distortion Index**

FDInet：通过特征失真指数防止DNN模型提取 cs.CR

13 pages, 7 figures

**SubmitDate**: 2023-06-20    [abs](http://arxiv.org/abs/2306.11338v1) [paper-pdf](http://arxiv.org/pdf/2306.11338v1)

**Authors**: Hongwei Yao, Zheng Li, Haiqin Weng, Feng Xue, Kui Ren, Zhan Qin

**Abstract**: Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINET, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary's queries, we reveal that the feature distribution of these queries deviates from that of the model's training set. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINET utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINET against six state-of-the-art extraction attacks on four benchmark datasets and four popular model architectures. Empirical results demonstrate the following findings FDINET proves to be highly effective in detecting model extraction, achieving a 100% detection accuracy on DFME and DaST. FDINET is highly efficient, using just 50 queries to raise an extraction alarm with an average confidence of 96.08% for GTSRB. FDINET exhibits the capability to identify colluding adversaries with an accuracy exceeding 91%. Additionally, it demonstrates the ability to detect two types of adaptive attacks.

摘要: 机器学习即服务(MLaaS)平台因其可访问性、成本效益、可扩展性和快速开发能力而广受欢迎。然而，最近的研究突显了MLaaS中基于云的模型对提取攻击的脆弱性。在本文中，我们介绍了FDINET，一种利用深度神经网络(DNN)模型特征分布的新型防御机制。具体地说，通过分析对手查询的特征分布，我们发现这些查询的特征分布偏离了模型训练集的特征分布。基于这一关键观察，我们提出了特征失真指数(FDI)，这是一种用来定量衡量所接收查询的特征分布偏差的度量。FDINET利用FDI来训练二进制检测器，并利用FDI的相似性从分布式抽取攻击中识别合谋对手。我们在四个基准数据集和四个流行的模型体系结构上进行了广泛的实验，以评估FDINET对六种最先进的提取攻击的攻击。实验结果表明，FDINET在检测模型提取方面具有很高的效率，在DFME和DAST上的检测准确率达到100%。FDINET的效率很高，仅使用50个查询就可以发出提取警报，GTSRB的平均置信度为96.08%。FDINET显示出识别串通对手的能力，准确率超过91%。此外，它还演示了检测两种类型的自适应攻击的能力。



## **4. Reversible Adversarial Examples with Beam Search Attack and Grayscale Invariance**

波束搜索攻击和灰度不变性的可逆对抗实例 cs.CR

Submitted to ICICS2023

**SubmitDate**: 2023-06-20    [abs](http://arxiv.org/abs/2306.11322v1) [paper-pdf](http://arxiv.org/pdf/2306.11322v1)

**Authors**: Haodong Zhang, Chi Man Pun, Xia Du

**Abstract**: Reversible adversarial examples (RAE) combine adversarial attacks and reversible data-hiding technology on a single image to prevent illegal access. Most RAE studies focus on achieving white-box attacks. In this paper, we propose a novel framework to generate reversible adversarial examples, which combines a novel beam search based black-box attack and reversible data hiding with grayscale invariance (RDH-GI). This RAE uses beam search to evaluate the adversarial gain of historical perturbations and guide adversarial perturbations. After the adversarial examples are generated, the framework RDH-GI embeds the secret data that can be recovered losslessly. Experimental results show that our method can achieve an average Peak Signal-to-Noise Ratio (PSNR) of at least 40dB compared to source images with limited query budgets. Our method can also achieve a targeted black-box reversible adversarial attack for the first time.

摘要: 可逆对抗性实例(RAE)在单个图像上结合了对抗性攻击和可逆数据隐藏技术，以防止非法访问。大多数RAE研究都集中在实现白盒攻击上。本文提出了一种新的生成可逆攻击实例的框架，该框架结合了一种新的基于波束搜索的黑盒攻击和具有灰度不变性的可逆数据隐藏(RDH-GI)。该RAE使用波束搜索来评估历史扰动的对抗性增益，并指导对抗性扰动。在生成对抗性样本后，框架RDH-GI嵌入了可以无损恢复的秘密数据。实验结果表明，与查询预算有限的源图像相比，我们的方法可以获得平均峰值信噪比(PSNR)至少40dB。我们的方法还可以首次实现有针对性的黑盒可逆对抗攻击。



## **5. Comparative Evaluation of Recent Universal Adversarial Perturbations in Image Classification**

图像分类中近期普遍存在的对抗性扰动的比较评价 cs.CV

18 pages,8 figures, 7 tables

**SubmitDate**: 2023-06-20    [abs](http://arxiv.org/abs/2306.11261v1) [paper-pdf](http://arxiv.org/pdf/2306.11261v1)

**Authors**: Juanjuan Weng, Zhiming Luo, Dazhen Lin, Shaozi Li

**Abstract**: The vulnerability of Convolutional Neural Networks (CNNs) to adversarial samples has recently garnered significant attention in the machine learning community. Furthermore, recent studies have unveiled the existence of universal adversarial perturbations (UAPs) that are image-agnostic and highly transferable across different CNN models. In this survey, our primary focus revolves around the recent advancements in UAPs specifically within the image classification task. We categorize UAPs into two distinct categories, i.e., noise-based attacks and generator-based attacks, thereby providing a comprehensive overview of representative methods within each category. By presenting the computational details of these methods, we summarize various loss functions employed for learning UAPs. Furthermore, we conduct a comprehensive evaluation of different loss functions within consistent training frameworks, including noise-based and generator-based. The evaluation covers a wide range of attack settings, including black-box and white-box attacks, targeted and untargeted attacks, as well as the examination of defense mechanisms.   Our quantitative evaluation results yield several important findings pertaining to the effectiveness of different loss functions, the selection of surrogate CNN models, the impact of training data and data size, and the training frameworks involved in crafting universal attackers. Finally, to further promote future research on universal adversarial attacks, we provide some visualizations of the perturbations and discuss the potential research directions.

摘要: 卷积神经网络(CNN)对敌意样本的脆弱性最近在机器学习界引起了极大的关注。此外，最近的研究揭示了普遍的对抗性扰动(UAP)的存在，这些UAP是与图像无关的，并且可以在不同的CNN模型中高度转移。在这项调查中，我们主要关注UAP的最新进展，特别是在图像分类任务中。我们将UAP分为两个不同的类别，即基于噪声的攻击和基于生成器的攻击，从而全面概述了每一类中具有代表性的方法。通过给出这些方法的计算细节，我们总结了用于学习UAP的各种损失函数。此外，我们在一致的训练框架内对不同的损失函数进行了综合评估，包括基于噪声的和基于生成器的。评估涵盖了广泛的攻击设置，包括黑盒和白盒攻击、定向攻击和非定向攻击，以及对防御机制的检查。我们的定量评估结果产生了几个重要的发现，涉及不同损失函数的有效性、代理CNN模型的选择、训练数据和数据大小的影响，以及制作通用攻击者所涉及的训练框架。最后，为了进一步促进通用对抗攻击的研究，我们提供了一些扰动的可视化，并讨论了潜在的研究方向。



## **6. Adversarial Robustness of Learning-based Static Malware Classifiers**

基于学习的静态恶意软件分类器的对抗健壮性 cs.CR

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2303.13372v2) [paper-pdf](http://arxiv.org/pdf/2303.13372v2)

**Authors**: Shoumik Saha, Wenxiao Wang, Yigitcan Kaya, Soheil Feizi, Tudor Dumitras

**Abstract**: Malware detection has long been a stage for an ongoing arms race between malware authors and anti-virus systems. Solutions that utilize machine learning (ML) gain traction as the scale of this arms race increases. This trend, however, makes performing attacks directly on ML an attractive prospect for adversaries. We study this arms race from both perspectives in the context of MalConv, a popular convolutional neural network-based malware classifier that operates on raw bytes of files. First, we show that MalConv is vulnerable to adversarial patch attacks: appending a byte-level patch to malware files bypasses detection 94.3% of the time. Moreover, we develop a universal adversarial patch (UAP) attack where a single patch can drop the detection rate in constant time of any malware file that contains it by 80%. These patches are effective even being relatively small with respect to the original file size -- between 2%-8%. As a countermeasure, we then perform window ablation that allows us to apply de-randomized smoothing, a modern certified defense to patch attacks in vision tasks, to raw files. The resulting `smoothed-MalConv' can detect over 80% of malware that contains the universal patch and provides certified robustness up to 66%, outlining a promising step towards robust malware detection. To our knowledge, we are the first to apply universal adversarial patch attack and certified defense using ablations on byte level in the malware field.

摘要: 恶意软件检测长期以来一直是恶意软件作者和反病毒系统之间持续军备竞赛的一个阶段。随着军备竞赛规模的扩大，利用机器学习(ML)的解决方案获得了吸引力。然而，这种趋势使得直接对ML进行攻击对对手来说是一个有吸引力的前景。我们在MalConv的背景下从两个角度研究了这场军备竞赛，MalConv是一种流行的基于卷积神经网络的恶意软件分类器，它对文件的原始字节进行操作。首先，我们发现MalConv容易受到恶意补丁的攻击：向恶意软件文件添加字节级补丁可以在94.3%的时间内绕过检测。此外，我们还开发了一种通用对手补丁(UAP)攻击，其中单个补丁可以在固定时间内使包含它的任何恶意软件文件的检测率下降80%。即使相对于原始文件大小相对较小--介于2%-8%之间，这些补丁也是有效的。作为对策，我们然后执行窗口消融，允许我们对RAW文件应用去随机化平滑，这是一种针对视觉任务中的补丁攻击的现代认证防御。由此产生的“平滑MalConv”可以检测包含通用补丁的80%以上的恶意软件，并提供高达66%的经验证的健壮性，概述了朝着健壮的恶意软件检测迈出的有希望的一步。据我们所知，我们是第一个在恶意软件领域应用通用对抗性补丁攻击和使用字节级烧蚀的认证防御的公司。



## **7. CosPGD: a unified white-box adversarial attack for pixel-wise prediction tasks**

CosPGD：一种针对像素预测任务的统一白盒对抗攻击 cs.CV

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2302.02213v2) [paper-pdf](http://arxiv.org/pdf/2302.02213v2)

**Authors**: Shashank Agnihotri, Steffen Jung, Margret Keuper

**Abstract**: While neural networks allow highly accurate predictions in many tasks, their lack of robustness towards even slight input perturbations hampers their deployment in many real-world applications. Recent research towards evaluating the robustness of neural networks such as the seminal projected gradient descent(PGD) attack and subsequent works have drawn significant attention, as they provide an effective insight into the quality of representations learned by the network. However, these methods predominantly focus on image classification tasks, while only a few approaches specifically address the analysis of pixel-wise prediction tasks such as semantic segmentation, optical flow, disparity estimation, and others, respectively. Thus, there is a lack of a unified adversarial robustness benchmarking tool(algorithm) that is applicable to all such pixel-wise prediction tasks. In this work, we close this gap and propose CosPGD, a novel white-box adversarial attack that allows optimizing dedicated attacks for any pixel-wise prediction task in a unified setting. It leverages the cosine similarity between the distributions over the predictions and ground truth (or target) to extend directly from classification tasks to regression settings. We outperform the SotA on semantic segmentation attacks in our experiments on PASCAL VOC2012 and CityScapes. Further, we set a new benchmark for adversarial attacks on optical flow, and image restoration displaying the ability to extend to any pixel-wise prediction task.

摘要: 虽然神经网络允许在许多任务中进行高精度的预测，但它们对即使是轻微的输入扰动缺乏稳健性，阻碍了它们在许多现实世界应用中的部署。最近关于评估神经网络的稳健性的研究，如种子投影梯度下降(PGD)攻击和后续工作，引起了人们的极大关注，因为它们提供了对网络学习的表示的质量的有效洞察。然而，这些方法主要集中在图像分类任务上，而只有少数方法分别具体地处理像素级预测任务的分析，例如语义分割、光流、视差估计等。因此，缺乏适用于所有这种逐像素预测任务的统一对抗性健壮性基准测试工具(算法)。在这项工作中，我们缩小了这一差距，并提出了CosPGD，一种新的白盒对抗攻击，允许在统一的环境下为任何像素级预测任务优化专用攻击。它利用预测上的分布和基本事实(或目标)之间的余弦相似性，直接从分类任务扩展到回归设置。在PASCAL VOC2012和CITYSCAPES上的实验中，我们在语义分割攻击上优于SOTA。此外，我们为光流的对抗性攻击和图像恢复设置了一个新的基准，显示了扩展到任何像素预测任务的能力。



## **8. Replay-based Recovery for Autonomous Robotic Vehicles from Sensor Deception Attacks**

基于重放的自主机器人对传感器欺骗攻击的恢复 cs.RO

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2209.04554v4) [paper-pdf](http://arxiv.org/pdf/2209.04554v4)

**Authors**: Pritam Dash, Guanpeng Li, Mehdi Karimibiuki, Karthik Pattabiraman

**Abstract**: Sensors are crucial for autonomous operation in robotic vehicles (RV). Unfortunately, RV sensors can be compromised by physical attacks such as tampering or spoofing, leading to a crash. In this paper, we present DeLorean, a modelfree recovery framework for recovering autonomous RVs from sensor deception attacks (SDA). DeLorean is designed to recover RVs even from a strong SDA in which the adversary targets multiple heterogeneous sensors simultaneously (even all the sensors). Under SDAs, DeLorean inspects the attack induced errors, identifies the targeted sensors, and prevents the erroneous sensor inputs from being used to derive actuator signals. DeLorean then replays historic state information in the RV's feedback control loop for a temporary mitigation and recovers the RV from SDA. Our evaluation on four real and two simulated RVs shows that DeLorean can recover RVs from SDAs, and ensure mission success in 90.7% of the cases on average.

摘要: 传感器对于机器人车辆(RV)的自主操作至关重要。不幸的是，房车传感器可能会受到物理攻击，如篡改或欺骗，导致崩溃。本文提出了一种无模型恢复框架DeLorean，用于从传感器欺骗攻击(SDA)中恢复自主房车。DeLorean被设计成即使在强大的SDA中也能恢复RV，在SDA中，对手同时瞄准多个不同的传感器(甚至所有传感器)。在SDAS下，DeLorean检查攻击引起的错误，识别目标传感器，并防止错误的传感器输入被用于推导执行器信号。然后，DeLorean在房车的反馈控制环路中重放历史状态信息以暂时缓解，并从SDA恢复房车。我们对4辆真实房车和2辆模拟房车的评估表明，DeLorean可以从SDA中恢复房车，并平均确保90.7%的任务成功。



## **9. Adversarial Training Should Be Cast as a Non-Zero-Sum Game**

对抗性训练应视为一种非零和博弈 cs.LG

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2306.11035v1) [paper-pdf](http://arxiv.org/pdf/2306.11035v1)

**Authors**: Alexander Robey, Fabian Latorre, George J. Pappas, Hamed Hassani, Volkan Cevher

**Abstract**: One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially-chosen perturbations of data. Despite the promise of this approach, algorithms based on this paradigm have not engendered sufficient levels of robustness, and suffer from pathological behavior like robust overfitting. To understand this shortcoming, we first show that the commonly used surrogate-based relaxation used in adversarial training algorithms voids all guarantees on the robustness of trained classifiers. The identification of this pitfall informs a novel non-zero-sum bilevel formulation of adversarial training, wherein each player optimizes a different objective function. Our formulation naturally yields a simple algorithmic framework that matches and in some cases outperforms state-of-the-art attacks, attains comparable levels of robustness to standard adversarial training algorithms, and does not suffer from robust overfitting.

摘要: 解决深层神经网络的对抗性脆弱性的一个重要方法是对抗性训练的两人零和范例，在这种范例中，预测者被训练来对抗对抗性选择的数据扰动。尽管这种方法前景看好，但基于这种范例的算法并没有产生足够的健壮性，并且存在健壮性过适应等病态行为。为了理解这一缺陷，我们首先证明了对抗性训练算法中常用的基于代理的松弛算法无效了对训练后的分类器的健壮性的所有保证。对这一陷阱的识别提供了一种新的对抗性训练的非零和双层公式，其中每个参与者优化一个不同的目标函数。我们的公式自然产生了一个简单的算法框架，该框架匹配并在某些情况下优于最先进的攻击，达到了与标准对抗性训练算法相当的健壮性水平，并且不会受到健壮过度匹配的影响。



## **10. Eigenpatches -- Adversarial Patches from Principal Components**

特征斑块--来自主成分的对抗性斑块 cs.CV

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2306.10963v1) [paper-pdf](http://arxiv.org/pdf/2306.10963v1)

**Authors**: Jens Bayer, Stefan Becker, David Münch, Michael Arens

**Abstract**: Adversarial patches are still a simple yet powerful white box attack that can be used to fool object detectors by suppressing possible detections. The patches of these so-called evasion attacks are computational expensive to produce and require full access to the attacked detector. This paper addresses the problem of computational expensiveness by analyzing 375 generated patches, calculating the principal components of these and show, that linear combinations of the resulting "eigenpatches" can be used to fool object detections successfully.

摘要: 对抗性补丁仍然是一种简单但强大的白盒攻击，可以通过抑制可能的检测来愚弄对象检测器。这些所谓的逃避攻击的补丁是计算昂贵的，并且需要完全访问被攻击的检测器。本文通过分析375个生成的面片，计算这些面片的主成分，从而解决了计算量过大的问题，并证明了所得到的“特征面片”的线性组合可以成功地愚弄目标检测。



## **11. Attack-Resilient Design for Connected and Automated Vehicles**

联网和自动化车辆的抗攻击设计 eess.SY

arXiv admin note: text overlap with arXiv:2109.01553

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2306.10925v1) [paper-pdf](http://arxiv.org/pdf/2306.10925v1)

**Authors**: Tianci Yang, Carlos Murguia, Dragan Nesic, Chau Yuen

**Abstract**: By sharing local sensor information via Vehicle-to-Vehicle (V2V) wireless communication networks, Cooperative Adaptive Cruise Control (CACC) is a technology that enables Connected and Automated Vehicles (CAVs) to drive autonomously on the highway in closely-coupled platoons. The use of CACC technologies increases safety and the traffic throughput, and decreases fuel consumption and CO2 emissions. However, CAVs heavily rely on embedded software, hardware, and communication networks that make them vulnerable to a range of cyberattacks. Cyberattacks to a particular CAV compromise the entire platoon as CACC schemes propagate corrupted data to neighboring vehicles potentially leading to traffic delays and collisions. Physics-based monitors can be used to detect the presence of False Data Injection (FDI) attacks to CAV sensors; however, unavoidable system disturbances and modelling uncertainty often translates to conservative detection results. Given enough system knowledge, adversaries are still able to launch a range of attacks that can surpass the detection scheme by hiding within the system disturbances and uncertainty -- we refer to this class of attacks as \textit{stealthy FDI attacks}. Stealthy attacks are hard to deal with as they affect the platoon dynamics without being noticed. In this manuscript, we propose a co-design methodology (built around a series convex programs) to synthesize distributed attack monitors and $H_{\infty}$ CACC controllers that minimize the joint effect of stealthy FDI attacks and system disturbances on the platoon dynamics while guaranteeing a prescribed platooning performance (in terms of tracking and string stability). Computer simulations are provided to illustrate the performance of out tools.

摘要: 通过车对车(V2V)无线通信网络共享本地传感器信息，协作自适应巡航控制(CACC)是一种使互联和自动车辆(CAV)能够在紧密耦合的排中自动在高速公路上行驶的技术。CACC技术的使用提高了安全性和交通吞吐量，降低了油耗和二氧化碳排放。然而，骑士队严重依赖嵌入式软件、硬件和通信网络，这使得他们容易受到一系列网络攻击。针对特定CAV的网络攻击危及整个排，因为CACC方案将损坏的数据传播到邻近的车辆，可能导致交通延误和碰撞。基于物理的监测器可以用来检测对CAV传感器的虚假数据注入(FDI)攻击的存在；然而，不可避免的系统干扰和建模不确定性通常会转化为保守的检测结果。在有足够的系统知识的情况下，攻击者仍然能够通过隐藏在系统中的干扰和不确定性来发起一系列可以超越检测方案的攻击--我们将这类攻击称为\Texttit{隐蔽的FDI攻击}。隐形攻击很难处理，因为它们在没有被注意到的情况下影响了排的动态。在这篇手稿中，我们提出了一种协同设计方法(建立在一系列凸规划的基础上)来综合分布式攻击监视器和$HINFTY$CACC控制器，以最小化隐身FDI攻击和系统干扰对排动力学的联合影响，同时保证指定的排性能(在跟踪和字符串稳定性方面)。为了说明OUT工具的性能，提供了计算机模拟。



## **12. On the Robustness of Dataset Inference**

关于数据集推理的稳健性 cs.LG

19 pages; Accepted to Transactions on Machine Learning Research  06/2023

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2210.13631v3) [paper-pdf](http://arxiv.org/pdf/2210.13631v3)

**Authors**: Sebastian Szyller, Rui Zhang, Jian Liu, N. Asokan

**Abstract**: Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs.   Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification) or computational cost. A fingerprinting technique, Dataset Inference (DI), has been shown to offer better robustness and efficiency than prior methods.   The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify an independent model trained with non-overlapping data from the same distribution as stolen. We further prove that DI also triggers FPs in realistic, non-linear suspect models. We then confirm empirically that DI in the black-box setting leads to FPs, with high confidence.   Second, we show that DI also suffers from false negatives (FNs) -- an adversary can fool DI (at the cost of incurring some accuracy loss) by regularising a stolen model's decision boundaries using adversarial training, thereby leading to an FN. To this end, we demonstrate that black-box DI fails to identify a model adversarially trained from a stolen dataset -- the setting where DI is the hardest to evade.   Finally, we discuss the implications of our findings, the viability of fingerprinting-based ownership verification in general, and suggest directions for future work.

摘要: 机器学习(ML)模型的训练成本很高，因为它们可能需要大量的数据、计算资源和技术专长。因此，它们构成了宝贵的知识产权，需要保护，不受想要窃取它们的对手的攻击。所有权验证技术允许模型盗窃攻击的受害者证明可疑模型实际上是从他们的模型中被盗的。虽然已经提出了一些基于水印或指纹的所有权验证技术，但它们大多在安全保证(装备良好的攻击者可以逃避验证)或计算代价方面存在不足。一种指纹技术，数据集推理(DI)，已经被证明比以前的方法提供了更好的稳健性和效率。DI的作者为线性(可疑)模型提供了正确性证明。然而，在相同设置的子空间中，我们证明DI存在高误报(FP)--它可能错误地识别使用来自相同分布的非重叠数据训练的独立模型。我们进一步证明，在现实的、非线性的可疑模型中，依赖注入也会触发FP。然后，我们以很高的置信度从经验上证实了黑盒设置中的DI会导致FP。其次，我们证明了DI也存在假阴性(FN)--对手可以通过使用对抗性训练来规则被盗模型的决策边界来愚弄DI(以招致一些准确性损失为代价)，从而导致FN。为此，我们演示了黑盒DI无法识别从窃取的数据集中恶意训练的模型--DI最难逃避的设置。最后，我们讨论了我们的发现的含义，基于指纹的所有权验证总体上的可行性，并对未来的工作提出了方向。



## **13. Hidden Backdoor Attack against Deep Learning-Based Wireless Signal Modulation Classifiers**

针对基于深度学习的无线信号调制分类器的隐藏后门攻击 eess.SP

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2306.10753v1) [paper-pdf](http://arxiv.org/pdf/2306.10753v1)

**Authors**: Yunsong Huang, Weicheng Liu, Hui-Ming Wang

**Abstract**: Recently, DL has been exploited in wireless communications such as modulation classification. However, due to the openness of wireless channel and unexplainability of DL, it is also vulnerable to adversarial attacks. In this correspondence, we investigate a so called hidden backdoor attack to modulation classification, where the adversary puts elaborately designed poisoned samples on the basis of IQ sequences into training dataset. These poisoned samples are hidden because it could not be found by traditional classification methods. And poisoned samples are same to samples with triggers which are patched samples in feature space. We show that the hidden backdoor attack can reduce the accuracy of modulation classification significantly with patched samples. At last, we propose activation cluster to detect abnormal samples in training dataset.

摘要: 最近，下行链路已被用于无线通信中，例如调制分类。然而，由于无线信道的开放性和下行链路的不可解释性，它也容易受到对手的攻击。在这种通信中，我们调查了所谓的隐藏后门攻击调制分类，其中对手将精心设计的基于IQ序列的有毒样本放入训练数据集中。这些有毒样本被隐藏起来，因为它不能被传统的分类方法发现。有毒样本与带有触发器的样本相同，后者是特征空间中的修补样本。结果表明，隐藏后门攻击会显著降低修补样本的调制分类准确率。最后，我们提出了激活聚类来检测训练数据集中的异常样本。



## **14. Adversarial Camouflage for Node Injection Attack on Graphs**

图上节点注入攻击的对抗性伪装 cs.LG

Submitted to Information Sciences. Code:  https://github.com/TaoShuchang/CANA

**SubmitDate**: 2023-06-19    [abs](http://arxiv.org/abs/2208.01819v3) [paper-pdf](http://arxiv.org/pdf/2208.01819v3)

**Authors**: Shuchang Tao, Qi Cao, Huawei Shen, Yunfan Wu, Liang Hou, Fei Sun, Xueqi Cheng

**Abstract**: Node injection attacks on Graph Neural Networks (GNNs) have received emerging attention due to their potential to significantly degrade GNN performance with high attack success rates. However, our study indicates these attacks often fail in practical scenarios, since defense/detection methods can easily identify and remove the injected nodes. To address this, we devote to camouflage node injection attack, making injected nodes appear normal and imperceptible to defense/detection methods. Unfortunately, the non-Euclidean nature of graph data and lack of intuitive prior present great challenges to the formalization, implementation, and evaluation of camouflage. In this paper, we first propose and define camouflage as distribution similarity between ego networks of injected nodes and normal nodes. Then for implementation, we propose an adversarial CAmouflage framework for Node injection Attack, namely CANA, to improve attack performance under defense/detection methods in practical scenarios. A novel camouflage metric is further designed under the guide of distribution similarity. Extensive experiments demonstrate that CANA can significantly improve the attack performance under defense/detection methods with higher camouflage or imperceptibility. This work urges us to raise awareness of the security vulnerabilities of GNNs in practical applications. The implementation of CANA is available at https://github.com/TaoShuchang/CANA.

摘要: 针对图神经网络(GNN)的节点注入攻击因其攻击成功率高而显著降低GNN性能而受到越来越多的关注。然而，我们的研究表明，这些攻击在实际场景中经常失败，因为防御/检测方法可以很容易地识别和删除注入的节点。为了解决这个问题，我们致力于伪装节点注入攻击，使注入的节点看起来正常，对防御/检测方法是不可察觉的。不幸的是，图形数据的非欧几里德性质和缺乏直观的先验知识给伪装的形式化、实现和评估带来了巨大的挑战。在本文中，我们首先提出并定义伪装为注入节点的EGO网络与正常节点之间的分布相似性。在实现上，我们提出了一种节点注入攻击的对抗性伪装框架CANA，以提高实际场景中防御/检测方法下的攻击性能。在分布相似性的指导下，进一步设计了一种新的伪装度量。大量实验表明，CANA能够显著提高伪装或隐蔽性较高的防御/检测方法下的攻击性能。这项工作促使我们提高对GNN在实际应用中的安全漏洞的认识。CANA的实施可在https://github.com/TaoShuchang/CANA.上获得



## **15. Intriguing Properties of Text-guided Diffusion Models**

文本引导扩散模型的有趣性质 cs.CV

Project page: https://sage-diffusion.github.io/

**SubmitDate**: 2023-06-18    [abs](http://arxiv.org/abs/2306.00974v3) [paper-pdf](http://arxiv.org/pdf/2306.00974v3)

**Authors**: Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, Alan Yuille

**Abstract**: Text-guided diffusion models (TDMs) are widely applied but can fail unexpectedly. Common failures include: (i) natural-looking text prompts generating images with the wrong content, or (ii) different random samples of the latent variables that generate vastly different, and even unrelated, outputs despite being conditioned on the same text prompt. In this work, we aim to study and understand the failure modes of TDMs in more detail. To achieve this, we propose SAGE, an adversarial attack on TDMs that uses image classifiers as surrogate loss functions, to search over the discrete prompt space and the high-dimensional latent space of TDMs to automatically discover unexpected behaviors and failure cases in the image generation. We make several technical contributions to ensure that SAGE finds failure cases of the diffusion model, rather than the classifier, and verify this in a human study. Our study reveals four intriguing properties of TDMs that have not been systematically studied before: (1) We find a variety of natural text prompts producing images that fail to capture the semantics of input texts. We categorize these failures into ten distinct types based on the underlying causes. (2) We find samples in the latent space (which are not outliers) that lead to distorted images independent of the text prompt, suggesting that parts of the latent space are not well-structured. (3) We also find latent samples that lead to natural-looking images which are unrelated to the text prompt, implying a potential misalignment between the latent and prompt spaces. (4) By appending a single adversarial token embedding to an input prompt we can generate a variety of specified target objects, while only minimally affecting the CLIP score. This demonstrates the fragility of language representations and raises potential safety concerns.

摘要: 文本引导扩散模型(TDM)被广泛应用，但可能会意外失败。常见的故障包括：(I)看起来自然的文本提示生成具有错误内容的图像，或(Ii)潜在变量的不同随机样本，尽管以相同的文本提示为条件，但生成的输出却截然不同，甚至是无关的。在这项工作中，我们旨在更详细地研究和理解TDM的故障模式。为此，我们提出了一种针对TDMS的对抗性攻击方法SAGE，它使用图像分类器作为代理损失函数，在TDMS的离散提示空间和高维潜在空间中进行搜索，自动发现图像生成中的意外行为和失败案例。我们做出了几项技术贡献，以确保SAGE找到扩散模型的故障案例，而不是分类器，并在人体研究中验证这一点。我们的研究揭示了TDM的四个以前没有被系统研究的有趣的性质：(1)我们发现各种自然的文本提示产生的图像无法捕捉到输入文本的语义。根据根本原因，我们将这些故障分为十种不同的类型。(2)我们在潜在空间(不是离群点)中发现了与文本提示无关的导致失真图像的样本，这表明潜在空间的部分结构不是良好的。(3)我们还发现潜在样本导致了与文本提示无关的看起来自然的图像，这意味着潜在空间和提示空间之间存在潜在的错位。(4)通过在输入提示中添加单个对抗性令牌，我们可以生成各种指定的目标对象，而对片段得分的影响最小。这表明了语言表达的脆弱性，并引发了潜在的安全问题。



## **16. Towards A Proactive ML Approach for Detecting Backdoor Poison Samples**

一种主动检测后门毒物样本的最大似然方法 cs.LG

USENIX Security 2023

**SubmitDate**: 2023-06-18    [abs](http://arxiv.org/abs/2205.13616v3) [paper-pdf](http://arxiv.org/pdf/2205.13616v3)

**Authors**: Xiangyu Qi, Tinghao Xie, Jiachen T. Wang, Tong Wu, Saeed Mahloujifar, Prateek Mittal

**Abstract**: Adversaries can embed backdoors in deep learning models by introducing backdoor poison samples into training datasets. In this work, we investigate how to detect such poison samples to mitigate the threat of backdoor attacks. First, we uncover a post-hoc workflow underlying most prior work, where defenders passively allow the attack to proceed and then leverage the characteristics of the post-attacked model to uncover poison samples. We reveal that this workflow does not fully exploit defenders' capabilities, and defense pipelines built on it are prone to failure or performance degradation in many scenarios. Second, we suggest a paradigm shift by promoting a proactive mindset in which defenders engage proactively with the entire model training and poison detection pipeline, directly enforcing and magnifying distinctive characteristics of the post-attacked model to facilitate poison detection. Based on this, we formulate a unified framework and provide practical insights on designing detection pipelines that are more robust and generalizable. Third, we introduce the technique of Confusion Training (CT) as a concrete instantiation of our framework. CT applies an additional poisoning attack to the already poisoned dataset, actively decoupling benign correlation while exposing backdoor patterns to detection. Empirical evaluations on 4 datasets and 14 types of attacks validate the superiority of CT over 14 baseline defenses.

摘要: 攻击者可以通过将后门毒药样本引入训练数据集中，在深度学习模型中嵌入后门。在这项工作中，我们研究如何检测此类毒物样本以减轻后门攻击的威胁。首先，我们揭示了大多数先前工作背后的事后工作流，在这种工作中，防御者被动地允许攻击继续进行，然后利用攻击后模型的特征来发现毒物样本。我们发现，这种工作流没有充分利用防御者的能力，在其上构建的防御管道在许多场景下容易出现故障或性能下降。其次，我们建议通过促进一种积极主动的心态来实现范式转变，在这种心态中，防御者主动参与整个模型培训和毒物检测管道，直接实施和放大攻击后模型的独特特征，以促进毒物检测。在此基础上，我们制定了一个统一的框架，并为设计更健壮和更具通用性的检测管道提供了实用的见解。第三，我们引入混淆训练(CT)技术作为我们的框架的具体实例。CT对已经中毒的数据集进行额外的中毒攻击，主动去耦合良性关联，同时暴露后门模式以供检测。在4个数据集和14种攻击类型上的经验评估验证了CT相对于14种基线防御的优越性。



## **17. Adversaries with Limited Information in the Friedkin--Johnsen Model**

Friedkin-Johnsen模型中信息有限的对手 cs.SI

To appear at KDD'23

**SubmitDate**: 2023-06-17    [abs](http://arxiv.org/abs/2306.10313v1) [paper-pdf](http://arxiv.org/pdf/2306.10313v1)

**Authors**: Sijing Tu, Stefan Neumann, Aristides Gionis

**Abstract**: In recent years, online social networks have been the target of adversaries who seek to introduce discord into societies, to undermine democracies and to destabilize communities. Often the goal is not to favor a certain side of a conflict but to increase disagreement and polarization. To get a mathematical understanding of such attacks, researchers use opinion-formation models from sociology, such as the Friedkin--Johnsen model, and formally study how much discord the adversary can produce when altering the opinions for only a small set of users. In this line of work, it is commonly assumed that the adversary has full knowledge about the network topology and the opinions of all users. However, the latter assumption is often unrealistic in practice, where user opinions are not available or simply difficult to estimate accurately.   To address this concern, we raise the following question: Can an attacker sow discord in a social network, even when only the network topology is known? We answer this question affirmatively. We present approximation algorithms for detecting a small set of users who are highly influential for the disagreement and polarization in the network. We show that when the adversary radicalizes these users and if the initial disagreement/polarization in the network is not very high, then our method gives a constant-factor approximation on the setting when the user opinions are known. To find the set of influential users, we provide a novel approximation algorithm for a variant of MaxCut in graphs with positive and negative edge weights. We experimentally evaluate our methods, which have access only to the network topology, and we find that they have similar performance as methods that have access to the network topology and all user opinions. We further present an NP-hardness proof, which was an open question by Chen and Racz [IEEE Trans. Netw. Sci. Eng., 2021].

摘要: 近年来，在线社交网络一直是试图在社会中制造不和谐、破坏民主和破坏社区稳定的敌人的目标。通常，目标不是偏袒冲突的某一方，而是增加分歧和两极分化。为了从数学上理解这类攻击，研究人员使用了社会学中的观点形成模型，如弗里德金-约翰森模型，并正式研究了当对手只为一小部分用户改变观点时，可以产生多大的不和谐。在这方面的工作中，通常假设对手完全了解网络拓扑和所有用户的意见。然而，后一种假设在实践中往往是不现实的，因为用户的意见是不可用的，或者只是很难准确估计。为了解决这一问题，我们提出了以下问题：即使只知道网络拓扑，攻击者也能在社交网络中挑拨离间吗？我们肯定地回答了这个问题。我们提出了一种近似算法，用于检测对网络中的不一致和极化有很大影响的一小部分用户。我们证明，当对手激化这些用户时，如果网络中的初始分歧/极化不是很高，那么当用户意见已知时，我们的方法在设置上给出一个恒定因子近似。为了寻找有影响力的用户集，我们给出了一种新的算法，用于计算边权为正负的图中MaxCut的一种变种。我们对我们的方法进行了实验评估，这些方法只能访问网络拓扑，我们发现它们具有与可以访问网络拓扑和所有用户意见的方法类似的性能。我们进一步给出了NP-硬度证明，这是Chen和racz[IEEE译文]提出的一个未决问题。奈特。SCI。Eng.，2021]。



## **18. Edge Learning for 6G-enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses**

支持6G的物联网的边缘学习：漏洞、数据集和防御的全面调查 cs.CR

**SubmitDate**: 2023-06-17    [abs](http://arxiv.org/abs/2306.10309v1) [paper-pdf](http://arxiv.org/pdf/2306.10309v1)

**Authors**: Mohamed Amine Ferrag, Othmane Friha, Burak Kantarci, Norbert Tihanyi, Lucas Cordeiro, Merouane Debbah, Djallel Hamouda, Muna Al-Hawawreh, Kim-Kwang Raymond Choo

**Abstract**: The ongoing deployment of the fifth generation (5G) wireless networks constantly reveals limitations concerning its original concept as a key driver of Internet of Everything (IoE) applications. These 5G challenges are behind worldwide efforts to enable future networks, such as sixth generation (6G) networks, to efficiently support sophisticated applications ranging from autonomous driving capabilities to the Metaverse. Edge learning is a new and powerful approach to training models across distributed clients while protecting the privacy of their data. This approach is expected to be embedded within future network infrastructures, including 6G, to solve challenging problems such as resource management and behavior prediction. This survey article provides a holistic review of the most recent research focused on edge learning vulnerabilities and defenses for 6G-enabled IoT. We summarize the existing surveys on machine learning for 6G IoT security and machine learning-associated threats in three different learning modes: centralized, federated, and distributed. Then, we provide an overview of enabling emerging technologies for 6G IoT intelligence. Moreover, we provide a holistic survey of existing research on attacks against machine learning and classify threat models into eight categories, including backdoor attacks, adversarial examples, combined attacks, poisoning attacks, Sybil attacks, byzantine attacks, inference attacks, and dropping attacks. In addition, we provide a comprehensive and detailed taxonomy and a side-by-side comparison of the state-of-the-art defense methods against edge learning vulnerabilities. Finally, as new attacks and defense technologies are realized, new research and future overall prospects for 6G-enabled IoT are discussed.

摘要: 作为万物互联(IoE)应用的关键驱动力，第五代(5G)无线网络的持续部署不断暴露出其原始概念的局限性。这些5G挑战是全球努力的背后，目的是使未来的网络(如第六代(6G)网络)能够有效支持从自动驾驶功能到Metverse的各种复杂应用。边缘学习是一种新的、功能强大的方法，用于跨分布式客户端训练模型，同时保护其数据的隐私。这种方法有望嵌入包括6G在内的未来网络基础设施，以解决资源管理和行为预测等具有挑战性的问题。本调查文章全面回顾了有关支持6G的物联网的边缘学习漏洞和防御的最新研究。我们总结了机器学习在集中式、联合式和分布式三种不同的学习模式下对6G物联网安全和机器学习相关威胁的现有研究。然后，我们将概述支持6G物联网智能的新兴技术。此外，我们对机器学习攻击的现有研究进行了全面的综述，并将威胁模型分为八类，包括后门攻击、对抗性例子、组合攻击、中毒攻击、Sybil攻击、拜占庭攻击、推理攻击和丢弃攻击。此外，我们还提供了全面而详细的分类以及针对边缘学习漏洞的最先进防御方法的并列比较。最后，随着新的攻击和防御技术的实现，讨论了6G物联网的新研究和未来的整体前景。



## **19. You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks**

您不需要健壮的机器学习来管理对手攻击风险 cs.LG

**SubmitDate**: 2023-06-16    [abs](http://arxiv.org/abs/2306.09951v1) [paper-pdf](http://arxiv.org/pdf/2306.09951v1)

**Authors**: Edward Raff, Michel Benaroch, Andrew L. Farris

**Abstract**: The robustness of modern machine learning (ML) models has become an increasing concern within the community. The ability to subvert a model into making errant predictions using seemingly inconsequential changes to input is startling, as is our lack of success in building models robust to this concern. Existing research shows progress, but current mitigations come with a high cost and simultaneously reduce the model's accuracy. However, such trade-offs may not be necessary when other design choices could subvert the risk. In this survey we review the current literature on attacks and their real-world occurrences, or limited evidence thereof, to critically evaluate the real-world risks of adversarial machine learning (AML) for the average entity. This is done with an eye toward how one would then mitigate these attacks in practice, the risks for production deployment, and how those risks could be managed. In doing so we elucidate that many AML threats do not warrant the cost and trade-offs of robustness due to a low likelihood of attack or availability of superior non-ML mitigations. Our analysis also recommends cases where an actor should be concerned about AML to the degree where robust ML models are necessary for a complete deployment.

摘要: 现代机器学习(ML)模型的稳健性已经成为社区内越来越关注的问题。通过对输入进行看似无关紧要的改变来颠覆模型做出错误预测的能力令人震惊，就像我们在构建针对这种担忧的强大模型方面缺乏成功一样。现有研究表明取得了进展，但目前的缓解措施代价高昂，同时降低了模型的准确性。然而，当其他设计选择可能会颠覆风险时，这样的权衡可能没有必要。在这项调查中，我们回顾了当前关于攻击及其真实世界发生的文献，或其有限的证据，以批判性地评估对抗性机器学习(AML)对于普通实体的真实世界风险。这样做的目的是考虑到如何在实践中减轻这些攻击、生产部署的风险以及如何管理这些风险。在此过程中，我们阐明了许多AML威胁并不保证健壮性的成本和权衡，因为攻击的可能性很低，或者可以获得更好的非ML缓解措施。我们的分析还推荐了参与者应该关注AML的情况，在这种程度上，健壮的ML模型是完整部署所必需的。



## **20. Adversarial Cheap Talk**

对抗性的低级谈资 cs.LG

To be published at ICML 2023. Project video and code are available at  https://sites.google.com/view/adversarial-cheap-talk

**SubmitDate**: 2023-06-16    [abs](http://arxiv.org/abs/2211.11030v3) [paper-pdf](http://arxiv.org/pdf/2211.11030v3)

**Authors**: Chris Lu, Timon Willi, Alistair Letcher, Jakob Foerster

**Abstract**: Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorithms. More specifically, we show that an ACT Adversary is capable of harming performance by interfering with the learner's function approximation, or instead helping the Victim's performance by outputting useful features. Finally, we show that an ACT Adversary can manipulate messages during train-time to directly and arbitrarily control the Victim at test-time. Project video and code are available at https://sites.google.com/view/adversarial-cheap-talk

摘要: 强化学习(RL)中的对抗性攻击通常假定具有访问受害者参数、环境或数据的高度特权。相反，本文提出了一种新的对抗性环境，称为廉价谈话MDP，在该环境中，对手只需将确定性消息附加到受害者的观察中，从而产生最小的影响范围。敌手不能掩盖基本事实、影响潜在环境动态或奖励信号、引入非平稳性、增加随机性、看到受害者的行为或获取他们的参数。此外，我们还提出了一个简单的元学习算法，称为对抗性廉价谈话(ACT)，以在这种情况下训练对手。我们证明，尽管在高度受限的环境下，接受过ACT训练的对手仍然会显著影响受害者的训练和测试表现。影响训练时间性能揭示了新的攻击向量，并提供了对现有RL算法的成功和失败模式的洞察。更具体地说，我们证明了ACT对手能够通过干扰学习者的函数逼近来损害性能，或者相反地通过输出有用的特征来帮助受害者的性能。最后，我们证明了ACT攻击者可以在训练时间内操纵消息，从而在测试时间直接任意控制受害者。项目视频和代码可在https://sites.google.com/view/adversarial-cheap-talk上获得



## **21. Query-Free Evasion Attacks Against Machine Learning-Based Malware Detectors with Generative Adversarial Networks**

基于产生式对抗网络的机器学习恶意软件检测器的无查询逃避攻击 cs.CR

**SubmitDate**: 2023-06-16    [abs](http://arxiv.org/abs/2306.09925v1) [paper-pdf](http://arxiv.org/pdf/2306.09925v1)

**Authors**: Daniel Gibert, Jordi Planes, Quan Le, Giulio Zizzo

**Abstract**: Malware detectors based on machine learning (ML) have been shown to be susceptible to adversarial malware examples. However, current methods to generate adversarial malware examples still have their limits. They either rely on detailed model information (gradient-based attacks), or on detailed outputs of the model - such as class probabilities (score-based attacks), neither of which are available in real-world scenarios. Alternatively, adversarial examples might be crafted using only the label assigned by the detector (label-based attack) to train a substitute network or an agent using reinforcement learning. Nonetheless, label-based attacks might require querying a black-box system from a small number to thousands of times, depending on the approach, which might not be feasible against malware detectors. This work presents a novel query-free approach to craft adversarial malware examples to evade ML-based malware detectors. To this end, we have devised a GAN-based framework to generate adversarial malware examples that look similar to benign executables in the feature space. To demonstrate the suitability of our approach we have applied the GAN-based attack to three common types of features usually employed by static ML-based malware detectors: (1) Byte histogram features, (2) API-based features, and (3) String-based features. Results show that our model-agnostic approach performs on par with MalGAN, while generating more realistic adversarial malware examples without requiring any query to the malware detectors. Furthermore, we have tested the generated adversarial examples against state-of-the-art multimodal and deep learning malware detectors, showing a decrease in detection performance, as well as a decrease in the average number of detections by the anti-malware engines in VirusTotal.

摘要: 基于机器学习(ML)的恶意软件检测器已被证明容易受到敌意恶意软件示例的影响。然而，当前生成敌意恶意软件示例的方法仍然有其局限性。它们要么依赖于详细的模型信息(基于梯度的攻击)，要么依赖于模型的详细输出--例如类别概率(基于分数的攻击)，这两者在现实世界的场景中都不可用。或者，可以仅使用检测器分配的标签(基于标签的攻击)来制作对抗性示例，以使用强化学习来训练替代网络或代理。尽管如此，基于标签的攻击可能需要查询黑匣子系统从少量到数千次，具体取决于方法，这在恶意软件检测器面前可能是不可行的。这项工作提出了一种新的无查询方法来构建恶意软件示例，以躲避基于ML的恶意软件检测器。为此，我们设计了一个基于GAN的框架来生成在特征空间中看起来类似于良性可执行文件的敌意恶意软件示例。为了证明我们的方法的适用性，我们将基于GAN的攻击应用于基于静态ML的恶意软件检测器通常使用的三种常见特征：(1)字节直方图特征，(2)基于API的特征，和(3)基于字符串的特征。结果表明，我们的模型无关方法的性能与MalGan相当，同时生成更真实的敌意恶意软件示例，而不需要向恶意软件检测器进行任何查询。此外，我们已经针对最先进的多模式和深度学习恶意软件检测器测试了生成的恶意示例，显示出检测性能的下降，以及VirusTotal中反恶意软件引擎的平均检测次数的下降。



## **22. Wasserstein distributional robustness of neural networks**

神经网络的Wasserstein分布稳健性 cs.LG

23 pages, 6 figures, 8 tables

**SubmitDate**: 2023-06-16    [abs](http://arxiv.org/abs/2306.09844v1) [paper-pdf](http://arxiv.org/pdf/2306.09844v1)

**Authors**: Xingjian Bai, Guangyi He, Yifan Jiang, Jan Obloj

**Abstract**: Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multi-step version. Our attack algorithms include Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) as special cases. Furthermore, we provide a new asymptotic estimate of the adversarial accuracy against distributional threat models. The bound is fast to compute and first-order accurate, offering new insights even for the pointwise AA. It also naturally yields out-of-sample performance guarantees. We conduct numerical experiments on the CIFAR-10 dataset using DNNs on RobustBench to illustrate our theoretical results. Our code is available at https://github.com/JanObloj/W-DRO-Adversarial-Methods.

摘要: 众所周知，深度神经网络容易受到对手攻击(AA)。对于图像识别任务，这意味着原始图像的微小扰动可能会导致图像被错误分类。这种攻击的设计以及对抗它们的对抗性训练方法都是深入研究的主题。我们使用沃瑟斯坦分布稳健优化(DRO)技术重塑了这个问题，并利用DRO敏感性分析的最新见解获得了新的贡献。我们考虑一组分布式威胁模型。与传统的逐点攻击不同，分布式威胁模型允许攻击者以非一致的方式干扰输入。我们将这些更普遍的攻击与超出样本的性能和奈特的不确定性联系在一起。为了评估神经网络的分布稳健性，我们提出了一种一阶AA算法及其多步算法。我们的攻击算法包括快速梯度符号法(FGSM)和投影梯度下降法(PGD)作为特例。此外，我们还对分布式威胁模型的对抗精度给出了一个新的渐近估计。边界计算速度快，一阶精度高，即使对于逐点的AA也提供了新的见解。它还自然而然地产生了超出样本的性能保证。为了验证我们的理论结果，我们在CIFAR-10数据集上进行了数值实验。我们的代码可以在https://github.com/JanObloj/W-DRO-Adversarial-Methods.上找到



## **23. TransFool: An Adversarial Attack against Neural Machine Translation Models**

TransFool：对神经机器翻译模型的敌意攻击 cs.CL

**SubmitDate**: 2023-06-16    [abs](http://arxiv.org/abs/2302.00944v2) [paper-pdf](http://arxiv.org/pdf/2302.00944v2)

**Authors**: Sahar Sadrizadeh, Ljiljana Dolamic, Pascal Frossard

**Abstract**: Deep neural networks have been shown to be vulnerable to small perturbations of their inputs, known as adversarial attacks. In this paper, we investigate the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and propose a new attack algorithm called TransFool. To fool NMT models, TransFool builds on a multi-term optimization problem and a gradient projection step. By integrating the embedding representation of a language model, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples. Experimental results demonstrate that, for different translation tasks and NMT architectures, our white-box attack can severely degrade the translation quality while the semantic similarity between the original and the adversarial sentences stays high. Moreover, we show that TransFool is transferable to unknown target models. Finally, based on automatic and human evaluations, TransFool leads to improvement in terms of success rate, semantic similarity, and fluency compared to the existing attacks both in white-box and black-box settings. Thus, TransFool permits us to better characterize the vulnerability of NMT models and outlines the necessity to design strong defense mechanisms and more robust NMT systems for real-life applications.

摘要: 深度神经网络已被证明容易受到其输入的微小扰动，即所谓的对抗性攻击。本文研究了神经机器翻译(NMT)模型对敌意攻击的脆弱性，提出了一种新的攻击算法TransFool。为了愚弄NMT模型，TransFool建立在多项优化问题和梯度投影步骤的基础上。通过集成语言模型的嵌入表示，我们在源语言中生成流畅的对抗性实例，这些实例与干净的样本保持较高的语义相似度。实验结果表明，对于不同的翻译任务和自然机器翻译体系结构，我们的白盒攻击可以在原句和对抗性句子之间保持较高语义相似度的情况下，严重降低翻译质量。此外，我们还证明了TransFool可以转移到未知目标模型。最后，基于自动和人工评估，TransFool在成功率、语义相似度和流畅度方面都比现有的白盒和黑盒攻击都有所提高。因此，TransFool使我们能够更好地刻画NMT模型的脆弱性，并概述为现实应用设计强大的防御机制和更健壮的NMT系统的必要性。



## **24. Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems**

事实破坏者：针对事实核查系统的证据操纵攻击的分类 cs.CR

**SubmitDate**: 2023-06-16    [abs](http://arxiv.org/abs/2209.03755v4) [paper-pdf](http://arxiv.org/pdf/2209.03755v4)

**Authors**: Sahar Abdelnabi, Mario Fritz

**Abstract**: Mis- and disinformation are a substantial global threat to our security and safety. To cope with the scale of online misinformation, researchers have been working on automating fact-checking by retrieving and verifying against relevant evidence. However, despite many advances, a comprehensive evaluation of the possible attack vectors against such systems is still lacking. Particularly, the automated fact-verification process might be vulnerable to the exact disinformation campaigns it is trying to combat. In this work, we assume an adversary that automatically tampers with the online evidence in order to disrupt the fact-checking model via camouflaging the relevant evidence or planting a misleading one. We first propose an exploratory taxonomy that spans these two targets and the different threat model dimensions. Guided by this, we design and propose several potential attack methods. We show that it is possible to subtly modify claim-salient snippets in the evidence and generate diverse and claim-aligned evidence. Thus, we highly degrade the fact-checking performance under many different permutations of the taxonomy's dimensions. The attacks are also robust against post-hoc modifications of the claim. Our analysis further hints at potential limitations in models' inference when faced with contradicting evidence. We emphasize that these attacks can have harmful implications on the inspectable and human-in-the-loop usage scenarios of such models, and we conclude by discussing challenges and directions for future defenses.

摘要: 错误和虚假信息是对我们的安全和安全的重大全球威胁。为了应对网上虚假信息的规模，研究人员一直在致力于通过检索和验证相关证据来实现事实核查的自动化。然而，尽管取得了许多进展，但仍然缺乏对针对此类系统的可能攻击媒介的全面评估。特别是，自动化的事实核查过程可能容易受到它试图打击的虚假信息运动的影响。在这项工作中，我们假设一个对手自动篡改在线证据，以便通过伪装相关证据或植入误导性证据来扰乱事实核查模型。我们首先提出了一种探索性分类，该分类跨越这两个目标和不同的威胁模型维度。在此指导下，我们设计并提出了几种潜在的攻击方法。我们表明，可以巧妙地修改证据中突出声明的片段，并生成多样化的与声明一致的证据。因此，在分类维度的许多不同排列下，我们极大地降低了事实检查的性能。这些攻击也对索赔的事后修改具有很强的抵御能力。我们的分析进一步暗示，在面对相互矛盾的证据时，模型的推理可能存在局限性。我们强调，这些攻击可能会对此类模型的可检查和人在环中使用场景产生有害影响，我们最后讨论了未来防御的挑战和方向。



## **25. Adversarial Image Color Transformations in Explicit Color Filter Space**

显式滤色空间中的对抗性图像颜色变换 cs.CV

Published at IEEE Transactions on Information Forensics and Security  2023. Code is available at  https://github.com/ZhengyuZhao/ACE/tree/master/Journal_version

**SubmitDate**: 2023-06-16    [abs](http://arxiv.org/abs/2011.06690v3) [paper-pdf](http://arxiv.org/pdf/2011.06690v3)

**Authors**: Zhengyu Zhao, Zhuoran Liu, Martha Larson

**Abstract**: Deep Neural Networks have been shown to be vulnerable to adversarial images. Conventional attacks strive for indistinguishable adversarial images with strictly restricted perturbations. Recently, researchers have moved to explore distinguishable yet non-suspicious adversarial images and demonstrated that color transformation attacks are effective. In this work, we propose Adversarial Color Filter (AdvCF), a novel color transformation attack that is optimized with gradient information in the parameter space of a simple color filter. In particular, our color filter space is explicitly specified so that we are able to provide a systematic analysis of model robustness against adversarial color transformations, from both the attack and defense perspectives. In contrast, existing color transformation attacks do not offer the opportunity for systematic analysis due to the lack of such an explicit space. We further demonstrate the effectiveness of our AdvCF in fooling image classifiers and also compare it with other color transformation attacks regarding their robustness to defenses and image acceptability through an extensive user study. We also highlight the human-interpretability of AdvCF and show its superiority over the state-of-the-art human-interpretable color transformation attack on both image acceptability and efficiency. Additional results provide interesting new insights into model robustness against AdvCF in another three visual tasks.

摘要: 深度神经网络已被证明容易受到敌意图像的影响。传统的攻击努力获得带有严格限制的扰动的难以区分的对抗性图像。最近，研究人员已经开始探索可区分但不可疑的对抗性图像，并证明了颜色变换攻击是有效的。在这项工作中，我们提出了对抗颜色过滤器(AdvCF)，这是一种新的颜色变换攻击，它利用简单颜色过滤器参数空间中的梯度信息进行优化。特别是，我们的滤色器空间被明确指定，以便我们能够从攻击和防御两个角度提供针对对抗性颜色变换的模型稳健性的系统分析。相比之下，现有的颜色变换攻击由于缺乏这种明确的空间而没有提供系统分析的机会。我们进一步证明了我们的AdvCF在愚弄图像分类器方面的有效性，并通过广泛的用户研究将其与其他颜色变换攻击在防御和图像可接受性方面的鲁棒性进行了比较。我们还强调了AdvCF的人类可解释性，并展示了它在图像可接受性和效率方面相对于最先进的人类可解释颜色变换攻击的优越性。其他结果为在另外三个可视化任务中针对AdvCF的模型健壮性提供了有趣的新见解。



## **26. Distributed Energy Resources Cybersecurity Outlook: Vulnerabilities, Attacks, Impacts, and Mitigations**

分布式能源网络安全展望：漏洞、攻击、影响和缓解 cs.CR

IEEE Systems Journal

**SubmitDate**: 2023-06-16    [abs](http://arxiv.org/abs/2205.11171v3) [paper-pdf](http://arxiv.org/pdf/2205.11171v3)

**Authors**: Ioannis Zografopoulos, Nikos D. Hatziargyriou, Charalambos Konstantinou

**Abstract**: The digitization and decentralization of the electric power grid are key thrusts for an economically and environmentally sustainable future. Towards this goal, distributed energy resources (DER), including rooftop solar panels, battery storage, electric vehicles, etc., are becoming ubiquitous in power systems. Power utilities benefit from DERs as they minimize operational costs; at the same time, DERs grant users and aggregators control over the power they produce and consume. DERs are interconnected, interoperable, and support remotely controllable features, thus, their cybersecurity is of cardinal importance. DER communication dependencies and the diversity of DER architectures widen the threat surface and aggravate the cybersecurity posture of power systems. In this work, we focus on security oversights that reside in the cyber and physical layers of DERs and can jeopardize grid operations. Existing works have underlined the impact of cyberattacks targeting DER assets, however, they either focus on specific system components (e.g., communication protocols), do not consider the mission-critical objectives of DERs, or neglect the adversarial perspective (e.g., adversary/attack models) altogether. To address these omissions, we comprehensively analyze adversarial capabilities and objectives when manipulating DER assets, and then present how protocol and device-level vulnerabilities can materialize into cyberattacks impacting power system operations. Finally, we provide mitigation strategies to thwart adversaries and directions for future DER cybersecurity research.

摘要: 电网的数字化和分散化是实现经济和环境可持续未来的关键推动力。为了实现这一目标，分布式能源(DER)在电力系统中变得无处不在，包括屋顶太阳能电池板、电池储存、电动汽车等。电力公用事业受益于DER，因为它们最大限度地降低了运营成本；同时，DER使用户和聚合器能够控制他们生产和消耗的电力。DER是互联的、可互操作的，并支持远程控制的功能，因此，其网络安全至关重要。DER通信的依赖性和DER体系结构的多样性扩大了威胁面，加剧了电力系统的网络安全态势。在这项工作中，我们重点关注驻留在DER的网络层和物理层并可能危及电网运营的安全疏忽。现有的工作强调了针对DER资产的网络攻击的影响，然而，它们要么专注于特定的系统组件(例如，通信协议)，没有考虑DER的关键任务目标，要么完全忽视了对抗性的观点(例如，对手/攻击模型)。为了解决这些疏漏，我们全面分析了操纵DER资产时的对抗能力和目标，然后介绍了协议和设备级漏洞如何转化为影响电力系统运行的网络攻击。最后，我们提供了挫败对手的缓解策略和未来网络安全研究的方向。



## **27. Inroads into Autonomous Network Defence using Explained Reinforcement Learning**

基于解释强化学习的自主网络防御研究 cs.CR

**SubmitDate**: 2023-06-15    [abs](http://arxiv.org/abs/2306.09318v1) [paper-pdf](http://arxiv.org/pdf/2306.09318v1)

**Authors**: Myles Foley, Mia Wang, Zoe M, Chris Hicks, Vasilios Mavroudis

**Abstract**: Computer network defence is a complicated task that has necessitated a high degree of human involvement. However, with recent advancements in machine learning, fully autonomous network defence is becoming increasingly plausible. This paper introduces an end-to-end methodology for studying attack strategies, designing defence agents and explaining their operation. First, using state diagrams, we visualise adversarial behaviour to gain insight about potential points of intervention and inform the design of our defensive models. We opt to use a set of deep reinforcement learning agents trained on different parts of the task and organised in a shallow hierarchy. Our evaluation shows that the resulting design achieves a substantial performance improvement compared to prior work. Finally, to better investigate the decision-making process of our agents, we complete our analysis with a feature ablation and importance study.

摘要: 计算机网络防御是一项复杂的任务，需要高度的人工参与。然而，随着最近机器学习的进步，完全自主的网络防御正变得越来越有可能。本文介绍了一种端到端的方法，用于研究攻击策略、设计防御代理并解释它们的操作。首先，我们使用状态图将敌对行为形象化，以洞察潜在的干预点，并为我们的防御模型的设计提供信息。我们选择使用一组针对任务不同部分进行培训的深度强化学习代理，并以浅层次进行组织。我们的评估结果表明，与以前的工作相比，所得到的设计实现了显著的性能改进。最后，为了更好地研究我们的代理的决策过程，我们用特征消融和重要性研究来完成我们的分析。



## **28. DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks in the Physical World**

DIFFender：物理世界中基于扩散的对抗性防御补丁攻击 cs.CV

**SubmitDate**: 2023-06-15    [abs](http://arxiv.org/abs/2306.09124v1) [paper-pdf](http://arxiv.org/pdf/2306.09124v1)

**Authors**: Caixin Kang, Yinpeng Dong, Zhengyi Wang, Shouwei Ruan, Hang Su, Xingxing Wei

**Abstract**: Adversarial attacks in the physical world, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is severely lacking. In this paper, we propose DIFFender, a novel defense method that leverages the pre-trained diffusion model to perform both localization and defense against potential adversarial patch attacks. DIFFender is designed as a pipeline consisting of two main stages: patch localization and restoration. In the localization stage, we exploit the intriguing properties of a diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ a text-guided diffusion model to eliminate adversarial regions in the image while preserving the integrity of the visual content. Additionally, we design a few-shot prompt-tuning algorithm to facilitate simple and efficient tuning, enabling the learned representations to easily transfer to downstream tasks, which optimize two stages jointly. We conduct extensive experiments on image classification and face recognition to demonstrate that DIFFender exhibits superior robustness under strong adaptive attacks and generalizes well across various scenarios, diverse classifiers, and multiple attack methods.

摘要: 物理世界中的对抗性攻击，特别是补丁攻击，对深度学习模型的健壮性和可靠性构成了严重威胁。开发针对补丁攻击的可靠防御对于现实世界的应用至关重要，但目前这一领域的研究严重缺乏。在本文中，我们提出了一种新的防御方法DIFFender，它利用预先训练的扩散模型来定位和防御潜在的敌意补丁攻击。DIFFender被设计为一个由两个主要阶段组成的管道：补丁定位和恢复。在本地化阶段，我们利用扩散模型的有趣性质来有效地识别敌方补丁的位置。在恢复阶段，我们使用文本引导的扩散模型来消除图像中的对抗性区域，同时保持视觉内容的完整性。此外，我们设计了几个镜头的提示调整算法，以便于简单有效的调整，使学习到的表示可以很容易地转移到下游任务，共同优化两个阶段。我们在图像分类和人脸识别上进行了大量的实验，证明了DIFFender在强自适应攻击下表现出了良好的鲁棒性，并且能够很好地适用于各种场景、不同的分类器和多种攻击方法。



## **29. The Effect of Length on Key Fingerprint Verification Security and Usability**

长度对密钥指纹验证安全性和可用性的影响 cs.CR

Accepted to International Conference on Availability, Reliability and  Security (ARES 2023)

**SubmitDate**: 2023-06-15    [abs](http://arxiv.org/abs/2306.04574v2) [paper-pdf](http://arxiv.org/pdf/2306.04574v2)

**Authors**: Dan Turner, Siamak F. Shahandashti, Helen Petrie

**Abstract**: In applications such as end-to-end encrypted instant messaging, secure email, and device pairing, users need to compare key fingerprints to detect impersonation and adversary-in-the-middle attacks. Key fingerprints are usually computed as truncated hashes of each party's view of the channel keys, encoded as an alphanumeric or numeric string, and compared out-of-band, e.g. manually, to detect any inconsistencies. Previous work has extensively studied the usability of various verification strategies and encoding formats, however, the exact effect of key fingerprint length on the security and usability of key fingerprint verification has not been rigorously investigated. We present a 162-participant study on the effect of numeric key fingerprint length on comparison time and error rate. While the results confirm some widely-held intuitions such as general comparison times and errors increasing significantly with length, a closer look reveals interesting nuances. The significant rise in comparison time only occurs when highly similar fingerprints are compared, and comparison time remains relatively constant otherwise. On errors, our results clearly distinguish between security non-critical errors that remain low irrespective of length and security critical errors that significantly rise, especially at higher fingerprint lengths. A noteworthy implication of this latter result is that Signal/WhatsApp key fingerprints provide a considerably lower level of security than usually assumed.

摘要: 在端到端加密即时消息、安全电子邮件和设备配对等应用中，用户需要比较密钥指纹来检测模仿和中间人攻击。密钥指纹通常被计算为每一方的频道密钥视图的截断散列，被编码为字母数字或数字字符串，并例如手动地进行带外比较以检测任何不一致。以往的工作已经广泛地研究了各种验证策略和编码格式的可用性，但还没有严格地研究密钥指纹长度对密钥指纹验证的安全性和可用性的确切影响。我们对162名参与者进行了一项关于数字密钥指纹长度对比较时间和错误率的影响的研究。虽然结果证实了一些普遍存在的直觉，如一般的比较时间和误差随着长度的增加而显著增加，但仔细观察会发现有趣的细微差别。只有当比较高度相似的指纹时，比较时间才会显著增加，否则比较时间保持相对恒定。在错误方面，我们的结果清楚地区分了无论长度如何都保持较低的安全非关键错误和显著上升的安全关键错误，特别是在较长的指纹长度时。后一种结果的一个值得注意的含义是，Signal/WhatsApp密钥指纹提供的安全级别比通常假设的要低得多。



## **30. Community Detection Attack against Collaborative Learning-based Recommender Systems**

基于协作学习的推荐系统的社区检测攻击 cs.IR

**SubmitDate**: 2023-06-15    [abs](http://arxiv.org/abs/2306.08929v1) [paper-pdf](http://arxiv.org/pdf/2306.08929v1)

**Authors**: Yacine Belal, Sonia Ben Mokhtar, Mohamed Maouche, Anthony Simonet-Boulogne

**Abstract**: Collaborative-learning based recommender systems emerged following the success of collaborative learning techniques such as Federated Learning (FL) and Gossip Learning (GL). In these systems, users participate in the training of a recommender system while keeping their history of consumed items on their devices. While these solutions seemed appealing for preserving the privacy of the participants at a first glance, recent studies have shown that collaborative learning can be vulnerable to a variety of privacy attacks. In this paper we propose a novel privacy attack called Community Detection Attack (CDA), which allows an adversary to discover the members of a community based on a set of items of her choice (e.g., discovering users interested in LGBT content). Through experiments on three real recommendation datasets and by using two state-of-the-art recommendation models, we assess the sensitivity of an FL-based recommender system as well as two flavors of Gossip Learning-based recommender systems to CDA. Results show that on all models and all datasets, the FL setting is more vulnerable to CDA than Gossip settings. We further evaluated two off-the-shelf mitigation strategies, namely differential privacy (DP) and a share less policy, which consists in sharing a subset of model parameters. Results show a better privacy-utility trade-off for the share less policy compared to DP especially in the Gossip setting.

摘要: 基于协作学习的推荐系统是在联邦学习(FL)和八卦学习(GL)等协作学习技术成功之后应运而生的。在这些系统中，用户参与推荐系统的培训，同时在他们的设备上保存他们的消费项目的历史。虽然这些解决方案乍一看似乎在保护参与者的隐私方面很有吸引力，但最近的研究表明，协作学习可能容易受到各种隐私攻击。在本文中，我们提出了一种新的隐私攻击，称为社区检测攻击(CDA)，它允许攻击者根据她选择的一组项目来发现社区成员(例如，发现对LGBT内容感兴趣的用户)。通过在三个真实推荐数据集上的实验，使用两种最新的推荐模型，我们评估了一个基于FL的推荐系统以及两种基于八卦学习的推荐系统对CDA的敏感度。结果表明，在所有模型和所有数据集上，FL设置比八卦设置更容易受到CDA的影响。我们进一步评估了两种现成的缓解策略，即差异隐私(DP)策略和共享较少策略，该策略包括共享模型参数的子集。结果表明，与DP相比，共享更少的策略具有更好的隐私效用权衡，尤其是在八卦环境下。



## **31. MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection**

MalProtect：基于ML的恶意软件检测中对抗恶意查询攻击的状态防御 cs.LG

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2302.10739v2) [paper-pdf](http://arxiv.org/pdf/2302.10739v2)

**Authors**: Aqib Rashid, Jose Such

**Abstract**: ML models are known to be vulnerable to adversarial query attacks. In these attacks, queries are iteratively perturbed towards a particular class without any knowledge of the target model besides its output. The prevalence of remotely-hosted ML classification models and Machine-Learning-as-a-Service platforms means that query attacks pose a real threat to the security of these systems. To deal with this, stateful defenses have been proposed to detect query attacks and prevent the generation of adversarial examples by monitoring and analyzing the sequence of queries received by the system. Several stateful defenses have been proposed in recent years. However, these defenses rely solely on similarity or out-of-distribution detection methods that may be effective in other domains. In the malware detection domain, the methods to generate adversarial examples are inherently different, and therefore we find that such detection mechanisms are significantly less effective. Hence, in this paper, we present MalProtect, which is a stateful defense against query attacks in the malware detection domain. MalProtect uses several threat indicators to detect attacks. Our results show that it reduces the evasion rate of adversarial query attacks by 80+\% in Android and Windows malware, across a range of attacker scenarios. In the first evaluation of its kind, we show that MalProtect outperforms prior stateful defenses, especially under the peak adversarial threat.

摘要: 众所周知，ML模型容易受到敌意查询攻击。在这些攻击中，查询被迭代地扰动到特定的类，除了其输出之外，不知道目标模型。远程托管的ML分类模型和机器学习即服务平台的流行意味着查询攻击对这些系统的安全构成了真正的威胁。为了解决这个问题，已经提出了状态防御来检测查询攻击，并通过监控和分析系统接收到的查询序列来防止敌对实例的生成。近年来，有人提出了几项有状态的辩护。然而，这些防御完全依赖于可能在其他领域有效的相似性或分布外检测方法。在恶意软件检测领域，生成恶意示例的方法本质上是不同的，因此我们发现这种检测机制的有效性显著降低。因此，在本文中，我们提出了MalProtect，它是恶意软件检测领域中针对查询攻击的一种状态防御。MalProtect使用多个威胁指示器来检测攻击。我们的结果表明，在各种攻击场景下，该算法将Android和Windows恶意软件中恶意查询攻击的逃避率降低了80%+\%。在该类型的第一次评估中，我们表明MalProtect的性能优于先前的状态防御，特别是在峰值敌意威胁下。



## **32. Augment then Smooth: Reconciling Differential Privacy with Certified Robustness**

先增强后平滑：使差异隐私与认证的健壮性相协调 cs.LG

25 pages, 19 figures

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.08656v1) [paper-pdf](http://arxiv.org/pdf/2306.08656v1)

**Authors**: Jiapeng Wu, Atiyeh Ashari Ghomi, David Glukhov, Jesse C. Cresswell, Franziska Boenisch, Nicolas Papernot

**Abstract**: Machine learning models are susceptible to a variety of attacks that can erode trust in their deployment. These threats include attacks against the privacy of training data and adversarial examples that jeopardize model accuracy. Differential privacy and randomized smoothing are effective defenses that provide certifiable guarantees for each of these threats, however, it is not well understood how implementing either defense impacts the other. In this work, we argue that it is possible to achieve both privacy guarantees and certified robustness simultaneously. We provide a framework called DP-CERT for integrating certified robustness through randomized smoothing into differentially private model training. For instance, compared to differentially private stochastic gradient descent on CIFAR10, DP-CERT leads to a 12-fold increase in certified accuracy and a 10-fold increase in the average certified radius at the expense of a drop in accuracy of 1.2%. Through in-depth per-sample metric analysis, we show that the certified radius correlates with the local Lipschitz constant and smoothness of the loss surface. This provides a new way to diagnose when private models will fail to be robust.

摘要: 机器学习模型容易受到各种攻击，这些攻击可能会侵蚀对其部署的信任。这些威胁包括对训练数据隐私的攻击，以及危及模型准确性的敌意例子。差异隐私和随机平滑是为这些威胁中的每一种提供可证明的保证的有效防御措施，然而，实施这两种防御措施对另一种威胁的影响还不是很清楚。在这项工作中，我们认为可以同时实现隐私保证和认证的健壮性。我们提供了一个称为DP-CERT的框架，用于将通过随机平滑验证的稳健性集成到不同的私有模型训练中。例如，与CIFAR10上的差分私有随机梯度下降相比，DP-CERT的认证精度提高了12倍，平均认证半径增加了10倍，但精度下降了1.2%。通过深入的逐样本度量分析，我们发现认证半径与损失曲面的局部Lipschitz常数和光滑度相关。这提供了一种新的方法来诊断何时私人车型将不再健壮。



## **33. A Unified Framework of Graph Information Bottleneck for Robustness and Membership Privacy**

面向健壮性和成员隐私的图信息瓶颈统一框架 cs.LG

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.08604v1) [paper-pdf](http://arxiv.org/pdf/2306.08604v1)

**Authors**: Enyan Dai, Limeng Cui, Zhengyang Wang, Xianfeng Tang, Yinghan Wang, Monica Cheng, Bing Yin, Suhang Wang

**Abstract**: Graph Neural Networks (GNNs) have achieved great success in modeling graph-structured data. However, recent works show that GNNs are vulnerable to adversarial attacks which can fool the GNN model to make desired predictions of the attacker. In addition, training data of GNNs can be leaked under membership inference attacks. This largely hinders the adoption of GNNs in high-stake domains such as e-commerce, finance and bioinformatics. Though investigations have been made in conducting robust predictions and protecting membership privacy, they generally fail to simultaneously consider the robustness and membership privacy. Therefore, in this work, we study a novel problem of developing robust and membership privacy-preserving GNNs. Our analysis shows that Information Bottleneck (IB) can help filter out noisy information and regularize the predictions on labeled samples, which can benefit robustness and membership privacy. However, structural noises and lack of labels in node classification challenge the deployment of IB on graph-structured data. To mitigate these issues, we propose a novel graph information bottleneck framework that can alleviate structural noises with neighbor bottleneck. Pseudo labels are also incorporated in the optimization to minimize the gap between the predictions on the labeled set and unlabeled set for membership privacy. Extensive experiments on real-world datasets demonstrate that our method can give robust predictions and simultaneously preserve membership privacy.

摘要: 图神经网络(GNN)在图结构数据建模方面取得了巨大的成功。然而，最近的研究表明，GNN很容易受到敌意攻击，这些攻击可以欺骗GNN模型做出所需的攻击者预测。此外，在成员关系推理攻击下，GNN的训练数据可能会被泄露。这在很大程度上阻碍了在电子商务、金融和生物信息学等高风险领域采用GNN。虽然已经在稳健预测和保护成员隐私方面进行了研究，但他们通常没有同时考虑稳健性和成员隐私。因此，在这项工作中，我们研究了一个新的问题，即开发健壮的、保护成员隐私的GNN。我们的分析表明，信息瓶颈(IB)可以帮助过滤噪声信息，并使对标记样本的预测正规化，这有利于稳健性和成员隐私。然而，结构噪声和节点分类中标签的缺乏对图结构数据上的IB的部署提出了挑战。为了缓解这些问题，我们提出了一种新的图信息瓶颈框架，该框架可以缓解带有邻居瓶颈的结构噪声。在优化过程中还加入了伪标签，以最大限度地减少对已标记集合和未标记集合的预测之间的差距，从而保证成员隐私。在真实数据集上的大量实验表明，我们的方法可以给出稳健的预测，同时保护成员隐私。



## **34. Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations**

基于非凸低阶半正定松弛的对抗性训练神经网络的紧认证 cs.LG

ICML 2023

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2211.17244v3) [paper-pdf](http://arxiv.org/pdf/2211.17244v3)

**Authors**: Hong-Ming Chiu, Richard Y. Zhang

**Abstract**: Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming (LP) techniques in particular face a "convex relaxation barrier" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming (MILP) and branch-and-bound (BnB) techniques. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming (SDP) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive SDP methods, while optimizing over dramatically fewer variables comparable to much weaker LP methods. Despite nonconvexity, we show how off-the-shelf local optimization algorithms can be used to achieve and to certify global optimality in polynomial time. Our experiments find that the nonconvex relaxation almost completely closes the gap towards exact certification of adversarially trained models.

摘要: 众所周知，对抗性训练可以产生高质量的神经网络模型，这些模型对对抗性扰动具有经验上的健壮性。然而，一旦一个模型经过对抗性的训练，人们往往希望得到一个证明，证明该模型对未来的所有攻击都是真正健壮的。不幸的是，当面对对手训练的模型时，所有现有的方法都在制作强大到足以实用的证书方面存在重大问题。线性规划(LP)技术尤其面临着一种“凸松弛障碍”，即使在使用混合整数线性规划(MILP)和分支定界(BNB)技术进行了改进之后，也无法进行高质量的认证。本文提出了一种基于半定规划(SDP)松弛的低阶限制的非凸证明技术。非凸松弛使强认证可与昂贵得多的SDP方法相媲美，同时优化的变量可比弱得多的线性规划方法少得多。尽管非凸性，我们展示了如何使用现成的局部优化算法来在多项式时间内实现和证明全局最优性。我们的实验发现，非凸松弛几乎完全弥合了对对抗性训练模型进行精确验证的差距。



## **35. Reliable Evaluation of Adversarial Transferability**

对抗性转移性的可靠评估 cs.CV

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.08565v1) [paper-pdf](http://arxiv.org/pdf/2306.08565v1)

**Authors**: Wenqian Yu, Jindong Gu, Zhijiang Li, Philip Torr

**Abstract**: Adversarial examples (AEs) with small adversarial perturbations can mislead deep neural networks (DNNs) into wrong predictions. The AEs created on one DNN can also fool another DNN. Over the last few years, the transferability of AEs has garnered significant attention as it is a crucial property for facilitating black-box attacks. Many approaches have been proposed to improve adversarial transferability. However, they are mainly verified across different convolutional neural network (CNN) architectures, which is not a reliable evaluation since all CNNs share some similar architectural biases. In this work, we re-evaluate 12 representative transferability-enhancing attack methods where we test on 18 popular models from 4 types of neural networks. Our reevaluation revealed that the adversarial transferability is often overestimated, and there is no single AE that can be transferred to all popular models. The transferability rank of previous attacking methods changes when under our comprehensive evaluation. Based on our analysis, we propose a reliable benchmark including three evaluation protocols. Adversarial transferability on our new benchmark is extremely low, which further confirms the overestimation of adversarial transferability. We release our benchmark at https://adv-trans-eval.github.io to facilitate future research, which includes code, model checkpoints, and evaluation protocols.

摘要: 具有小的对抗性扰动的对抗性示例(AE)可能会将深度神经网络(DNN)误导到错误的预测中。在一个DNN上创建的AE也可以欺骗另一个DNN。在过去的几年里，AE的可转移性引起了人们的极大关注，因为它是促进黑盒攻击的关键属性。已经提出了许多方法来提高对抗性转移能力。然而，它们主要是在不同的卷积神经网络(CNN)结构上进行验证的，这不是一个可靠的评估，因为所有的卷积神经网络都有一些相似的结构偏差。在这项工作中，我们重新评估了12种具有代表性的可转移性增强攻击方法，并在4种神经网络的18个流行模型上进行了测试。我们的重新评估表明，对抗性的可转移性经常被高估，并且没有一个单一的AE可以转移到所有流行的模型。在我们的综合评估下，以往进攻方法的可转换性排名发生了变化。基于我们的分析，我们提出了一个可靠的基准测试，包括三个评估协议。我们新基准的对抗性可转让性极低，这进一步证实了对对抗性可转移性的高估。我们在https://adv-trans-eval.github.io上发布我们的基准测试，以促进未来的研究，其中包括代码、模型检查点和评估协议。



## **36. LMD: A Learnable Mask Network to Detect Adversarial Examples for Speaker Verification**

LMD：一种用于说话人确认的可学习掩码网络 eess.AS

13 pages, 9 figures

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2211.00825v2) [paper-pdf](http://arxiv.org/pdf/2211.00825v2)

**Authors**: Xing Chen, Jie Wang, Xiao-Lei Zhang, Wei-Qiang Zhang, Kunde Yang

**Abstract**: Although the security of automatic speaker verification (ASV) is seriously threatened by recently emerged adversarial attacks, there have been some countermeasures to alleviate the threat. However, many defense approaches not only require the prior knowledge of the attackers but also possess weak interpretability. To address this issue, in this paper, we propose an attacker-independent and interpretable method, named learnable mask detector (LMD), to separate adversarial examples from the genuine ones. It utilizes score variation as an indicator to detect adversarial examples, where the score variation is the absolute discrepancy between the ASV scores of an original audio recording and its transformed audio synthesized from its masked complex spectrogram. A core component of the score variation detector is to generate the masked spectrogram by a neural network. The neural network needs only genuine examples for training, which makes it an attacker-independent approach. Its interpretability lies that the neural network is trained to minimize the score variation of the targeted ASV, and maximize the number of the masked spectrogram bins of the genuine training examples. Its foundation is based on the observation that, masking out the vast majority of the spectrogram bins with little speaker information will inevitably introduce a large score variation to the adversarial example, and a small score variation to the genuine example. Experimental results with 12 attackers and two representative ASV systems show that our proposed method outperforms five state-of-the-art baselines. The extensive experimental results can also be a benchmark for the detection-based ASV defenses.

摘要: 尽管自动说话人确认(ASV)的安全性受到最近出现的敌意攻击的严重威胁，但已经有一些对策来缓解这种威胁。然而，许多防御方法不仅需要攻击者的先验知识，而且具有较弱的可解释性。针对这一问题，本文提出了一种独立于攻击者且可解释的方法，称为可学习掩码检测器(LMD)，用于区分敌意实例和真实实例。它利用分数变化作为检测敌意例子的指标，其中分数变化是原始音频记录的ASV分数与从其掩蔽的复谱图合成的变换音频之间的绝对差异。分数变化检测器的核心部件是通过神经网络生成被屏蔽的谱图。神经网络只需要真实的样本进行训练，这使它成为一种独立于攻击者的方法。它的可解释性在于神经网络被训练来最小化目标ASV的分数差异，并最大化真实训练样本的掩蔽谱图库的数量。它的基础是观察到，在几乎没有说话人信息的情况下掩蔽绝大多数的语谱库将不可避免地给对抗性例子带来大的分数变化，而对真实的例子带来小的分数变化。对12个攻击者和两个有代表性的ASV系统的实验结果表明，我们提出的方法的性能超过了五个最先进的基线。广泛的实验结果也可以作为基于检测的ASV防御的基准。



## **37. COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models**

封面：对语言模型中基于提示的学习的启发式贪婪对抗性攻击 cs.CL

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.05659v2) [paper-pdf](http://arxiv.org/pdf/2306.05659v2)

**Authors**: Zihao Tan, Qingliang Chen, Wenbin Zhu, Yongjian Huang

**Abstract**: Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate and attack speed. Further experimental studies indicate that our proposed method also displays good capabilities in scenarios with varying shot counts, template lengths and query counts, exhibiting good generalizability.

摘要: 基于提示的学习已被证明是预训练语言模型(PLM)中的一种有效方法，特别是在资源较少的场景中，如少镜头场景。然而，PLM的可信性至关重要，基于提示的模板中已经显示出潜在的漏洞，这些漏洞可能会误导语言模型的预测，导致严重的安全问题。在本文中，我们将通过在黑盒场景中对人工模板提出一种基于提示的对抗性攻击来揭示PLM的一些漏洞。首先，我们分别设计了字字级和词级启发式方法来打破人工模板。在此基础上，提出了一种基于上述启发式破坏性方法的贪婪算法。最后，我们在BERT系列模型的三个变种和八个数据集上对我们的方法进行了评估。综合实验结果从攻击成功率和攻击速度两个方面验证了该方法的有效性。进一步的实验研究表明，该方法在镜头数、模板长度和查询次数不同的场景中也表现出了良好的性能，表现出良好的泛化能力。



## **38. A Relaxed Optimization Approach for Adversarial Attacks against Neural Machine Translation Models**

神经机器翻译模型对抗性攻击的一种松弛优化方法 cs.CL

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.08492v1) [paper-pdf](http://arxiv.org/pdf/2306.08492v1)

**Authors**: Sahar Sadrizadeh, Clément Barbier, Ljiljana Dolamic, Pascal Frossard

**Abstract**: In this paper, we propose an optimization-based adversarial attack against Neural Machine Translation (NMT) models. First, we propose an optimization problem to generate adversarial examples that are semantically similar to the original sentences but destroy the translation generated by the target NMT model. This optimization problem is discrete, and we propose a continuous relaxation to solve it. With this relaxation, we find a probability distribution for each token in the adversarial example, and then we can generate multiple adversarial examples by sampling from these distributions. Experimental results show that our attack significantly degrades the translation quality of multiple NMT models while maintaining the semantic similarity between the original and adversarial sentences. Furthermore, our attack outperforms the baselines in terms of success rate, similarity preservation, effect on translation quality, and token error rate. Finally, we propose a black-box extension of our attack by sampling from an optimized probability distribution for a reference model whose gradients are accessible.

摘要: 本文针对神经机器翻译(NMT)模型提出了一种基于优化的敌意攻击方法。首先，我们提出了一个优化问题，以生成与原始句子语义相似但破坏目标NMT模型生成的翻译的对抗性实例。这个优化问题是离散的，我们提出了一种连续松弛法来解决它。通过这种松弛，我们找到了对抗性实例中每个令牌的概率分布，然后我们可以从这些分布中采样来生成多个对抗性实例。实验结果表明，我们的攻击在保持原始句子和对抗性句子之间的语义相似性的同时，显著降低了多个自然机器翻译模型的翻译质量。此外，我们的攻击在成功率、相似性保持、对翻译质量的影响和令牌错误率方面都优于基线。最后，我们提出了我们的攻击的一个黑盒扩展，通过从一个梯度可访问的参考模型的优化概率分布中采样来实现。



## **39. X-Detect: Explainable Adversarial Patch Detection for Object Detectors in Retail**

X-Detect：零售业目标检测器的可解释敌意补丁检测 cs.CV

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.08422v1) [paper-pdf](http://arxiv.org/pdf/2306.08422v1)

**Authors**: Omer Hofman, Amit Giloni, Yarin Hayun, Ikuya Morikawa, Toshiya Shimizu, Yuval Elovici, Asaf Shabtai

**Abstract**: Object detection models, which are widely used in various domains (such as retail), have been shown to be vulnerable to adversarial attacks. Existing methods for detecting adversarial attacks on object detectors have had difficulty detecting new real-life attacks. We present X-Detect, a novel adversarial patch detector that can: i) detect adversarial samples in real time, allowing the defender to take preventive action; ii) provide explanations for the alerts raised to support the defender's decision-making process, and iii) handle unfamiliar threats in the form of new attacks. Given a new scene, X-Detect uses an ensemble of explainable-by-design detectors that utilize object extraction, scene manipulation, and feature transformation techniques to determine whether an alert needs to be raised. X-Detect was evaluated in both the physical and digital space using five different attack scenarios (including adaptive attacks) and the COCO dataset and our new Superstore dataset. The physical evaluation was performed using a smart shopping cart setup in real-world settings and included 17 adversarial patch attacks recorded in 1,700 adversarial videos. The results showed that X-Detect outperforms the state-of-the-art methods in distinguishing between benign and adversarial scenes for all attack scenarios while maintaining a 0% FPR (no false alarms) and providing actionable explanations for the alerts raised. A demo is available.

摘要: 目标检测模型被广泛应用于各个领域(如零售)，已被证明容易受到对手攻击。现有的用于检测对象检测器上的敌意攻击的方法已经很难检测到新的现实生活中的攻击。我们提出了X-Detect，这是一种新型的对抗性补丁检测器，它可以：i)实时检测对手样本，允许防御者采取预防措施；ii)为支持防御者决策过程而发出的警报提供解释；iii)处理新攻击形式的陌生威胁。给定一个新场景，X-Detect使用一组可通过设计解释的检测器，这些检测器利用对象提取、场景操作和特征转换技术来确定是否需要发出警报。X-Detect在物理和数字空间中使用五种不同的攻击场景(包括自适应攻击)以及Coco数据集和我们新的Superstore数据集进行了评估。物理评估是使用真实世界设置中的智能购物车进行的，包括1700个对抗性视频中记录的17个对抗性补丁攻击。结果表明，X-Detect在区分所有攻击场景的良性和敌意场景方面优于最先进的方法，同时保持0%的FPR(无错误警报)，并为发出的警报提供可行的解释。现已提供演示。



## **40. Global-Local Processing in Convolutional Neural Networks**

卷积神经网络中的全局-局部处理 cs.CV

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.08336v1) [paper-pdf](http://arxiv.org/pdf/2306.08336v1)

**Authors**: Zahra Rezvani, Soroor Shekarizeh, Mohammad Sabokrou

**Abstract**: Convolutional Neural Networks (CNNs) have achieved outstanding performance on image processing challenges. Actually, CNNs imitate the typically developed human brain structures at the micro-level (Artificial neurons). At the same time, they distance themselves from imitating natural visual perception in humans at the macro architectures (high-level cognition). Recently it has been investigated that CNNs are highly biased toward local features and fail to detect the global aspects of their input. Nevertheless, the literature offers limited clues on this problem. To this end, we propose a simple yet effective solution inspired by the unconscious behavior of the human pupil. We devise a simple module called Global Advantage Stream (GAS) to learn and capture the holistic features of input samples (i.e., the global features). Then, the GAS features were combined with a CNN network as a plug-and-play component called the Global/Local Processing (GLP) model. The experimental results confirm that this stream improves the accuracy with an insignificant additional computational/temporal load and makes the network more robust to adversarial attacks. Furthermore, investigating the interpretation of the model shows that it learns a more holistic representation similar to the perceptual system of healthy humans

摘要: 卷积神经网络(CNN)在图像处理方面取得了优异的性能。实际上，CNN在微观层面上模仿了典型的人类大脑结构(人工神经元)。与此同时，他们在宏观架构(高级认知)上与模仿人类的自然视觉知觉保持距离。最近的研究表明，CNN高度偏向于局部特征，并且无法检测其输入的全局方面。然而，文献对这个问题提供的线索有限。为此，我们提出了一种简单而有效的解决方案，灵感来自于人类的无意识行为。我们设计了一个名为Global Advantage Stream(GAS)的简单模块来学习和捕获输入样本的整体特征(即全局特征)。然后，将GAS功能与CNN网络组合为称为全局/本地处理(GLP)模型的即插即用组件。实验结果证实，该流在不增加计算/时间开销的情况下提高了准确率，并使网络对对手攻击具有更强的鲁棒性。此外，研究该模型的解释表明，它学习了一种更整体的表示，类似于健康人类的感知系统



## **41. On the Robustness of Latent Diffusion Models**

关于潜在扩散模型的稳健性 cs.CV

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.08257v1) [paper-pdf](http://arxiv.org/pdf/2306.08257v1)

**Authors**: Jianping Zhang, Zhuoer Xu, Shiwen Cui, Changhua Meng, Weibin Wu, Michael R. Lyu

**Abstract**: Latent diffusion models achieve state-of-the-art performance on a variety of generative tasks, such as image synthesis and image editing. However, the robustness of latent diffusion models is not well studied. Previous works only focus on the adversarial attacks against the encoder or the output image under white-box settings, regardless of the denoising process. Therefore, in this paper, we aim to analyze the robustness of latent diffusion models more thoroughly. We first study the influence of the components inside latent diffusion models on their white-box robustness. In addition to white-box scenarios, we evaluate the black-box robustness of latent diffusion models via transfer attacks, where we consider both prompt-transfer and model-transfer settings and possible defense mechanisms. However, all these explorations need a comprehensive benchmark dataset, which is missing in the literature. Therefore, to facilitate the research of the robustness of latent diffusion models, we propose two automatic dataset construction pipelines for two kinds of image editing models and release the whole dataset. Our code and dataset are available at \url{https://github.com/jpzhang1810/LDM-Robustness}.

摘要: 潜在扩散模型在各种生成性任务中实现了最先进的性能，例如图像合成和图像编辑。然而，潜扩散模型的稳健性还没有得到很好的研究。以往的工作只关注白盒环境下对编码器或输出图像的敌意攻击，而没有考虑去噪过程。因此，在本文中，我们旨在更深入地分析潜在扩散模型的稳健性。我们首先研究了潜扩散模型中各分量对其白盒稳健性的影响。除了白盒场景外，我们还通过传输攻击评估了潜在扩散模型的黑盒稳健性，其中我们同时考虑了提示传输和模型传输设置以及可能的防御机制。然而，所有这些探索都需要一个全面的基准数据集，而文献中缺少这一数据集。因此，为了便于研究潜在扩散模型的健壮性，我们针对两种图像编辑模型提出了两种数据集自动构建流水线，并发布了整个数据集。我们的代码和数据集可在\url{https://github.com/jpzhang1810/LDM-Robustness}.上获得



## **42. CARSO: Counter-Adversarial Recall of Synthetic Observations**

卡索：合成观察的反对抗性召回 cs.CV

20 pages, 5 figures, 10 tables; Update: removed visual artifacts from  some figures, fixed typos/capitalisation, typographic/pagination improvements

**SubmitDate**: 2023-06-14    [abs](http://arxiv.org/abs/2306.06081v2) [paper-pdf](http://arxiv.org/pdf/2306.06081v2)

**Authors**: Emanuele Ballarin, Alessio Ansuini, Luca Bortolussi

**Abstract**: In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/emaballarin/CARSO .

摘要: 在这篇文章中，我们提出了一种新的图像分类对抗性防御机制--CARSO--受认知神经科学的启发。该方法是对抗性训练的协同补充，并依赖于被攻击分类器的内部表示的知识。利用生成模型进行对抗性净化，在这种表示的条件下，对待最终分类的输入的重构进行采样。通过对不同图像数据集和分类器体系结构的各种强大自适应攻击的成熟基准进行的实验评估表明，CARSO能够比仅使用最先进的对手训练更好地防御分类器--并且具有可容忍的干净准确性代价。此外，防御体系结构成功地有效地保护自己免受不可预见的威胁，以及适合愚弄随机防御的端到端攻击。代码和预先培训的模型可在https://github.com/emaballarin/CARSO上找到。



## **43. White-Box Adversarial Policies in Deep Reinforcement Learning**

深度强化学习中的白盒对抗策略 cs.AI

Code is available at  https://github.com/thestephencasper/white_box_rarl

**SubmitDate**: 2023-06-13    [abs](http://arxiv.org/abs/2209.02167v2) [paper-pdf](http://arxiv.org/pdf/2209.02167v2)

**Authors**: Stephen Casper, Taylor Killian, Gabriel Kreiman, Dylan Hadfield-Menell

**Abstract**: In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we take inspiration from the literature on white-box attacks to train more effective adversarial policies. We study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can achieve higher initial and asymptotic performance against a target agent than black-box controls. Code is available at https://github.com/thestephencasper/lm_white_box_attacks

摘要: 在强化学习(RL)中，可以通过训练对抗代理来制定对抗策略，以最小化目标代理的回报。以前的工作已经研究了这些攻击的黑盒版本，其中对手只观察世界状态，并将目标代理视为环境的任何其他部分。然而，这没有考虑到问题中的额外结构。在这项工作中，我们从白盒攻击的文献中获得灵感，以训练更有效的对抗策略。我们研究了白盒对抗策略，并表明访问目标代理的内部状态有助于识别其漏洞。我们做出了两项贡献。(1)我们引入了白盒对抗策略，其中攻击者在每个时间步同时观察目标的内部状态和世界状态。我们制定了使用这些策略攻击双人游戏中的代理和文本生成语言模型的方法。(2)我们证明了这些策略可以获得比黑箱控制更高的初始性能和针对目标代理的渐近性能。代码可在https://github.com/thestephencasper/lm_white_box_attacks上找到



## **44. Class Attribute Inference Attacks: Inferring Sensitive Class Information by Diffusion-Based Attribute Manipulations**

类别属性推断攻击：通过基于扩散的属性操作推断敏感类别信息 cs.LG

46 pages, 37 figures, 5 tables

**SubmitDate**: 2023-06-13    [abs](http://arxiv.org/abs/2303.09289v2) [paper-pdf](http://arxiv.org/pdf/2303.09289v2)

**Authors**: Lukas Struppek, Dominik Hintersdorf, Felix Friedrich, Manuel Brack, Patrick Schramowski, Kristian Kersting

**Abstract**: Neural network-based image classifiers are powerful tools for computer vision tasks, but they inadvertently reveal sensitive attribute information about their classes, raising concerns about their privacy. To investigate this privacy leakage, we introduce the first Class Attribute Inference Attack (CAIA), which leverages recent advances in text-to-image synthesis to infer sensitive attributes of individual classes in a black-box setting, while remaining competitive with related white-box attacks. Our extensive experiments in the face recognition domain show that CAIA can accurately infer undisclosed sensitive attributes, such as an individual's hair color, gender, and racial appearance, which are not part of the training labels. Interestingly, we demonstrate that adversarial robust models are even more vulnerable to such privacy leakage than standard models, indicating that a trade-off between robustness and privacy exists.

摘要: 基于神经网络的图像分类器是计算机视觉任务的强大工具，但它们无意中泄露了有关其类别的敏感属性信息，引发了对其隐私的担忧。为了调查这种隐私泄露，我们引入了第一类属性推理攻击(CAIA)，它利用文本到图像合成的最新进展来推断黑盒环境中个别类的敏感属性，同时保持与相关白盒攻击的竞争力。我们在人脸识别领域的广泛实验表明，CAIA可以准确地推断出未披露的敏感属性，如个人的头发颜色、性别和种族外观，这些属性不属于训练标签的一部分。有趣的是，我们证明了对抗性稳健模型比标准模型更容易受到这种隐私泄露的影响，这表明存在稳健性和隐私之间的权衡。



## **45. Finite Gaussian Neurons: Defending against adversarial attacks by making neural networks say "I don't know"**

有限高斯神经元：通过让神经网络说出“我不知道”来防御敌意攻击 cs.LG

PhD thesis

**SubmitDate**: 2023-06-13    [abs](http://arxiv.org/abs/2306.07796v1) [paper-pdf](http://arxiv.org/pdf/2306.07796v1)

**Authors**: Felix Grezes

**Abstract**: Since 2014, artificial neural networks have been known to be vulnerable to adversarial attacks, which can fool the network into producing wrong or nonsensical outputs by making humanly imperceptible alterations to inputs. While defenses against adversarial attacks have been proposed, they usually involve retraining a new neural network from scratch, a costly task. In this work, I introduce the Finite Gaussian Neuron (FGN), a novel neuron architecture for artificial neural networks. My works aims to: - easily convert existing models to Finite Gaussian Neuron architecture, - while preserving the existing model's behavior on real data, - and offering resistance against adversarial attacks. I show that converted and retrained Finite Gaussian Neural Networks (FGNN) always have lower confidence (i.e., are not overconfident) in their predictions over randomized and Fast Gradient Sign Method adversarial images when compared to classical neural networks, while maintaining high accuracy and confidence over real MNIST images. To further validate the capacity of Finite Gaussian Neurons to protect from adversarial attacks, I compare the behavior of FGNs to that of Bayesian Neural Networks against both randomized and adversarial images, and show how the behavior of the two architectures differs. Finally I show some limitations of the FGN models by testing them on the more complex SPEECHCOMMANDS task, against the stronger Carlini-Wagner and Projected Gradient Descent adversarial attacks.

摘要: 自2014年以来，人工神经网络一直被认为容易受到对抗性攻击，这些攻击可以通过对输入进行人类无法察觉的改变来愚弄网络产生错误或毫无意义的输出。虽然有人提出了防御对手攻击的建议，但它们通常涉及从头开始重新训练新的神经网络，这是一项代价高昂的任务。在这项工作中，我介绍了有限高斯神经元(FGN)，一种新的人工神经网络的神经元结构。我的工作目标是：-轻松地将现有模型转换为有限的高斯神经元架构-同时保留现有模型在真实数据上的行为-并提供对对手攻击的抵抗。结果表明，与经典神经网络相比，经过转换和再训练的有限高斯神经网络(FGNN)在对随机和快速梯度符号法对手图像的预测中总是具有较低的置信度(即不过分自信)，而在真实MNIST图像上保持较高的精度和置信度。为了进一步验证有限高斯神经元抵御敌意攻击的能力，我比较了FGNs和贝叶斯神经网络在随机图像和敌意图像下的行为，并展示了这两种体系结构的行为如何不同。最后，我通过在更复杂的SPEECHCOMMANDS任务中测试FGN模型的一些局限性，对抗更强大的Carlini-Wagner和预测的梯度下降对手攻击。



## **46. Area is all you need: repeatable elements make stronger adversarial attacks**

面积就是你所需要的：可重复的元素构成更强的对抗性攻击 cs.CV

**SubmitDate**: 2023-06-13    [abs](http://arxiv.org/abs/2306.07768v1) [paper-pdf](http://arxiv.org/pdf/2306.07768v1)

**Authors**: Dillon Niederhut

**Abstract**: Over the last decade, deep neural networks have achieved state of the art in computer vision tasks. These models, however, are susceptible to unusual inputs, known as adversarial examples, that cause them to misclassify or otherwise fail to detect objects. Here, we provide evidence that the increasing success of adversarial attacks is primarily due to increasing their size. We then demonstrate a method for generating the largest possible adversarial patch by building a adversarial pattern out of repeatable elements. This approach achieves a new state of the art in evading detection by YOLOv2 and YOLOv3. Finally, we present an experiment that fails to replicate the prior success of several attacks published in this field, and end with some comments on testing and reproducibility.

摘要: 在过去的十年里，深度神经网络在计算机视觉任务中达到了最先进的水平。然而，这些模型很容易受到异常输入的影响，这些输入被称为对抗性示例，导致它们错误分类或无法检测到对象。在这里，我们提供的证据表明，对抗性攻击的日益成功主要是由于其规模的增加。然后，我们演示了一种通过从可重复元素中构建对抗性模式来生成可能最大的对抗性补丁的方法。该方法在躲避YOLOv2和YOLOv3的检测方面达到了新的技术水平。最后，我们给出了一个未能复制该领域已发表的几种攻击的先前成功的实验，并以对测试和重复性的一些评论结束。



## **47. PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts**

PromptBitch：评估大型语言模型在对抗性提示下的稳健性 cs.CL

Technical report; 23 pages; code is at:  https://github.com/microsoft/promptbench

**SubmitDate**: 2023-06-13    [abs](http://arxiv.org/abs/2306.04528v2) [paper-pdf](http://arxiv.org/pdf/2306.04528v2)

**Authors**: Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie

**Abstract**: The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,032 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our findings demonstrate that contemporary LLMs are vulnerable to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. We make our code, prompts, and methodologies to generate adversarial prompts publicly accessible, thereby enabling and encouraging collaborative exploration in this pivotal field: https://github.com/microsoft/promptbench.

摘要: 学术界和工业界对大型语言模型(LLM)的依赖日益增加，这就要求我们必须全面了解它们对提示的稳健性。为了响应这一关键需求，我们引入了PromptBtch，这是一个健壮性基准，旨在衡量LLMS对敌意提示的弹性。这项研究使用了过多的对抗性文本攻击，目标是多个层面的提示：字符、单词、句子和语义。这些提示随后被用于不同的任务，如情感分析、自然语言推理、阅读理解、机器翻译和数学解题。我们的研究产生了4032个对抗性提示，仔细评估了8个任务和13个数据集，总共有567,084个测试样本。我们的研究结果表明，当代的LLM容易受到对抗性提示的影响。此外，我们还提供了全面的分析，以了解即时健壮性及其可转移性背后的奥秘。然后，我们提供了有洞察力的健壮性分析和实用的即时撰写建议，这对研究人员和日常用户都是有益的。我们将生成对抗性提示的代码、提示和方法公之于众，从而支持并鼓励在这个关键领域进行协作探索：https://github.com/microsoft/promptbench.



## **48. Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios**

隐私推理--非IID场景下联合学习的隐形后门攻击 cs.LG

It can be accepted IJCNN

**SubmitDate**: 2023-06-13    [abs](http://arxiv.org/abs/2306.08011v1) [paper-pdf](http://arxiv.org/pdf/2306.08011v1)

**Authors**: Haochen Mei, Gaolei Li, Jun Wu, Longfei Zheng

**Abstract**: Federated learning (FL) naturally faces the problem of data heterogeneity in real-world scenarios, but this is often overlooked by studies on FL security and privacy. On the one hand, the effectiveness of backdoor attacks on FL may drop significantly under non-IID scenarios. On the other hand, malicious clients may steal private data through privacy inference attacks. Therefore, it is necessary to have a comprehensive perspective of data heterogeneity, backdoor, and privacy inference. In this paper, we propose a novel privacy inference-empowered stealthy backdoor attack (PI-SBA) scheme for FL under non-IID scenarios. Firstly, a diverse data reconstruction mechanism based on generative adversarial networks (GANs) is proposed to produce a supplementary dataset, which can improve the attacker's local data distribution and support more sophisticated strategies for backdoor attacks. Based on this, we design a source-specified backdoor learning (SSBL) strategy as a demonstration, allowing the adversary to arbitrarily specify which classes are susceptible to the backdoor trigger. Since the PI-SBA has an independent poisoned data synthesis process, it can be integrated into existing backdoor attacks to improve their effectiveness and stealthiness in non-IID scenarios. Extensive experiments based on MNIST, CIFAR10 and Youtube Aligned Face datasets demonstrate that the proposed PI-SBA scheme is effective in non-IID FL and stealthy against state-of-the-art defense methods.

摘要: 联邦学习(FL)自然会面临现实场景中数据异构性的问题，但这一点往往被FL安全和隐私方面的研究所忽视。一方面，在非IID场景下，对FL的后门攻击效果可能会大幅下降。另一方面，恶意客户端可能会通过隐私推理攻击窃取隐私数据。因此，有必要对数据异构性、后门和隐私推断有一个全面的视角。提出了一种新的基于隐私推理的隐蔽后门攻击方案(PI-SBA)，用于非IID场景下的FL攻击。首先，提出了一种基于产生式对抗网络(GANS)的多样化数据重构机制，生成一个补充数据集，改善攻击者的局部数据分布，支持更复杂的后门攻击策略。在此基础上，我们设计了一种来源指定的后门学习(SSBL)策略作为演示，允许攻击者任意指定哪些类容易受到后门触发器的影响。由于PI-SBA具有独立的有毒数据合成过程，因此可以将其集成到现有的后门攻击中，以提高其在非IID场景中的有效性和隐蔽性。基于MNIST、CIFAR10和YouTube对齐人脸数据集的大量实验表明，所提出的PI-SBA算法在非IID人脸识别中是有效的，并且对现有的防御方法具有较好的隐蔽性。



## **49. Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems**

恶意：一种新的对抗深度假冒和欺骗检测系统的对抗性卷积噪声攻击 eess.AS

Accepted at INTERSPEECH 2023

**SubmitDate**: 2023-06-13    [abs](http://arxiv.org/abs/2306.07655v1) [paper-pdf](http://arxiv.org/pdf/2306.07655v1)

**Authors**: Michele Panariello, Wanying Ge, Hemlata Tak, Massimiliano Todisco, Nicholas Evans

**Abstract**: We present Malafide, a universal adversarial attack against automatic speaker verification (ASV) spoofing countermeasures (CMs). By introducing convolutional noise using an optimised linear time-invariant filter, Malafide attacks can be used to compromise CM reliability while preserving other speech attributes such as quality and the speaker's voice. In contrast to other adversarial attacks proposed recently, Malafide filters are optimised independently of the input utterance and duration, are tuned instead to the underlying spoofing attack, and require the optimisation of only a small number of filter coefficients. Even so, they degrade CM performance estimates by an order of magnitude, even in black-box settings, and can also be configured to overcome integrated CM and ASV subsystems. Integrated solutions that use self-supervised learning CMs, however, are more robust, under both black-box and white-box settings.

摘要: 我们提出了一种针对自动说话人验证(ASV)欺骗对策(CMS)的通用对抗性攻击--恶意攻击。通过使用优化的线性时不变滤波器引入卷积噪声，恶意攻击可以用来损害CM的可靠性，同时保留其他语音属性，如质量和说话人的声音。与最近提出的其他敌意攻击不同，恶意过滤器独立于输入发音和持续时间进行优化，而是根据潜在的欺骗攻击进行调整，并且只需要优化少量的过滤器系数。即便如此，即使在黑盒设置中，它们也会将CM性能估计降低一个数量级，并且还可以配置为克服集成的CM和ASV子系统。然而，使用自我监督学习CMS的集成解决方案在黑盒和白盒设置下都更健壮。



## **50. DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation**

DHBE：基于受限对抗性蒸馏的深度神经网络无数据整体后门擦除 cs.LG

It has been accepted by asiaccs

**SubmitDate**: 2023-06-13    [abs](http://arxiv.org/abs/2306.08009v1) [paper-pdf](http://arxiv.org/pdf/2306.08009v1)

**Authors**: Zhicong Yan, Shenghong Li, Ruijie Zhao, Yuan Tian, Yuanyuan Zhao

**Abstract**: Backdoor attacks have emerged as an urgent threat to Deep Neural Networks (DNNs), where victim DNNs are furtively implanted with malicious neurons that could be triggered by the adversary. To defend against backdoor attacks, many works establish a staged pipeline to remove backdoors from victim DNNs: inspecting, locating, and erasing. However, in a scenario where a few clean data can be accessible, such pipeline is fragile and cannot erase backdoors completely without sacrificing model accuracy. To address this issue, in this paper, we propose a novel data-free holistic backdoor erasing (DHBE) framework. Instead of the staged pipeline, the DHBE treats the backdoor erasing task as a unified adversarial procedure, which seeks equilibrium between two different competing processes: distillation and backdoor regularization. In distillation, the backdoored DNN is distilled into a proxy model, transferring its knowledge about clean data, yet backdoors are simultaneously transferred. In backdoor regularization, the proxy model is holistically regularized to prevent from infecting any possible backdoor transferred from distillation. These two processes jointly proceed with data-free adversarial optimization until a clean, high-accuracy proxy model is obtained. With the novel adversarial design, our framework demonstrates its superiority in three aspects: 1) minimal detriment to model accuracy, 2) high tolerance for hyperparameters, and 3) no demand for clean data. Extensive experiments on various backdoor attacks and datasets are performed to verify the effectiveness of the proposed framework. Code is available at \url{https://github.com/yanzhicong/DHBE}

摘要: 后门攻击已经成为对深度神经网络(DNN)的紧迫威胁，受害者DNN被秘密植入可能由对手触发的恶意神经元。为了防御后门攻击，许多工作建立了一个分阶段的管道来删除受害者DNN的后门：检查、定位和擦除。然而，在可以访问少数干净数据的情况下，这样的管道是脆弱的，无法在不牺牲模型精度的情况下完全擦除后门。针对这一问题，本文提出了一种新的无数据整体后门擦除(DHBE)框架。与阶段性管道不同，DHBE将后门擦除任务视为统一的对抗性程序，寻求两个不同竞争过程之间的平衡：蒸馏和后门正规化。在蒸馏过程中，后置的DNN被蒸馏成代理模型，传递其关于干净数据的知识，但同时传递后门。在后门正规化中，代理模型被整体正规化，以防止感染从蒸馏转移的任何可能的后门。这两个过程共同进行无数据的对抗性优化，直到获得干净、高精度的代理模型。通过新的对抗性设计，我们的框架在三个方面显示了其优越性：1)对模型精度的损害最小，2)对超参数的容忍度高，3)不需要干净的数据。在各种后门攻击和数据集上进行了大量的实验，以验证该框架的有效性。代码位于\url{https://github.com/yanzhicong/DHBE}



