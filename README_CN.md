# Latest Adversarial Attack Papers
**update at 2024-01-12 11:34:50**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model**

GE-AdvGAN：基于梯度编辑的对抗性生成模型提高对抗性样本的可转移性 cs.CV

Accepted by SIAM International Conference on Data Mining (SDM24)

**SubmitDate**: 2024-01-11    [abs](http://arxiv.org/abs/2401.06031v1) [paper-pdf](http://arxiv.org/pdf/2401.06031v1)

**Authors**: Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Kim-Kwang Raymond Choo

**Abstract**: Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN

摘要: 诸如生成性对抗性网络(GANS)之类的对抗性生成模型被广泛应用于生成各种类型的数据，即图像、文本和音频。相应地，其良好的性能导致了白盒和黑盒攻击场景下基于GAN的对抗性攻击方法。可转移黑盒攻击的重要性在于它们能够在不同的模型和环境中有效，更紧密地与现实世界的应用程序保持一致。然而，就这类方法的可转让对抗性例子而言，保持业绩仍然具有挑战性。同时，我们观察到一些增强的基于梯度的可转移对抗性攻击算法需要较长的对抗性样本生成时间。因此，在这项工作中，我们提出了一种新的算法GE-AdvGAN，在提高算法效率的同时，增强了对抗性样本的可传递性。主要方法是通过优化发电机参数的训练过程。通过功能和特征的相似性分析，我们引入了一种新的梯度编辑机制，并验证了其在各种模型上生成可移植样本的可行性。此外，通过利用频域信息来确定梯度编辑方向，GE-AdvGAN可以生成高度可转移的对抗性样本，同时与最新的可转移对抗性攻击算法相比，可以最大限度地减少执行时间。通过在不同数据集上的大规模实验，对GE-AdvGAN算法的性能进行了综合评估，结果表明了该算法的优越性。我们算法的代码可以在https://github.com/LMBTough/GE-advGAN上找到



## **2. Combating Adversarial Attacks with Multi-Agent Debate**

用多智能体辩论对抗对抗性攻击 cs.CL

**SubmitDate**: 2024-01-11    [abs](http://arxiv.org/abs/2401.05998v1) [paper-pdf](http://arxiv.org/pdf/2401.05998v1)

**Authors**: Steffi Chern, Zhen Fan, Andy Liu

**Abstract**: While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics.

摘要: 虽然最先进的语言模型已经取得了令人印象深刻的结果，但它们仍然容易受到推理时对抗性攻击，例如由红色团队ARXIV：2209.07858生成的对抗性提示。为了提高语言模型生成的总体质量，提出了一种方法是多主体辩论，即语言模型通过讨论和反馈进行自我评估。我们在当前最先进的语言模型之间实现了多代理辩论，并评估了模型在单代理和多代理设置下对红色团队攻击的敏感度。我们发现，当越狱或能力较差的模型被迫与非越狱或更有能力的模型辩论时，多智能体辩论可以降低模型毒性。我们还发现，通过多代理交互的普遍使用，情况也有了轻微的改善。通过嵌入聚类对敌意提示内容进行分类，分析了不同模型对不同类型攻击主题的敏感度。



## **3. Localized adversarial artifacts for compressed sensing MRI**

用于压缩传感磁共振成像的局部化对抗性伪影 eess.IV

14 pages, 7 figures

**SubmitDate**: 2024-01-11    [abs](http://arxiv.org/abs/2206.05289v2) [paper-pdf](http://arxiv.org/pdf/2206.05289v2)

**Authors**: Rima Alaifari, Giovanni S. Alberti, Tandri Gauksson

**Abstract**: As interest in deep neural networks (DNNs) for image reconstruction tasks grows, their reliability has been called into question (Antun et al., 2020; Gottschling et al., 2020). However, recent work has shown that, compared to total variation (TV) minimization, when appropriately regularized, DNNs show similar robustness to adversarial noise in terms of $\ell^2$-reconstruction error (Genzel et al., 2022). We consider a different notion of robustness, using the $\ell^\infty$-norm, and argue that localized reconstruction artifacts are a more relevant defect than the $\ell^2$-error. We create adversarial perturbations to undersampled magnetic resonance imaging measurements (in the frequency domain) which induce severe localized artifacts in the TV-regularized reconstruction. Notably, the same attack method is not as effective against DNN based reconstruction. Finally, we show that this phenomenon is inherent to reconstruction methods for which exact recovery can be guaranteed, as with compressed sensing reconstructions with $\ell^1$- or TV-minimization.

摘要: 随着人们对用于图像重建任务的深度神经网络(DNN)的兴趣与日俱增，其可靠性受到质疑(Antun等人，2020；Gottschling等人，2020)。然而，最近的工作表明，与总变分(TV)最小化相比，当适当地正则化时，DNN在重构误差方面表现出类似于对抗噪声的稳健性(Genzel等人，2022)。我们考虑一种不同的健壮性概念，使用$\ell^\ininty$-范数，并认为局部重建构件是比$\ell^2$-错误更相关的缺陷。我们对欠采样磁共振成像测量(在频域)造成对抗性扰动，从而在TV正则化重建中导致严重的局部化伪影。值得注意的是，相同的攻击方法对基于DNN的重建并不有效。最后，我们证明了这种现象是可以保证精确恢复的重建方法所固有的，就像压缩感知重建中的最小化或TV最小化一样。



## **4. Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models**

朝向稳健剪枝：一种自适应的语言模型知识保留剪枝策略 cs.CL

**SubmitDate**: 2024-01-11    [abs](http://arxiv.org/abs/2310.13191v3) [paper-pdf](http://arxiv.org/pdf/2310.13191v3)

**Authors**: Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu

**Abstract**: The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models.

摘要: 修剪目标最近已经超越了语言模型中的精确度和稀疏性，扩展到了健壮性。尽管如此，现有的方法在不断增加模型稀疏性的同时努力增强对敌对攻击的鲁棒性，并且需要重新训练过程。随着人类步入大型语言模型时代，这些问题变得日益突出。本文提出语言模型的稳健性与它们所包含的预训练知识的程度成正比。因此，我们提出了一种训练后剪枝策略，旨在忠实地复制密集语言模型的嵌入空间和特征空间，目的是在剪枝过程中保存更多的预先训练的知识。在这种设置中，每一层的重建误差不仅源于自身，还包括来自前几层的累积误差，然后进行自适应校正。与其他最先进的基线相比，我们的方法在精确度、稀疏性、健壮性和剪枝成本之间表现出了更好的平衡，在数据集Sst2、IMDB和AgNews上使用ERT，标志着在语言模型中朝着健壮剪枝迈出了重要的一步。



## **5. Dynamics-aware Adversarial Attack of Adaptive Neural Networks**

自适应神经网络的动态感知敌意攻击 cs.CV

arXiv admin note: text overlap with arXiv:2112.09428

**SubmitDate**: 2024-01-11    [abs](http://arxiv.org/abs/2210.08159v4) [paper-pdf](http://arxiv.org/pdf/2210.08159v4)

**Authors**: An Tao, Yueqi Duan, Yingqi Wang, Jiwen Lu, Jie Zhou

**Abstract**: In this paper, we investigate the dynamics-aware adversarial attack problem of adaptive neural networks. Most existing adversarial attack algorithms are designed under a basic assumption -- the network architecture is fixed throughout the attack process. However, this assumption does not hold for many recently proposed adaptive neural networks, which adaptively deactivate unnecessary execution units based on inputs to improve computational efficiency. It results in a serious issue of lagged gradient, making the learned attack at the current step ineffective due to the architecture change afterward. To address this issue, we propose a Leaded Gradient Method (LGM) and show the significant effects of the lagged gradient. More specifically, we reformulate the gradients to be aware of the potential dynamic changes of network architectures, so that the learned attack better "leads" the next step than the dynamics-unaware methods when network architecture changes dynamically. Extensive experiments on representative types of adaptive neural networks for both 2D images and 3D point clouds show that our LGM achieves impressive adversarial attack performance compared with the dynamic-unaware attack methods. Code is available at https://github.com/antao97/LGM.

摘要: 本文研究了自适应神经网络的动态感知对抗攻击问题。大多数现有的对抗性攻击算法都是在一个基本假设下设计的--网络体系结构在整个攻击过程中是固定的。然而，这一假设对最近提出的许多自适应神经网络并不成立，这些自适应神经网络基于输入自适应地停用不必要的执行单元来提高计算效率。它导致了严重的梯度滞后问题，使得当前步骤的学习攻击由于之后的体系结构变化而无效。为了解决这个问题，我们提出了一种引导梯度法(LGM)，并展示了滞后梯度的显著影响。更具体地说，我们重新制定了梯度，以了解网络体系结构的潜在动态变化，以便在网络体系结构动态变化时，学习到的攻击比不知道动态变化的方法更好地“引导”下一步。在典型的自适应神经网络上对2D图像和3D点云进行的大量实验表明，与动态未知攻击方法相比，我们的LGM具有令人印象深刻的对抗性攻击性能。代码可在https://github.com/antao97/LGM.上找到



## **6. The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models**

魔鬼代言人：使用扩散模型粉碎不可利用数据的幻觉 cs.LG

Accepted to the 2024 IEEE Conference on Secure and Trustworthy  Machine Learning (SatML)

**SubmitDate**: 2024-01-11    [abs](http://arxiv.org/abs/2303.08500v2) [paper-pdf](http://arxiv.org/pdf/2303.08500v2)

**Authors**: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie

**Abstract**: Protecting personal data against exploitation of machine learning models is crucial. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data "unexploitable." This paper provides a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can counteract the effectiveness of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training even under distribution mismatch between the diffusion model and the protected data. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over. Our implementation is available at this repository: https://github.com/hmdolatabadi/AVATAR.

摘要: 保护个人数据免受机器学习模型的利用至关重要。最近，可用性攻击显示出巨大的希望，可以提供额外的一层保护，防止未经授权使用数据来训练神经网络。这些方法的目的是在干净的数据中添加难以察觉的噪声，以便神经网络无法从受保护的数据中提取有意义的模式，声称它们可以使个人数据“无法利用”。这篇论文提供了针对这种方法的强有力的对策，表明不可利用的数据可能只是一种错觉。特别是，我们利用扩散模型的力量，并表明精心设计的去噪过程可以抵消数据保护扰动的有效性。我们对算法进行了严格的分析，并从理论上证明了所需的去噪量与数据保护扰动的大小直接相关。我们的方法被称为阿凡达，在各种情况下针对最近的一系列可用性攻击提供最先进的性能，即使在扩散模型和受保护数据之间的分布不匹配的情况下，性能也优于对手训练。我们的发现呼吁进行更多的研究，让个人数据无法被利用，这表明这一目标远未结束。我们的实现可从以下存储库获得：https://github.com/hmdolatabadi/AVATAR.



## **7. A general theory for robust clustering via trimmed mean**

基于截尾均值的鲁棒聚类的一般理论 math.ST

51 pages

**SubmitDate**: 2024-01-10    [abs](http://arxiv.org/abs/2401.05574v1) [paper-pdf](http://arxiv.org/pdf/2401.05574v1)

**Authors**: Soham Jana, Jianqing Fan, Sanjeev Kulkarni

**Abstract**: Clustering is a fundamental tool in statistical machine learning in the presence of heterogeneous data. Many recent results focus primarily on optimal mislabeling guarantees, when data are distributed around centroids with sub-Gaussian errors. Yet, the restrictive sub-Gaussian model is often invalid in practice, since various real-world applications exhibit heavy tail distributions around the centroids or suffer from possible adversarial attacks that call for robust clustering with a robust data-driven initialization. In this paper, we introduce a hybrid clustering technique with a novel multivariate trimmed mean type centroid estimate to produce mislabeling guarantees under a weak initialization condition for general error distributions around the centroids. A matching lower bound is derived, up to factors depending on the number of clusters. In addition, our approach also produces the optimal mislabeling even in the presence of adversarial outliers. Our results reduce to the sub-Gaussian case when errors follow sub-Gaussian distributions. To solve the problem thoroughly, we also present novel data-driven robust initialization techniques and show that, with probabilities approaching one, these initial centroid estimates are sufficiently good for the subsequent clustering algorithm to achieve the optimal mislabeling rates. Furthermore, we demonstrate that the Lloyd algorithm is suboptimal for more than two clusters even when errors are Gaussian, and for two clusters when errors distributions have heavy tails. Both simulated data and real data examples lend further support to both of our robust initialization procedure and clustering algorithm.

摘要: 聚类是统计机器学习中存在异构数据的基本工具。许多最近的结果主要集中在最佳的错误标记的保证，当数据分布在亚高斯误差的质心周围。然而，限制性亚高斯模型在实践中通常是无效的，因为各种现实世界的应用程序在质心周围表现出厚尾分布，或者遭受可能的对抗性攻击，这些攻击需要具有强大的数据驱动初始化的强大聚类。在本文中，我们引入了一种混合聚类技术与一种新的多元修剪平均型质心估计，以产生错误标记的保证下，一般误差分布的质心周围的弱初始化条件。一个匹配的下限推导出，取决于集群的数量的因素。此外，即使存在对抗性离群值，我们的方法也会产生最佳错误标记。当误差服从亚高斯分布时，我们的结果减少到亚高斯的情况。为了彻底解决这个问题，我们还提出了新的数据驱动的鲁棒初始化技术，并表明，概率接近1，这些初始质心估计是足够好的后续聚类算法，以实现最佳的错误标记率。此外，我们证明了劳埃德算法是次优的两个以上的集群，即使错误是高斯，和两个集群时，错误分布有沉重的尾巴。模拟数据和真实数据的例子都进一步支持我们强大的初始化过程和聚类算法。



## **8. LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack**

LimeAttack：文本硬标签对抗攻击的局部解释方法 cs.CL

18 pages, 38th AAAI Main Track

**SubmitDate**: 2024-01-10    [abs](http://arxiv.org/abs/2308.00319v2) [paper-pdf](http://arxiv.org/pdf/2308.00319v2)

**Authors**: Hai Zhu, Zhaoqing Yang, Weiwei Shang, Yuren Wu

**Abstract**: Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt gradients or confidence scores to calculate word importance ranking and generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of model queries and the attack success rate is restricted by adversary initialization. In this paper, we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experiments show that LimeAttack achieves the better attacking performance compared with existing hard-label attack under the same query budget. In addition, we evaluate the effectiveness of LimeAttack on large language models, and results indicate that adversarial examples remain a significant threat to large language models. The adversarial examples crafted by LimeAttack are highly transferable and effectively improve model robustness in adversarial training.

摘要: 自然语言处理模型很容易受到敌意例子的影响。以前的文本对抗性攻击采用梯度或置信度分数来计算单词重要性排名并生成对抗性实例。然而，这些信息在现实世界中是不可用的。因此，我们将重点放在一种更现实和更具挑战性的环境中，即硬标签攻击，在这种情况下，攻击者只能查询模型并获得离散的预测标签。现有的硬标签攻击算法倾向于通过随机替换来初始化对抗性样本，然后利用复杂的启发式算法来优化对抗性扰动。这些方法需要大量的模型查询，攻击成功率受对手初始化的制约。本文提出了一种新的硬标签攻击算法LimeAttack，该算法利用一种局部可解释的方法来近似单词重要性排序，然后采用波束搜索来寻找最优解。大量实验表明，在相同的查询开销下，LimeAttack的攻击性能优于现有的硬标签攻击。此外，我们评估了LimeAttack在大型语言模型上的有效性，结果表明，对抗性例子仍然是对大型语言模型的重大威胁。LimeAttack生成的对抗性实例具有很强的可移植性，有效地提高了对抗性训练中模型的稳健性。



## **9. A Theoretical View of Linear Backpropagation and Its Convergence**

线性反向传播及其收敛的理论观点 cs.LG

This paper is accepted by IEEE Transactions on Pattern Analysis and  Machine Intelligence

**SubmitDate**: 2024-01-10    [abs](http://arxiv.org/abs/2112.11018v2) [paper-pdf](http://arxiv.org/pdf/2112.11018v2)

**Authors**: Ziang Li, Yiwen Guo, Haodi Liu, Changshui Zhang

**Abstract**: Backpropagation (BP) is widely used for calculating gradients in deep neural networks (DNNs). Applied often along with stochastic gradient descent (SGD) or its variants, BP is considered as a de-facto choice in a variety of machine learning tasks including DNN training and adversarial attack/defense. Recently, a linear variant of BP named LinBP was introduced for generating more transferable adversarial examples for performing black-box attacks, by Guo et al. Although it has been shown empirically effective in black-box attacks, theoretical studies and convergence analyses of such a method is lacking. This paper serves as a complement and somewhat an extension to Guo et al.'s paper, by providing theoretical analyses on LinBP in neural-network-involved learning tasks, including adversarial attack and model training. We demonstrate that, somewhat surprisingly, LinBP can lead to faster convergence in these tasks in the same hyper-parameter settings, compared to BP. We confirm our theoretical results with extensive experiments.

摘要: BP算法被广泛用于深度神经网络(DNN)中的梯度计算。BP经常与随机梯度下降(SGD)或其变体一起应用，被认为是包括DNN训练和对抗性攻击/防御在内的各种机器学习任务的事实上的选择。最近，Guo等人引入了BP的线性变体LinBP，以生成更多可移植的对抗实例来执行黑盒攻击。虽然它在黑盒攻击中已被证明是有效的，但缺乏对该方法的理论研究和收敛分析。本文是对郭等人S论文的补充和某种程度上的扩展，通过对LINBP在神经网络相关学习任务(包括对抗性攻击和模型训练)中的理论分析，我们证明了在相同的超参数设置下，LINBP可以比BP在这些任务中更快地收敛。我们用大量的实验证实了我们的理论结果。



## **10. Lyapunov-Stable Deep Equilibrium Models**

Lyapunov稳定的深度平衡模型 cs.LG

**SubmitDate**: 2024-01-10    [abs](http://arxiv.org/abs/2304.12707v3) [paper-pdf](http://arxiv.org/pdf/2304.12707v3)

**Authors**: Haoyu Chu, Shikui Wei, Ting Liu, Yao Zhao, Yuto Miyatake

**Abstract**: Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the Lyapunov stability of the DEQ model's fixed points, which enables the proposed model to resist minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we orthogonalize the layers after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models under well-known adversarial attacks, and experimental results demonstrate significant improvement in robustness. Furthermore, we show that the LyaDEQ model can be combined with other defense methods, such as adversarial training, to achieve even better adversarial robustness.

摘要: 深度平衡(DEQ)模型是一类很有前途的隐层模型，它通过求解单个非线性层的不动点来摒弃传统的深度模型。尽管它们取得了成功，但这些模型的固定点的稳定性仍然知之甚少。将DEQ模型视为非线性动态系统，利用Lyapunov理论，提出了一种具有可证明稳定性的鲁棒DEQ模型LyaDEQ。我们方法的关键是确保DEQ模型不动点的Lyapunov稳定性，从而使所提出的模型能够抵抗较小的初始扰动。为了避免Lyapunov稳定不动点位置较近造成的对抗性差，我们对Lyapunov稳定模后的各层进行正交化，以分离不同的不动点。我们对LyaDEQ模型进行了评估，实验结果表明，LyaDEQ模型在稳健性方面有显著的提高。此外，我们还证明了LyaDEQ模型可以与其他防御方法相结合，例如对抗训练，以获得更好的对抗健壮性。



## **11. Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks**

基于学习的增强型成员推理攻击难度校正 cs.CR

10 pages, 14 figures

**SubmitDate**: 2024-01-10    [abs](http://arxiv.org/abs/2401.04929v1) [paper-pdf](http://arxiv.org/pdf/2401.04929v1)

**Authors**: Haonan Shi, Tu Ouyang, An Wang

**Abstract**: Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network classifier to determine membership. The experiment results show that LDC-MIA can improve TPR at low FPR by up to 4x compared to the other difficulty calibration based MIAs. It also has the highest Area Under ROC curve (AUC) across all datasets. Our method's cost is comparable with most of the existing MIAs, but is orders of magnitude more efficient than one of the state-of-the-art methods, LiRA, while achieving similar performance.

摘要: 机器学习模型，特别是深度神经网络，目前是从医疗保健到金融的各种应用程序的组成部分。然而，使用敏感数据来训练这些模型会引发对隐私和安全的担忧。出现的一种验证训练模型是否保护隐私的方法是成员推理攻击(MIA)，它允许对手确定特定数据点是否属于模型训练数据集的一部分。虽然文献中已经提出了一系列的MIA，但只有少数几个MIA能在低假阳性率(FPR)区域(0.01%~1%)获得高的真阳性率(TPR)。要使MIA在实际环境中发挥实际作用，这是需要考虑的关键因素。在本文中，我们提出了一种新的MIA方法，旨在显著改善低FPR下的TPR。我们的方法，称为基于学习的MIA难度校准(LDC-MIA)，使用神经网络分类器来确定成员身份，根据数据记录的硬度来表征数据记录。实验结果表明，与其他基于难度校正的MIA相比，LDC-MIA可以在较低的误码率下将TPR提高4倍。在所有数据集中，它也具有最高的ROC曲线下面积(AUC)。我们的方法的成本与大多数现有的MIA相当，但效率比最先进的方法之一LIRA高出数量级，同时实现了类似的性能。



## **12. AdvSQLi: Generating Adversarial SQL Injections against Real-world WAF-as-a-service**

AdvSQLi：针对真实世界的WAF-as-a-Service生成敌意SQL注入 cs.CR

Accepted by IEEE Transactions on Information Forensics and Security  (IEEE TIFS)

**SubmitDate**: 2024-01-09    [abs](http://arxiv.org/abs/2401.02615v3) [paper-pdf](http://arxiv.org/pdf/2401.02615v3)

**Authors**: Zhenqing Qu, Xiang Ling, Ting Wang, Xiang Chen, Shouling Ji, Chunming Wu

**Abstract**: As the first defensive layer that attacks would hit, the web application firewall (WAF) plays an indispensable role in defending against malicious web attacks like SQL injection (SQLi). With the development of cloud computing, WAF-as-a-service, as one kind of Security-as-a-service, has been proposed to facilitate the deployment, configuration, and update of WAFs in the cloud. Despite its tremendous popularity, the security vulnerabilities of WAF-as-a-service are still largely unknown, which is highly concerning given its massive usage. In this paper, we propose a general and extendable attack framework, namely AdvSQLi, in which a minimal series of transformations are performed on the hierarchical tree representation of the original SQLi payload, such that the generated SQLi payloads can not only bypass WAF-as-a-service under black-box settings but also keep the same functionality and maliciousness as the original payload. With AdvSQLi, we make it feasible to inspect and understand the security vulnerabilities of WAFs automatically, helping vendors make products more secure. To evaluate the attack effectiveness and efficiency of AdvSQLi, we first employ two public datasets to generate adversarial SQLi payloads, leading to a maximum attack success rate of 100% against state-of-the-art ML-based SQLi detectors. Furthermore, to demonstrate the immediate security threats caused by AdvSQLi, we evaluate the attack effectiveness against 7 WAF-as-a-service solutions from mainstream vendors and find all of them are vulnerable to AdvSQLi. For instance, AdvSQLi achieves an attack success rate of over 79% against the F5 WAF. Through in-depth analysis of the evaluation results, we further condense out several general yet severe flaws of these vendors that cannot be easily patched.

摘要: Web应用防火墙(WAF)作为攻击攻击的第一防御层，在防御SQL注入(SQLI)等恶意Web攻击中发挥着不可或缺的作用。随着云计算的发展，WAF-as-a-Service作为安全即服务的一种，被提出以方便WAF在云中的部署、配置和更新。尽管网站管家非常受欢迎，但它的安全漏洞仍然很大程度上是未知的，这是高度关注的，因为它的大量使用。本文提出了一个通用的、可扩展的攻击框架AdvSQLi，该框架对原始SQLI负载的层次树表示进行最小一系列的变换，使得生成的SQLI负载不仅可以在黑盒环境下绕过WAF-as-a-Service，而且保持了与原始负载相同的功能和恶意。有了AdvSQLi，我们就可以自动检查和了解无线局域网的安全漏洞，帮助供应商让产品更安全。为了评估AdvSQLI的攻击效果和效率，我们首先使用两个公开的数据集来生成对抗性的SQLI有效负载，导致对最先进的基于ML的SQLI检测器的最大攻击成功率为100%。此外，为了展示AdvSQLi带来的直接安全威胁，我们对来自主流厂商的7个网站管家即服务解决方案进行了攻击有效性评估，发现它们都容易受到AdvSQLi的攻击。例如，AdvSQLi对F5 WAF的攻击成功率超过79%。通过对评估结果的深入分析，我们进一步提炼出了这些供应商普遍存在的几个不容易修补的严重缺陷。



## **13. ROIC-DM: Robust Text Inference and Classification via Diffusion Model**

ROIC-DM：基于扩散模型的稳健文本推理与分类 cs.CL

under review

**SubmitDate**: 2024-01-09    [abs](http://arxiv.org/abs/2401.03514v2) [paper-pdf](http://arxiv.org/pdf/2401.03514v2)

**Authors**: Shilong Yuan, Wei Yuan, Hongzhi Yin, Tieke He

**Abstract**: While language models have made many milestones in text inference and classification tasks, they remain susceptible to adversarial attacks that can lead to unforeseen outcomes. Existing works alleviate this problem by equipping language models with defense patches. However, these defense strategies often rely on impractical assumptions or entail substantial sacrifices in model performance. Consequently, enhancing the resilience of the target model using such defense mechanisms is a formidable challenge. This paper introduces an innovative model for robust text inference and classification, built upon diffusion models (ROIC-DM). Benefiting from its training involving denoising stages, ROIC-DM inherently exhibits greater robustness compared to conventional language models. Moreover, ROIC-DM can attain comparable, and in some cases, superior performance to language models, by effectively incorporating them as advisory components. Extensive experiments conducted with several strong textual adversarial attacks on three datasets demonstrate that (1) ROIC-DM outperforms traditional language models in robustness, even when the latter are fortified with advanced defense mechanisms; (2) ROIC-DM can achieve comparable and even better performance than traditional language models by using them as advisors.

摘要: 虽然语言模型在文本推理和分类任务中取得了许多里程碑，但它们仍然容易受到可能导致不可预见结果的对抗性攻击。现有的工作通过为语言模型配备防御补丁来缓解这个问题。然而，这些防御策略往往依赖于不切实际的假设，或者需要在模型性能上做出实质性的牺牲。因此，使用这种防御机制提高目标模型的弹性是一个巨大的挑战。本文介绍了一种基于扩散模型的稳健文本推理和分类模型(ROIC-DM)。由于其训练涉及去噪阶段，ROIC-DM固有地表现出比传统语言模型更强的稳健性。此外，通过有效地将语言模型合并为咨询组件，ROIC-DM可以获得与语言模型相当的、甚至在某些情况下优于语言模型的性能。在三个数据集上进行的大量实验表明：(1)ROIC-DM在稳健性方面优于传统的语言模型，即使后者有先进的防御机制；(2)ROIC-DM可以获得与传统语言模型相当甚至更好的性能，通过使用它们作为顾问。



## **14. Exploring Adversarial Robustness of LiDAR-Camera Fusion Model in Autonomous Driving**

自动驾驶中LiDAR-Camera融合模型的对抗稳健性研究 cs.RO

**SubmitDate**: 2024-01-09    [abs](http://arxiv.org/abs/2312.01468v2) [paper-pdf](http://arxiv.org/pdf/2312.01468v2)

**Authors**: Bo Yang, Xiaoyu Ji, Zizhi Jin, Yushi Cheng, Wenyuan Xu

**Abstract**: Our study assesses the adversarial robustness of LiDAR-camera fusion models in 3D object detection. We introduce an attack technique that, by simply adding a limited number of physically constrained adversarial points above a car, can make the car undetectable by the fusion model. Experimental results reveal that even without changes to the image data channel, the fusion model can be deceived solely by manipulating the LiDAR data channel. This finding raises safety concerns in the field of autonomous driving. Further, we explore how the quantity of adversarial points, the distance between the front-near car and the LiDAR-equipped car, and various angular factors affect the attack success rate. We believe our research can contribute to the understanding of multi-sensor robustness, offering insights and guidance to enhance the safety of autonomous driving.

摘要: 我们的研究评估了LiDAR-相机融合模型在3D目标检测中的对抗健壮性。我们介绍了一种攻击技术，只需在汽车上方添加有限数量的物理约束对手点，就可以使汽车无法被融合模型检测到。实验结果表明，即使在不改变图像数据通道的情况下，仅通过操纵LiDAR数据通道也可以欺骗融合模型。这一发现引发了自动驾驶领域的安全担忧。在此基础上，进一步探讨了攻击点的数量、前近车与装备激光雷达的车之间的距离以及各种角度因素对攻击成功率的影响。我们相信，我们的研究将有助于理解多传感器的稳健性，为提高自动驾驶的安全性提供见解和指导。



## **15. Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study**

分数阶连续动态耦合图神经网络的稳健性研究 cs.LG

in Proc. AAAI Conference on Artificial Intelligence, Vancouver,  Canada, Feb. 2024

**SubmitDate**: 2024-01-09    [abs](http://arxiv.org/abs/2401.04331v1) [paper-pdf](http://arxiv.org/pdf/2401.04331v1)

**Authors**: Qiyu Kang, Kai Zhao, Yang Song, Yihang Xie, Yanan Zhao, Sijie Wang, Rui She, Wee Peng Tay

**Abstract**: In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph neural FDE models. We establish a theoretical foundation outlining the robustness characteristics of graph neural FDE models, highlighting that they maintain more stringent output perturbation bounds in the face of input and graph topology disturbances, compared to their integer-order counterparts. Our empirical evaluations further confirm the enhanced robustness of graph neural FDE models, highlighting their potential in adversarially robust applications.

摘要: 在这项工作中，我们严格研究了图神经分数阶微分方程(FDE)模型的稳健性。该框架通过实现时间分数Caputo导数，扩展了传统的图神经(整数阶)常微分方程(ODE)模型。利用分数阶微积分，我们的模型可以在特征更新过程中考虑长期记忆，不同于传统的图神经节点模型中看到的无记忆的马尔可夫更新。图神经FDE模型相对于图神经ODE模型的优越性已经在没有攻击或扰动的环境中得到了证实。虽然已有文献证明传统的图神经FDE模型在对抗攻击下具有一定程度的稳定性和韧性，但图神经FDE模型的稳健性，特别是在对抗条件下的稳健性，在很大程度上还没有被探索。本文对图神经FDE模型的稳健性进行了详细的评估。我们建立了一个理论基础，概述了图神经FDE模型的稳健性特征，强调了它们在面对输入和图的拓扑扰动时，与其整数阶对应模型相比，保持了更严格的输出摄动界。我们的经验评估进一步证实了图神经FDE模型的增强的稳健性，突出了它们在相反的健壮应用中的潜力。



## **16. Cross-Class Feature Augmentation for Class Incremental Learning**

面向类增量学习的跨类特征增强 cs.CV

**SubmitDate**: 2024-01-09    [abs](http://arxiv.org/abs/2304.01899v3) [paper-pdf](http://arxiv.org/pdf/2304.01899v3)

**Authors**: Taehoon Kim, Jaeyoo Park, Bohyung Han

**Abstract**: We propose a novel class incremental learning approach by incorporating a feature augmentation technique motivated by adversarial attacks. We employ a classifier learned in the past to complement training examples rather than simply play a role as a teacher for knowledge distillation towards subsequent models. The proposed approach has a unique perspective to utilize the previous knowledge in class incremental learning since it augments features of arbitrary target classes using examples in other classes via adversarial attacks on a previously learned classifier. By allowing the cross-class feature augmentations, each class in the old tasks conveniently populates samples in the feature space, which alleviates the collapse of the decision boundaries caused by sample deficiency for the previous tasks, especially when the number of stored exemplars is small. This idea can be easily incorporated into existing class incremental learning algorithms without any architecture modification. Extensive experiments on the standard benchmarks show that our method consistently outperforms existing class incremental learning methods by significant margins in various scenarios, especially under an environment with an extremely limited memory budget.

摘要: 我们提出了一种新的类增量学习方法，该方法结合了一种基于对抗性攻击的特征增强技术。我们使用过去学习的分类器来补充训练实例，而不是简单地扮演老师的角色，将知识升华到后续模型。该方法在类增量学习中利用先验知识具有独特的视角，因为它通过对先前学习的分类器的对抗性攻击，利用其他类中的例子来扩充任意目标类的特征。通过允许跨类特征扩充，旧任务中的每一类都可以方便地在特征空间中填充样本，从而缓解了以前任务由于样本不足而导致的决策边界崩溃，特别是在存储的样本数量较少的情况下。这种思想可以很容易地融入到现有的类增量学习算法中，而不需要修改任何体系结构。在标准基准测试上的大量实验表明，在各种情况下，特别是在内存预算极其有限的环境下，我们的方法始终比现有的类增量学习方法有显著的优势。



## **17. Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging**

漏洞揭开面纱：对病理成像多模式视觉语言模型的敌意攻击 eess.IV

**SubmitDate**: 2024-01-08    [abs](http://arxiv.org/abs/2401.02565v2) [paper-pdf](http://arxiv.org/pdf/2401.02565v2)

**Authors**: Jai Prakash Veerla, Poojitha Thota, Partha Sai Guttikonda, Shirin Nilizadeh, Jacob M. Luber

**Abstract**: In the dynamic landscape of medical artificial intelligence, this study explores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP) model, a Vision Language Foundation model, under targeted adversarial conditions. Leveraging the Kather Colon dataset with 7,180 H&E images across nine tissue types, our investigation employs Projected Gradient Descent (PGD) adversarial attacks to intentionally induce misclassifications. The outcomes reveal a 100% success rate in manipulating PLIP's predictions, underscoring its susceptibility to adversarial perturbations. The qualitative analysis of adversarial examples delves into the interpretability challenges, shedding light on nuanced changes in predictions induced by adversarial manipulations. These findings contribute crucial insights into the interpretability, domain adaptation, and trustworthiness of Vision Language Models in medical imaging. The study emphasizes the pressing need for robust defenses to ensure the reliability of AI models.

摘要: 在医学人工智能的动态场景中，本研究探索了视觉语言基础模型-病理语言图像预训练(PLIP)模型在有针对性的对抗条件下的脆弱性。利用Kather Colon数据集，其中包含9种组织类型的7,180张H&E图像，我们的调查使用了投影梯度下降(PGD)对抗性攻击来故意诱导错误分类。结果显示，PLIP操纵预测的成功率为100%，突显出其易受对手干扰的影响。对抗性例子的定性分析深入到了可解释性的挑战，揭示了对抗性操纵导致的预测的细微变化。这些发现为医学成像中视觉语言模型的可解释性、领域适应性和可信性提供了重要的见解。该研究强调，迫切需要强大的防御措施，以确保人工智能模型的可靠性。



## **18. The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks**

分布式联合学习网络中对抗性节点放置的影响 cs.CR

Submitted to ICC 2024 conference

**SubmitDate**: 2024-01-08    [abs](http://arxiv.org/abs/2311.07946v2) [paper-pdf](http://arxiv.org/pdf/2311.07946v2)

**Authors**: Adam Piaseczny, Eric Ruzomberka, Rohit Parasnis, Christopher G. Brinton

**Abstract**: As Federated Learning (FL) grows in popularity, new decentralized frameworks are becoming widespread. These frameworks leverage the benefits of decentralized environments to enable fast and energy-efficient inter-device communication. However, this growing popularity also intensifies the need for robust security measures. While existing research has explored various aspects of FL security, the role of adversarial node placement in decentralized networks remains largely unexplored. This paper addresses this gap by analyzing the performance of decentralized FL for various adversarial placement strategies when adversaries can jointly coordinate their placement within a network. We establish two baseline strategies for placing adversarial node: random placement and network centrality-based placement. Building on this foundation, we propose a novel attack algorithm that prioritizes adversarial spread over adversarial centrality by maximizing the average network distance between adversaries. We show that the new attack algorithm significantly impacts key performance metrics such as testing accuracy, outperforming the baseline frameworks by between 9% and 66.5% for the considered setups. Our findings provide valuable insights into the vulnerabilities of decentralized FL systems, setting the stage for future research aimed at developing more secure and robust decentralized FL frameworks.

摘要: 随着联邦学习(FL)的流行，新的去中心化框架正在变得广泛。这些框架利用分散环境的优势，实现快速、节能的设备间通信。然而，这种日益增长的人气也加剧了采取强有力的安全措施的必要性。虽然现有的研究已经探索了FL安全的各个方面，但敌意节点放置在分散网络中的作用在很大程度上仍未被探索。本文通过分析当对手可以在一个网络内联合协调他们的放置时，分散的FL在不同的对手放置策略下的性能来解决这一差距。我们建立了两种放置敌意节点的基线策略：随机放置和基于网络中心性的放置。在此基础上，我们提出了一种新的攻击算法，该算法通过最大化对手之间的平均网络距离来优先考虑对手的传播而不是对手的中心。我们发现，新的攻击算法显著影响了测试准确率等关键性能指标，在所考虑的设置下，性能比基准框架高出9%到66.5%。我们的发现对去中心化FL系统的脆弱性提供了有价值的见解，为未来旨在开发更安全和健壮的去中心化FL框架的研究奠定了基础。



## **19. Logits Poisoning Attack in Federated Distillation**

联合蒸馏中的洛吉斯中毒攻击 cs.LG

13 pages, 3 figures, 5 tables

**SubmitDate**: 2024-01-08    [abs](http://arxiv.org/abs/2401.03685v1) [paper-pdf](http://arxiv.org/pdf/2401.03685v1)

**Authors**: Yuhan Tang, Zhiyuan Wu, Bo Gao, Tian Wen, Yuwei Wang, Sheng Sun

**Abstract**: Federated Distillation (FD) is a novel and promising distributed machine learning paradigm, where knowledge distillation is leveraged to facilitate a more efficient and flexible cross-device knowledge transfer in federated learning. By optimizing local models with knowledge distillation, FD circumvents the necessity of uploading large-scale model parameters to the central server, simultaneously preserving the raw data on local clients. Despite the growing popularity of FD, there is a noticeable gap in previous works concerning the exploration of poisoning attacks within this framework. This can lead to a scant understanding of the vulnerabilities to potential adversarial actions. To this end, we introduce FDLA, a poisoning attack method tailored for FD. FDLA manipulates logit communications in FD, aiming to significantly degrade model performance on clients through misleading the discrimination of private samples. Through extensive simulation experiments across a variety of datasets, attack scenarios, and FD configurations, we demonstrate that LPA effectively compromises client model accuracy, outperforming established baseline algorithms in this regard. Our findings underscore the critical need for robust defense mechanisms in FD settings to mitigate such adversarial threats.

摘要: 联合蒸馏(FD)是一种新颖的、有前途的分布式机器学习范式，其中利用知识蒸馏来促进联合学习中更高效、更灵活的跨设备知识转移。通过知识提炼优化本地模型，FD避免了将大规模模型参数上传到中心服务器的需要，同时将原始数据保存在本地客户端。尽管FD越来越受欢迎，但在这一框架内关于中毒攻击的探索存在着明显的空白。这可能导致对潜在敌对行动的脆弱性缺乏了解。为此，我们引入了FDLA，一种为FD量身定做的中毒攻击方法。FDLA操纵FD中的Logit通信，旨在通过误导私人样本的区分来显著降低客户的模型性能。通过对各种数据集、攻击场景和FD配置进行广泛的模拟实验，我们证明了LPA有效地折衷了客户端模型的准确性，在这方面优于已建立的基准算法。我们的发现强调了在FD环境中迫切需要强大的防御机制来缓解这种对抗性威胁。



## **20. Assessing the Influence of Different Types of Probing on Adversarial Decision-Making in a Deception Game**

在欺骗游戏中评估不同类型的探测对对抗性决策的影响 cs.CR

**SubmitDate**: 2024-01-08    [abs](http://arxiv.org/abs/2310.10662v3) [paper-pdf](http://arxiv.org/pdf/2310.10662v3)

**Authors**: Md Abu Sayed, Mohammad Ariful Islam Khan, Bryant A Allsup, Joshua Zamora, Palvi Aggarwal

**Abstract**: Deception, which includes leading cyber-attackers astray with false information, has shown to be an effective method of thwarting cyber-attacks. There has been little investigation of the effect of probing action costs on adversarial decision-making, despite earlier studies on deception in cybersecurity focusing primarily on variables like network size and the percentage of honeypots utilized in games. Understanding human decision-making when prompted with choices of various costs is essential in many areas such as in cyber security. In this paper, we will use a deception game (DG) to examine different costs of probing on adversarial decisions. To achieve this we utilized an IBLT model and a delayed feedback mechanism to mimic knowledge of human actions. Our results were taken from an even split of deception and no deception to compare each influence. It was concluded that probing was slightly taken less as the cost of probing increased. The proportion of attacks stayed relatively the same as the cost of probing increased. Although a constant cost led to a slight decrease in attacks. Overall, our results concluded that the different probing costs do not have an impact on the proportion of attacks whereas it had a slightly noticeable impact on the proportion of probing.

摘要: 欺骗，包括主要的网络攻击者误导虚假信息，已被证明是挫败网络攻击的有效方法。尽管早期关于网络安全中的欺骗的研究主要集中在网络规模和游戏中使用的蜜罐百分比等变量上，但关于探测行动成本对对抗性决策的影响的调查很少。在网络安全等许多领域，当被提示选择各种成本时，理解人类的决策是至关重要的。在本文中，我们将使用一个欺骗游戏(DG)来检查探测对手决策的不同成本。为了实现这一点，我们利用了IBLT模型和延迟反馈机制来模拟人类行为的知识。我们的结果取自欺骗性和非欺骗性的平分，以比较每种影响。结论是，随着探测成本的增加，探测的使用略有减少。随着探测成本的增加，攻击的比例保持相对不变。虽然不变的成本导致攻击略有减少。总体而言，我们的结果得出结论，不同的探测成本对攻击的比例没有影响，而对探测的比例有稍微明显的影响。



## **21. Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience**

提升防御：在对抗性训练和模型复原力水印之间架起桥梁 cs.LG

Accepted at DAI Workshop, AAAI 2024

**SubmitDate**: 2024-01-07    [abs](http://arxiv.org/abs/2312.14260v2) [paper-pdf](http://arxiv.org/pdf/2312.14260v2)

**Authors**: Janvi Thakkar, Giulio Zizzo, Sergio Maffeis

**Abstract**: Machine learning models are being used in an increasing number of critical applications; thus, securing their integrity and ownership is critical. Recent studies observed that adversarial training and watermarking have a conflicting interaction. This work introduces a novel framework to integrate adversarial training with watermarking techniques to fortify against evasion attacks and provide confident model verification in case of intellectual property theft. We use adversarial training together with adversarial watermarks to train a robust watermarked model. The key intuition is to use a higher perturbation budget to generate adversarial watermarks compared to the budget used for adversarial training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets to evaluate our proposed technique on various model stealing attacks. The results obtained consistently outperform the existing baseline in terms of robustness performance and further prove the resilience of this defense against pruning and fine-tuning removal attacks.

摘要: 机器学习模型正在越来越多的关键应用中使用；因此，确保它们的完整性和所有权至关重要。最近的研究发现，对抗性训练和水印之间存在相互冲突的作用。这项工作引入了一种新的框架，将对抗性训练与水印技术相结合，以加强对逃避攻击的防御，并在知识产权被盗的情况下提供可信的模型验证。我们使用对抗性训练和对抗性水印相结合的方法来训练一个健壮的水印模型。关键的直觉是，与用于对抗性训练的预算相比，使用更高的扰动预算来生成对抗性水印，从而避免冲突。我们使用MNIST和Fashion-MNIST数据集来评估我们提出的针对各种模型窃取攻击的技术。得到的结果在稳健性性能方面始终优于现有的基线，并进一步证明了该防御措施对剪枝和微调删除攻击的弹性。



## **22. Data-Driven Subsampling in the Presence of an Adversarial Actor**

对抗性参与者在场情况下的数据驱动子抽样 cs.LG

Accepted for publication at ICMLCN 2024

**SubmitDate**: 2024-01-07    [abs](http://arxiv.org/abs/2401.03488v1) [paper-pdf](http://arxiv.org/pdf/2401.03488v1)

**Authors**: Abu Shafin Mohammad Mahdee Jameel, Ahmed P. Mohamed, Jinho Yi, Aly El Gamal, Akshay Malhotra

**Abstract**: Deep learning based automatic modulation classification (AMC) has received significant attention owing to its potential applications in both military and civilian use cases. Recently, data-driven subsampling techniques have been utilized to overcome the challenges associated with computational complexity and training time for AMC. Beyond these direct advantages of data-driven subsampling, these methods also have regularizing properties that may improve the adversarial robustness of the modulation classifier. In this paper, we investigate the effects of an adversarial attack on an AMC system that employs deep learning models both for AMC and for subsampling. Our analysis shows that subsampling itself is an effective deterrent to adversarial attacks. We also uncover the most efficient subsampling strategy when an adversarial attack on both the classifier and the subsampler is anticipated.

摘要: 基于深度学习的自动调制分类(AMC)因其在军事和民用方面的潜在应用而受到广泛关注。最近，数据驱动的子采样技术已经被用来克服与AMC的计算复杂性和训练时间相关的挑战。除了数据驱动的子采样的这些直接优势之外，这些方法还具有可提高调制分类器的对抗性的正则化特性。在本文中，我们研究了敌意攻击对AMC系统的影响，该系统对AMC和子采样都采用了深度学习模型。我们的分析表明，二次抽样本身就是一种有效的对抗攻击的威慑。当分类器和子采样器都受到敌意攻击时，我们还发现了最有效的子采样策略。



## **23. Token-Modification Adversarial Attacks for Natural Language Processing: A Survey**

自然语言处理中的标记修改攻击：综述 cs.CL

Version 3: edited and expanded

**SubmitDate**: 2024-01-07    [abs](http://arxiv.org/abs/2103.00676v3) [paper-pdf](http://arxiv.org/pdf/2103.00676v3)

**Authors**: Tom Roth, Yansong Gao, Alsharif Abuadbba, Surya Nepal, Wei Liu

**Abstract**: Many adversarial attacks target natural language processing systems, most of which succeed through modifying the individual tokens of a document. Despite the apparent uniqueness of each of these attacks, fundamentally they are simply a distinct configuration of four components: a goal function, allowable transformations, a search method, and constraints. In this survey, we systematically present the different components used throughout the literature, using an attack-independent framework which allows for easy comparison and categorisation of components. Our work aims to serve as a comprehensive guide for newcomers to the field and to spark targeted research into refining the individual attack components.

摘要: 许多敌意攻击的目标是自然语言处理系统，其中大多数攻击是通过修改文档的单个令牌来成功的。尽管这些攻击看起来都是独一无二的，但从根本上说，它们只是四个组件的不同配置：目标函数、允许的转换、搜索方法和约束。在本调查中，我们系统地介绍了整个文献中使用的不同组件，使用了与攻击无关的框架，该框架允许对组件进行轻松的比较和分类。我们的工作旨在为该领域的新手提供全面的指导，并引发有针对性的研究，以完善个别攻击组件。



## **24. Affinity Uncertainty-based Hard Negative Mining in Graph Contrastive Learning**

图对比学习中基于亲和力不确定性的硬否定挖掘 cs.LG

Accepted to TNNLS

**SubmitDate**: 2024-01-07    [abs](http://arxiv.org/abs/2301.13340v2) [paper-pdf](http://arxiv.org/pdf/2301.13340v2)

**Authors**: Chaoxi Niu, Guansong Pang, Ling Chen

**Abstract**: Hard negative mining has shown effective in enhancing self-supervised contrastive learning (CL) on diverse data types, including graph CL (GCL). The existing hardness-aware CL methods typically treat negative instances that are most similar to the anchor instance as hard negatives, which helps improve the CL performance, especially on image data. However, this approach often fails to identify the hard negatives but leads to many false negatives on graph data. This is mainly due to that the learned graph representations are not sufficiently discriminative due to oversmooth representations and/or non-independent and identically distributed (non-i.i.d.) issues in graph data. To tackle this problem, this article proposes a novel approach that builds a discriminative model on collective affinity information (i.e., two sets of pairwise affinities between the negative instances and the anchor instance) to mine hard negatives in GCL. In particular, the proposed approach evaluates how confident/uncertain the discriminative model is about the affinity of each negative instance to an anchor instance to determine its hardness weight relative to the anchor instance. This uncertainty information is then incorporated into the existing GCL loss functions via a weighting term to enhance their performance. The enhanced GCL is theoretically grounded that the resulting GCL loss is equivalent to a triplet loss with an adaptive margin being exponentially proportional to the learned uncertainty of each negative instance. Extensive experiments on ten graph datasets show that our approach does the following: 1) consistently enhances different state-of-the-art (SOTA) GCL methods in both graph and node classification tasks and 2) significantly improves their robustness against adversarial attacks. Code is available at https://github.com/mala-lab/AUGCL.

摘要: 硬负挖掘在增强包括图对比学习(GCL)在内的不同数据类型上的自我监督对比学习(CL)方面表现出了有效的效果。现有的硬度感知CL方法通常将与锚实例最相似的否定实例视为硬否定，这有助于提高CL的性能，特别是在图像数据上。然而，这种方法往往不能识别硬否定，而是导致图数据上的许多假否定。这主要是因为由于过度光滑的表示和/或非独立且同分布的(非I.I.D.)，学习的图表示不具有足够的区分性。图形数据中的问题。针对这一问题，本文提出了一种基于群体亲和力信息的判别模型(即否定实例和锚定实例之间的两组成对亲和度)来挖掘GCL中的硬否定。特别地，该方法评估判别模型关于每个否定实例对锚实例的亲和度的置信度/不确定性，以确定其相对于锚实例的硬度权重。然后，这种不确定性信息通过加权项被合并到现有的GCL损失函数中，以提高它们的性能。增强型GCL的理论基础是，由此产生的GCL损失等同于三重损失，其自适应裕度与每个负实例的学习不确定性成指数正比。在10个图数据集上的大量实验表明，我们的方法做了以下工作：1)在图和节点分类任务中一致地提高了不同的最新技术(SOTA)GCL方法；2)显著提高了它们对对手攻击的健壮性。代码可在https://github.com/mala-lab/AUGCL.上找到



## **25. Data-Dependent Stability Analysis of Adversarial Training**

对抗性训练的数据依赖稳定性分析 cs.LG

**SubmitDate**: 2024-01-06    [abs](http://arxiv.org/abs/2401.03156v1) [paper-pdf](http://arxiv.org/pdf/2401.03156v1)

**Authors**: Yihan Wang, Shuang Liu, Xiao-Shan Gao

**Abstract**: Stability analysis is an essential aspect of studying the generalization ability of deep learning, as it involves deriving generalization bounds for stochastic gradient descent-based training algorithms. Adversarial training is the most widely used defense against adversarial example attacks. However, previous generalization bounds for adversarial training have not included information regarding the data distribution. In this paper, we fill this gap by providing generalization bounds for stochastic gradient descent-based adversarial training that incorporate data distribution information. We utilize the concepts of on-average stability and high-order approximate Lipschitz conditions to examine how changes in data distribution and adversarial budget can affect robust generalization gaps. Our derived generalization bounds for both convex and non-convex losses are at least as good as the uniform stability-based counterparts which do not include data distribution information. Furthermore, our findings demonstrate how distribution shifts from data poisoning attacks can impact robust generalization.

摘要: 稳定性分析是研究深度学习泛化能力的一个重要方面，因为它涉及到推导基于随机梯度下降的训练算法的泛化界限。对抗性训练是对抗对抗性范例攻击最广泛使用的防御手段。然而，以前的对抗性训练的泛化界限没有包括关于数据分布的信息。在本文中，我们通过提供包含数据分布信息的基于随机梯度下降的对抗性训练的泛化界限来填补这一空白。我们利用平均稳定性和高阶近似Lipschitz条件的概念来检验数据分布和对抗性预算的变化如何影响稳健的泛化差距。我们推导出的凸损失和非凸损失的泛化界至少与基于一致稳定性的相应界一样好，后者不包括数据分布信息。此外，我们的发现表明，数据中毒攻击的分布变化如何影响健壮的泛化。



## **26. Transferable Learned Image Compression-Resistant Adversarial Perturbations**

可转移的学习图像抗压缩对抗扰动 cs.CV

Accepted as poster at Data Compression Conference 2024 (DCC 2024)

**SubmitDate**: 2024-01-06    [abs](http://arxiv.org/abs/2401.03115v1) [paper-pdf](http://arxiv.org/pdf/2401.03115v1)

**Authors**: Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen

**Abstract**: Adversarial attacks can readily disrupt the image classification system, revealing the vulnerability of DNN-based recognition tasks. While existing adversarial perturbations are primarily applied to uncompressed images or compressed images by the traditional image compression method, i.e., JPEG, limited studies have investigated the robustness of models for image classification in the context of DNN-based image compression. With the rapid evolution of advanced image compression, DNN-based learned image compression has emerged as the promising approach for transmitting images in many security-critical applications, such as cloud-based face recognition and autonomous driving, due to its superior performance over traditional compression. Therefore, there is a pressing need to fully investigate the robustness of a classification system post-processed by learned image compression. To bridge this research gap, we explore the adversarial attack on a new pipeline that targets image classification models that utilize learned image compressors as pre-processing modules. Furthermore, to enhance the transferability of perturbations across various quality levels and architectures of learned image compression models, we introduce a saliency score-based sampling method to enable the fast generation of transferable perturbation. Extensive experiments with popular attack methods demonstrate the enhanced transferability of our proposed method when attacking images that have been post-processed with different learned image compression models.

摘要: 对抗性攻击很容易破坏图像分类系统，揭示了基于DNN的识别任务的脆弱性。虽然现有的对抗性扰动主要应用于未压缩图像或通过传统图像压缩方法压缩的图像，即，JPEG，有限的研究已经调查了在基于DNN的图像压缩的背景下用于图像分类的模型的鲁棒性。随着高级图像压缩的快速发展，基于DNN的学习图像压缩已成为许多安全关键应用（如基于云的人脸识别和自动驾驶）中传输图像的有前途的方法，因为它比传统压缩具有更优越的性能。因此，迫切需要充分研究学习图像压缩后处理的分类系统的鲁棒性。为了弥合这一研究差距，我们探索了对一个新管道的对抗性攻击，该管道针对利用学习的图像压缩器作为预处理模块的图像分类模型。此外，为了增强扰动在各种质量级别和学习图像压缩模型架构中的可转移性，我们引入了一种基于显著性分数的采样方法，以快速生成可转移的扰动。广泛的实验与流行的攻击方法证明了增强的可移植性，我们所提出的方法时，攻击的图像已与不同的学习图像压缩模型后处理。



## **27. Lotto: Secure Participant Selection against Adversarial Servers in Federated Learning**

乐透：联合学习中对抗敌意服务器的安全参与者选择 cs.CR

20 pages, 14 figures

**SubmitDate**: 2024-01-05    [abs](http://arxiv.org/abs/2401.02880v1) [paper-pdf](http://arxiv.org/pdf/2401.02880v1)

**Authors**: Zhifeng Jiang, Peng Ye, Shiqi He, Wei Wang, Ruichuan Chen, Bo Li

**Abstract**: In Federated Learning (FL), common privacy-preserving technologies, such as secure aggregation and distributed differential privacy, rely on the critical assumption of an honest majority among participants to withstand various attacks. In practice, however, servers are not always trusted, and an adversarial server can strategically select compromised clients to create a dishonest majority, thereby undermining the system's security guarantees. In this paper, we present Lotto, an FL system that addresses this fundamental, yet underexplored issue by providing secure participant selection against an adversarial server. Lotto supports two selection algorithms: random and informed. To ensure random selection without a trusted server, Lotto enables each client to autonomously determine their participation using verifiable randomness. For informed selection, which is more vulnerable to manipulation, Lotto approximates the algorithm by employing random selection within a refined client pool. Our theoretical analysis shows that Lotto effectively restricts the number of server-selected compromised clients, thus ensuring an honest majority among participants. Large-scale experiments further reveal that Lotto achieves time-to-accuracy performance comparable to that of insecure selection methods, indicating a low computational overhead for secure selection.

摘要: 在联邦学习(FL)中，常见的隐私保护技术，如安全聚合和分布式差异隐私，依赖于参与者之间诚实多数的关键假设来抵御各种攻击。然而，在实践中，服务器并不总是可信的，敌意服务器可以策略性地选择受攻击的客户端来制造不诚实的多数，从而破坏系统的安全保证。在本文中，我们提出了乐透，一个FL系统，解决了这个基本的，但探索不足的问题，通过提供安全的参与者选择对抗敌对的服务器。乐透支持两种选择算法：随机和通知。为了确保在没有可信服务器的情况下随机选择，乐透使每个客户端能够使用可验证的随机性自主确定他们的参与。对于更容易受到操纵的知情选择，乐透通过在改进的客户机池中使用随机选择来近似算法。我们的理论分析表明，乐透有效地限制了服务器选择的受攻击客户端的数量，从而确保了参与者中诚实的多数。大规模实验进一步表明，乐透算法的时间精度性能与非安全选择方法相当，表明安全选择方法具有较低的计算开销。



## **28. PromptBench: A Unified Library for Evaluation of Large Language Models**

PromptBitch：大型语言模型评估的统一库 cs.AI

An extension to PromptBench (arXiv:2306.04528) for unified evaluation  of LLMs using the same name; code: https://github.com/microsoft/promptbench

**SubmitDate**: 2024-01-05    [abs](http://arxiv.org/abs/2312.07910v2) [paper-pdf](http://arxiv.org/pdf/2312.07910v2)

**Authors**: Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie

**Abstract**: The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.

摘要: 大型语言模型(LLM)的评估对于评估其性能和降低潜在的安全风险至关重要。在本文中，我们介绍了一个用于评估LLMS的统一库PromptBitch.它由几个易于研究人员使用和扩展的关键组件组成：即时构建、即时工程、数据集和模型加载、对抗性即时攻击、动态评估协议和分析工具。PromptBitch是一个开放的、通用的、灵活的研究代码库，可以在创建新的基准、部署下游应用程序和设计新的评估协议方面促进原创研究。该代码可在https://github.com/microsoft/promptbench上获得，并将继续受到支持。



## **29. Enhancing targeted transferability via feature space fine-tuning**

通过特征空间微调增强目标可转移性 cs.CV

9 pages, 10 figures, accepted by 2024ICASSP

**SubmitDate**: 2024-01-05    [abs](http://arxiv.org/abs/2401.02727v1) [paper-pdf](http://arxiv.org/pdf/2401.02727v1)

**Authors**: Hui Zeng, Biwei Chen, Anjie Peng

**Abstract**: Adversarial examples (AEs) have been extensively studied due to their potential for privacy protection and inspiring robust neural networks. However, making a targeted AE transferable across unknown models remains challenging. In this paper, to alleviate the overfitting dilemma common in an AE crafted by existing simple iterative attacks, we propose fine-tuning it in the feature space. Specifically, starting with an AE generated by a baseline attack, we encourage the features that contribute to the target class and discourage the features that contribute to the original class in a middle layer of the source model. Extensive experiments demonstrate that only a few iterations of fine-tuning can boost existing attacks in terms of targeted transferability nontrivially and universally. Our results also verify that the simple iterative attacks can yield comparable or even better transferability than the resource-intensive methods, which rely on training target-specific classifiers or generators with additional data. The code is available at: github.com/zengh5/TA_feature_FT.

摘要: 对抗样本（AE）由于其隐私保护和激发强大神经网络的潜力而得到了广泛的研究。然而，使目标AE在未知模型之间可转移仍然具有挑战性。在本文中，为了缓解现有简单迭代攻击制作的AE中常见的过拟合困境，我们提出在特征空间中对其进行微调。具体来说，从基线攻击生成的AE开始，我们鼓励对目标类有贡献的功能，并阻止对源模型中间层中的原始类有贡献的功能。大量的实验表明，只有几次迭代的微调可以提高现有的攻击在有针对性的可转移性的非平凡和普遍。我们的研究结果还验证了简单的迭代攻击可以产生与资源密集型方法相当甚至更好的可转移性，资源密集型方法依赖于使用额外数据训练特定于目标的分类器或生成器。该代码可在github.com/zengh5/TA_feature_FT上获得。



## **30. Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration**

校准攻击：一种针对校准的对抗性攻击框架 cs.LG

**SubmitDate**: 2024-01-05    [abs](http://arxiv.org/abs/2401.02718v1) [paper-pdf](http://arxiv.org/pdf/2401.02718v1)

**Authors**: Stephen Obadinma, Xiaodan Zhu, Hongyu Guo

**Abstract**: We introduce a new framework of adversarial attacks, named calibration attacks, in which the attacks are generated and organized to trap victim models to be miscalibrated without altering their original accuracy, hence seriously endangering the trustworthiness of the models and any decision-making based on their confidence scores. Specifically, we identify four novel forms of calibration attacks: underconfidence attacks, overconfidence attacks, maximum miscalibration attacks, and random confidence attacks, in both the black-box and white-box setups. We then test these new attacks on typical victim models with comprehensive datasets, demonstrating that even with a relatively low number of queries, the attacks can create significant calibration mistakes. We further provide detailed analyses to understand different aspects of calibration attacks. Building on that, we investigate the effectiveness of widely used adversarial defences and calibration methods against these types of attacks, which then inspires us to devise two novel defences against such calibration attacks.

摘要: 我们引入了一种新的对抗性攻击框架，称为校准攻击，在不改变受害者模型原有精度的情况下，生成和组织攻击以捕获需要错误校准的受害者模型，从而严重危及模型的可信性和任何基于其置信度的决策。具体地说，我们识别了四种新的校准攻击形式：在黑盒和白盒设置中的低信任度攻击、过度自信攻击、最大误校准度攻击和随机信任攻击。然后，我们使用全面的数据集在典型的受害者模型上测试这些新攻击，表明即使查询次数相对较少，攻击也可能造成严重的校准错误。我们进一步提供了详细的分析，以了解校准攻击的不同方面。在此基础上，我们研究了广泛使用的对抗性防御和校准方法对这类攻击的有效性，这促使我们设计了两种针对此类校准攻击的新防御方法。



## **31. Game Theory for Adversarial Attacks and Defenses**

对抗性攻防的博弈论 cs.LG

With the agreement of my coauthors, I would like to withdraw the  manuscript "Game Theory for Adversarial Attacks and Defenses". Some  experimental procedures were not included in the manuscript, which makes a  part of important claims not meaningful

**SubmitDate**: 2024-01-05    [abs](http://arxiv.org/abs/2110.06166v4) [paper-pdf](http://arxiv.org/pdf/2110.06166v4)

**Authors**: Shorya Sharma

**Abstract**: Adversarial attacks can generate adversarial inputs by applying small but intentionally worst-case perturbations to samples from the dataset, which leads to even state-of-the-art deep neural networks outputting incorrect answers with high confidence. Hence, some adversarial defense techniques are developed to improve the security and robustness of the models and avoid them being attacked. Gradually, a game-like competition between attackers and defenders formed, in which both players would attempt to play their best strategies against each other while maximizing their own payoffs. To solve the game, each player would choose an optimal strategy against the opponent based on the prediction of the opponent's strategy choice. In this work, we are on the defensive side to apply game-theoretic approaches on defending against attacks. We use two randomization methods, random initialization and stochastic activation pruning, to create diversity of networks. Furthermore, we use one denoising technique, super resolution, to improve models' robustness by preprocessing images before attacks. Our experimental results indicate that those three methods can effectively improve the robustness of deep-learning neural networks.

摘要: 对抗性攻击可以通过对数据集的样本施加小但故意的最坏情况扰动来生成对抗性输入，这导致即使是最先进的深度神经网络也会以高置信度输出错误的答案。因此，一些对抗性防御技术的发展，以提高模型的安全性和鲁棒性，避免他们受到攻击。渐渐地，进攻者和防守者之间形成了一种类似游戏的竞争，双方都试图在最大化自己收益的同时，对对方采取最佳策略。为了解决这个问题，每个玩家都会根据对对手策略选择的预测来选择一个最佳策略。在这项工作中，我们是在防守端应用博弈论的方法来抵御攻击。我们使用两种随机化方法，随机初始化和随机激活修剪，以创建网络的多样性。此外，我们使用一种去噪技术，超分辨率，通过预处理图像攻击前，以提高模型的鲁棒性。实验结果表明，这三种方法都能有效提高深度学习神经网络的鲁棒性。



## **32. Secure Control of Connected and Automated Vehicles Using Trust-Aware Robust Event-Triggered Control Barrier Functions**

使用信任感知的健壮事件触发控制屏障功能实现互联和自动化车辆的安全控制 eess.SY

arXiv admin note: substantial text overlap with arXiv:2305.16818

**SubmitDate**: 2024-01-05    [abs](http://arxiv.org/abs/2401.02306v2) [paper-pdf](http://arxiv.org/pdf/2401.02306v2)

**Authors**: H M Sabbir Ahmad, Ehsan Sabouni, Akua Dickson, Wei Xiao, Christos G. Cassandras, Wenchao Li

**Abstract**: We address the security of a network of Connected and Automated Vehicles (CAVs) cooperating to safely navigate through a conflict area (e.g., traffic intersections, merging roadways, roundabouts). Previous studies have shown that such a network can be targeted by adversarial attacks causing traffic jams or safety violations ending in collisions. We focus on attacks targeting the V2X communication network used to share vehicle data and consider as well uncertainties due to noise in sensor measurements and communication channels. To combat these, motivated by recent work on the safe control of CAVs, we propose a trust-aware robust event-triggered decentralized control and coordination framework that can provably guarantee safety. We maintain a trust metric for each vehicle in the network computed based on their behavior and used to balance the tradeoff between conservativeness (when deeming every vehicle as untrustworthy) and guaranteed safety and security. It is important to highlight that our framework is invariant to the specific choice of the trust framework. Based on this framework, we propose an attack detection and mitigation scheme which has twofold benefits: (i) the trust framework is immune to false positives, and (ii) it provably guarantees safety against false positive cases. We use extensive simulations (in SUMO and CARLA) to validate the theoretical guarantees and demonstrate the efficacy of our proposed scheme to detect and mitigate adversarial attacks.

摘要: 我们致力于解决互联和自动化车辆(CAV)网络的安全问题，这些车辆通过协作安全地通过冲突区域(例如，交通路口、合并道路、环形交叉路口)。以前的研究表明，这样的网络可以成为导致交通拥堵或以碰撞结束的安全违规行为的对抗性攻击的目标。我们专注于针对用于共享车辆数据的V2X通信网络的攻击，并考虑由于传感器测量和通信通道中的噪声而产生的不确定性。为了应对这些问题，基于最近在CAV安全控制方面的工作，我们提出了一个信任感知的、健壮的、事件触发的分布式控制和协调框架，该框架能够有效地保证安全。我们为网络中的每辆车维护一个基于其行为计算的信任度量，用于平衡保守性(当认为每辆车不值得信任时)与保证的安全和保障之间的权衡。必须强调的是，我们的框架与信任框架的具体选择是不变的。基于该框架，我们提出了一种攻击检测和缓解方案，该方案具有两个优点：(I)信任框架不受误报的影响；(Ii)它可证明地保证了对误报情况的安全性。我们使用大量的仿真(在相扑和CALA中)来验证理论上的保证，并展示了我们所提出的方案在检测和缓解敌意攻击方面的有效性。



## **33. MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack**

MalModel：利用黑盒后门攻击隐藏移动深度学习模型中的恶意负载 cs.CR

Due to the limitation "The abstract field cannot be longer than 1,920  characters", the abstract here is shorter than that in the PDF file

**SubmitDate**: 2024-01-05    [abs](http://arxiv.org/abs/2401.02659v1) [paper-pdf](http://arxiv.org/pdf/2401.02659v1)

**Authors**: Jiayi Hua, Kailong Wang, Meizhen Wang, Guangdong Bai, Xiapu Luo, Haoyu Wang

**Abstract**: Mobile malware has become one of the most critical security threats in the era of ubiquitous mobile computing. Despite the intensive efforts from security experts to counteract it, recent years have still witnessed a rapid growth of identified malware samples. This could be partly attributed to the newly-emerged technologies that may constantly open up under-studied attack surfaces for the adversaries. One typical example is the recently-developed mobile machine learning (ML) framework that enables storing and running deep learning (DL) models on mobile devices. Despite obvious advantages, this new feature also inadvertently introduces potential vulnerabilities (e.g., on-device models may be modified for malicious purposes). In this work, we propose a method to generate or transform mobile malware by hiding the malicious payloads inside the parameters of deep learning models, based on a strategy that considers four factors (layer type, layer number, layer coverage and the number of bytes to replace). Utilizing the proposed method, we can run malware in DL mobile applications covertly with little impact on the model performance (i.e., as little as 0.4% drop in accuracy and at most 39ms latency overhead).

摘要: 在移动计算无处不在的时代，移动恶意软件已经成为最关键的安全威胁之一。尽管安全专家做出了密集的努力来对抗它，但近年来识别出的恶意软件样本仍然快速增长。这在一定程度上可以归因于新出现的技术，这些技术可能会不断为对手打开研究不足的攻击面。一个典型的例子是最近开发的移动机器学习(ML)框架，该框架允许在移动设备上存储和运行深度学习(DL)模型。尽管有明显的优势，但这一新功能也在不经意间引入了潜在的漏洞(例如，设备上的模型可能会被恶意修改)。在这项工作中，我们提出了一种通过将恶意负载隐藏在深度学习模型的参数中来生成或转换移动恶意软件的方法，该方法考虑了四个因素(层类型、层数、层覆盖率和替换字节数)。利用该方法，我们可以在不影响模型性能的情况下，隐蔽地在下行移动应用程序中运行恶意软件(即，准确率仅下降0.4%，延迟开销最多39ms)。



## **34. A Random Ensemble of Encrypted models for Enhancing Robustness against Adversarial Examples**

一种加密模型的随机集成以增强对敌意示例的稳健性 cs.CR

4 pages

**SubmitDate**: 2024-01-05    [abs](http://arxiv.org/abs/2401.02633v1) [paper-pdf](http://arxiv.org/pdf/2401.02633v1)

**Authors**: Ryota Iijima, Sayaka Shiota, Hitoshi Kiya

**Abstract**: Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In addition, AEs have adversarial transferability, which means AEs generated for a source model can fool another black-box model (target model) with a non-trivial probability. In previous studies, it was confirmed that the vision transformer (ViT) is more robust against the property of adversarial transferability than convolutional neural network (CNN) models such as ConvMixer, and moreover encrypted ViT is more robust than ViT without any encryption. In this article, we propose a random ensemble of encrypted ViT models to achieve much more robust models. In experiments, the proposed scheme is verified to be more robust against not only black-box attacks but also white-box ones than convention methods.

摘要: 众所周知，深度神经网络(DNN)很容易受到敌意例子(AEs)的攻击。此外，AEs具有对抗性，这意味着为一个源模型生成的AEs可以以非平凡的概率愚弄另一个黑盒模型(目标模型)。在以往的研究中，已经证实视觉转换器(VIT)比卷积神经网络(CNN)模型(如ConvMixer)具有更好的鲁棒性，而且加密后的VIT比未加密的VIT具有更好的鲁棒性。在本文中，我们提出了一种加密的VIT模型的随机集成，以实现更健壮的模型。实验证明，该方案不仅对黑盒攻击，而且对白盒攻击都比传统方法具有更好的鲁棒性。



## **35. A Practical Survey on Emerging Threats from AI-driven Voice Attacks: How Vulnerable are Commercial Voice Control Systems?**

关于人工智能驱动的语音攻击的新兴威胁的实用调查：商业语音控制系统有多脆弱？ cs.CR

14 pages

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2312.06010v2) [paper-pdf](http://arxiv.org/pdf/2312.06010v2)

**Authors**: Yuanda Wang, Qiben Yan, Nikolay Ivanov, Xun Chen

**Abstract**: The emergence of Artificial Intelligence (AI)-driven audio attacks has revealed new security vulnerabilities in voice control systems. While researchers have introduced a multitude of attack strategies targeting voice control systems (VCS), the continual advancements of VCS have diminished the impact of many such attacks. Recognizing this dynamic landscape, our study endeavors to comprehensively assess the resilience of commercial voice control systems against a spectrum of malicious audio attacks. Through extensive experimentation, we evaluate six prominent attack techniques across a collection of voice control interfaces and devices. Contrary to prevailing narratives, our results suggest that commercial voice control systems exhibit enhanced resistance to existing threats. Particularly, our research highlights the ineffectiveness of white-box attacks in black-box scenarios. Furthermore, the adversaries encounter substantial obstacles in obtaining precise gradient estimations during query-based interactions with commercial systems, such as Apple Siri and Samsung Bixby. Meanwhile, we find that current defense strategies are not completely immune to advanced attacks. Our findings contribute valuable insights for enhancing defense mechanisms in VCS. Through this survey, we aim to raise awareness within the academic community about the security concerns of VCS and advocate for continued research in this crucial area.

摘要: 人工智能(AI)驱动的音频攻击的出现揭示了语音控制系统中的新安全漏洞。虽然研究人员针对语音控制系统(VCS)引入了多种攻击策略，但VCS的不断进步削弱了许多此类攻击的影响。认识到这种动态格局，我们的研究努力全面评估商业语音控制系统对一系列恶意音频攻击的弹性。通过广泛的实验，我们评估了一系列语音控制接口和设备上的六种重要攻击技术。与流行的说法相反，我们的结果表明，商业语音控制系统对现有威胁的抵抗力增强了。特别是，我们的研究突出了白盒攻击在黑盒场景中的无效。此外，在与Apple Siri和Samsung Bixby等商业系统进行基于查询的交互期间，对手在获得精确的梯度估计方面遇到了巨大的障碍。与此同时，我们发现，当前的防御策略并不能完全免受高级攻击的影响。我们的发现为增强VCS的防御机制提供了有价值的见解。通过这项调查，我们旨在提高学术界对VCS安全问题的认识，并倡导在这一关键领域继续研究。



## **36. Evasive Hardware Trojan through Adversarial Power Trace**

通过对抗性电源跟踪规避硬件木马 cs.CR

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2401.02342v1) [paper-pdf](http://arxiv.org/pdf/2401.02342v1)

**Authors**: Behnam Omidi, Khaled N. Khasawneh, Ihsen Alouani

**Abstract**: The globalization of the Integrated Circuit (IC) supply chain, driven by time-to-market and cost considerations, has made ICs vulnerable to hardware Trojans (HTs). Against this threat, a promising approach is to use Machine Learning (ML)-based side-channel analysis, which has the advantage of being a non-intrusive method, along with efficiently detecting HTs under golden chip-free settings. In this paper, we question the trustworthiness of ML-based HT detection via side-channel analysis. We introduce a HT obfuscation (HTO) approach to allow HTs to bypass this detection method. Rather than theoretically misleading the model by simulated adversarial traces, a key aspect of our approach is the design and implementation of adversarial noise as part of the circuitry, alongside the HT. We detail HTO methodologies for ASICs and FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly, we found that HTO can be implemented with only a single transistor for ASIC designs to generate adversarial power traces that can fool the defense with 100% efficiency. We also efficiently implemented our approach on a Spartan 6 Xilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii) ring-oscillator-based design. Additionally, we assess the efficiency of countermeasures like spectral domain analysis, and we show that an adaptive attacker can still design evasive HTOs by constraining the design with a spectral noise budget. In addition, while adversarial training (AT) offers higher protection against evasive HTs, AT models suffer from a considerable utility loss, potentially rendering them unsuitable for such security application. We believe this research represents a significant step in understanding and exploiting ML vulnerabilities in a hardware security context, and we make all resources and designs openly available online: https://dev.d18uu4lqwhbmka.amplifyapp.com

摘要: 集成电路（IC）供应链的全球化，受上市时间和成本考虑的驱动，使IC容易受到硬件木马（HT）的攻击。针对这种威胁，一种有前途的方法是使用基于机器学习（ML）的侧信道分析，它具有非侵入性方法的优点，同时可以在黄金无芯片设置下有效检测HT。在本文中，我们质疑的可信度ML为基础的HT检测通过侧信道分析。我们引入了HT混淆（HTO）的方法，让HT绕过这种检测方法。我们的方法的一个关键方面是设计和实现对抗性噪声作为电路的一部分，而不是通过模拟对抗性痕迹在理论上误导模型。我们详细介绍了ASIC和FPGA的HTO方法，并使用TrustHub基准评估我们的方法。有趣的是，我们发现HTO可以仅用ASIC设计的单个晶体管来实现，以生成可以以100%的效率欺骗防御的对抗性功率迹线。我们还使用2种不同的变体在Spartan 6 Xilinx FPGA上有效地实现了我们的方法：（i）基于DSP切片的设计，以及（ii）基于环形振荡器的设计。此外，我们评估的对策，如频谱域分析的效率，我们表明，自适应攻击者仍然可以设计规避HTO通过限制设计与频谱噪声预算。此外，虽然对抗训练（AT）提供了更高的保护，以防止规避HT，AT模型遭受了相当大的效用损失，可能使他们不适合这种安全应用。我们相信这项研究代表了在硬件安全环境中理解和利用ML漏洞的重要一步，我们将所有资源和设计公开在线提供：https://dev.d18uu4lqwhbmka.amplifyapp.com



## **37. Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It**

用于虚假新闻检测的对抗性数据中毒：如何使模型在不修改目标新闻的情况下对其进行错误分类 cs.LG

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2312.15228v2) [paper-pdf](http://arxiv.org/pdf/2312.15228v2)

**Authors**: Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccini, Irene Amerini, Fabrizio Silvestri

**Abstract**: Fake news detection models are critical to countering disinformation but can be manipulated through adversarial attacks. In this position paper, we analyze how an attacker can compromise the performance of an online learning detector on specific news content without being able to manipulate the original target news. In some contexts, such as social networks, where the attacker cannot exert complete control over all the information, this scenario can indeed be quite plausible. Therefore, we show how an attacker could potentially introduce poisoning data into the training data to manipulate the behavior of an online learning method. Our initial findings reveal varying susceptibility of logistic regression models based on complexity and attack type.

摘要: 假新闻检测模型对于打击虚假信息至关重要，但可以通过对抗性攻击进行操纵。在这份立场文件中，我们分析了攻击者如何在无法操纵原始目标新闻的情况下，损害在线学习检测器对特定新闻内容的性能。在某些情况下，如社交网络，攻击者无法完全控制所有信息，这种情况确实是相当合理的。因此，我们展示了攻击者如何潜在地将中毒数据引入到训练数据中，以操纵在线学习方法的行为。我们的初步研究结果揭示了基于复杂性和攻击类型的逻辑回归模型的不同易感性。



## **38. Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective**

对抗性机器学习中的攻击：基于生命周期的系统综述 cs.LG

35 pages, 4 figures, 10 tables, 313 reference papers

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2302.09457v2) [paper-pdf](http://arxiv.org/pdf/2302.09457v2)

**Authors**: Baoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, Siwei Lyu

**Abstract**: Adversarial machine learning (AML) studies the adversarial phenomenon of machine learning, which may make inconsistent or unexpected predictions with humans. Some paradigms have been recently developed to explore this adversarial phenomenon occurring at different stages of a machine learning system, such as backdoor attack occurring at the pre-training, in-training and inference stage; weight attack occurring at the post-training, deployment and inference stage; adversarial attack occurring at the inference stage. However, although these adversarial paradigms share a common goal, their developments are almost independent, and there is still no big picture of AML. In this work, we aim to provide a unified perspective to the AML community to systematically review the overall progress of this field. We firstly provide a general definition about AML, and then propose a unified mathematical framework to covering existing attack paradigms. According to the proposed unified framework, we build a full taxonomy to systematically categorize and review existing representative methods for each paradigm. Besides, using this unified framework, it is easy to figure out the connections and differences among different attack paradigms, which may inspire future researchers to develop more advanced attack paradigms. Finally, to facilitate the viewing of the built taxonomy and the related literature in adversarial machine learning, we further provide a website, \ie, \url{http://adversarial-ml.com}, where the taxonomies and literature will be continuously updated.

摘要: 对抗性机器学习(AML)研究机器学习中的对抗性现象，它可能会做出与人类不一致或意想不到的预测。最近已经发展了一些范例来探索这种发生在机器学习系统的不同阶段的对抗性现象，例如发生在预训练、训练中和推理阶段的后门攻击；发生在训练后、部署和推理阶段的权重攻击；发生在推理阶段的对抗性攻击。然而，尽管这些对抗性范式有着共同的目标，但它们的发展几乎是独立的，仍然没有AML的大图景。在这项工作中，我们旨在为AML社区提供一个统一的视角，以便系统地审查这一领域的整体进展。我们首先给出了AML的一般定义，然后提出了一个统一的数学框架来覆盖现有的攻击范型。根据提出的统一框架，我们建立了一个完整的分类法，对每个范式的现有代表性方法进行了系统的分类和审查。此外，使用这个统一的框架，可以很容易地找出不同攻击范式之间的联系和区别，这可能会启发未来的研究人员开发更高级的攻击范式。最后，为了便于在对抗性机器学习中查看所建立的分类和相关文献，我们还提供了一个网站，\即\url{http://adversarial-ml.com}，，在那里分类和文献将不断更新。



## **39. Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing**

基于增量随机平滑的视觉语言模型快速认证 cs.CV

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2311.09024v2) [paper-pdf](http://arxiv.org/pdf/2311.09024v2)

**Authors**: A K Nirala, A Joshi, C Hegde, S Sarkar

**Abstract**: A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild.   In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base "training" set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.

摘要: 像CLIP这样的深度视觉语言模型的一个主要好处是，它们实现了零镜头开放式词汇分类；用户能够在推理时通过自然语言提示来定义新的类别标签。然而，尽管基于片段的零射击分类器在一系列域转换中表现出了具有竞争力的性能，但它们仍然非常容易受到对手的攻击。因此，确保这些模型的稳健性对于它们在野外的可靠部署至关重要。在这项工作中，我们引入了开放词汇认证(OVC)，这是一种通过随机平滑技术为CLIP等开放词汇模型设计的快速认证方法。对于给定的基本“训练”提示集及其相应的认证片段分类器，OVC依赖于这样的观察，即具有新提示的分类器可以被视为基本训练集中附近分类器的扰动版本。因此，OVC可以使用一种增量随机平滑的变体来快速验证新的分类器。通过使用缓存技巧，我们在新奇提示的认证过程中实现了大约两个数量级的加速。为了获得进一步的(启发式)加速比，OVC使用多变量正态分布来近似给定输入的嵌入空间，从而绕过了通过视觉主干的前向传递进行采样的需要。我们通过在CIFAR-10和ImageNet测试数据集上使用多个视觉语言主干进行实验评估，验证了OVC的有效性。



## **40. InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models**

InstructTA：针对大型视觉语言模型的指令调整的定向攻击 cs.CV

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2312.01886v2) [paper-pdf](http://arxiv.org/pdf/2312.01886v2)

**Authors**: Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang

**Abstract**: Large vision-language models (LVLMs) have demonstrated their incredible capability in image understanding and response generation. However, this rich visual interaction also makes LVLMs vulnerable to adversarial examples. In this paper, we formulate a novel and practical gray-box attack scenario that the adversary can only access the visual encoder of the victim LVLM, without the knowledge of its prompts (which are often proprietary for service providers and not publicly available) and its underlying large language model (LLM). This practical setting poses challenges to the cross-prompt and cross-model transferability of targeted adversarial attack, which aims to confuse the LVLM to output a response that is semantically similar to the attacker's chosen target text. To this end, we propose an instruction-tuned targeted attack (dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with high transferability. Initially, we utilize a public text-to-image generative model to "reverse" the target response into a target image, and employ GPT-4 to infer a reasonable instruction $\boldsymbol{p}^\prime$ from the target response. We then form a local surrogate model (sharing the same visual encoder with the victim LVLM) to extract instruction-aware features of an adversarial image example and the target image, and minimize the distance between these two features to optimize the adversarial example. To further improve the transferability, we augment the instruction $\boldsymbol{p}^\prime$ with instructions paraphrased from an LLM. Extensive experiments demonstrate the superiority of our proposed method in targeted attack performance and transferability.

摘要: 大型视觉语言模型在图像理解和响应生成方面表现出了令人难以置信的能力。然而，这种丰富的视觉交互也使LVLM容易受到对抗性例子的攻击。本文提出了一种新颖实用的灰盒攻击方案，即攻击者只能访问受害者LVLM的可视编码器，而不知道其提示(通常是服务提供商的专有提示，而不是公开可用的)及其底层的大型语言模型(LLM)。这一实际设置对目标对抗性攻击的跨提示和跨模型可转移性提出了挑战，其目的是混淆LVLM以输出与攻击者选择的目标文本在语义上相似的响应。为此，我们提出了一种指令调谐的定向攻击(InstructTA)，对具有高可转移性的LVLMS进行定向对抗性攻击。首先，我们利用一个公开的文本到图像的生成模型将目标响应“反转”成目标图像，并使用GPT-4从目标响应中推断出合理的指令符号。然后，我们形成一个局部代理模型(与受害者LVLM共享相同的视觉编码器)来提取对抗性图像示例和目标图像的指令感知特征，并最小化这两个特征之间的距离以优化对抗性示例。为了进一步提高可转移性，我们用转译自LLM的指令扩充了指令$\boldSymbol{p}^\Prime$。大量实验证明了该方法在目标攻击性能和可转移性方面的优越性。



## **41. DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification**

DiffAttack：针对基于扩散的对抗性净化的逃避攻击 cs.CR

Accepted to NeurIPS 2023

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2311.16124v2) [paper-pdf](http://arxiv.org/pdf/2311.16124v2)

**Authors**: Mintong Kang, Dawn Song, Bo Li

**Abstract**: Diffusion-based purification defenses leverage diffusion models to remove crafted perturbations of adversarial examples and achieve state-of-the-art robustness. Recent studies show that even advanced attacks cannot break such defenses effectively, since the purification process induces an extremely deep computational graph which poses the potential problem of gradient obfuscation, high memory cost, and unbounded randomness. In this paper, we propose a unified framework DiffAttack to perform effective and efficient attacks against diffusion-based purification defenses, including both DDPM and score-based approaches. In particular, we propose a deviated-reconstruction loss at intermediate diffusion steps to induce inaccurate density gradient estimation to tackle the problem of vanishing/exploding gradients. We also provide a segment-wise forwarding-backwarding algorithm, which leads to memory-efficient gradient backpropagation. We validate the attack effectiveness of DiffAttack compared with existing adaptive attacks on CIFAR-10 and ImageNet. We show that DiffAttack decreases the robust accuracy of models compared with SOTA attacks by over 20% on CIFAR-10 under $\ell_\infty$ attack $(\epsilon=8/255)$, and over 10% on ImageNet under $\ell_\infty$ attack $(\epsilon=4/255)$. We conduct a series of ablations studies, and we find 1) DiffAttack with the deviated-reconstruction loss added over uniformly sampled time steps is more effective than that added over only initial/final steps, and 2) diffusion-based purification with a moderate diffusion length is more robust under DiffAttack.

摘要: 基于扩散的净化防御利用扩散模型来消除对抗性示例的精心设计的扰动，并实现最先进的健壮性。最近的研究表明，即使是高级攻击也不能有效地打破这种防御，因为净化过程会产生非常深的计算图，这会带来梯度混淆、高存储成本和无限随机性的潜在问题。在本文中，我们提出了一个统一的框架DiffAttack来执行对基于扩散的净化防御的有效和高效的攻击，包括DDPM和基于分数的方法。特别是，我们提出了中间扩散步骤的偏差重建损失，以导致不准确的密度梯度估计，以解决梯度消失/爆炸的问题。我们还提出了一种分段向前-向后算法，这导致了内存效率较高的梯度反向传播。与已有的针对CIFAR-10和ImageNet的自适应攻击相比，验证了DiffAttack的攻击有效性。我们发现，与SOTA攻击相比，DiffAttack在$\ell_\inty$攻击$(\epsilon=8/255)$下对CIFAR-10的鲁棒性准确率降低了20%以上，在ImageNet上在$\ell_\inty$攻击$(\epsilon=4/255)$下降低了10%以上。我们进行了一系列的烧蚀研究，我们发现1)DiffAttack在均匀采样的时间步长上添加偏差重建损耗比仅在初始/最终步骤上添加的DiffAttack更有效，2)在DiffAttack下，具有适度扩散长度的基于扩散的净化更稳健。



## **42. CBD: A Certified Backdoor Detector Based on Local Dominant Probability**

CBD：一种基于局部支配概率的认证后门检测器 cs.LG

Accepted to NeurIPS 2023

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2310.17498v2) [paper-pdf](http://arxiv.org/pdf/2310.17498v2)

**Authors**: Zhen Xiang, Zidi Xiong, Bo Li

**Abstract**: Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first certified backdoor detector (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic local dominant probability. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves comparable or even higher detection accuracy than state-of-the-art detectors, and it in addition provides detection certification. Notably, for backdoor attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\% (84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and TinyImageNet, respectively, with low false positive rates.

摘要: 后门攻击是深度神经网络的常见威胁。在测试过程中，嵌入后门触发器的样本将被后门模型错误分类为对抗性目标，而没有后门触发器的样本将被正确分类。在本文中，我们提出了第一个认证的后门检测器（CBD），这是基于一种新的，可调的共形预测方案的基础上，我们提出的统计局部优势概率。对于任何被检查的分类器，CBD提供1）检测推断，2）保证攻击对于相同分类域是可检测的条件，以及3）假阳性率的概率上限。我们的理论研究结果表明，具有更强的测试时间噪声和更小的扰动幅度的触发器的攻击更有可能被检测到的保证。此外，我们在四个基准数据集上进行了广泛的实验，考虑了各种后门类型，如BadNet，CB和Blend。CBD实现了与最先进的检测器相当甚至更高的检测精度，此外还提供检测认证。值得注意的是，对于攻击成功率超过90%的随机扰动触发器的后门攻击，CBD分别达到100%（98%）、100%（84%）、98%（98%）和72%（40%）的经验值。在四个基准数据集GTSRB，SVHN，CIFAR-10和TinyImageNet上，分别获得了（认证的）检测真阳性率，假阳性率较低。



## **43. DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks**

DeepTaster：基于对抗性扰动的指纹识别在深度神经网络中使用的专有数据集 cs.CR

**SubmitDate**: 2024-01-04    [abs](http://arxiv.org/abs/2211.13535v2) [paper-pdf](http://arxiv.org/pdf/2211.13535v2)

**Authors**: Seonhye Park, Alsharif Abuadbba, Shuo Wang, Kristen Moore, Yansong Gao, Hyoungshick Kim, Surya Nepal

**Abstract**: Training deep neural networks (DNNs) requires large datasets and powerful computing resources, which has led some owners to restrict redistribution without permission. Watermarking techniques that embed confidential data into DNNs have been used to protect ownership, but these can degrade model performance and are vulnerable to watermark removal attacks. Recently, DeepJudge was introduced as an alternative approach to measuring the similarity between a suspect and a victim model. While DeepJudge shows promise in addressing the shortcomings of watermarking, it primarily addresses situations where the suspect model copies the victim's architecture. In this study, we introduce DeepTaster, a novel DNN fingerprinting technique, to address scenarios where a victim's data is unlawfully used to build a suspect model. DeepTaster can effectively identify such DNN model theft attacks, even when the suspect model's architecture deviates from the victim's. To accomplish this, DeepTaster generates adversarial images with perturbations, transforms them into the Fourier frequency domain, and uses these transformed images to identify the dataset used in a suspect model. The underlying premise is that adversarial images can capture the unique characteristics of DNNs built with a specific dataset. To demonstrate the effectiveness of DeepTaster, we evaluated the effectiveness of DeepTaster by assessing its detection accuracy on three datasets (CIFAR10, MNIST, and Tiny-ImageNet) across three model architectures (ResNet18, VGG16, and DenseNet161). We conducted experiments under various attack scenarios, including transfer learning, pruning, fine-tuning, and data augmentation. Specifically, in the Multi-Architecture Attack scenario, DeepTaster was able to identify all the stolen cases across all datasets, while DeepJudge failed to detect any of the cases.

摘要: 训练深度神经网络(DNN)需要大数据集和强大的计算资源，这导致一些所有者在未经许可的情况下限制重新分发。将机密数据嵌入到DNN中的水印技术已被用于保护所有权，但这些技术会降低模型性能，并且容易受到水印移除攻击。最近，深度法官被引入作为衡量嫌疑人和受害者模型之间相似性的另一种方法。虽然DeepJustice在解决水印的缺点方面表现出了希望，但它主要解决了可疑模型复制受害者的体系结构的情况。在这项研究中，我们引入了DeepTaster，一种新的DNN指纹识别技术，以解决受害者的数据被非法用于建立嫌疑人模型的情况。DeepTaster可以有效地识别这种DNN模型盗窃攻击，即使当可疑模型的架构与受害者的架构不同时也是如此。为了实现这一点，DeepTaster生成带有扰动的对抗性图像，将它们转换到傅立叶频域，并使用这些转换后的图像来识别可疑模型中使用的数据集。其基本前提是敌意图像可以捕捉到用特定数据集构建的DNN的独特特征。为了证明DeepTaster的有效性，我们通过评估DeepTaster在三个模型体系结构(ResNet18、VGG16和DenseNet161)的三个数据集(CIFAR10、MNIST和Tiny-ImageNet)上的检测精度来评估DeepTaster的有效性。我们在不同的攻击场景下进行了实验，包括迁移学习、剪枝、微调和数据增强。具体地说，在多架构攻击场景中，DeepTaster能够识别所有数据集的所有被盗案例，而DeepJustice未能检测到任何案例。



## **44. Integrated Cyber-Physical Resiliency for Power Grids under IoT-Enabled Dynamic Botnet Attacks**

物联网动态僵尸网络攻击下电网的综合网络物理弹性 eess.SY

**SubmitDate**: 2024-01-03    [abs](http://arxiv.org/abs/2401.01963v1) [paper-pdf](http://arxiv.org/pdf/2401.01963v1)

**Authors**: Yuhan Zhao, Juntao Chen, Quanyan Zhu

**Abstract**: The wide adoption of Internet of Things (IoT)-enabled energy devices improves the quality of life, but simultaneously, it enlarges the attack surface of the power grid system. The adversary can gain illegitimate control of a large number of these devices and use them as a means to compromise the physical grid operation, a mechanism known as the IoT botnet attack. This paper aims to improve the resiliency of cyber-physical power grids to such attacks. Specifically, we use an epidemic model to understand the dynamic botnet formation, which facilitates the assessment of the cyber layer vulnerability of the grid. The attacker aims to exploit this vulnerability to enable a successful physical compromise, while the system operator's goal is to ensure a normal operation of the grid by mitigating cyber risks. We develop a cross-layer game-theoretic framework for strategic decision-making to enhance cyber-physical grid resiliency. The cyber-layer game guides the system operator on how to defend against the botnet attacker as the first layer of defense, while the dynamic game strategy at the physical layer further counteracts the adversarial behavior in real time for improved physical resilience. A number of case studies on the IEEE-39 bus system are used to corroborate the devised approach.

摘要: 物联网（IoT）能源设备的广泛采用提高了生活质量，但同时也扩大了电网系统的攻击面。攻击者可以非法控制大量这些设备，并将其用作破坏物理网格操作的手段，这种机制称为物联网僵尸网络攻击。本文旨在提高网络物理电网对此类攻击的弹性。具体来说，我们使用流行病模型来了解动态僵尸网络的形成，这有利于评估网格的网络层脆弱性。攻击者的目标是利用此漏洞成功进行物理攻击，而系统运营商的目标是通过减轻网络风险来确保电网的正常运行。我们开发了一个跨层的博弈论框架的战略决策，以提高网络物理网格弹性。网络层游戏指导系统操作员如何防御僵尸网络攻击者作为第一层防御，而物理层的动态游戏策略进一步实时抵消对抗行为，以提高物理弹性。IEEE-39总线系统上的一些案例研究被用来证实设计的方法。



## **45. Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports**

从网络威胁情报报告中挖掘时态攻击模式 cs.CR

A modified version of this pre-print is submitted to IEEE  Transactions on Software Engineering, and is under review

**SubmitDate**: 2024-01-03    [abs](http://arxiv.org/abs/2401.01883v1) [paper-pdf](http://arxiv.org/pdf/2401.01883v1)

**Authors**: Md Rayhanur Rahman, Brandon Wroblewski, Quinn Matthews, Brantley Morgan, Tim Menzies, Laurie Williams

**Abstract**: Defending from cyberattacks requires practitioners to operate on high-level adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack incidents describe the chain of malicious actions with respect to time. To avoid repeating cyberattack incidents, practitioners must proactively identify and defend against recurring chain of actions - which we refer to as temporal attack patterns. Automatically mining the patterns among actions provides structured and actionable information on the adversary behavior of past cyberattacks. The goal of this paper is to aid security practitioners in prioritizing and proactive defense against cyberattacks by mining temporal attack patterns from cyberthreat intelligence reports. To this end, we propose ChronoCTI, an automated pipeline for mining temporal attack patterns from cyberthreat intelligence (CTI) reports of past cyberattacks. To construct ChronoCTI, we build the ground truth dataset of temporal attack patterns and apply state-of-the-art large language models, natural language processing, and machine learning techniques. We apply ChronoCTI on a set of 713 CTI reports, where we identify 124 temporal attack patterns - which we categorize into nine pattern categories. We identify that the most prevalent pattern category is to trick victim users into executing malicious code to initiate the attack, followed by bypassing the anti-malware system in the victim network. Based on the observed patterns, we advocate organizations to train users about cybersecurity best practices, introduce immutable operating systems with limited functionalities, and enforce multi-user authentications. Moreover, we advocate practitioners to leverage the automated mining capability of ChronoCTI and design countermeasures against the recurring attack patterns.

摘要: 防御网络攻击需要从业者对高级别的对手行为进行操作。网络威胁情报（CTI）报告对过去的网络攻击事件描述了恶意行为的时间链。为了避免重复的网络攻击事件，从业人员必须主动识别和防御重复的行动链-我们称之为时间攻击模式。自动挖掘行为之间的模式提供了关于过去网络攻击的对手行为的结构化和可操作的信息。本文的目标是通过从网络威胁情报报告中挖掘时间攻击模式，帮助安全从业人员优先考虑和主动防御网络攻击。为此，我们提出了ChronoCTI，这是一个自动化的管道，用于从过去网络攻击的网络威胁情报（CTI）报告中挖掘时间攻击模式。为了构建ChronoCTI，我们构建了时间攻击模式的真实数据集，并应用了最先进的大型语言模型、自然语言处理和机器学习技术。我们将ChronoCTI应用于一组713份CTI报告，其中我们确定了124种时间攻击模式-我们将其分为9种模式类别。我们发现，最普遍的模式类别是欺骗受害者用户执行恶意代码发起攻击，然后绕过受害者网络中的反恶意软件系统。根据观察到的模式，我们建议组织对用户进行网络安全最佳实践培训，引入功能有限的不可变操作系统，并实施多用户身份验证。此外，我们提倡从业人员利用ChronoCTI的自动挖掘功能，并针对反复出现的攻击模式设计对策。



## **46. Attackers reveal their arsenal: An investigation of adversarial techniques in CTI reports**

攻击者展示他们的武器库：CTI报告中的对抗性技术调查 cs.CR

This version is submitted to ACM Transactions on Privacy and  Security. This version is under review

**SubmitDate**: 2024-01-03    [abs](http://arxiv.org/abs/2401.01865v1) [paper-pdf](http://arxiv.org/pdf/2401.01865v1)

**Authors**: Md Rayhanur Rahman, Setu Kumar Basak, Rezvan Mahdavi Hezaveh, Laurie Williams

**Abstract**: Context: Cybersecurity vendors often publish cyber threat intelligence (CTI) reports, referring to the written artifacts on technical and forensic analysis of the techniques used by the malware in APT attacks. Objective: The goal of this research is to inform cybersecurity practitioners about how adversaries form cyberattacks through an analysis of adversarial techniques documented in cyberthreat intelligence reports. Dataset: We use 594 adversarial techniques cataloged in MITRE ATT\&CK. We systematically construct a set of 667 CTI reports that MITRE ATT\&CK used as citations in the descriptions of the cataloged adversarial techniques. Methodology: We analyze the frequency and trend of adversarial techniques, followed by a qualitative analysis of the implementation of techniques. Next, we perform association rule mining to identify pairs of techniques recurring in APT attacks. We then perform qualitative analysis to identify the underlying relations among the techniques in the recurring pairs. Findings: The set of 667 CTI reports documents 10,370 techniques in total, and we identify 19 prevalent techniques accounting for 37.3\% of documented techniques. We also identify 425 statistically significant recurring pairs and seven types of relations among the techniques in these pairs. The top three among the seven relationships suggest that techniques used by the malware inter-relate with one another in terms of (a) abusing or affecting the same system assets, (b) executing in sequences, and (c) overlapping in their implementations. Overall, the study quantifies how adversaries leverage techniques through malware in APT attacks based on publicly reported documents. We advocate organizations prioritize their defense against the identified prevalent techniques and actively hunt for potential malicious intrusion based on the identified pairs of techniques.

摘要: 背景：网络安全供应商经常发布网络威胁情报(CTI)报告，提到对恶意软件在APT攻击中使用的技术进行技术和法医分析的书面人工制品。目标：本研究的目标是通过分析网络威胁情报报告中记录的对抗技术，向网络安全从业者提供有关对手如何形成网络攻击的信息。数据集：我们使用了MITRE ATT-CK收录的594种对抗性技术。我们系统地构建了一组667篇CTI报告，MITRE ATT-CK在编目对抗技术的描述中引用了这些报告。方法：我们分析对抗性技术的频率和趋势，然后对技术的实施进行定性分析。接下来，我们执行关联规则挖掘来识别在APT攻击中重复出现的技术对。然后，我们进行定性分析，以确定循环对中的技术之间的潜在关系。结果：这组667份CTI报告共记录了10,370种技术，我们确定了19种流行技术，占记录技术的37.3%。我们还确定了425个具有统计意义的重复对，以及这些对中的技术之间的七种类型的关系。七种关系中的前三种表明恶意软件使用的技术在以下方面相互关联：(A)滥用或影响相同的系统资产，(B)按顺序执行，以及(C)实现中的重叠。总体而言，这项研究基于公开报道的文件，量化了对手如何通过恶意软件在APT攻击中利用技术。我们主张组织优先防御已识别的流行技术，并根据已识别的技术对积极寻找潜在的恶意入侵。



## **47. Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems**

分布式防骗系统中的局部差分私有嵌入模型 cs.CR

**SubmitDate**: 2024-01-03    [abs](http://arxiv.org/abs/2401.02450v1) [paper-pdf](http://arxiv.org/pdf/2401.02450v1)

**Authors**: Iker Perez, Jason Wong, Piotr Skalski, Stuart Burrell, Richard Mortier, Derek McAuley, David Sutton

**Abstract**: Global financial crime activity is driving demand for machine learning solutions in fraud prevention. However, prevention systems are commonly serviced to financial institutions in isolation, and few provisions exist for data sharing due to fears of unintentional leaks and adversarial attacks. Collaborative learning advances in finance are rare, and it is hard to find real-world insights derived from privacy-preserving data processing systems. In this paper, we present a collaborative deep learning framework for fraud prevention, designed from a privacy standpoint, and awarded at the recent PETs Prize Challenges. We leverage latent embedded representations of varied-length transaction sequences, along with local differential privacy, in order to construct a data release mechanism which can securely inform externally hosted fraud and anomaly detection models. We assess our contribution on two distributed data sets donated by large payment networks, and demonstrate robustness to popular inference-time attacks, along with utility-privacy trade-offs analogous to published work in alternative application domains.

摘要: 全球金融犯罪活动正在推动对预防欺诈的机器学习解决方案的需求。然而，预防系统通常是单独向金融机构提供服务的，由于担心无意泄露和对抗性攻击，几乎没有关于数据共享的规定。金融领域的协作学习进展非常罕见，而且很难从保护隐私的数据处理系统中找到现实世界的见解。在这篇文章中，我们提出了一个协作式深度学习框架，用于预防欺诈，从隐私的角度设计，并在最近的PETS奖挑战中获奖。我们利用可变长度交易序列的潜在嵌入表示以及本地差异隐私来构建一种数据发布机制，该机制可以安全地通知外部托管的欺诈和异常检测模型。我们评估了我们在大型支付网络捐赠的两个分布式数据集上的贡献，并展示了对流行的推理时间攻击的健壮性，以及类似于在替代应用领域发布的工作的实用隐私权衡。



## **48. Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement**

基于注意力细化的抗补丁攻击的语义分割 cs.CV

30 pages, 3 figures, 12 tables

**SubmitDate**: 2024-01-03    [abs](http://arxiv.org/abs/2401.01750v1) [paper-pdf](http://arxiv.org/pdf/2401.01750v1)

**Authors**: Zheng Yuan, Jie Zhang, Yude Wang, Shiguang Shan, Xilin Chen

**Abstract**: The attention mechanism has been proven effective on various visual tasks in recent years. In the semantic segmentation task, the attention mechanism is applied in various methods, including the case of both Convolution Neural Networks (CNN) and Vision Transformer (ViT) as backbones. However, we observe that the attention mechanism is vulnerable to patch-based adversarial attacks. Through the analysis of the effective receptive field, we attribute it to the fact that the wide receptive field brought by global attention may lead to the spread of the adversarial patch. To address this issue, in this paper, we propose a Robust Attention Mechanism (RAM) to improve the robustness of the semantic segmentation model, which can notably relieve the vulnerability against patch-based attacks. Compared to the vallina attention mechanism, RAM introduces two novel modules called Max Attention Suppression and Random Attention Dropout, both of which aim to refine the attention matrix and limit the influence of a single adversarial patch on the semantic segmentation results of other positions. Extensive experiments demonstrate the effectiveness of our RAM to improve the robustness of semantic segmentation models against various patch-based attack methods under different attack settings.

摘要: 近年来，注意机制在各种视觉任务中被证明是有效的。在语义分割任务中，注意力机制应用于各种方法中，包括卷积神经网络（CNN）和视觉Transformer（ViT）作为骨干的情况。然而，我们观察到注意力机制容易受到基于补丁的对抗性攻击。通过对有效感受野的分析，我们将其归因于全球注意力带来的广泛感受野可能导致对抗补丁的传播。为了解决这个问题，在本文中，我们提出了一个健壮的注意力机制（RAM），以提高语义分割模型的鲁棒性，这可以显着减轻对补丁的攻击的脆弱性。与vallina注意力机制相比，RAM引入了两个新的模块，称为Max Attention Suppression和Random Attention Dropout，这两个模块都旨在细化注意力矩阵，并限制单个对抗补丁对其他位置的语义分割结果的影响。大量的实验表明，我们的RAM的有效性，以提高对各种基于补丁的攻击方法在不同的攻击设置下的语义分割模型的鲁棒性。



## **49. An Initial Investigation of Neural Replay Simulator for Over-the-Air Adversarial Perturbations to Automatic Speaker Verification**

用于自动说话人确认的空中对抗扰动神经重放模拟器的初步研究 cs.SD

Accepted in ICASSP 2024

**SubmitDate**: 2024-01-03    [abs](http://arxiv.org/abs/2310.05354v4) [paper-pdf](http://arxiv.org/pdf/2310.05354v4)

**Authors**: Jiaqi Li, Li Wang, Liumeng Xue, Lei Wang, Zhizheng Wu

**Abstract**: Deep Learning has advanced Automatic Speaker Verification (ASV) in the past few years. Although it is known that deep learning-based ASV systems are vulnerable to adversarial examples in digital access, there are few studies on adversarial attacks in the context of physical access, where a replay process (i.e., over the air) is involved. An over-the-air attack involves a loudspeaker, a microphone, and a replaying environment that impacts the movement of the sound wave. Our initial experiment confirms that the replay process impacts the effectiveness of the over-the-air attack performance. This study performs an initial investigation towards utilizing a neural replay simulator to improve over-the-air adversarial attack robustness. This is achieved by using a neural waveform synthesizer to simulate the replay process when estimating the adversarial perturbations. Experiments conducted on the ASVspoof2019 dataset confirm that the neural replay simulator can considerably increase the success rates of over-the-air adversarial attacks. This raises the concern for adversarial attacks on speaker verification in physical access applications.

摘要: 在过去的几年里，深度学习发展了自动说话人确认(ASV)。虽然众所周知，基于深度学习的ASV系统在数字访问中容易受到敌意攻击，但在涉及重播过程(即空中重播)的物理访问环境中，很少有关于对抗性攻击的研究。空中攻击包括扬声器、麦克风和影响声波移动的重放环境。我们的初步实验证实，重放过程会影响空中攻击性能的有效性。本研究对利用神经重放模拟器来提高空中对抗攻击的稳健性进行了初步的研究。这是通过使用神经波形合成器来模拟在估计对抗性扰动时的重播过程来实现的。在ASVspoof2019数据集上进行的实验证实，神经重放模拟器可以显著提高空中对抗性攻击的成功率。这引起了人们对物理访问应用中说话人验证的对抗性攻击的关注。



## **50. Will 6G be Semantic Communications? Opportunities and Challenges from Task Oriented and Secure Communications to Integrated Sensing**

6G会成为语义通信吗？从任务导向和安全通信到集成传感的机遇和挑战 cs.NI

**SubmitDate**: 2024-01-03    [abs](http://arxiv.org/abs/2401.01531v1) [paper-pdf](http://arxiv.org/pdf/2401.01531v1)

**Authors**: Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus

**Abstract**: This paper explores opportunities and challenges of task (goal)-oriented and semantic communications for next-generation (NextG) communication networks through the integration of multi-task learning. This approach employs deep neural networks representing a dedicated encoder at the transmitter and multiple task-specific decoders at the receiver, collectively trained to handle diverse tasks including semantic information preservation, source input reconstruction, and integrated sensing and communications. To extend the applicability from point-to-point links to multi-receiver settings, we envision the deployment of decoders at various receivers, where decentralized learning addresses the challenges of communication load and privacy concerns, leveraging federated learning techniques that distribute model updates across decentralized nodes. However, the efficacy of this approach is contingent on the robustness of the employed deep learning models. We scrutinize potential vulnerabilities stemming from adversarial attacks during both training and testing phases. These attacks aim to manipulate both the inputs at the encoder at the transmitter and the signals received over the air on the receiver side, highlighting the importance of fortifying semantic communications against potential multi-domain exploits. Overall, the joint and robust design of task-oriented communications, semantic communications, and integrated sensing and communications in a multi-task learning framework emerges as the key enabler for context-aware, resource-efficient, and secure communications ultimately needed in NextG network systems.

摘要: 通过整合多任务学习，探索面向任务(目标)和语义通信的下一代通信网络的机遇和挑战。这种方法采用深度神经网络，在发送端代表一个专用编码器，在接收端代表多个特定于任务的解码器，共同训练以处理包括语义信息保存、源输入重建以及集成传感和通信在内的各种任务。为了将适用性从点对点链路扩展到多接收器设置，我们设想在不同的接收器上部署解码器，其中分散学习利用跨分散节点分发模型更新的联合学习技术来解决通信负载和隐私问题的挑战。然而，这种方法的有效性取决于所采用的深度学习模型的稳健性。我们在培训和测试阶段仔细检查来自对抗性攻击的潜在漏洞。这些攻击的目的是同时操纵发送器编码器的输入和接收器端通过空中接收的信号，突显加强语义通信以抵御潜在的多域利用的重要性。总体而言，面向任务的通信、语义通信以及多任务学习框架中的集成传感和通信的联合稳健设计成为下一代网络系统最终需要的情景感知、资源高效和安全通信的关键推动因素。



