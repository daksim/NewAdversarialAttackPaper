# Latest Adversarial Attack Papers
**update at 2023-11-21 19:08:07**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. BrainWash: A Poisoning Attack to Forget in Continual Learning**

洗脑：在持续学习中忘记的毒药攻击 cs.LG

**SubmitDate**: 2023-11-20    [abs](http://arxiv.org/abs/2311.11995v1) [paper-pdf](http://arxiv.org/pdf/2311.11995v1)

**Authors**: Ali Abbasi, Parsa Nooralinejad, Hamed Pirsiavash, Soheil Kolouri

**Abstract**: Continual learning has gained substantial attention within the deep learning community, offering promising solutions to the challenging problem of sequential learning. Yet, a largely unexplored facet of this paradigm is its susceptibility to adversarial attacks, especially with the aim of inducing forgetting. In this paper, we introduce "BrainWash," a novel data poisoning method tailored to impose forgetting on a continual learner. By adding the BrainWash noise to a variety of baselines, we demonstrate how a trained continual learner can be induced to forget its previously learned tasks catastrophically, even when using these continual learning baselines. An important feature of our approach is that the attacker requires no access to previous tasks' data and is armed merely with the model's current parameters and the data belonging to the most recent task. Our extensive experiments highlight the efficacy of BrainWash, showcasing degradation in performance across various regularization-based continual learning methods.

摘要: 持续学习在深度学习界得到了广泛的关注，为顺序学习这一具有挑战性的问题提供了有希望的解决方案。然而，这一范式的一个很大程度上没有被探索的方面是它对敌意攻击的敏感性，特别是以诱导遗忘为目的。在这篇文章中，我们介绍了“洗脑”，一种新的数据中毒方法，专门为不断学习的人强加遗忘。通过将洗脑噪声添加到各种基线中，我们演示了如何诱导训练有素的持续学习者灾难性地忘记其先前学习的任务，即使使用这些持续学习基线也是如此。我们方法的一个重要特征是攻击者不需要访问以前任务的数据，并且只用模型的当前参数和属于最近任务的数据武装起来。我们广泛的实验突出了洗脑的有效性，展示了各种基于正则化的持续学习方法在表现上的下降。



## **2. Generating Valid and Natural Adversarial Examples with Large Language Models**

使用大型语言模型生成有效的自然对抗性实例 cs.CL

Submitted to the IEEE for possible publication

**SubmitDate**: 2023-11-20    [abs](http://arxiv.org/abs/2311.11861v1) [paper-pdf](http://arxiv.org/pdf/2311.11861v1)

**Authors**: Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen

**Abstract**: Deep learning-based natural language processing (NLP) models, particularly pre-trained language models (PLMs), have been revealed to be vulnerable to adversarial attacks. However, the adversarial examples generated by many mainstream word-level adversarial attack models are neither valid nor natural, leading to the loss of semantic maintenance, grammaticality, and human imperceptibility. Based on the exceptional capacity of language understanding and generation of large language models (LLMs), we propose LLM-Attack, which aims at generating both valid and natural adversarial examples with LLMs. The method consists of two stages: word importance ranking (which searches for the most vulnerable words) and word synonym replacement (which substitutes them with their synonyms obtained from LLMs). Experimental results on the Movie Review (MR), IMDB, and Yelp Review Polarity datasets against the baseline adversarial attack models illustrate the effectiveness of LLM-Attack, and it outperforms the baselines in human and GPT-4 evaluation by a significant margin. The model can generate adversarial examples that are typically valid and natural, with the preservation of semantic meaning, grammaticality, and human imperceptibility.

摘要: 基于深度学习的自然语言处理(NLP)模型，特别是预先训练的语言模型(PLM)，已经被发现容易受到对手的攻击。然而，许多主流的词级对抗性攻击模型生成的对抗性实例既不有效也不自然，导致失去了语义维护、语法和人类的不可见性。基于语言理解和生成大型语言模型(LLMS)的卓越能力，我们提出了LLM-Attack，旨在利用LLMS生成有效的和自然的对抗性实例。该方法包括两个阶段：词重要性排序(搜索最易受攻击的词)和词同义词替换(用从LLMS获得的同义词替换它们)。在Movie Review(MR)、IMDB和Yelp Review极性数据集上针对基线敌意攻击模型的实验结果表明了LLM-Attack的有效性，并且它在人类和GPT-4评估中的表现明显优于基线。该模型可以生成典型的有效和自然的对抗性例子，同时保留了语义、语法和人类的不可察觉。



## **3. Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems**

超越边界：对人工智能系统可转移攻击的全面综述 cs.CR

**SubmitDate**: 2023-11-20    [abs](http://arxiv.org/abs/2311.11796v1) [paper-pdf](http://arxiv.org/pdf/2311.11796v1)

**Authors**: Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan

**Abstract**: Artificial Intelligence (AI) systems such as autonomous vehicles, facial recognition, and speech recognition systems are increasingly integrated into our daily lives. However, despite their utility, these AI systems are vulnerable to a wide range of attacks such as adversarial, backdoor, data poisoning, membership inference, model inversion, and model stealing attacks. In particular, numerous attacks are designed to target a particular model or system, yet their effects can spread to additional targets, referred to as transferable attacks. Although considerable efforts have been directed toward developing transferable attacks, a holistic understanding of the advancements in transferable attacks remains elusive. In this paper, we comprehensively explore learning-based attacks from the perspective of transferability, particularly within the context of cyber-physical security. We delve into different domains -- the image, text, graph, audio, and video domains -- to highlight the ubiquitous and pervasive nature of transferable attacks. This paper categorizes and reviews the architecture of existing attacks from various viewpoints: data, process, model, and system. We further examine the implications of transferable attacks in practical scenarios such as autonomous driving, speech recognition, and large language models (LLMs). Additionally, we outline the potential research directions to encourage efforts in exploring the landscape of transferable attacks. This survey offers a holistic understanding of the prevailing transferable attacks and their impacts across different domains.

摘要: 自动驾驶汽车、面部识别和语音识别系统等人工智能(AI)系统越来越多地融入我们的日常生活。然而，尽管这些人工智能系统具有实用性，但它们容易受到各种攻击，如对抗性攻击、后门攻击、数据中毒攻击、成员关系推理攻击、模型反转攻击和模型窃取攻击。具体地说，许多攻击旨在针对特定型号或系统，但其影响可能会扩散到其他目标，称为可转移攻击。尽管已经做出了相当大的努力来开发可转移攻击，但对可转移攻击的进展仍难以全面了解。在本文中，我们从可转移性的角度，特别是在网络-物理安全的背景下，全面地探讨了基于学习的攻击。我们深入研究不同的领域--图像、文本、图形、音频和视频域--以突出可转移攻击的无处不在和普遍存在的性质。本文从数据、过程、模型和系统等不同角度对现有攻击的体系结构进行了分类和回顾。我们进一步研究了可转移攻击在实际场景中的含义，如自动驾驶、语音识别和大型语言模型(LLM)。此外，我们概述了潜在的研究方向，以鼓励在探索可转移攻击的图景方面的努力。这项调查提供了对流行的可转移攻击及其跨不同领域的影响的全面了解。



## **4. AdvGen: Physical Adversarial Attack on Face Presentation Attack Detection Systems**

AdvGen：人脸呈现攻击检测系统的物理对抗性攻击 cs.CV

10 pages, 9 figures, Accepted to the International Joint Conference  on Biometrics (IJCB 2023)

**SubmitDate**: 2023-11-20    [abs](http://arxiv.org/abs/2311.11753v1) [paper-pdf](http://arxiv.org/pdf/2311.11753v1)

**Authors**: Sai Amrit Patnaik, Shivali Chansoriya, Anil K. Jain, Anoop M. Namboodiri

**Abstract**: Evaluating the risk level of adversarial images is essential for safely deploying face authentication models in the real world. Popular approaches for physical-world attacks, such as print or replay attacks, suffer from some limitations, like including physical and geometrical artifacts. Recently, adversarial attacks have gained attraction, which try to digitally deceive the learning strategy of a recognition system using slight modifications to the captured image. While most previous research assumes that the adversarial image could be digitally fed into the authentication systems, this is not always the case for systems deployed in the real world. This paper demonstrates the vulnerability of face authentication systems to adversarial images in physical world scenarios. We propose AdvGen, an automated Generative Adversarial Network, to simulate print and replay attacks and generate adversarial images that can fool state-of-the-art PADs in a physical domain attack setting. Using this attack strategy, the attack success rate goes up to 82.01%. We test AdvGen extensively on four datasets and ten state-of-the-art PADs. We also demonstrate the effectiveness of our attack by conducting experiments in a realistic, physical environment.

摘要: 评估敌意图像的风险级别对于在现实世界中安全地部署人脸认证模型至关重要。流行的物理世界攻击方法，如打印或重放攻击，受到一些限制，比如包括物理和几何伪像。最近，敌意攻击越来越受到关注，这种攻击试图通过对捕获的图像进行轻微修改来数字欺骗识别系统的学习策略。虽然以前的大多数研究都假设敌意图像可以数字地输入身份验证系统，但对于现实世界中部署的系统来说，情况并不总是如此。本文论证了人脸认证系统在现实世界场景中对敌意图像的脆弱性。我们提出了一个自动生成的对抗性网络AdvGen来模拟打印和重放攻击，并生成可以在物理域攻击环境中愚弄最先进的PAD的对抗性图像。使用该攻击策略，攻击成功率可达82.01%。我们在四个数据集和十个最先进的PAD上广泛测试AdvGen。我们还通过在现实的物理环境中进行实验来证明我们的攻击的有效性。



## **5. Rethinking the Backward Propagation for Adversarial Transferability**

关于对抗性转移的后向传播的再思考 cs.CV

Accepted by NeurIPS 2023

**SubmitDate**: 2023-11-20    [abs](http://arxiv.org/abs/2306.12685v2) [paper-pdf](http://arxiv.org/pdf/2306.12685v2)

**Authors**: Xiaosen Wang, Kangheng Tong, Kun He

**Abstract**: Transfer-based attacks generate adversarial examples on the surrogate model, which can mislead other black-box models without access, making it promising to attack real-world applications. Recently, several works have been proposed to boost adversarial transferability, in which the surrogate model is usually overlooked. In this work, we identify that non-linear layers (e.g., ReLU, max-pooling, etc.) truncate the gradient during backward propagation, making the gradient w.r.t. input image imprecise to the loss function. We hypothesize and empirically validate that such truncation undermines the transferability of adversarial examples. Based on these findings, we propose a novel method called Backward Propagation Attack (BPA) to increase the relevance between the gradient w.r.t. input image and loss function so as to generate adversarial examples with higher transferability. Specifically, BPA adopts a non-monotonic function as the derivative of ReLU and incorporates softmax with temperature to smooth the derivative of max-pooling, thereby mitigating the information loss during the backward propagation of gradients. Empirical results on the ImageNet dataset demonstrate that not only does our method substantially boost the adversarial transferability, but it is also general to existing transfer-based attacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA.

摘要: 基于传输的攻击在代理模型上生成敌意示例，这可能会在无法访问的情况下误导其他黑盒模型，使其有可能攻击现实世界的应用程序。最近，已有一些关于提高对抗性转移能力的工作被提出，但其中的代理模型往往被忽视。在这项工作中，我们确定了非线性层(例如，RELU、最大池等)。在反向传播过程中截断梯度，使梯度w.r.t.输入图像不精确到损失函数。我们假设和经验验证，这种截断破坏了对抗性例子的可转移性。基于这些发现，我们提出了一种新的方法，称为反向传播攻击(BPA)，以提高梯度之间的相关性。输入图像和损失函数，生成具有较高可转移性的对抗性实例。具体地说，BPA采用非单调函数作为RELU的导数，并将Softmax与温度相结合以平滑max-Pooling的导数，从而减少了梯度反向传播过程中的信息损失。在ImageNet数据集上的实验结果表明，我们的方法不仅大大提高了攻击的对抗性可转移性，而且对现有的基于传输的攻击也是通用的。代码可在https://github.com/Trustworthy-AI-Group/RPA.上找到



## **6. Understanding Variation in Subpopulation Susceptibility to Poisoning Attacks**

了解亚群对中毒攻击易感性的差异 cs.LG

18 pages, 11 figures

**SubmitDate**: 2023-11-20    [abs](http://arxiv.org/abs/2311.11544v1) [paper-pdf](http://arxiv.org/pdf/2311.11544v1)

**Authors**: Evan Rose, Fnu Suya, David Evans

**Abstract**: Machine learning is susceptible to poisoning attacks, in which an attacker controls a small fraction of the training data and chooses that data with the goal of inducing some behavior unintended by the model developer in the trained model. We consider a realistic setting in which the adversary with the ability to insert a limited number of data points attempts to control the model's behavior on a specific subpopulation. Inspired by previous observations on disparate effectiveness of random label-flipping attacks on different subpopulations, we investigate the properties that can impact the effectiveness of state-of-the-art poisoning attacks against different subpopulations. For a family of 2-dimensional synthetic datasets, we empirically find that dataset separability plays a dominant role in subpopulation vulnerability for less separable datasets. However, well-separated datasets exhibit more dependence on individual subpopulation properties. We further discover that a crucial subpopulation property is captured by the difference in loss on the clean dataset between the clean model and a target model that misclassifies the subpopulation, and a subpopulation is much easier to attack if the loss difference is small. This property also generalizes to high-dimensional benchmark datasets. For the Adult benchmark dataset, we show that we can find semantically-meaningful subpopulation properties that are related to the susceptibilities of a selected group of subpopulations. The results in this paper are accompanied by a fully interactive web-based visualization of subpopulation poisoning attacks found at https://uvasrg.github.io/visualizing-poisoning

摘要: 机器学习很容易受到中毒攻击，在这种攻击中，攻击者控制着一小部分训练数据，并选择这些数据，目的是在训练的模型中诱导一些模型开发人员意想不到的行为。我们考虑一种现实的设置，在这种情况下，具有插入有限数量数据点的能力的对手试图控制模型在特定子群上的行为。受先前关于随机标签翻转攻击对不同子群的不同有效性的观察的启发，我们调查了可以影响针对不同子群的最新毒化攻击的有效性的属性。对于一类2维合成数据集，我们的经验发现，数据集可分性在较少可分性数据集的子总体脆弱性中起主导作用。然而，分离良好的数据集表现出对单个子总体属性的更多依赖。我们进一步发现，一个关键的子总体属性是通过CLEAN模型和错误分类的目标模型之间在CLEAN数据集上的损失差异来捕捉的，并且如果损失差异很小，子总体更容易受到攻击。此属性也适用于高维基准数据集。对于成人基准数据集，我们表明我们可以找到与选定的一组子群体的易感性相关的语义上有意义的子群体属性。本文的结果伴随着在https://uvasrg.github.io/visualizing-poisoning发现的亚群中毒攻击的完全交互的基于网络的可视化



## **7. Assessing Prompt Injection Risks in 200+ Custom GPTs**

评估200多个定制GPT的即时注射风险 cs.CR

**SubmitDate**: 2023-11-20    [abs](http://arxiv.org/abs/2311.11538v1) [paper-pdf](http://arxiv.org/pdf/2311.11538v1)

**Authors**: Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, Xinyu Xing

**Abstract**: In the rapidly evolving landscape of artificial intelligence, ChatGPT has been widely used in various applications. The new feature: customization of ChatGPT models by users to cater to specific needs has opened new frontiers in AI utility. However, this study reveals a significant security vulnerability inherent in these user-customized GPTs: prompt injection attacks. Through comprehensive testing of over 200 user-designed GPT models via adversarial prompts, we demonstrate that these systems are susceptible to prompt injections. Through prompt injection, an adversary can not only extract the customized system prompts but also access the uploaded files. This paper provides a first-hand analysis of the prompt injection, alongside the evaluation of the possible mitigation of such attacks. Our findings underscore the urgent need for robust security frameworks in the design and deployment of customizable GPT models. The intent of this paper is to raise awareness and prompt action in the AI community, ensuring that the benefits of GPT customization do not come at the cost of compromised security and privacy.

摘要: 在快速发展的人工智能版图中，ChatGPT已被广泛应用于各种应用。新功能：用户根据特定需求定制ChatGPT型号，开辟了人工智能实用程序的新领域。然而，这项研究揭示了这些用户定制的GPT固有的一个重大安全漏洞：提示注入攻击。通过通过对抗性提示对200多个用户设计的GPT模型进行全面测试，我们证明了这些系统容易受到快速注入的影响。通过提示注入，攻击者不仅可以提取定制的系统提示，还可以访问上传的文件。本文提供了对快速注入的第一手分析，并评估了此类攻击的可能缓解。我们的发现强调了在设计和部署可定制的GPT模型时迫切需要强大的安全框架。本文的目的是提高AI社区的意识并迅速采取行动，确保GPT定制的好处不会以牺牲安全和隐私为代价。



## **8. Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information**

基于困惑度量和上下文信息的令牌级敌意提示检测 cs.CL

**SubmitDate**: 2023-11-20    [abs](http://arxiv.org/abs/2311.11509v1) [paper-pdf](http://arxiv.org/pdf/2311.11509v1)

**Authors**: Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, Vishy Swaminathan

**Abstract**: In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that lead to undesirable outputs. The inherent vulnerability of LLMs stems from their input-output mechanisms, especially when presented with intensely out-of-distribution (OOD) inputs. This paper proposes a token-level detection method to identify adversarial prompts, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity and incorporate neighboring token information to encourage the detection of contiguous adversarial prompt sequences. As a result, we propose two methods: one that identifies each token as either being part of an adversarial prompt or not, and another that estimates the probability of each token being part of an adversarial prompt.

摘要: 近年来，大型语言模型(LLM)已经成为各种应用中的关键工具。然而，这些模型容易受到敌意提示攻击，攻击者可以仔细策划导致不良输出的输入字符串。低成本管理的内在脆弱性源于其投入-产出机制，特别是在投入严重失配(OOD)的情况下。提出了一种令牌级检测方法来识别敌意提示，利用LLM的能力来预测下一个令牌的概率。我们测量模型的困惑程度，并结合相邻令牌信息来鼓励对连续对抗性提示序列的检测。因此，我们提出了两种方法：一种是识别每个令牌是不是对抗性提示的一部分，另一种是估计每个令牌是对抗性提示的一部分的概率。



## **9. Interpretable Computer Vision Models through Adversarial Training: Unveiling the Robustness-Interpretability Connection**

通过对抗训练可解释的计算机视觉模型：揭示鲁棒性-可解释性连接 cs.CV

13 pages, 19 figures, 6 tables

**SubmitDate**: 2023-11-19    [abs](http://arxiv.org/abs/2307.02500v2) [paper-pdf](http://arxiv.org/pdf/2307.02500v2)

**Authors**: Delyan Boychev

**Abstract**: With the perpetual increase of complexity of the state-of-the-art deep neural networks, it becomes a more and more challenging task to maintain their interpretability. Our work aims to evaluate the effects of adversarial training utilized to produce robust models - less vulnerable to adversarial attacks. It has been shown to make computer vision models more interpretable. Interpretability is as essential as robustness when we deploy the models to the real world. To prove the correlation between these two problems, we extensively examine the models using local feature-importance methods (SHAP, Integrated Gradients) and feature visualization techniques (Representation Inversion, Class Specific Image Generation). Standard models, compared to robust are more susceptible to adversarial attacks, and their learned representations are less meaningful to humans. Conversely, these models focus on distinctive regions of the images which support their predictions. Moreover, the features learned by the robust model are closer to the real ones.

摘要: 随着最新的深度神经网络的复杂性不断增加，保持其可解释性成为一项越来越具有挑战性的任务。我们的工作旨在评估用于产生健壮模型的对抗性训练的效果--较不容易受到对抗性攻击。它已被证明使计算机视觉模型更易于解释。当我们将模型部署到真实世界时，可解释性与健壮性同样重要。为了证明这两个问题之间的相关性，我们使用局部特征重要性方法(Shap，集成梯度)和特征可视化技术(表示反转，类特定图像生成)对模型进行了广泛的检验。与稳健模型相比，标准模型更容易受到对抗性攻击，其学习的表示对人类的意义较小。相反，这些模型关注的是图像中支持其预测的独特区域。此外，稳健模型所学习的特征更接近真实的特征。



## **10. Revisiting and Advancing Adversarial Training Through A Simple Baseline**

通过一条简单的基线重新审视和推进对抗性训练 cs.CV

11 pages, 8 figures

**SubmitDate**: 2023-11-19    [abs](http://arxiv.org/abs/2306.07613v2) [paper-pdf](http://arxiv.org/pdf/2306.07613v2)

**Authors**: Hong Liu

**Abstract**: In this paper, we delve into the essential components of adversarial training which is a pioneering defense technique against adversarial attacks. We indicate that some factors such as the loss function, learning rate scheduler, and data augmentation, which are independent of the model architecture, will influence adversarial robustness and generalization. When these factors are controlled for, we introduce a simple baseline approach, termed SimpleAT, that performs competitively with recent methods and mitigates robust overfitting. We conduct extensive experiments on CIFAR-10/100 and Tiny-ImageNet, which validate the robustness of SimpleAT against state-of-the-art adversarial attackers such as AutoAttack. Our results also demonstrate that SimpleAT exhibits good performance in the presence of various image corruptions, such as those found in the CIFAR-10-C. In addition, we empirically show that SimpleAT is capable of reducing the variance in model predictions, which is considered the primary contributor to robust overfitting. Our results also reveal the connections between SimpleAT and many advanced state-of-the-art adversarial defense methods.

摘要: 在这篇文章中，我们深入研究了对抗攻击的一种开创性防御技术--对抗性训练的基本组成部分。我们指出，损失函数、学习速率调度器和数据扩充等与模型结构无关的因素会影响对手的健壮性和泛化能力。当这些因素被控制时，我们引入了一种简单的基线方法，称为SimpleAT，它的性能与最近的方法具有竞争力，并减轻了稳健的过拟合。我们在CIFAR-10/100和Tiny-ImageNet上进行了大量的实验，验证了SimpleAT对AutoAttack等最先进的敌意攻击者的健壮性。我们的结果还表明，SimpleAT在存在各种图像损坏时表现出良好的性能，例如在CIFAR-10-C中发现的那些图像损坏。此外，我们的经验表明，SimpleAT能够减少模型预测中的方差，这被认为是稳健过拟合的主要贡献者。我们的结果还揭示了SimpleAT与许多先进的对抗性防御方法之间的联系。



## **11. Robust Network Pruning With Sparse Entropic Wasserstein Regression**

基于稀疏熵Wasserstein回归的稳健网络剪枝 cs.AI

submitted to ICLR 2024

**SubmitDate**: 2023-11-19    [abs](http://arxiv.org/abs/2310.04918v2) [paper-pdf](http://arxiv.org/pdf/2310.04918v2)

**Authors**: Lei You, Hei Victor Cheng

**Abstract**: This study tackles the issue of neural network pruning that inaccurate gradients exist when computing the empirical Fisher Information Matrix (FIM). We introduce an entropic Wasserstein regression (EWR) formulation, capitalizing on the geometric attributes of the optimal transport (OT) problem. This is analytically showcased to excel in noise mitigation by adopting neighborhood interpolation across data points. The unique strength of the Wasserstein distance is its intrinsic ability to strike a balance between noise reduction and covariance information preservation. Extensive experiments performed on various networks show comparable performance of the proposed method with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.

摘要: 该研究解决了在计算经验Fisher信息矩阵(FIM)时存在梯度不准确的神经网络修剪问题。利用最优运输(OT)问题的几何属性，我们引入了一个熵Wasserstein回归(EWR)公式。分析表明，通过采用跨数据点的邻域内插，这在噪声缓解方面表现出色。沃瑟斯坦距离的独特优势在于它在降噪和保留协方差信息之间取得平衡的内在能力。在不同网络上进行的大量实验表明，该方法的性能与最新的网络剪枝算法(SOTA)相当。当网络规模或目标稀疏度较大时，我们提出的方法的性能优于SOTA，当存在噪声梯度时，增益甚至更大，可能来自噪声数据、模拟记忆或敌对攻击。值得注意的是，我们提出的方法在剩余不到四分之一的网络参数的情况下，对MobileNetV1实现了6%的准确率提高和8%的测试损失改善。



## **12. Adversarial Prompt Tuning for Vision-Language Models**

视觉语言模型的对抗性提示调整 cs.CV

**SubmitDate**: 2023-11-19    [abs](http://arxiv.org/abs/2311.11261v1) [paper-pdf](http://arxiv.org/pdf/2311.11261v1)

**Authors**: Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang

**Abstract**: With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code will be available upon publication of the paper.

摘要: 随着多通道学习的快速发展，诸如CLIP等预先训练的视觉语言模型在弥合视觉和语言通道之间的差距方面显示出了显著的能力。然而，这些模型仍然容易受到敌意攻击，特别是在图像模式方面，这带来了相当大的安全风险。本文介绍了对抗性提示调优(AdvPT)技术，这是一种在VLMS中增强图像编码器对抗性稳健性的新技术。AdvPT创新性地利用可学习的文本提示，并将其与对抗性图像嵌入相结合，以解决VLM中固有的漏洞，而无需进行广泛的参数培训或修改模型体系结构。我们证明，AdvPT提高了对白盒和黑盒攻击的抵抗力，并与现有的基于图像处理的防御技术相结合，显示出协同效应，进一步增强了防御能力。全面的实验分析提供了对对抗性即时调整的见解，这是一种致力于通过修改文本输入来提高对对抗性图像的抵抗力的新范式，为未来稳健的多通道学习研究铺平了道路。这些发现为增强VLM的安全性开辟了新的可能性。我们的代码将在论文发表后提供。



## **13. Untargeted Black-box Attacks for Social Recommendations**

针对社交推荐的无目标黑匣子攻击 cs.SI

Preprint. Under review

**SubmitDate**: 2023-11-19    [abs](http://arxiv.org/abs/2311.07127v2) [paper-pdf](http://arxiv.org/pdf/2311.07127v2)

**Authors**: Wenqi Fan, Shijie Wang, Xiao-yong Wei, Xiaowei Mei, Qing Li

**Abstract**: The rise of online social networks has facilitated the evolution of social recommender systems, which incorporate social relations to enhance users' decision-making process. With the great success of Graph Neural Networks in learning node representations, GNN-based social recommendations have been widely studied to model user-item interactions and user-user social relations simultaneously. Despite their great successes, recent studies have shown that these advanced recommender systems are highly vulnerable to adversarial attacks, in which attackers can inject well-designed fake user profiles to disrupt recommendation performances. While most existing studies mainly focus on targeted attacks to promote target items on vanilla recommender systems, untargeted attacks to degrade the overall prediction performance are less explored on social recommendations under a black-box scenario. To perform untargeted attacks on social recommender systems, attackers can construct malicious social relationships for fake users to enhance the attack performance. However, the coordination of social relations and item profiles is challenging for attacking black-box social recommendations. To address this limitation, we first conduct several preliminary studies to demonstrate the effectiveness of cross-community connections and cold-start items in degrading recommendations performance. Specifically, we propose a novel framework Multiattack based on multi-agent reinforcement learning to coordinate the generation of cold-start item profiles and cross-community social relations for conducting untargeted attacks on black-box social recommendations. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of our proposed attacking framework under the black-box setting.

摘要: 在线社交网络的兴起促进了社交推荐系统的发展，社交推荐系统整合了社会关系，以增强用户的决策过程。随着图神经网络在学习节点表示方面的巨大成功，基于GNN的社交推荐被广泛研究以同时建模用户-项目交互和用户-用户社会关系。尽管它们取得了巨大的成功，但最近的研究表明，这些先进的推荐系统非常容易受到对手攻击，攻击者可以注入精心设计的虚假用户配置文件来破坏推荐性能。虽然现有的研究主要集中于在普通推荐系统上通过定向攻击来推广目标项，但在黑盒场景下，针对社交推荐的非定向攻击以降低整体预测性能的研究较少。为了对社交推荐系统进行无针对性的攻击，攻击者可以为虚假用户构建恶意的社交关系，以提高攻击性能。然而，社交关系和项目简介的协调对于攻击黑箱社交推荐是具有挑战性的。为了解决这一局限性，我们首先进行了几项初步研究，以证明跨社区联系和冷启动项目在降低推荐性能方面的有效性。具体地说，我们提出了一种新的基于多智能体强化学习的多攻击框架，用于协调冷启动项目配置文件的生成和跨社区社会关系的生成，以对黑盒社交推荐进行无针对性的攻击。在各种真实数据集上的综合实验证明了我们提出的攻击框架在黑盒环境下的有效性。



## **14. Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies**

健壮的网络切片：多代理策略、对抗性攻击和防御策略 cs.LG

Published in IEEE Transactions on Machine Learning in Communications  and Networking (TMLCN)

**SubmitDate**: 2023-11-19    [abs](http://arxiv.org/abs/2311.11206v1) [paper-pdf](http://arxiv.org/pdf/2311.11206v1)

**Authors**: Feng Wang, M. Cenk Gursoy, Senem Velipasalar

**Abstract**: In this paper, we present a multi-agent deep reinforcement learning (deep RL) framework for network slicing in a dynamic environment with multiple base stations and multiple users. In particular, we propose a novel deep RL framework with multiple actors and centralized critic (MACC) in which actors are implemented as pointer networks to fit the varying dimension of input. We evaluate the performance of the proposed deep RL algorithm via simulations to demonstrate its effectiveness. Subsequently, we develop a deep RL based jammer with limited prior information and limited power budget. The goal of the jammer is to minimize the transmission rates achieved with network slicing and thus degrade the network slicing agents' performance. We design a jammer with both listening and jamming phases and address jamming location optimization as well as jamming channel optimization via deep RL. We evaluate the jammer at the optimized location, generating interference attacks in the optimized set of channels by switching between the jamming phase and listening phase. We show that the proposed jammer can significantly reduce the victims' performance without direct feedback or prior knowledge on the network slicing policies. Finally, we devise a Nash-equilibrium-supervised policy ensemble mixed strategy profile for network slicing (as a defensive measure) and jamming. We evaluate the performance of the proposed policy ensemble algorithm by applying on the network slicing agents and the jammer agent in simulations to show its effectiveness.

摘要: 在本文中，我们提出了一个多智能体深度强化学习（deep RL）框架，用于在具有多个基站和多个用户的动态环境中进行网络切片。特别是，我们提出了一种新的深度RL框架，其中包含多个演员和集中式评论家（MACC），其中演员被实现为指针网络，以适应输入的不同维度。我们通过模拟来评估所提出的深度RL算法的性能，以证明其有效性。随后，我们开发了一种基于深度RL的干扰机，具有有限的先验信息和有限的功率预算。干扰器的目标是最小化通过网络切片实现的传输速率，从而降低网络切片代理的性能。我们设计了一个具有监听和干扰两个阶段的干扰机，并通过深度RL解决了干扰位置优化和干扰信道优化问题。我们在优化的位置评估干扰机，通过在干扰阶段和监听阶段之间切换，在优化的信道集合中生成干扰攻击。我们表明，所提出的干扰可以显着降低受害者的性能没有直接的反馈或先验知识的网络切片政策。最后，我们设计了一个纳什均衡监督的政策集成混合策略配置文件的网络切片（作为一种防御措施）和干扰。我们通过在网络切片代理和干扰代理上的仿真来评估所提出的策略集成算法的性能，以显示其有效性。



## **15. Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications**

视觉应用中基于注意力的物理对抗性攻击实时防御 cs.CV

**SubmitDate**: 2023-11-19    [abs](http://arxiv.org/abs/2311.11191v1) [paper-pdf](http://arxiv.org/pdf/2311.11191v1)

**Authors**: Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo

**Abstract**: Deep neural networks exhibit excellent performance in computer vision tasks, but their vulnerability to real-world adversarial attacks, achieved through physical objects that can corrupt their predictions, raises serious security concerns for their application in safety-critical domains. Existing defense methods focus on single-frame analysis and are characterized by high computational costs that limit their applicability in multi-frame scenarios, where real-time decisions are crucial.   To address this problem, this paper proposes an efficient attention-based defense mechanism that exploits adversarial channel-attention to quickly identify and track malicious objects in shallow network layers and mask their adversarial effects in a multi-frame setting. This work advances the state of the art by enhancing existing over-activation techniques for real-world adversarial attacks to make them usable in real-time applications. It also introduces an efficient multi-frame defense framework, validating its efficacy through extensive experiments aimed at evaluating both defense performance and computational cost.

摘要: 深度神经网络在计算机视觉任务中表现出优异的性能，但它们在现实世界中通过物理对象实现的对手攻击的脆弱性会破坏它们的预测，这给它们在安全关键领域的应用带来了严重的安全问题。现有的防御方法侧重于单帧分析，并且具有计算成本高的特点，这限制了它们在多帧场景中的适用性，在多帧场景中，实时决策至关重要。针对这一问题，本文提出了一种高效的基于注意力的防御机制，该机制利用对抗性通道注意力来快速识别和跟踪浅层网络中的恶意对象，并在多帧环境下掩盖它们的对抗性效果。这项工作通过增强现有的针对现实世界对抗性攻击的过度激活技术来提高技术水平，使它们能够用于实时应用程序。它还引入了一种高效的多帧防御框架，通过旨在评估防御性能和计算成本的广泛实验来验证其有效性。



## **16. Improving Adversarial Transferability by Stable Diffusion**

通过稳定扩散提高对手的可转移性 cs.CV

**SubmitDate**: 2023-11-18    [abs](http://arxiv.org/abs/2311.11017v1) [paper-pdf](http://arxiv.org/pdf/2311.11017v1)

**Authors**: Jiayang Liu, Siyu Zhu, Siyuan Liang, Jie Zhang, Han Fang, Weiming Zhang, Ee-Chien Chang

**Abstract**: Deep neural networks (DNNs) are susceptible to adversarial examples, which introduce imperceptible perturbations to benign samples, deceiving DNN predictions. While some attack methods excel in the white-box setting, they often struggle in the black-box scenario, particularly against models fortified with defense mechanisms. Various techniques have emerged to enhance the transferability of adversarial attacks for the black-box scenario. Among these, input transformation-based attacks have demonstrated their effectiveness. In this paper, we explore the potential of leveraging data generated by Stable Diffusion to boost adversarial transferability. This approach draws inspiration from recent research that harnessed synthetic data generated by Stable Diffusion to enhance model generalization. In particular, previous work has highlighted the correlation between the presence of both real and synthetic data and improved model generalization. Building upon this insight, we introduce a novel attack method called Stable Diffusion Attack Method (SDAM), which incorporates samples generated by Stable Diffusion to augment input images. Furthermore, we propose a fast variant of SDAM to reduce computational overhead while preserving high adversarial transferability. Our extensive experimental results demonstrate that our method outperforms state-of-the-art baselines by a substantial margin. Moreover, our approach is compatible with existing transfer-based attacks to further enhance adversarial transferability.

摘要: 深度神经网络(DNN)很容易受到敌意例子的影响，这些例子给良性样本带来了难以察觉的扰动，欺骗了DNN的预测。虽然一些攻击方法在白盒设置中表现出色，但它们在黑盒场景中往往举步维艰，特别是针对具有防御机制的模型。已经出现了各种技术来增强针对黑盒情况的对抗性攻击的可转移性。其中，基于输入变换的攻击已经证明了它们的有效性。在本文中，我们探索了利用稳定扩散产生的数据来提高对抗转移的潜力。这种方法从最近的研究中获得灵感，这些研究利用稳定扩散产生的合成数据来增强模型泛化。特别是，以前的工作强调了真实数据和合成数据的存在与改进的模型泛化之间的相关性。基于这一认识，我们提出了一种新的攻击方法，称为稳定扩散攻击方法(SDAM)，它结合了稳定扩散产生的样本来增强输入图像。此外，我们提出了一种SDAM的快速变体来减少计算开销，同时保持了较高的对抗性可转移性。我们广泛的实验结果表明，我们的方法比最先进的基线有很大的优势。此外，我们的方法与现有的基于传输的攻击是兼容的，以进一步增强对抗的可转移性。



## **17. Security of quantum key distribution from generalised entropy accumulation**

广义熵积累下量子密钥分配的安全性 quant-ph

30 pages

**SubmitDate**: 2023-11-18    [abs](http://arxiv.org/abs/2203.04993v2) [paper-pdf](http://arxiv.org/pdf/2203.04993v2)

**Authors**: Tony Metger, Renato Renner

**Abstract**: The goal of quantum key distribution (QKD) is to establish a secure key between two parties connected by an insecure quantum channel. To use a QKD protocol in practice, one has to prove that a finite size key is secure against general attacks: no matter the adversary's attack, they cannot gain useful information about the key. A much simpler task is to prove security against collective attacks, where the adversary is assumed to behave identically and independently in each round. In this work, we provide a formal framework for general QKD protocols and show that for any protocol that can be expressed in this framework, security against general attacks reduces to security against collective attacks, which in turn reduces to a numerical computation. Our proof relies on a recently developed information-theoretic tool called generalised entropy accumulation and can handle generic prepare-and-measure protocols directly without switching to an entanglement-based version.

摘要: 量子密钥分发(QKD)的目标是在通过不安全的量子信道连接的双方之间建立安全密钥。要在实践中使用QKD协议，必须证明有限大小的密钥对一般攻击是安全的：无论对手进行攻击，他们都无法获得有关密钥的有用信息。一个简单得多的任务是证明对集体攻击的安全性，假设对手在每一轮中的行为都是相同的和独立的。在这项工作中，我们为一般的量子密钥分发协议提供了一个形式化的框架，并证明了对于任何可以在该框架中表达的协议，对一般攻击的安全性归结为对集体攻击的安全性，而集体攻击的安全性又归结为数值计算。我们的证明依赖于最近开发的一种称为广义熵累积的信息论工具，可以直接处理通用的准备和测量协议，而不需要切换到基于纠缠的版本。



## **18. PACOL: Poisoning Attacks Against Continual Learners**

PACOL：针对持续学习者的中毒攻击 cs.LG

**SubmitDate**: 2023-11-18    [abs](http://arxiv.org/abs/2311.10919v1) [paper-pdf](http://arxiv.org/pdf/2311.10919v1)

**Authors**: Huayu Li, Gregory Ditzler

**Abstract**: Continual learning algorithms are typically exposed to untrusted sources that contain training data inserted by adversaries and bad actors. An adversary can insert a small number of poisoned samples, such as mislabeled samples from previously learned tasks, or intentional adversarial perturbed samples, into the training datasets, which can drastically reduce the model's performance. In this work, we demonstrate that continual learning systems can be manipulated by malicious misinformation and present a new category of data poisoning attacks specific for continual learners, which we refer to as {\em Poisoning Attacks Against Continual Learners} (PACOL). The effectiveness of labeling flipping attacks inspires PACOL; however, PACOL produces attack samples that do not change the sample's label and produce an attack that causes catastrophic forgetting. A comprehensive set of experiments shows the vulnerability of commonly used generative replay and regularization-based continual learning approaches against attack methods. We evaluate the ability of label-flipping and a new adversarial poison attack, namely PACOL proposed in this work, to force the continual learning system to forget the knowledge of a learned task(s). More specifically, we compared the performance degradation of continual learning systems trained on benchmark data streams with and without poisoning attacks. Moreover, we discuss the stealthiness of the attacks in which we test the success rate of data sanitization defense and other outlier detection-based defenses for filtering out adversarial samples.

摘要: 持续学习算法通常暴露于不可信的来源，这些来源包含由对手和不良行为者插入的训练数据。敌手可以在训练数据集中插入少量有毒样本，例如来自先前学习任务的错误标记的样本，或者故意的对抗性扰动样本，这可能会显著降低模型的性能。在这项工作中，我们证明了持续学习系统可以被恶意错误信息操纵，并提出了一种新的针对持续学习者的数据中毒攻击，我们称之为针对持续学习者的中毒攻击(PACOL)。标签翻转攻击的有效性启发了PACOL；然而，PACOL生成的攻击样本不会改变样本的标签，并产生导致灾难性遗忘的攻击。一组全面的实验表明，常用的生成性回放和基于正则化的连续学习方法对攻击方法的脆弱性。我们评估了标签翻转和本文提出的一种新的对抗性毒物攻击，即PACOL，以迫使持续学习系统忘记学习任务的知识的能力(S)。更具体地说，我们比较了在基准数据流上训练的连续学习系统在有和没有中毒攻击的情况下性能下降的情况。此外，我们还讨论了攻击的隐蔽性，测试了数据净化防御和其他基于孤立点检测的防御措施过滤对手样本的成功率。



## **19. Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models**

鹦鹉训练的对抗性例子：将黑匣子音频攻击的实用性推向说话人识别模型 cs.SD

**SubmitDate**: 2023-11-17    [abs](http://arxiv.org/abs/2311.07780v2) [paper-pdf](http://arxiv.org/pdf/2311.07780v2)

**Authors**: Rui Duan, Zhe Qu, Leah Ding, Yao Liu, Zhuo Lu

**Abstract**: Audio adversarial examples (AEs) have posed significant security challenges to real-world speaker recognition systems. Most black-box attacks still require certain information from the speaker recognition model to be effective (e.g., keeping probing and requiring the knowledge of similarity scores). This work aims to push the practicality of the black-box attacks by minimizing the attacker's knowledge about a target speaker recognition model. Although it is not feasible for an attacker to succeed with completely zero knowledge, we assume that the attacker only knows a short (or a few seconds) speech sample of a target speaker. Without any probing to gain further knowledge about the target model, we propose a new mechanism, called parrot training, to generate AEs against the target model. Motivated by recent advancements in voice conversion (VC), we propose to use the one short sentence knowledge to generate more synthetic speech samples that sound like the target speaker, called parrot speech. Then, we use these parrot speech samples to train a parrot-trained(PT) surrogate model for the attacker. Under a joint transferability and perception framework, we investigate different ways to generate AEs on the PT model (called PT-AEs) to ensure the PT-AEs can be generated with high transferability to a black-box target model with good human perceptual quality. Real-world experiments show that the resultant PT-AEs achieve the attack success rates of 45.8% - 80.8% against the open-source models in the digital-line scenario and 47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon Echo, and Google Home, in the over-the-air scenario.

摘要: 音频对抗性例子(AEs)对真实说话人识别系统提出了巨大的安全挑战。大多数黑盒攻击仍然需要说话人识别模型中的某些信息才能有效(例如，保持探测并需要知道相似性得分)。这项工作旨在通过最小化攻击者对目标说话人识别模型的了解来推动黑盒攻击的实用性。虽然攻击者在完全零知识的情况下成功是不可行的，但我们假设攻击者只知道目标说话人的一小段(或几秒钟)语音样本。在没有任何关于目标模型的进一步知识的情况下，我们提出了一种新的机制，称为鹦鹉训练，以生成针对目标模型的AE。受语音转换领域最新进展的启发，我们提出了利用一小句话的知识来生成更多听起来像目标说话人的合成语音样本，称为鹦鹉语音。然后，我们使用这些鹦鹉语音样本为攻击者训练一个鹦鹉训练(PT)代理模型。在可转移性和感知联合框架下，我们研究了在PT模型(称为PT-AEs)上生成AEs的不同方法，以确保生成的PT-AEs能够高可转移性地生成具有良好人类感知质量的黑盒目标模型。真实世界实验表明，在数字线路场景中，所生成的PT-AE对开源模型的攻击成功率为45.8%-80.8%，在空中场景中，对Apple HomePod(Siri)、Amazon Echo和Google Home等智能设备的攻击成功率为47.9%-58.3%。



## **20. From Principle to Practice: Vertical Data Minimization for Machine Learning**

从原理到实践：机器学习中的垂直数据最小化 cs.LG

Accepted at IEEE S&P 2024

**SubmitDate**: 2023-11-17    [abs](http://arxiv.org/abs/2311.10500v1) [paper-pdf](http://arxiv.org/pdf/2311.10500v1)

**Authors**: Robin Staab, Nikola Jovanović, Mislav Balunović, Martin Vechev

**Abstract**: Aiming to train and deploy predictive models, organizations collect large amounts of detailed client data, risking the exposure of private information in the event of a breach. To mitigate this, policymakers increasingly demand compliance with the data minimization (DM) principle, restricting data collection to only that data which is relevant and necessary for the task. Despite regulatory pressure, the problem of deploying machine learning models that obey DM has so far received little attention. In this work, we address this challenge in a comprehensive manner. We propose a novel vertical DM (vDM) workflow based on data generalization, which by design ensures that no full-resolution client data is collected during training and deployment of models, benefiting client privacy by reducing the attack surface in case of a breach. We formalize and study the corresponding problem of finding generalizations that both maximize data utility and minimize empirical privacy risk, which we quantify by introducing a diverse set of policy-aligned adversarial scenarios. Finally, we propose a range of baseline vDM algorithms, as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that outperforms all baselines across several settings. We plan to release our code as a publicly available library, helping advance the standardization of DM for machine learning. Overall, we believe our work can help lay the foundation for further exploration and adoption of DM principles in real-world applications.

摘要: 为了训练和部署预测模型，组织收集了大量详细的客户数据，在发生入侵时冒着私人信息暴露的风险。为了缓解这一问题，政策制定者越来越多地要求遵守数据最小化(DM)原则，将数据收集仅限于与任务相关和必要的数据。尽管面临监管压力，但到目前为止，部署服从DM的机器学习模型的问题几乎没有得到关注。在这项工作中，我们以全面的方式应对这一挑战。我们提出了一种基于数据泛化的垂直数据挖掘(VDM)工作流，该工作流在设计上确保了在模型的训练和部署过程中不收集全分辨率的客户数据，从而减少了在发生攻击时的攻击面，从而有利于客户隐私。我们形式化并研究了相应的问题，即找到既最大化数据效用又最小化经验隐私风险的概括，我们通过引入一组与策略一致的对抗场景来量化这些概括。最后，我们提出了一系列的基线VDM算法，以及隐私感知树(PAT)，这是一种特别有效的VDM算法，其性能在几种设置下都优于所有基线。我们计划将我们的代码作为一个公开的库发布，帮助推进机器学习的DM标准化。总体而言，我们相信我们的工作可以为进一步探索和采用DM原理在现实世界中的应用奠定基础。



## **21. Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting**

打破界限：深度无线流量预测中的性能和稳健性平衡 cs.LG

Accepted for presentation at the ARTMAN workshop, part of the ACM  Conference on Computer and Communications Security (CCS), 2023

**SubmitDate**: 2023-11-17    [abs](http://arxiv.org/abs/2311.09790v2) [paper-pdf](http://arxiv.org/pdf/2311.09790v2)

**Authors**: Romain Ilbert, Thai V. Hoang, Zonghua Zhang, Themis Palpanas

**Abstract**: Balancing the trade-off between accuracy and robustness is a long-standing challenge in time series forecasting. While most of existing robust algorithms have achieved certain suboptimal performance on clean data, sustaining the same performance level in the presence of data perturbations remains extremely hard. In this paper, we study a wide array of perturbation scenarios and propose novel defense mechanisms against adversarial attacks using real-world telecom data. We compare our strategy against two existing adversarial training algorithms under a range of maximal allowed perturbations, defined using $\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid strategy, which is composed of a classifier to detect adversarial examples, a denoiser to eliminate noise from the perturbed data samples, and a standard forecaster, achieves the best performance on both clean and perturbed data. Our optimal model can retain up to $92.02\%$ the performance of the original forecasting model in terms of Mean Squared Error (MSE) on clean data, while being more robust than the standard adversarially trained models on perturbed data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing methods on normal and perturbed data, respectively. In addition, the components of our models can be trained in parallel, resulting in better computational efficiency. Our results indicate that we can optimally balance the trade-off between the performance and robustness of forecasting models by improving the classifier and denoiser, even in the presence of sophisticated and destructive poisoning attacks.

摘要: 在精度和稳健性之间权衡是时间序列预测中的一个长期挑战。虽然大多数现有的稳健算法在干净的数据上已经取得了一定的次优性能，但在存在数据扰动的情况下保持相同的性能水平仍然是非常困难的。在本文中，我们研究了大量的扰动场景，并利用真实的电信数据提出了针对敌意攻击的新的防御机制。在最大允许扰动的范围内，我们将我们的策略与两种已有的对抗性训练算法进行了比较，这些扰动是由[0.1，0.4]$中的$\ell_{inty}$-Norm，$\定义的。我们的研究结果表明，我们的混合策略由检测敌意样本的分类器、消除扰动数据样本中的噪声的消噪器和标准预测器组成，在干净和扰动数据上都取得了最好的性能。我们的最优模型在对干净数据的均方误差(MSE)方面可以保持原始预测模型高达92.02美元的性能，同时比标准的对抗性训练的模型对扰动数据的预测更加稳健。其均方根误差分别比正常数据和扰动数据的比较方法低2.71倍和2.51倍。此外，我们的模型的组件可以并行训练，从而产生更好的计算效率。我们的结果表明，即使在存在复杂和破坏性的中毒攻击的情况下，我们也可以通过改进分类器和去噪器来最佳地平衡预测模型的性能和稳健性。



## **22. Laccolith: Hypervisor-Based Adversary Emulation with Anti-Detection**

Laccolith：基于系统管理程序的反检测对手仿真 cs.CR

**SubmitDate**: 2023-11-17    [abs](http://arxiv.org/abs/2311.08274v2) [paper-pdf](http://arxiv.org/pdf/2311.08274v2)

**Authors**: Vittorio Orbinato, Marco Carlo Feliciano, Domenico Cotroneo, Roberto Natella

**Abstract**: Advanced Persistent Threats (APTs) represent the most threatening form of attack nowadays since they can stay undetected for a long time. Adversary emulation is a proactive approach for preparing against these attacks. However, adversary emulation tools lack the anti-detection abilities of APTs. We introduce Laccolith, a hypervisor-based solution for adversary emulation with anti-detection to fill this gap. We also present an experimental study to compare Laccolith with MITRE CALDERA, a state-of-the-art solution for adversary emulation, against five popular anti-virus products. We found that CALDERA cannot evade detection, limiting the realism of emulated attacks, even when combined with a state-of-the-art anti-detection framework. Our experiments show that Laccolith can hide its activities from all the tested anti-virus products, thus making it suitable for realistic emulations.

摘要: 高级持续威胁(APT)是当今最具威胁性的攻击形式，因为它们可以长时间保持不被发现。对手模拟是为应对这些攻击做准备的一种主动方法。然而，敌方仿真工具缺乏APTS的抗检测能力。我们引入了Laccolith，这是一种基于系统管理程序的解决方案，用于具有反检测功能的对手模拟，以填补这一空白。我们还提供了一项实验研究，以比较Laccolith和MITRE Caldera，这是一种最先进的对手模拟解决方案，与五种流行的反病毒产品进行比较。我们发现，Caldera无法逃避检测，从而限制了模拟攻击的真实性，即使与最先进的反检测框架结合使用也是如此。我们的实验表明，漆柱可以对所有测试的抗病毒产品隐藏其活性，从而使其适合于真实的模拟。



## **23. Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models**

打破时间一致性：使用图像模型生成视频通用对抗性扰动 cs.CV

ICCV 2023

**SubmitDate**: 2023-11-17    [abs](http://arxiv.org/abs/2311.10366v1) [paper-pdf](http://arxiv.org/pdf/2311.10366v1)

**Authors**: Hee-Seon Kim, Minji Son, Minbeom Kim, Myung-Joon Kwon, Changick Kim

**Abstract**: As video analysis using deep learning models becomes more widespread, the vulnerability of such models to adversarial attacks is becoming a pressing concern. In particular, Universal Adversarial Perturbation (UAP) poses a significant threat, as a single perturbation can mislead deep learning models on entire datasets. We propose a novel video UAP using image data and image model. This enables us to take advantage of the rich image data and image model-based studies available for video applications. However, there is a challenge that image models are limited in their ability to analyze the temporal aspects of videos, which is crucial for a successful video attack. To address this challenge, we introduce the Breaking Temporal Consistency (BTC) method, which is the first attempt to incorporate temporal information into video attacks using image models. We aim to generate adversarial videos that have opposite patterns to the original. Specifically, BTC-UAP minimizes the feature similarity between neighboring frames in videos. Our approach is simple but effective at attacking unseen video models. Additionally, it is applicable to videos of varying lengths and invariant to temporal shifts. Our approach surpasses existing methods in terms of effectiveness on various datasets, including ImageNet, UCF-101, and Kinetics-400.

摘要: 随着使用深度学习模型的视频分析变得越来越普遍，这类模型对对手攻击的脆弱性正成为一个紧迫的问题。特别是，通用对抗性扰动(UAP)构成了一个重大威胁，因为单个扰动可能会误导整个数据集上的深度学习模型。利用图像数据和图像模型，提出了一种新的视频UAP。这使我们能够利用可用于视频应用的丰富的图像数据和基于图像模型的研究。然而，存在一个挑战，即图像模型在分析视频的时间方面的能力有限，这对成功的视频攻击至关重要。为了应对这一挑战，我们引入了打破时间一致性(BTC)方法，这是首次尝试将时间信息融入到使用图像模型的视频攻击中。我们的目标是生成与原始视频模式相反的对抗性视频。具体地说，BTC-UAP将视频中相邻帧之间的特征相似度降至最低。我们的方法简单但有效地攻击了看不见的视频模型。此外，它还适用于不同长度和不随时间移位的视频。我们的方法在各种数据集上的有效性方面超过了现有的方法，包括ImageNet、UCF-101和Kinetics-400。



## **24. Quantum Public-Key Encryption with Tamper-Resilient Public Keys from One-Way Functions**

基于单向函数防篡改公钥的量子公钥加密 quant-ph

48 pages

**SubmitDate**: 2023-11-17    [abs](http://arxiv.org/abs/2304.01800v3) [paper-pdf](http://arxiv.org/pdf/2304.01800v3)

**Authors**: Fuyuki Kitagawa, Tomoyuki Morimae, Ryo Nishimaki, Takashi Yamakawa

**Abstract**: We construct quantum public-key encryption from one-way functions. In our construction, public keys are quantum, but ciphertexts are classical. Quantum public-key encryption from one-way functions (or weaker primitives such as pseudorandom function-like states) are also proposed in some recent works [Morimae-Yamakawa, eprint:2022/1336; Coladangelo, eprint:2023/282; Barooti-Grilo-Malavolta-Sattath-Vu-Walter, eprint:2023/877]. However, they have a huge drawback: they are secure only when quantum public keys can be transmitted to the sender (who runs the encryption algorithm) without being tampered with by the adversary, which seems to require unsatisfactory physical setup assumptions such as secure quantum channels. Our construction is free from such a drawback: it guarantees the secrecy of the encrypted messages even if we assume only unauthenticated quantum channels. Thus, the encryption is done with adversarially tampered quantum public keys. Our construction is the first quantum public-key encryption that achieves the goal of classical public-key encryption, namely, to establish secure communication over insecure channels, based only on one-way functions. Moreover, we show a generic compiler to upgrade security against chosen plaintext attacks (CPA security) into security against chosen ciphertext attacks (CCA security) only using one-way functions. As a result, we obtain CCA secure quantum public-key encryption based only on one-way functions.

摘要: 我们用单向函数构造量子公钥加密。在我们的构造中，公钥是量子的，但密文是经典的。最近的一些工作也提出了基于单向函数(或更弱的基元，如伪随机函数类状态)的量子公钥加密[Morimae-Yamakawa，ePrint：2022/1336；Coladangelo，ePrint：2023/282；Barooti-Grilo-Malavolta-Sattath-Vu-Walter，ePrint：2023/877]。然而，它们有一个巨大的缺点：只有当量子公钥可以传输给发送者(运行加密算法)而不被对手篡改时，它们才是安全的，这似乎需要不令人满意的物理设置假设，如安全量子通道。我们的构造没有这样的缺点：它保证了加密消息的保密性，即使我们假设只有未经验证的量子通道。因此，加密是用恶意篡改的量子公钥完成的。我们的构造是第一个量子公钥加密，它实现了经典公钥加密的目标，即仅基于单向函数在不安全的通道上建立安全通信。此外，我们还给出了一个通用编译器，该编译器仅使用单向函数将针对选择明文攻击的安全性(CPA安全性)升级为针对选择密文攻击(CCA安全性)的安全性。由此，我们得到了仅基于单向函数的CCA安全量子公钥加密。



## **25. You Cannot Escape Me: Detecting Evasions of SIEM Rules in Enterprise Networks**

你无法逃脱我：检测企业网络中SIEM规则的规避 cs.CR

To be published in Proceedings of the 33rd USENIX Security Symposium  (USENIX Security 2024)

**SubmitDate**: 2023-11-16    [abs](http://arxiv.org/abs/2311.10197v1) [paper-pdf](http://arxiv.org/pdf/2311.10197v1)

**Authors**: Rafael Uetz, Marco Herzog, Louis Hackländer, Simon Schwarz, Martin Henze

**Abstract**: Cyberattacks have grown into a major risk for organizations, with common consequences being data theft, sabotage, and extortion. Since preventive measures do not suffice to repel attacks, timely detection of successful intruders is crucial to stop them from reaching their final goals. For this purpose, many organizations utilize Security Information and Event Management (SIEM) systems to centrally collect security-related events and scan them for attack indicators using expert-written detection rules. However, as we show by analyzing a set of widespread SIEM detection rules, adversaries can evade almost half of them easily, allowing them to perform common malicious actions within an enterprise network without being detected. To remedy these critical detection blind spots, we propose the idea of adaptive misuse detection, which utilizes machine learning to compare incoming events to SIEM rules on the one hand and known-benign events on the other hand to discover successful evasions. Based on this idea, we present AMIDES, an open-source proof-of-concept adaptive misuse detection system. Using four weeks of SIEM events from a large enterprise network and more than 500 hand-crafted evasions, we show that AMIDES successfully detects a majority of these evasions without any false alerts. In addition, AMIDES eases alert analysis by assessing which rules were evaded. Its computational efficiency qualifies AMIDES for real-world operation and hence enables organizations to significantly reduce detection blind spots with moderate effort.

摘要: 网络攻击已经成为组织的主要风险，常见的后果是数据被盗、破坏和敲诈勒索。由于预防措施不足以击退攻击，及时发现成功的入侵者对于阻止他们实现最终目标至关重要。为此，许多组织利用安全信息和事件管理(SIEM)系统集中收集与安全相关的事件，并使用专家编写的检测规则扫描它们的攻击指标。然而，正如我们通过分析一组广泛使用的SIEM检测规则所表明的那样，攻击者可以很容易地避开其中的近一半，使他们能够在不被检测到的情况下在企业网络内执行常见的恶意操作。为了弥补这些关键的检测盲点，我们提出了自适应误用检测的思想，一方面利用机器学习将传入事件与SIEM规则进行比较，另一方面利用已知良性事件来发现成功的规避。基于这一思想，我们提出了一个开源的概念验证自适应误用检测系统AMIDES。使用来自大型企业网络的四周SIEM事件和500多个手工创建的规避，我们表明AMIDES成功检测到了大多数此类规避，而没有任何错误警报。此外，AMIDES通过评估哪些规则被规避来简化警报分析。它的计算效率使AMADS有资格在现实世界中运行，因此使组织能够以适度的努力显著减少检测盲点。



## **26. Differentiable JPEG: The Devil is in the Details**

与众不同的JPEG：魔鬼在细节中 cs.CV

Accepted at WACV 2024. Project page:  https://christophreich1996.github.io/differentiable_jpeg/

**SubmitDate**: 2023-11-16    [abs](http://arxiv.org/abs/2309.06978v2) [paper-pdf](http://arxiv.org/pdf/2309.06978v2)

**Authors**: Christoph Reich, Biplob Debnath, Deep Patel, Srimat Chakradhar

**Abstract**: JPEG remains one of the most widespread lossy image coding methods. However, the non-differentiable nature of JPEG restricts the application in deep learning pipelines. Several differentiable approximations of JPEG have recently been proposed to address this issue. This paper conducts a comprehensive review of existing diff. JPEG approaches and identifies critical details that have been missed by previous methods. To this end, we propose a novel diff. JPEG approach, overcoming previous limitations. Our approach is differentiable w.r.t. the input image, the JPEG quality, the quantization tables, and the color conversion parameters. We evaluate the forward and backward performance of our diff. JPEG approach against existing methods. Additionally, extensive ablations are performed to evaluate crucial design choices. Our proposed diff. JPEG resembles the (non-diff.) reference implementation best, significantly surpassing the recent-best diff. approach by $3.47$dB (PSNR) on average. For strong compression rates, we can even improve PSNR by $9.51$dB. Strong adversarial attack results are yielded by our diff. JPEG, demonstrating the effective gradient approximation. Our code is available at https://github.com/necla-ml/Diff-JPEG.

摘要: JPEG仍然是应用最广泛的有损图像编码方法之一。然而，JPEG的不可微特性限制了其在深度学习管道中的应用。为了解决这个问题，最近已经提出了几种JPEG的可微近似。本文对现有的DIFF进行了全面的回顾。JPEG处理并确定了以前方法遗漏的关键细节。为此，我们提出了一个新颖的Diff。JPEG方法，克服了以前的限制。我们的方法是可微的W.r.t。输入图像、JPEG质量、量化表和颜色转换参数。我们评估了DIFF的向前和向后性能。JPEG方法与现有方法的对比。此外，还进行了广泛的消融，以评估关键的设计选择。我们提议的不同之处。JPEG与(Non-Diff.)参考实现最好，大大超过了最近最好的差异。平均接近3.47美元分贝(PSNR)。对于强压缩率，我们甚至可以将PSNR提高9.51美元分贝。强大的对抗性攻击结果是由我们的差异产生的。JPEG格式，演示了有效的渐变近似。我们的代码可以在https://github.com/necla-ml/Diff-JPEG.上找到



## **27. Towards more Practical Threat Models in Artificial Intelligence Security**

人工智能安全中更实用的威胁模型 cs.CR

18 pages, 4 figures, 7 tables, under submission

**SubmitDate**: 2023-11-16    [abs](http://arxiv.org/abs/2311.09994v1) [paper-pdf](http://arxiv.org/pdf/2311.09994v1)

**Authors**: Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Alexandre Alahi

**Abstract**: Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI. For example, while models are often studied in isolation, they form part of larger ML pipelines in practice. Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical. We take a first step towards describing the full extent of this disparity. To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with \textbf{271} industrial practitioners. On the one hand, we find that all existing threat models are indeed applicable. On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings. Our paper is thus a call for action to study more practical threat models in artificial intelligence security.

摘要: 最近的研究发现了人工智能安全研究和实践之间的差距：学术界研究的威胁并不总是反映人工智能的实际使用和安全风险。例如，虽然模型通常是孤立研究的，但实际上它们构成了更大的ML管道的一部分。最近的研究也提出，学术攻击引入的对抗性操纵是不切实际的。我们朝着描述这种差距的全面程度迈出了第一步。为此，我们回顾了人工智能安全研究中研究最多的六种攻击的威胁模型，并通过对工业从业者的调查，将它们与实际应用相匹配。一方面，我们发现所有现有的威胁模型确实都是适用的。另一方面，存在严重的不匹配：研究往往对攻击者过于慷慨，假设他们可以访问现实世界中不常见的信息。因此，我们的论文呼吁采取行动，研究人工智能安全中更实用的威胁模型。



## **28. Hijacking Large Language Models via Adversarial In-Context Learning**

通过对抗性情境学习劫持大型语言模型 cs.LG

**SubmitDate**: 2023-11-16    [abs](http://arxiv.org/abs/2311.09948v1) [paper-pdf](http://arxiv.org/pdf/2311.09948v1)

**Authors**: Yao Qiang, Xiangyu Zhou, Dongxiao Zhu

**Abstract**: In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.

摘要: 尽管ICL的表现很有希望，但它在范例的选择和排列上存在不稳定的问题。此外，精心设计的敌意攻击对ICL的健壮性构成了显著的威胁。然而，现有的攻击要么容易检测，要么依赖外部模型，要么缺乏对ICL的特异性。在各种任务和数据集上的大量实验结果证明了LLM劫持攻击的有效性，导致人们将注意力分散到对抗性令牌上，从而导致目标不想要的输出。



## **29. Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-start**

低水平收缩的两层优化：无热启动的最优样本复杂性 stat.ML

Corrected Remark 18 + other small edits. Code at  https://github.com/CSML-IIT-UCL/bioptexps

**SubmitDate**: 2023-11-16    [abs](http://arxiv.org/abs/2202.03397v4) [paper-pdf](http://arxiv.org/pdf/2202.03397v4)

**Authors**: Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo

**Abstract**: We analyse a general class of bilevel problems, in which the upper-level problem consists in the minimization of a smooth objective function and the lower-level problem is to find the fixed point of a smooth contraction map. This type of problems include instances of meta-learning, equilibrium models, hyperparameter optimization and data poisoning adversarial attacks. Several recent works have proposed algorithms which warm-start the lower-level problem, i.e.~they use the previous lower-level approximate solution as a staring point for the lower-level solver. This warm-start procedure allows one to improve the sample complexity in both the stochastic and deterministic settings, achieving in some cases the order-wise optimal sample complexity. However, there are situations, e.g., meta learning and equilibrium models, in which the warm-start procedure is not well-suited or ineffective. In this work we show that without warm-start, it is still possible to achieve order-wise (near) optimal sample complexity. In particular, we propose a simple method which uses (stochastic) fixed point iterations at the lower-level and projected inexact gradient descent at the upper-level, that reaches an $\epsilon$-stationary point using $O(\epsilon^{-2})$ and $\tilde{O}(\epsilon^{-1})$ samples for the stochastic and the deterministic setting, respectively. Finally, compared to methods using warm-start, our approach yields a simpler analysis that does not need to study the coupled interactions between the upper-level and lower-level iterates.

摘要: 我们分析了一类一般的两层问题，其中上层问题在于光滑目标函数的最小化，下层问题是寻找光滑压缩映射的不动点。这类问题包括元学习、均衡模型、超参数优化和数据中毒攻击等实例。最近的一些工作已经提出了暖启动下层问题的算法，即它们使用先前的下层近似解作为下层求解器的起点。这种热启动过程允许人们在随机和确定性设置下改善样本复杂性，在某些情况下实现顺序最优的样本复杂性。然而，在一些情况下，例如元学习和平衡模型，热启动程序不是很适合或无效的。在这项工作中，我们证明了在没有热启动的情况下，仍然有可能达到阶次(接近)最优的样本复杂性。特别地，我们提出了一种简单的方法，它在下层使用(随机)不动点迭代，在上层使用投影的不精确梯度下降，在随机和确定设置下分别使用$O(epsilon^{-2})$和$tilde{O}(epsilon^{-1})$样本达到$-epsilon$-固定点。最后，与使用热启动的方法相比，我们的方法产生了更简单的分析，不需要研究上层和下层迭代之间的耦合作用。



## **30. On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models**

基于人工反馈的强化学习在大型语言模型中的可开发性 cs.AI

**SubmitDate**: 2023-11-16    [abs](http://arxiv.org/abs/2311.09641v1) [paper-pdf](http://arxiv.org/pdf/2311.09641v1)

**Authors**: Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao

**Abstract**: Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.

摘要: 带人反馈的强化学习(RLHF)是一种将大语言模型与人的偏好相匹配的方法，在大语言模型对齐中起着重要作用。尽管RLHF有其优势，但它依靠人工注释者对文本进行排名，如果任何敌意注释者(即攻击者)通过对任何恶意文本进行排名来操纵排名分数，从而对LLM进行敌意操作，这可能会引入潜在的安全漏洞。为了评估RLHF的红团队对抗人类偏好数据中毒的能力，我们提出了一种毒化攻击方法RankPoison，该方法针对候选者选择偏好翻转来达到某些恶意行为(例如，生成更长的序列，这会增加计算成本)。利用RankPoison生成的有毒数据集，我们可以在不损害原始安全对齐性能的情况下，对LLM进行中毒攻击，生成更长的令牌。此外，应用RankPoison，我们还成功地实现了一个后门攻击，在带有触发词的问题下，LLMS可以生成更长的答案。我们的发现突出了RLHF中的关键安全挑战，强调了对LLM采用更强大的比对方法的必要性。



## **31. HAL 9000: Skynet's Risk Manager**

HAL 9000：天网的风险经理 cs.CR

18 pages, 9 figures

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2311.09449v1) [paper-pdf](http://arxiv.org/pdf/2311.09449v1)

**Authors**: Tadeu Freitas, Mário Neto, Inês Dutra, João Soares, Manuel Correia, Rolando Martins

**Abstract**: Intrusion Tolerant Systems (ITSs) are a necessary component for cyber-services/infrastructures. Additionally, as cyberattacks follow a multi-domain attack surface, a similar defensive approach should be applied, namely, the use of an evolving multi-disciplinary solution that combines ITS, cybersecurity and Artificial Intelligence (AI). With the increased popularity of AI solutions, due to Big Data use-case scenarios and decision support and automation scenarios, new opportunities to apply Machine Learning (ML) algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS can augment its intrusion tolerance capability, by learning from previous attacks and from known vulnerabilities. As such, this work's contribution is twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and incorporates new components to increase its intrusion tolerance capability and its adaptability to new adversaries; (2) an improved Risk Manager design that leverages AI to improve ITSs by automatically assessing OS risks to intrusions, and advise with safer configurations. One of the reasons that intrusions are successful is due to bad configurations or slow adaptability to new threats. This can be caused by the dependency that systems have for human intervention. One of the characteristics in Skynet and HAL 9000 design is the removal of human intervention. Being fully automatized lowers the chance of successful intrusions caused by human error. Our experiments using Skynet, shows that HAL is able to choose 15% safer configurations than the state-of-the-art risk manager.

摘要: 入侵容忍系统（ITS）是网络服务/基础设施的必要组成部分。此外，由于网络攻击遵循多域攻击面，因此应采用类似的防御方法，即使用结合ITS，网络安全和人工智能（AI）的不断发展的多学科解决方案。随着人工智能解决方案的日益普及，由于大数据用例场景以及决策支持和自动化场景，应用机器学习（ML）算法的新机会已经出现，即ITS授权。使用ML算法，ITS可以通过从以前的攻击和已知漏洞中学习来增强其入侵容忍能力。因此，这项工作的贡献是双重的：（1）ITS架构（天网）基于最先进的技术，并结合了新的组件，以提高其入侵容忍能力和对新对手的适应性;（2）改进的风险管理器设计，利用人工智能通过自动评估操作系统入侵风险来改进ITS，并建议更安全的配置。入侵成功的原因之一是由于配置不佳或对新威胁的适应性缓慢。这可能是由系统对人为干预的依赖性造成的。天网和HAL 9000设计的特点之一是消除了人为干预。完全自动化降低了由人为错误造成的成功入侵的机会。我们使用天网的实验表明，HAL能够选择比最先进的风险管理器安全15%的配置。



## **32. How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities**

开源LLM的可信度有多高？恶意演示下的评估显示其漏洞 cs.CL

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2311.09447v1) [paper-pdf](http://arxiv.org/pdf/2311.09447v1)

**Authors**: Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun

**Abstract**: The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose an enhanced Chain of Utterances-based (CoU) prompting strategy by incorporating meticulously crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.

摘要: 开源大型语言模型(LLM)的快速发展显著地推动了人工智能的发展。然而，人们对他们的可信性仍知之甚少。在缺乏足够可信度的情况下大规模部署这些模型可能会带来重大风险，这突显了迅速发现这些问题的必要性。在这项工作中，我们对开源LLM的可信性进行了评估，从八个不同的方面对它们进行了仔细的审查，包括毒性、刻板印象、道德、幻觉、公平性、奉承、隐私和对对手演示的健壮性。我们提出了一种增强的基于话语链(CUU)的提示策略，该策略结合了精心制作的恶意演示来进行可信度攻击。我们广泛的实验涵盖了最近一系列具有代表性的开源LLM，包括Vicuna、MPT、Falcon、Mistral和Llama 2。经验结果强调了我们攻击策略在不同方面的有效性。更有趣的是，我们的结果分析显示，在一般NLP任务中性能优越的模型并不总是具有更大的可信度；事实上，较大的模型可能更容易受到攻击。此外，经过指令调整、专注于指令遵循的模型往往更容易受到影响，尽管针对安全对齐的微调LLM被证明在减轻对手信任攻击方面是有效的。



## **33. Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models**

超越检测：揭开辱骂语言模型中的公平漏洞 cs.CL

Under review

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2311.09428v1) [paper-pdf](http://arxiv.org/pdf/2311.09428v1)

**Authors**: Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu

**Abstract**: This work investigates the potential of undermining both fairness and detection performance in abusive language detection. In a dynamic and complex digital world, it is crucial to investigate the vulnerabilities of these detection models to adversarial fairness attacks to improve their fairness robustness. We propose a simple yet effective framework FABLE that leverages backdoor attacks as they allow targeted control over the fairness and detection performance. FABLE explores three types of trigger designs (i.e., rare, artificial, and natural triggers) and novel sampling strategies. Specifically, the adversary can inject triggers into samples in the minority group with the favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the effectiveness of FABLE attacking fairness and utility in abusive language detection.

摘要: 这项工作调查了在辱骂语言检测中同时破坏公平性和检测性能的可能性。在动态和复杂的数字世界中，研究这些检测模型对敌意公平攻击的脆弱性，以提高其公平性健壮性是至关重要的。我们提出了一个简单而有效的框架寓言，它利用了后门攻击，因为它们允许对公平性和检测性能进行有针对性的控制。Fable探索了三种类型的触发器设计(即罕见的、人工的和自然的触发器)和新颖的抽样策略。具体地说，敌手可以向少数群体中具有有利结果的样本注入触发器(即“非滥用”)，并将其标签翻转到不利结果，即“滥用”。在基准数据集上的实验证明了寓言攻击的有效性、公平性和实用性。



## **34. UMD: Unsupervised Model Detection for X2X Backdoor Attacks**

UMD：X2X后门攻击的无监督模型检测 cs.LG

Proceedings of the 40th International Conference on Machine Learning

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2305.18651v4) [paper-pdf](http://arxiv.org/pdf/2305.18651v4)

**Authors**: Zhen Xiang, Zidi Xiong, Bo Li

**Abstract**: Backdoor (Trojan) attack is a common threat to deep neural networks, where samples from one or more source classes embedded with a backdoor trigger will be misclassified to adversarial target classes. Existing methods for detecting whether a classifier is backdoor attacked are mostly designed for attacks with a single adversarial target (e.g., all-to-one attack). To the best of our knowledge, without supervision, no existing methods can effectively address the more general X2X attack with an arbitrary number of source classes, each paired with an arbitrary target class. In this paper, we propose UMD, the first Unsupervised Model Detection method that effectively detects X2X backdoor attacks via a joint inference of the adversarial (source, target) class pairs. In particular, we first define a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach. Then, these selected class pairs are jointly assessed based on an aggregation of their reverse-engineered trigger size for detection inference, using a robust and unsupervised anomaly detector we proposed. We conduct comprehensive evaluations on CIFAR-10, GTSRB, and Imagenette dataset, and show that our unsupervised UMD outperforms SOTA detectors (even with supervision) by 17%, 4%, and 8%, respectively, in terms of the detection accuracy against diverse X2X attacks. We also show the strong detection performance of UMD against several strong adaptive attacks.

摘要: 后门(特洛伊木马)攻击是深度神经网络的常见威胁，来自嵌入后门触发器的一个或多个源类的样本将被错误分类为对抗性目标类。现有的检测分类器是否被后门攻击的方法大多是针对单个敌对目标的攻击而设计的(例如，All-to-One攻击)。就我们所知，在没有监督的情况下，没有任何现有方法可以有效地应对具有任意数量的源类的更通用的X2X攻击，每个源类都与任意的目标类配对。在本文中，我们提出了第一种无监督模型检测方法UMD，它通过联合推理对手(源、目标)类对来有效地检测X2X后门攻击。特别是，我们首先定义了一种新的可转移性统计量来度量和选择基于所提出的聚类方法的假定的后门类对的子集。然后，使用我们提出的健壮和无监督的异常检测器，基于它们的反向工程触发大小的聚集来联合评估这些选择的类对以用于检测推理。我们在CIFAR-10、GTSRB和Imagenette数据集上进行了综合评估，结果表明，在对各种X2X攻击的检测准确率方面，我们的无监督UMD分别比SOTA检测器(即使有监督)提高了17%、4%和8%。我们还展示了UMD对几种强自适应攻击的强检测性能。



## **35. Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD**

梯度看起来很相似：DP-SGD的敏感度经常被高估 cs.LG

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2307.00310v2) [paper-pdf](http://arxiv.org/pdf/2307.00310v2)

**Authors**: Anvith Thudi, Hengrui Jia, Casey Meehan, Ilia Shumailov, Nicolas Papernot

**Abstract**: Differentially private stochastic gradient descent (DP-SGD) is the canonical approach to private deep learning. While the current privacy analysis of DP-SGD is known to be tight in some settings, several empirical results suggest that models trained on common benchmark datasets leak significantly less privacy for many datapoints. Yet, despite past attempts, a rigorous explanation for why this is the case has not been reached. Is it because there exist tighter privacy upper bounds when restricted to these dataset settings, or are our attacks not strong enough for certain datapoints? In this paper, we provide the first per-instance (i.e., ``data-dependent") DP analysis of DP-SGD. Our analysis captures the intuition that points with similar neighbors in the dataset enjoy better data-dependent privacy than outliers. Formally, this is done by modifying the per-step privacy analysis of DP-SGD to introduce a dependence on the distribution of model updates computed from a training dataset. We further develop a new composition theorem to effectively use this new per-step analysis to reason about an entire training run. Put all together, our evaluation shows that this novel DP-SGD analysis allows us to now formally show that DP-SGD leaks significantly less privacy for many datapoints (when trained on common benchmarks) than the current data-independent guarantee. This implies privacy attacks will necessarily fail against many datapoints if the adversary does not have sufficient control over the possible training datasets.

摘要: 差分私人随机梯度下降(DP-SGD)是私人深度学习的典型方法。虽然目前DP-SGD的隐私分析在某些情况下是严格的，但一些经验结果表明，在公共基准数据集上训练的模型对于许多数据点来说泄露的隐私要少得多。然而，尽管过去曾尝试过，但对于为什么会出现这种情况，还没有达成一个严格的解释。是因为限制到这些数据集设置时存在更严格的隐私上限，还是因为我们的攻击对某些数据点不够强大？在这篇文章中，我们提供了DP-SGD的第一个逐实例(即“数据依赖”)DP分析。我们的分析抓住了这样一种直觉，即数据集中具有相似邻居的点比离群值享有更好的数据依赖隐私。形式上，这是通过修改DP-SGD的每一步隐私分析来实现的，以引入对从训练数据集计算的模型更新的分布的依赖。我们进一步开发了一个新的合成定理，以有效地使用这个新的逐步分析来推理整个训练运行。综上所述，我们的评估表明，这种新颖的DP-SGD分析允许我们现在正式地表明，DP-SGD对于许多数据点(当根据公共基准进行训练时)的隐私泄露显著低于当前的数据独立保证。这意味着如果对手对可能的训练数据集没有足够的控制，针对许多数据点的隐私攻击必然会失败。



## **36. Frontier Language Models are not Robust to Adversarial Arithmetic, or "What do I need to say so you agree 2+2=5?**

前沿语言模型对对抗性算术或“我需要说什么才能让你同意2+2=5？” cs.CL

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2311.07587v2) [paper-pdf](http://arxiv.org/pdf/2311.07587v2)

**Authors**: C. Daniel Freeman, Laura Culp, Aaron Parisi, Maxwell L Bileschi, Gamaleldin F Elsayed, Alex Rizkowsky, Isabelle Simpson, Alex Alemi, Azade Nova, Ben Adlam, Bernd Bohnet, Gaurav Mishra, Hanie Sedghi, Igor Mordatch, Izzeddin Gur, Jaehoon Lee, JD Co-Reyes, Jeffrey Pennington, Kelvin Xu, Kevin Swersky, Kshiteej Mahajan, Lechao Xiao, Rosanne Liu, Simon Kornblith, Noah Constant, Peter J. Liu, Roman Novak, Yundi Qian, Noah Fiedel, Jascha Sohl-Dickstein

**Abstract**: We introduce and study the problem of adversarial arithmetic, which provides a simple yet challenging testbed for language model alignment. This problem is comprised of arithmetic questions posed in natural language, with an arbitrary adversarial string inserted before the question is complete. Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and even to steer models to a particular wrong answer. We additionally provide a simple algorithm for finding successful attacks by querying those same models, which we name "prompt inversion rejection sampling" (PIRS). We finally show that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops. However, we were not able to make a language model fully robust against adversarial arithmetic attacks.

摘要: 我们引入并研究了对抗性算法问题，为语言模型对齐提供了一个简单但具有挑战性的试验台。这个问题由自然语言提出的算术问题组成，在问题完成之前插入一个任意的敌意字符串。即使是在1位数加法问题的简单设置中，也很容易找到令所有测试模型(包括Palm2、GPT4、Claude2)表现不佳的对抗性提示，甚至会将模型引导到特定的错误答案。此外，我们还提供了一个简单的算法，用于通过查询这些相同的模型来发现成功的攻击，我们将其命名为“即时反转拒绝采样”(PIRS)。最后，我们证明了模型可以通过强化学习和代理构成环来部分加强对这些攻击的抵抗。然而，我们不能使语言模型对敌意算术攻击完全健壮。



## **37. Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts**

通过系统提示的自我对抗性攻击越狱GPT-4V cs.CR

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2311.09127v1) [paper-pdf](http://arxiv.org/pdf/2311.09127v1)

**Authors**: Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun

**Abstract**: Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities in model APIs. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully steal the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking, which could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.

摘要: 现有关于越狱多模式大型语言模型(MLLMS)的工作主要集中在模型输入中的对抗性示例，对模型API中的漏洞关注较少。为了填补这一研究空白，我们开展了以下工作：1)在GPT-4V中发现了一个系统即时泄漏漏洞。通过精心设计的对话，我们成功窃取了GPT-4V的内部系统提示。2)基于获得的系统提示，提出了一种新的基于系统提示的MLLM越狱攻击方法SASP(Self-Aversarial Attack by System Prompt)。通过使用GPT-4作为针对自己的红色团队工具，我们的目标是利用被盗的系统提示来搜索潜在的越狱提示。此外，为了追求更好的性能，我们还在GPT-4的S分析的基础上增加了人工修改，进一步将攻击成功率提高到98.7%。3)评估了修改系统提示对越狱攻击的防御效果。结果表明，设计适当的系统提示可以显著降低越狱成功率。总体而言，我们的工作为加强MLLM安全提供了新的见解，展示了系统提示在越狱中的重要作用，这可以被用来极大地提高越狱成功率，同时也保持了防御越狱的潜力。



## **38. Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing**

基于增量随机平滑的视觉语言模型快速认证 cs.CV

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2311.09024v1) [paper-pdf](http://arxiv.org/pdf/2311.09024v1)

**Authors**: A K Nirala, A Joshi, C Hegde, S Sarkar

**Abstract**: A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild.   In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base "training" set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.

摘要: 像CLIP这样的深度视觉语言模型的一个主要好处是，它们实现了零镜头开放式词汇分类；用户能够在推理时通过自然语言提示来定义新的类别标签。然而，尽管基于片段的零射击分类器在一系列域转换中表现出了具有竞争力的性能，但它们仍然非常容易受到对手的攻击。因此，确保这些模型的稳健性对于它们在野外的可靠部署至关重要。在这项工作中，我们引入了开放词汇认证(OVC)，这是一种通过随机平滑技术为CLIP等开放词汇模型设计的快速认证方法。对于给定的基本“训练”提示集及其相应的认证片段分类器，OVC依赖于这样的观察，即具有新提示的分类器可以被视为基本训练集中附近分类器的扰动版本。因此，OVC可以使用一种增量随机平滑的变体来快速验证新的分类器。为了获得进一步的(启发式)加速比，OVC使用多变量正态分布来近似给定输入的嵌入空间，从而绕过了通过视觉主干的前向传递进行采样的需要。我们通过在CIFAR-10和ImageNet测试数据集上使用多个视觉语言主干进行实验评估，验证了OVC的有效性。



## **39. Adversarial Attacks to Reward Machine-based Reinforcement Learning**

奖励基于机器的强化学习的对抗性攻击 cs.LG

Thesis Supervisor: Prof. Federico Cerutti (Universit\`a degli Studi  di Brescia, IT)

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2311.09014v1) [paper-pdf](http://arxiv.org/pdf/2311.09014v1)

**Authors**: Lorenzo Nodari

**Abstract**: In recent years, Reward Machines (RMs) have stood out as a simple yet effective automata-based formalism for exposing and exploiting task structure in reinforcement learning settings. Despite their relevance, little to no attention has been directed to the study of their security implications and robustness to adversarial scenarios, likely due to their recent appearance in the literature. With my thesis, I aim to provide the first analysis of the security of RM-based reinforcement learning techniques, with the hope of motivating further research in the field, and I propose and evaluate a novel class of attacks on RM-based techniques: blinding attacks.

摘要: 近年来，奖励机器(RMS)作为一种简单而有效的基于自动机的形式化方法已经脱颖而出，用于揭示和开发强化学习环境中的任务结构。尽管它们具有相关性，但很少或根本没有人关注它们的安全影响和对抗情景的稳健性，这可能是因为它们最近出现在文献中。本文旨在对基于RM的强化学习技术的安全性进行首次分析，以期推动该领域的进一步研究，并提出并评估了一类针对基于RM的强化学习技术的新型攻击：盲攻击。



## **40. Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing**

用自适应平滑提高分类器的精度和稳健性 cs.LG

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2301.12554v3) [paper-pdf](http://arxiv.org/pdf/2301.12554v3)

**Authors**: Yatong Bai, Brendon G. Anderson, Aerin Kim, Somayeh Sojoudi

**Abstract**: While prior research has proposed a plethora of methods that build neural classifiers robust against adversarial robustness, practitioners are still reluctant to adopt them due to their unacceptably severe clean accuracy penalties. This paper significantly alleviates this accuracy-robustness trade-off by mixing the output probabilities of a standard classifier and a robust classifier, where the standard network is optimized for clean accuracy and is not robust in general. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key to this improvement. In addition to providing intuitions and empirical evidence, we theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The proposed flexible method, termed "adaptive smoothing", can work in conjunction with existing or even future methods that improve clean accuracy, robustness, or adversary detection. Our empirical evaluation considers strong attack methods, including AutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves an 85.21% clean accuracy while maintaining a 38.72% $\ell_\infty$-AutoAttacked ($\epsilon = 8/255$) accuracy, becoming the second most robust method on the RobustBench CIFAR-100 benchmark as of submission, while improving the clean accuracy by ten percentage points compared with all listed models. The code that implements our method is available at https://github.com/Bai-YT/AdaptiveSmoothing.

摘要: 虽然先前的研究已经提出了大量的方法来构建对抗对抗性鲁棒性的神经分类器，但由于其不可接受的严重清洁准确性惩罚，从业者仍然不愿意采用它们。本文通过混合标准分类器和鲁棒分类器的输出概率来显着解释这种准确性-鲁棒性权衡，其中标准网络针对干净的准确性进行了优化，并且通常不鲁棒。我们表明，鲁棒的基础分类器的正确和不正确的例子的信心差是这种改进的关键。除了提供直观和经验证据，我们从理论上证明了现实假设下的混合分类器的鲁棒性。此外，我们将对抗性输入检测器适配到混合网络中，该混合网络自适应地调整两个基础模型的混合，进一步降低了实现鲁棒性的准确性损失。所提出的灵活的方法，称为“自适应平滑”，可以与现有的，甚至未来的方法，提高清洁的准确性，鲁棒性，或对手检测。我们的经验评估考虑了强大的攻击方法，包括自动攻击和自适应攻击。在CIFAR-100数据集上，我们的方法实现了85.21%的干净准确率，同时保持了38.72%的$\ell_\infty$-AutoAttacked（$\null = 8/255$）准确率，成为截至提交时RobustBench CIFAR-100基准测试中第二强大的方法，同时与所有列出的模型相比，将干净准确率提高了10个百分点。实现我们方法的代码可以在https://github.com/Bai-YT/AdaptiveSmoothing上找到。



## **41. On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers**

人工智能分类器对抗健壮性度量的存在性、唯一性和可伸缩性 stat.ML

16 pages, 3 figures

**SubmitDate**: 2023-11-15    [abs](http://arxiv.org/abs/2310.14421v4) [paper-pdf](http://arxiv.org/pdf/2310.14421v4)

**Authors**: Illia Horenko

**Abstract**: Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.

摘要: 提出并证明了(局部)唯一可逆分类器、广义线性模型(GLM)和熵人工智能(EAI)的最小对抗路径(MAP)和最小对抗距离(MAD)的存在唯一性和显式解析计算的简单可验证的数学条件.MAP和MAD的实际计算，以及它们对各种人工智能工具(用于神经元网络、增强随机森林、GLM和EAI)的比较和解释，在常见的合成基准上进行了演示：在双瑞士辊螺旋及其扩展上，以及在两个生物医学数据问题上(用于健康保险索赔预测和心脏病发作死亡分类)。在生物医学应用方面，它展示了MAP如何在可访问的控制变量的预定义子集中提供独特的、最小限度的患者特定风险缓解干预。



## **42. DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Pre-trained Language Models**

Dala：一种基于分布感知LORA的针对预训练语言模型的对抗性攻击 cs.CL

First two authors contribute equally

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.08598v1) [paper-pdf](http://arxiv.org/pdf/2311.08598v1)

**Authors**: Yibo Wang, Xiangjue Dong, James Caverlee, Philip S. Yu

**Abstract**: Pre-trained language models (PLMs) that achieve success in applications are susceptible to adversarial attack methods that are capable of generating adversarial examples with minor perturbations. Although recent attack methods can achieve a relatively high attack success rate (ASR), our observation shows that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit lower confidence levels and higher distance to the training data distribution. As a result, they are easy to detect using very simple detection methods, diminishing the actual effectiveness of these attack methods. To solve this problem, we propose a Distribution-Aware LoRA-based Adversarial Attack (DALA) method, which considers the distribution shift of adversarial examples to improve attack effectiveness under detection methods. We further design a new evaluation metric NASR combining ASR and detection for the attack task. We conduct experiments on four widely-used datasets and validate the attack effectiveness on ASR and NASR of the adversarial examples generated by DALA on the BERT-base model and the black-box LLaMA2-7b model.

摘要: 在应用中取得成功的预先训练的语言模型(PLM)容易受到对抗性攻击方法的影响，这些方法能够以较小的扰动生成对抗性示例。虽然目前的攻击方法可以达到相对较高的攻击成功率，但我们的观察表明，生成的对抗性实例与原始实例相比具有不同的数据分布。具体地说，这些对抗性例子表现出较低的置信度和较高的训练数据分布距离。因此，使用非常简单的检测方法很容易检测到它们，从而降低了这些攻击方法的实际有效性。为了解决这一问题，我们提出了一种基于分布感知LORA的对抗性攻击方法(DALA)，该方法考虑了对抗性实例的分布偏移，以提高检测方法下的攻击效率。我们进一步设计了一种新的ASR和检测相结合的评估指标NASR，用于攻击任务。我们在四个广泛使用的数据集上进行了实验，并验证了Dala在Bert-base模型和黑盒LLaMA2-7b模型上生成的对抗性实例对ASR和Nasr的攻击有效性。



## **43. Physical Adversarial Examples for Multi-Camera Systems**

多摄像机系统的物理对抗实例 cs.CV

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.08539v1) [paper-pdf](http://arxiv.org/pdf/2311.08539v1)

**Authors**: Ana Răduţoiu, Jan-Philipp Schulze, Philip Sperl, Konstantin Böttinger

**Abstract**: Neural networks build the foundation of several intelligent systems, which, however, are known to be easily fooled by adversarial examples. Recent advances made these attacks possible even in air-gapped scenarios, where the autonomous system observes its surroundings by, e.g., a camera. We extend these ideas in our research and evaluate the robustness of multi-camera setups against such physical adversarial examples. This scenario becomes ever more important with the rise in popularity of autonomous vehicles, which fuse the information of several cameras for their driving decision. While we find that multi-camera setups provide some robustness towards past attack methods, we see that this advantage reduces when optimizing on multiple perspectives at once. We propose a novel attack method that we call Transcender-MC, where we incorporate online 3D renderings and perspective projections in the training process. Moreover, we motivate that certain data augmentation techniques can facilitate the generation of successful adversarial examples even further. Transcender-MC is 11% more effective in successfully attacking multi-camera setups than state-of-the-art methods. Our findings offer valuable insights regarding the resilience of object detection in a setup with multiple cameras and motivate the need of developing adequate defense mechanisms against them.

摘要: 神经网络建立了几个智能系统的基础，然而，众所周知，这些系统很容易被敌对的例子愚弄。最近的进步使这些攻击甚至在气隙场景中也成为可能，在这种场景中，自主系统通过例如相机来观察周围环境。我们将这些想法推广到我们的研究中，并评估了多摄像机设置对此类物理对手示例的稳健性。随着自动驾驶汽车的普及，这种情况变得越来越重要。自动驾驶汽车融合了几个摄像头的信息，用于做出驾驶决策。虽然我们发现多摄像头设置对过去的攻击方法提供了一些稳健性，但我们看到，当同时优化多个视角时，这种优势会减弱。我们提出了一种新的攻击方法，我们称之为Transcender-MC，其中我们在训练过程中结合了在线3D渲染和透视投影。此外，我们鼓励某些数据增强技术可以进一步促进成功的对抗性例子的生成。Transcender-MC在成功攻击多摄像头设置方面比最先进的方法有效11%。我们的发现提供了关于多摄像头设置中目标检测的弹性的有价值的见解，并激励了开发针对它们的足够的防御机制的需要。



## **44. Alignment is not sufficient to prevent large language models from generating harmful information: A psychoanalytic perspective**

对齐不足以防止大型语言模型产生有害信息：从精神分析的角度 cs.CL

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.08487v1) [paper-pdf](http://arxiv.org/pdf/2311.08487v1)

**Authors**: Zi Yin, Wei Ding, Jia Liu

**Abstract**: Large Language Models (LLMs) are central to a multitude of applications but struggle with significant risks, notably in generating harmful content and biases. Drawing an analogy to the human psyche's conflict between evolutionary survival instincts and societal norm adherence elucidated in Freud's psychoanalysis theory, we argue that LLMs suffer a similar fundamental conflict, arising between their inherent desire for syntactic and semantic continuity, established during the pre-training phase, and the post-training alignment with human values. This conflict renders LLMs vulnerable to adversarial attacks, wherein intensifying the models' desire for continuity can circumvent alignment efforts, resulting in the generation of harmful information. Through a series of experiments, we first validated the existence of the desire for continuity in LLMs, and further devised a straightforward yet powerful technique, such as incomplete sentences, negative priming, and cognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle to prevent the generation of harmful information. In summary, our study uncovers the root of LLMs' vulnerabilities to adversarial attacks, hereby questioning the efficacy of solely relying on sophisticated alignment methods, and further advocates for a new training idea that integrates modal concepts alongside traditional amodal concepts, aiming to endow LLMs with a more nuanced understanding of real-world contexts and ethical considerations.

摘要: 大型语言模型(LLM)是众多应用程序的核心，但面临着巨大的风险，特别是在生成有害内容和偏见方面。通过类比弗洛伊德精神分析理论中阐明的人类心理在进化生存本能和遵守社会规范之间的冲突，我们认为LLMS在训练前建立的对句法和语义连续性的内在愿望与训练后与人类价值观的一致性之间存在着类似的根本冲突。这种冲突使LLM容易受到对抗性攻击，其中加强模型对连续性的渴望可以绕过对齐工作，从而导致有害信息的产生。通过一系列实验，我们首先验证了LLMS中对连续性的渴望的存在，并进一步设计了一种简单而强大的技术，如不完整句子、负启动和认知不协调情景，以证明即使是高级LLMS也难以防止有害信息的产生。综上所述，我们的研究揭示了LLMS易受敌意攻击的根源，由此质疑单纯依赖复杂的对齐方法的有效性，并进一步倡导一种新的训练思想，将情态概念与传统的非模态概念相结合，旨在赋予LLMS对现实世界背景和伦理考虑的更细微的理解。



## **45. The Perception-Robustness Tradeoff in Deterministic Image Restoration**

确定性图像恢复中的感知-稳健性权衡 eess.IV

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.09253v1) [paper-pdf](http://arxiv.org/pdf/2311.09253v1)

**Authors**: Guy Ohayon, Tomer Michaeli, Michael Elad

**Abstract**: We study the behavior of deterministic methods for solving inverse problems in imaging. These methods are commonly designed to achieve two goals: (1) attaining high perceptual quality, and (2) generating reconstructions that are consistent with the measurements. We provide a rigorous proof that the better a predictor satisfies these two requirements, the larger its Lipschitz constant must be, regardless of the nature of the degradation involved. In particular, to approach perfect perceptual quality and perfect consistency, the Lipschitz constant of the model must grow to infinity. This implies that such methods are necessarily more susceptible to adversarial attacks. We demonstrate our theory on single image super-resolution algorithms, addressing both noisy and noiseless settings. We also show how this undesired behavior can be leveraged to explore the posterior distribution, thereby allowing the deterministic model to imitate stochastic methods.

摘要: 我们研究了求解成像反问题的确定性方法的行为。这些方法通常被设计来实现两个目标：(1)获得高感知质量，(2)产生与测量一致的重建。我们提供了一个严格的证明，无论所涉及的退化的性质如何，预报器满足这两个条件越好，其Lipschitz常数必然越大。特别是，为了接近完美的感知质量和完美的一致性，模型的Lipschitz常数必须增长到无穷大。这意味着这种方法必然更容易受到对抗性攻击。我们演示了我们关于单图像超分辨率算法的理论，解决了噪声和无噪声设置。我们还展示了如何利用这种不受欢迎的行为来探索后验分布，从而允许确定性模型模拟随机方法。



## **46. Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction**

Scale-MIA：一种基于潜在空间重构的安全联邦学习可扩展模型反转攻击 cs.LG

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.05808v2) [paper-pdf](http://arxiv.org/pdf/2311.05808v2)

**Authors**: Shanghao Shi, Ning Wang, Yang Xiao, Chaoyu Zhang, Yi Shi, Y. Thomas Hou, Wenjing Lou

**Abstract**: Federated learning is known for its capability to safeguard participants' data privacy. However, recently emerged model inversion attacks (MIAs) have shown that a malicious parameter server can reconstruct individual users' local data samples through model updates. The state-of-the-art attacks either rely on computation-intensive search-based optimization processes to recover each input batch, making scaling difficult, or they involve the malicious parameter server adding extra modules before the global model architecture, rendering the attacks too conspicuous and easily detectable.   To overcome these limitations, we propose Scale-MIA, a novel MIA capable of efficiently and accurately recovering training samples of clients from the aggregated updates, even when the system is under the protection of a robust secure aggregation protocol. Unlike existing approaches treating models as black boxes, Scale-MIA recognizes the importance of the intricate architecture and inner workings of machine learning models. It identifies the latent space as the critical layer for breaching privacy and decomposes the complex recovery task into an innovative two-step process to reduce computation complexity. The first step involves reconstructing the latent space representations (LSRs) from the aggregated model updates using a closed-form inversion mechanism, leveraging specially crafted adversarial linear layers. In the second step, the whole input batches are recovered from the LSRs by feeding them into a fine-tuned generative decoder.   We implemented Scale-MIA on multiple commonly used machine learning models and conducted comprehensive experiments across various settings. The results demonstrate that Scale-MIA achieves excellent recovery performance on different datasets, exhibiting high reconstruction rates, accuracy, and attack efficiency on a larger scale compared to state-of-the-art MIAs.

摘要: 联合学习以其保护参与者数据隐私的能力而闻名。然而，最近出现的模型反转攻击(MIA)表明，恶意参数服务器可以通过模型更新来重建个人用户的本地数据样本。最先进的攻击要么依赖于基于搜索的计算密集型优化过程来恢复每个输入批次，使扩展变得困难，要么涉及恶意参数服务器在全局模型体系结构之前添加额外的模块，使攻击过于显眼和容易检测。为了克服这些局限性，我们提出了Scale-MIA，一种新的MIA，即使在系统处于健壮的安全聚合协议的保护下，也能够从聚合的更新中高效而准确地恢复客户端的训练样本。与将模型视为黑盒的现有方法不同，Scale-MIA认识到机器学习模型复杂的体系结构和内部工作原理的重要性。它将潜在空间识别为侵犯隐私的关键层，并将复杂的恢复任务分解为一个创新的两步过程，以降低计算复杂度。第一步涉及使用闭合形式的反转机制从聚集的模型更新重构潜在空间表示(LSR)，利用特制的对抗性线性层。在第二步中，通过将输入批次馈送到微调的产生式解码器，从LSR中恢复整个输入批次。我们在多种常用的机器学习模型上实现了Scale-MIA，并在不同的环境下进行了全面的实验。结果表明，Scale-MIA在不同的数据集上表现出了良好的恢复性能，与现有的MIA相比，在更大的范围内表现出更高的重建率、准确性和攻击效率。



## **47. A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily**

披着羊皮的狼：广义嵌套越狱提示可以轻松愚弄大型语言模型 cs.CL

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.08268v1) [paper-pdf](http://arxiv.org/pdf/2311.08268v1)

**Authors**: Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang

**Abstract**: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on another white-box model, compromising generalization or jailbreak efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we offer detailed analysis and discussion from the perspective of prompt execution priority on the failure of LLMs' defense. We hope that our research can catalyze both the academic community and LLMs vendors towards the provision of safer and more regulated Large Language Models.

摘要: 大型语言模型(LLM)，如ChatGPT和GPT-4，旨在提供有用和安全的响应。然而，被称为“越狱”的对抗性提示可能会绕过安全措施，导致LLMS生成有害内容。探索越狱提示可以帮助更好地揭示LLM的弱点，并进一步指导我们确保它们的安全。不幸的是，现有的越狱方法要么受到复杂的手动设计的影响，要么需要在另一个白盒模型上进行优化，从而影响通用性或越狱效率。本文将越狱提示攻击概括为两个方面：(1)提示重写和(2)场景嵌套。在此基础上，我们提出了ReNeLLM，这是一个利用LLM自身生成有效越狱提示的自动化框架。广泛的实验表明，与现有的基准相比，ReNeLLM显著提高了攻击成功率，同时大大降低了时间成本。我们的研究也揭示了现有防御方法在保护低密度脂蛋白方面的不足。最后，从即时执行优先权的角度对有限责任公司的抗辩失败进行了详细的分析和讨论。我们希望我们的研究能够促进学术界和LLMS供应商提供更安全和更规范的大型语言模型。



## **48. On The Relationship Between Universal Adversarial Attacks And Sparse Representations**

泛对抗攻击与稀疏表示的关系 cs.CV

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.08265v1) [paper-pdf](http://arxiv.org/pdf/2311.08265v1)

**Authors**: Dana Weitzner, Raja Giryes

**Abstract**: The prominent success of neural networks, mainly in computer vision tasks, is increasingly shadowed by their sensitivity to small, barely perceivable adversarial perturbations in image input.   In this work, we aim at explaining this vulnerability through the framework of sparsity.   We show the connection between adversarial attacks and sparse representations, with a focus on explaining the universality and transferability of adversarial examples in neural networks.   To this end, we show that sparse coding algorithms, and the neural network-based learned iterative shrinkage thresholding algorithm (LISTA) among them, suffer from this sensitivity, and that common attacks on neural networks can be expressed as attacks on the sparse representation of the input image. The phenomenon that we observe holds true also when the network is agnostic to the sparse representation and dictionary, and thus can provide a possible explanation for the universality and transferability of adversarial attacks.   The code is available at https://github.com/danawr/adversarial_attacks_and_sparse_representations.

摘要: 神经网络的突出成功，主要是在计算机视觉任务中，越来越多地被它们对图像输入中微小的、几乎不可感知的对抗性扰动的敏感性所掩盖。   在这项工作中，我们的目标是通过稀疏性的框架来解释这种脆弱性。   我们展示了对抗性攻击和稀疏表示之间的联系，重点是解释神经网络中对抗性示例的普遍性和可转移性。   为此，我们表明，稀疏编码算法，其中基于神经网络的学习迭代收缩阈值算法（LISTA），遭受这种敏感性，神经网络的常见攻击可以表示为对输入图像的稀疏表示的攻击。当网络对稀疏表示和字典不可知时，我们观察到的现象也是如此，因此可以为对抗性攻击的普遍性和可转移性提供可能的解释。   该代码可在https://github.com/danawr/adversarial_attacks_and_sparse_representations上获得。



## **49. The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks**

分布式联合学习网络中对抗性节点放置的影响 cs.CR

Submitted to ICC 2023 conference

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.07946v1) [paper-pdf](http://arxiv.org/pdf/2311.07946v1)

**Authors**: Adam Piaseczny, Eric Ruzomberka, Rohit Parasnis, Christopher G. Brinton

**Abstract**: As Federated Learning (FL) grows in popularity, new decentralized frameworks are becoming widespread. These frameworks leverage the benefits of decentralized environments to enable fast and energy-efficient inter-device communication. However, this growing popularity also intensifies the need for robust security measures. While existing research has explored various aspects of FL security, the role of adversarial node placement in decentralized networks remains largely unexplored. This paper addresses this gap by analyzing the performance of decentralized FL for various adversarial placement strategies when adversaries can jointly coordinate their placement within a network. We establish two baseline strategies for placing adversarial node: random placement and network centrality-based placement. Building on this foundation, we propose a novel attack algorithm that prioritizes adversarial spread over adversarial centrality by maximizing the average network distance between adversaries. We show that the new attack algorithm significantly impacts key performance metrics such as testing accuracy, outperforming the baseline frameworks by between 9% and 66.5% for the considered setups. Our findings provide valuable insights into the vulnerabilities of decentralized FL systems, setting the stage for future research aimed at developing more secure and robust decentralized FL frameworks.

摘要: 随着联邦学习(FL)的流行，新的去中心化框架正在变得广泛。这些框架利用分散环境的优势，实现快速、节能的设备间通信。然而，这种日益增长的人气也加剧了采取强有力的安全措施的必要性。虽然现有的研究已经探索了FL安全的各个方面，但敌意节点放置在分散网络中的作用在很大程度上仍未被探索。本文通过分析当对手可以在一个网络内联合协调他们的放置时，分散的FL在不同的对手放置策略下的性能来解决这一差距。我们建立了两种放置敌意节点的基线策略：随机放置和基于网络中心性的放置。在此基础上，我们提出了一种新的攻击算法，该算法通过最大化对手之间的平均网络距离来优先考虑对手的传播而不是对手的中心。我们发现，新的攻击算法显著影响了测试准确率等关键性能指标，在所考虑的设置下，性能比基准框架高出9%到66.5%。我们的发现对去中心化FL系统的脆弱性提供了有价值的见解，为未来旨在开发更安全和健壮的去中心化FL框架的研究奠定了基础。



## **50. Towards Improving Robustness Against Common Corruptions in Object Detectors Using Adversarial Contrastive Learning**

使用对抗性对比学习提高目标检测器对常见错误的鲁棒性 cs.CV

**SubmitDate**: 2023-11-14    [abs](http://arxiv.org/abs/2311.07928v1) [paper-pdf](http://arxiv.org/pdf/2311.07928v1)

**Authors**: Shashank Kotyan, Danilo Vasconcellos Vargas

**Abstract**: Neural networks have revolutionized various domains, exhibiting remarkable accuracy in tasks like natural language processing and computer vision. However, their vulnerability to slight alterations in input samples poses challenges, particularly in safety-critical applications like autonomous driving. Current approaches, such as introducing distortions during training, fall short in addressing unforeseen corruptions. This paper proposes an innovative adversarial contrastive learning framework to enhance neural network robustness simultaneously against adversarial attacks and common corruptions. By generating instance-wise adversarial examples and optimizing contrastive loss, our method fosters representations that resist adversarial perturbations and remain robust in real-world scenarios. Subsequent contrastive learning then strengthens the similarity between clean samples and their adversarial counterparts, fostering representations resistant to both adversarial attacks and common distortions. By focusing on improving performance under adversarial and real-world conditions, our approach aims to bolster the robustness of neural networks in safety-critical applications, such as autonomous vehicles navigating unpredictable weather conditions. We anticipate that this framework will contribute to advancing the reliability of neural networks in challenging environments, facilitating their widespread adoption in mission-critical scenarios.

摘要: 神经网络已经彻底改变了各个领域，在自然语言处理和计算机视觉等任务中表现出非凡的准确性。然而，它们容易受到输入样本轻微变化的影响，这带来了挑战，特别是在自动驾驶等安全关键型应用中。目前的办法，例如在培训期间引入扭曲做法，不足以解决不可预见的腐败问题。本文提出了一个创新的对抗性对比学习框架，以提高神经网络的鲁棒性，同时对抗对抗性攻击和常见的腐败。通过生成实例对抗性示例和优化对比损失，我们的方法培养了抵抗对抗性扰动并在现实世界场景中保持鲁棒性的表示。随后的对比学习加强了干净样本与其对抗样本之间的相似性，培养了抵抗对抗攻击和常见扭曲的表征。通过专注于提高在对抗性和真实世界条件下的性能，我们的方法旨在增强神经网络在安全关键应用中的鲁棒性，例如在不可预测的天气条件下导航的自动驾驶汽车。我们预计，该框架将有助于提高神经网络在具有挑战性的环境中的可靠性，促进其在关键任务场景中的广泛采用。



