# Latest Adversarial Attack Papers
**update at 2022-03-25 15:01:23**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. A Perturbation Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow**

一种用于评估光流稳健性的扰动约束对抗性攻击 cs.CV

**SubmitDate**: 2022-03-24    [paper-pdf](http://arxiv.org/pdf/2203.13214v1)

**Authors**: Jenny Schmalfuss, Philipp Scholze, Andrés Bruhn

**Abstracts**: Recent optical flow methods are almost exclusively judged in terms of accuracy, while analyzing their robustness is often neglected. Although adversarial attacks offer a useful tool to perform such an analysis, current attacks on optical flow methods rather focus on real-world attacking scenarios than on a worst case robustness assessment. Hence, in this work, we propose a novel adversarial attack - the Perturbation Constrained Flow Attack (PCFA) - that emphasizes destructivity over applicability as a real-world attack. More precisely, PCFA is a global attack that optimizes adversarial perturbations to shift the predicted flow towards a specified target flow, while keeping the L2 norm of the perturbation below a chosen bound. Our experiments not only demonstrate PCFA's applicability in white- and black-box settings, but also show that it finds stronger adversarial samples for optical flow than previous attacking frameworks. Moreover, based on these strong samples, we provide the first common ranking of optical flow methods in the literature considering both prediction quality and adversarial robustness, indicating that high quality methods are not necessarily robust. Our source code will be publicly available.

摘要: 目前的光流方法几乎完全是根据精度来判断的，而对它们的稳健性分析往往被忽视。虽然对抗性攻击提供了执行这种分析的有用工具，但是当前对光流方法的攻击更多地集中在真实世界的攻击场景上，而不是最坏情况下的健壮性评估。因此，在这项工作中，我们提出了一种新的对抗性攻击-扰动约束流攻击(PCFA)-作为一种现实世界的攻击，它强调破坏性而不是适用性。更准确地说，PCFA是一种全局攻击，它优化对抗性扰动，将预测流向指定的目标流移动，同时将扰动的L2范数保持在选定的界限以下。我们的实验不仅证明了PCFA在白盒和黑盒环境下的适用性，而且表明它发现的光流攻击样本比以前的攻击框架更具对抗性。此外，基于这些强样本，我们给出了文献中第一个同时考虑预测质量和对抗鲁棒性的光流方法的通用排名，表明高质量的方法不一定是健壮的。我们的源代码将向公众开放。



## **2. Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity**

基于频率驱动的语义相似度潜伏攻击 cs.CV

CVPR 2022 conference (accepted), 18 pages, 17 figure

**SubmitDate**: 2022-03-24    [paper-pdf](http://arxiv.org/pdf/2203.05151v4)

**Authors**: Cheng Luo, Qinliang Lin, Weicheng Xie, Bizhu Wu, Jinheng Xie, Linlin Shen

**Abstracts**: Current adversarial attack research reveals the vulnerability of learning-based classifiers against carefully crafted perturbations. However, most existing attack methods have inherent limitations in cross-dataset generalization as they rely on a classification layer with a closed set of categories. Furthermore, the perturbations generated by these methods may appear in regions easily perceptible to the human visual system (HVS). To circumvent the former problem, we propose a novel algorithm that attacks semantic similarity on feature representations. In this way, we are able to fool classifiers without limiting attacks to a specific dataset. For imperceptibility, we introduce the low-frequency constraint to limit perturbations within high-frequency components, ensuring perceptual similarity between adversarial examples and originals. Extensive experiments on three datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online platforms indicate that our attack can yield misleading and transferable adversarial examples across architectures and datasets. Additionally, visualization results and quantitative performance (in terms of four different metrics) show that the proposed algorithm generates more imperceptible perturbations than the state-of-the-art methods. Code is made available at.

摘要: 目前的对抗性攻击研究揭示了基于学习的分类器对精心设计的扰动的脆弱性。然而，大多数现有的攻击方法在跨数据集泛化方面存在固有的局限性，因为它们依赖于具有封闭类别集的分类层。此外，这些方法产生的扰动可能出现在人类视觉系统(HVS)容易察觉的区域。针对上述问题，我们提出了一种攻击特征表示语义相似度的新算法。通过这种方式，我们能够愚弄分类器，而不会将攻击限制在特定的数据集。对于不可感知性，我们引入了低频约束来限制高频分量内的扰动，以确保对抗性示例与原始示例之间的感知相似性。在三个数据集(CIFAR-10、CIFAR-100和ImageNet-1K)和三个公共在线平台上的广泛实验表明，我们的攻击可以产生跨体系结构和数据集的误导性和可转移的敌意示例。此外，可视化结果和量化性能(在四个不同的度量方面)表明，所提出的算法比现有的方法产生更多的不可察觉的扰动。代码可在上获得。



## **3. Imperceptible Transfer Attack and Defense on 3D Point Cloud Classification**

三维点云分类的潜移式攻防 cs.CV

**SubmitDate**: 2022-03-24    [paper-pdf](http://arxiv.org/pdf/2111.10990v2)

**Authors**: Daizong Liu, Wei Hu

**Abstracts**: Although many efforts have been made into attack and defense on the 2D image domain in recent years, few methods explore the vulnerability of 3D models. Existing 3D attackers generally perform point-wise perturbation over point clouds, resulting in deformed structures or outliers, which is easily perceivable by humans. Moreover, their adversarial examples are generated under the white-box setting, which frequently suffers from low success rates when transferred to attack remote black-box models. In this paper, we study 3D point cloud attacks from two new and challenging perspectives by proposing a novel Imperceptible Transfer Attack (ITA): 1) Imperceptibility: we constrain the perturbation direction of each point along its normal vector of the neighborhood surface, leading to generated examples with similar geometric properties and thus enhancing the imperceptibility. 2) Transferability: we develop an adversarial transformation model to generate the most harmful distortions and enforce the adversarial examples to resist it, improving their transferability to unknown black-box models. Further, we propose to train more robust black-box 3D models to defend against such ITA attacks by learning more discriminative point cloud representations. Extensive evaluations demonstrate that our ITA attack is more imperceptible and transferable than state-of-the-arts and validate the superiority of our defense strategy.

摘要: 虽然近年来人们在二维图像领域的攻防方面做了很多努力，但很少有方法研究三维模型的脆弱性。现有的3D攻击者一般对点云进行逐点摄动，产生变形的结构或离群点，这很容易被人察觉到。此外，它们的对抗性例子是在白盒环境下产生的，当转移到攻击远程黑盒模型时，白盒模型的成功率往往很低。本文从两个新的具有挑战性的角度对三维点云攻击进行了研究，提出了一种新的不可感知性转移攻击(ITA)：1)不可感知性：我们约束每个点沿其邻域曲面的法向量的扰动方向，从而生成具有相似几何性质的示例，从而增强了不可感知性。2)可转换性：我们建立了一个对抗性转换模型来产生最有害的扭曲，并加强了对抗性例子来抵抗它，提高了它们到未知黑盒模型的可转移性。此外，我们建议通过学习更具区别性的点云表示来训练更健壮的黑盒3D模型来防御此类ITA攻击。广泛的评估表明，我们的ITA攻击比最先进的攻击更具隐蔽性和可移动性，验证了我们防御战略的优越性。



## **4. Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation**

提高模型稳健性的对抗性训练？预测和解释都要看 cs.CL

AAAI 2022

**SubmitDate**: 2022-03-23    [paper-pdf](http://arxiv.org/pdf/2203.12709v1)

**Authors**: Hanjie Chen, Yangfeng Ji

**Abstracts**: Neural language models show vulnerability to adversarial examples which are semantically similar to their original counterparts with a few words replaced by their synonyms. A common way to improve model robustness is adversarial training which follows two steps-collecting adversarial examples by attacking a target model, and fine-tuning the model on the augmented dataset with these adversarial examples. The objective of traditional adversarial training is to make a model produce the same correct predictions on an original/adversarial example pair. However, the consistency between model decision-makings on two similar texts is ignored. We argue that a robust model should behave consistently on original/adversarial example pairs, that is making the same predictions (what) based on the same reasons (how) which can be reflected by consistent interpretations. In this work, we propose a novel feature-level adversarial training method named FLAT. FLAT aims at improving model robustness in terms of both predictions and interpretations. FLAT incorporates variational word masks in neural networks to learn global word importance and play as a bottleneck teaching the model to make predictions based on important words. FLAT explicitly shoots at the vulnerability problem caused by the mismatch between model understandings on the replaced words and their synonyms in original/adversarial example pairs by regularizing the corresponding global word importance scores. Experiments show the effectiveness of FLAT in improving the robustness with respect to both predictions and interpretations of four neural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks on four text classification tasks. The models trained via FLAT also show better robustness than baseline models on unforeseen adversarial examples across different attacks.

摘要: 神经语言模型显示出对敌意例子的脆弱性，这些例子在语义上与它们的原始对应物相似，但有几个单词被它们的同义词取代。提高模型鲁棒性的一种常见方法是对抗性训练，它遵循两个步骤-通过攻击目标模型来收集对抗性示例，并使用这些对抗性示例在扩充的数据集上对模型进行微调。传统对抗性训练的目标是使模型在原始/对抗性示例对上产生相同的正确预测。然而，两个相似文本上的模型决策之间的一致性被忽略了。我们认为一个健壮的模型应该在原始/对抗性示例对上表现一致，即基于相同的原因(如何)做出相同的预测，这些预测可以被一致的解释反映出来。在这项工作中，我们提出了一种新的特征级对抗性训练方法，称为Flat。Flat旨在提高模型在预测和解释方面的稳健性。Flat在神经网络中引入变量词掩码来学习全局词的重要性，并作为瓶颈，教导模型基于重要词进行预测。Flat通过正则化相应的全局单词重要性分数，显式地解决了替换单词与其在原始/对抗性示例对中的同义词之间的模型理解不匹配所造成的脆弱性问题。实验表明，对于4种文本分类任务上的两种敌意攻击，Flat能有效地提高4种神经网络模型(LSTM、CNN、BERT和DeBERTa)的预测和解释鲁棒性。通过Flat训练的模型在不同攻击的不可预见的对抗性实例上也表现出比基线模型更好的鲁棒性。



## **5. Enhancing Classifier Conservativeness and Robustness by Polynomiality**

利用多项式增强分类器的保守性和稳健性 cs.LG

IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022

**SubmitDate**: 2022-03-23    [paper-pdf](http://arxiv.org/pdf/2203.12693v1)

**Authors**: Ziqi Wang, Marco Loog

**Abstracts**: We illustrate the detrimental effect, such as overconfident decisions, that exponential behavior can have in methods like classical LDA and logistic regression. We then show how polynomiality can remedy the situation. This, among others, leads purposefully to random-level performance in the tails, away from the bulk of the training data. A directly related, simple, yet important technical novelty we subsequently present is softRmax: a reasoned alternative to the standard softmax function employed in contemporary (deep) neural networks. It is derived through linking the standard softmax to Gaussian class-conditional models, as employed in LDA, and replacing those by a polynomial alternative. We show that two aspects of softRmax, conservativeness and inherent gradient regularization, lead to robustness against adversarial attacks without gradient obfuscation.

摘要: 我们举例说明了指数行为在经典的LDA和Logistic回归等方法中可能产生的不利影响，如过度自信的决策。然后，我们展示了多项式是如何弥补这种情况的。这在其他方面中，故意导致尾部的随机级别的性能，而不是大量的训练数据。我们随后提出的一个直接相关、简单但重要的技术创新是softRmax：当代(深层)神经网络中采用的标准Softmax函数的合理替代方案。它是通过将标准Softmax链接到LDA中使用的高斯类条件模型，并用多项式替代来推导出来的。我们证明了软Rmax的两个方面，保守性和固有的梯度正则化，使得在没有梯度混淆的情况下对敌意攻击具有鲁棒性。



## **6. Explainability-Aware One Point Attack for Point Cloud Neural Networks**

基于可解释性的点云神经网络单点攻击 cs.CV

**SubmitDate**: 2022-03-23    [paper-pdf](http://arxiv.org/pdf/2110.04158v3)

**Authors**: Hanxiao Tan, Helena Kotthaus

**Abstracts**: With the proposition of neural networks for point clouds, deep learning has started to shine in the field of 3D object recognition while researchers have shown an increased interest to investigate the reliability of point cloud networks by adversarial attacks. However, most of the existing studies aim to deceive humans or defense algorithms, while the few that address the operation principles of the models themselves remain flawed in terms of critical point selection. In this work, we propose two adversarial methods: One Point Attack (OPA) and Critical Traversal Attack (CTA), which incorporate the explainability technologies and aim to explore the intrinsic operating principle of point cloud networks and their sensitivity against critical points perturbations. Our results show that popular point cloud networks can be deceived with almost $100\%$ success rate by shifting only one point from the input instance. In addition, we show the interesting impact of different point attribution distributions on the adversarial robustness of point cloud networks. Finally, we discuss how our approaches facilitate the explainability study for point cloud networks. To the best of our knowledge, this is the first point-cloud-based adversarial approach concerning explainability. Our code is available at https://github.com/Explain3D/Exp-One-Point-Atk-PC.

摘要: 随着神经网络在点云领域的提出，深度学习开始在三维物体识别领域闪耀光芒，而研究人员对利用对抗性攻击来研究点云网络的可靠性也表现出越来越大的兴趣。然而，现有的大多数研究都是为了欺骗人类或防御算法，而少数针对模型本身操作原理的研究在临界点选择方面仍然存在缺陷。在这项工作中，我们提出了两种对抗方法：单点攻击(OPA)和临界遍历攻击(CTA)，它们融合了可解释性技术，旨在探索点云网络的内在工作原理及其对临界点扰动的敏感性。我们的结果表明，只要从输入实例中移动一个点，流行的点云网络就可以被欺骗，成功率接近100美元。此外，我们还展示了不同的点属性分布对点云网络对抗健壮性的影响。最后，我们讨论了我们的方法如何促进点云网络的可解释性研究。据我们所知，这是第一种基于点云的对抗性解释方法。我们的代码可在https://github.com/Explain3D/Exp-One-Point-Atk-PC.获得



## **7. Adversarial Fine-tuning for Backdoor Defense: Connecting Backdoor Attacks to Adversarial Attacks**

对抗性后门防御微调：将后门攻击与对抗性攻击联系起来 cs.CV

**SubmitDate**: 2022-03-23    [paper-pdf](http://arxiv.org/pdf/2202.06312v2)

**Authors**: Bingxu Mu, Zhenxing Niu, Le Wang, Xue Wang, Rong Jin, Gang Hua

**Abstracts**: Deep neural networks (DNNs) are known to be vulnerable to both backdoor attacks as well as adversarial attacks. In the literature, these two types of attacks are commonly treated as distinct problems and solved separately, since they belong to training-time and inference-time attacks respectively. However, in this paper we find an intriguing connection between them: for a model planted with backdoors, we observe that its adversarial examples have similar behaviors as its triggered samples, i.e., both activate the same subset of DNN neurons. It indicates that planting a backdoor into a model will significantly affect the model's adversarial examples. Based on this observations, we design a new Adversarial Fine-Tuning (AFT) algorithm to defend against backdoor attacks. We empirically show that, against 5 state-of-the-art backdoor attacks, our AFT can effectively erase the backdoor triggers without obvious performance degradation on clean samples and significantly outperforms existing defense methods.

摘要: 众所周知，深度神经网络(DNNs)既容易受到后门攻击，也容易受到敌意攻击。在文献中，这两类攻击由于分别属于训练时间攻击和推理时间攻击，通常被视为不同的问题并分别解决。然而，在本文中，我们发现了它们之间一个有趣的联系：对于一个植入后门的模型，我们观察到其敌对示例与其触发样本具有相似的行为，即两者都激活了相同的DNN神经元子集。这表明在模型中植入后门将显著影响模型的对抗性示例。在此基础上，我们设计了一种新的对抗性微调(AFT)算法来防御后门攻击。我们的实验表明，对于5种最先进的后门攻击，我们的AFT可以有效地清除后门触发，而在干净的样本上没有明显的性能下降，并且显著优于现有的防御方法。



## **8. Input-specific Attention Subnetworks for Adversarial Detection**

用于敌意检测的输入特定关注子网络 cs.CL

Accepted at Findings of ACL 2022, 14 pages, 6 Tables and 9 Figures

**SubmitDate**: 2022-03-23    [paper-pdf](http://arxiv.org/pdf/2203.12298v1)

**Authors**: Emil Biju, Anirudh Sriram, Pratyush Kumar, Mitesh M Khapra

**Abstracts**: Self-attention heads are characteristic of Transformer models and have been well studied for interpretability and pruning. In this work, we demonstrate an altogether different utility of attention heads, namely for adversarial detection. Specifically, we propose a method to construct input-specific attention subnetworks (IAS) from which we extract three features to discriminate between authentic and adversarial inputs. The resultant detector significantly improves (by over 7.5%) the state-of-the-art adversarial detection accuracy for the BERT encoder on 10 NLU datasets with 11 different adversarial attack types. We also demonstrate that our method (a) is more accurate for larger models which are likely to have more spurious correlations and thus vulnerable to adversarial attack, and (b) performs well even with modest training sets of adversarial examples.

摘要: 自关注磁头是变压器模型的特征，在可解释性和剪枝方面已经得到了很好的研究。在这项工作中，我们展示了注意力头部的一种完全不同的用途，即用于敌意检测。具体地说，我们提出了一种构造输入特定关注子网络(IAS)的方法，从IAS中提取三个特征来区分真实输入和敌意输入。该检测器在具有11种不同攻击类型的10个NLU数据集上显著提高了BERT编码器的最新敌方检测精度(超过7.5%)。我们还证明了我们的方法(A)对于较大的模型更准确，这些模型可能具有更多的伪相关性，因此容易受到对抗性攻击，并且(B)即使在适度的对抗性示例训练集的情况下，我们的方法也表现得很好。



## **9. Integrity Fingerprinting of DNN with Double Black-box Design and Verification**

基于双黑盒设计和验证的DNN完整性指纹分析 cs.CR

**SubmitDate**: 2022-03-23    [paper-pdf](http://arxiv.org/pdf/2203.10902v2)

**Authors**: Shuo Wang, Sharif Abuadbba, Sidharth Agarwal, Kristen Moore, Surya Nepal, Salil Kanhere

**Abstracts**: Cloud-enabled Machine Learning as a Service (MLaaS) has shown enormous promise to transform how deep learning models are developed and deployed. Nonetheless, there is a potential risk associated with the use of such services since a malicious party can modify them to achieve an adverse result. Therefore, it is imperative for model owners, service providers, and end-users to verify whether the deployed model has not been tampered with or not. Such verification requires public verifiability (i.e., fingerprinting patterns are available to all parties, including adversaries) and black-box access to the deployed model via APIs. Existing watermarking and fingerprinting approaches, however, require white-box knowledge (such as gradient) to design the fingerprinting and only support private verifiability, i.e., verification by an honest party.   In this paper, we describe a practical watermarking technique that enables black-box knowledge in fingerprint design and black-box queries during verification. The service ensures the integrity of cloud-based services through public verification (i.e. fingerprinting patterns are available to all parties, including adversaries). If an adversary manipulates a model, this will result in a shift in the decision boundary. Thus, the underlying principle of double-black watermarking is that a model's decision boundary could serve as an inherent fingerprint for watermarking. Our approach captures the decision boundary by generating a limited number of encysted sample fingerprints, which are a set of naturally transformed and augmented inputs enclosed around the model's decision boundary in order to capture the inherent fingerprints of the model. We evaluated our watermarking approach against a variety of model integrity attacks and model compression attacks.

摘要: 支持云的机器学习即服务(MLaaS)在转变深度学习模型的开发和部署方式方面显示出巨大的潜力。尽管如此，使用此类服务仍存在潜在风险，因为恶意方可能会对其进行修改以达到不利的结果。因此，模型所有者、服务提供商和最终用户必须验证部署的模型是否未被篡改。这样的验证需要公开的可验证性(即，指纹模式对所有各方都可用，包括对手)，并且需要通过API对部署的模型进行黑盒访问。然而，现有的水印和指纹方法需要白盒知识(如梯度)来设计指纹，并且只支持私密可验证性，即由诚实的一方进行验证。在本文中，我们描述了一种实用的水印技术，它能够在指纹设计中提供黑盒知识，并在验证过程中提供黑盒查询。该服务通过公开验证(即包括对手在内的各方均可使用指纹模式)来确保云服务的完整性。如果对手操纵模型，这将导致决策边界的移动。因此，双黑水印的基本原理是模型的决策边界可以作为水印的固有指纹。我们的方法通过生成有限数量的包裹样本指纹来捕获决策边界，这些指纹是围绕模型决策边界的一组自然转换和扩充的输入，以捕获模型的固有指纹。我们针对各种模型完整性攻击和模型压缩攻击对我们的水印方法进行了评估。



## **10. Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon**

阴影可能是危险的：自然现象对物理世界的隐秘而有效的对抗性攻击 cs.CV

This paper has been accepted by CVPR2022. Code:  https://github.com/hncszyq/ShadowAttack

**SubmitDate**: 2022-03-23    [paper-pdf](http://arxiv.org/pdf/2203.03818v3)

**Authors**: Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji

**Abstracts**: Estimating the risk level of adversarial examples is essential for safely deploying machine learning models in the real world. One popular approach for physical-world attacks is to adopt the "sticker-pasting" strategy, which however suffers from some limitations, including difficulties in access to the target or printing by valid colors. A new type of non-invasive attacks emerged recently, which attempt to cast perturbation onto the target by optics based tools, such as laser beam and projector. However, the added optical patterns are artificial but not natural. Thus, they are still conspicuous and attention-grabbed, and can be easily noticed by humans. In this paper, we study a new type of optical adversarial examples, in which the perturbations are generated by a very common natural phenomenon, shadow, to achieve naturalistic and stealthy physical-world adversarial attack under the black-box setting. We extensively evaluate the effectiveness of this new attack on both simulated and real-world environments. Experimental results on traffic sign recognition demonstrate that our algorithm can generate adversarial examples effectively, reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets respectively, while continuously misleading a moving camera over 95% of the time in real-world scenarios. We also offer discussions about the limitations and the defense mechanism of this attack.

摘要: 估计对抗性示例的风险水平对于在现实世界中安全地部署机器学习模型是至关重要的。物理世界攻击的一种流行方法是采用“粘贴”策略，但该策略受到一些限制，包括难以接近目标或以有效颜色打印。最近出现了一种新型的非侵入性攻击，它试图通过激光束和投影仪等基于光学的工具对目标进行摄动。然而，添加的光学图案是人造的，但不是自然的。因此，它们仍然是引人注目和引人注目的，很容易被人类注意到。本文研究了一种新的光学对抗实例，其中的扰动是由一种非常常见的自然现象--阴影产生的，从而在黑盒环境下实现了自然主义的、隐身的物理世界对抗攻击。我们广泛评估了这种新攻击在模拟和真实环境中的有效性。在交通标志识别上的实验结果表明，该算法能够有效地生成对抗性样本，在LISA和GTSRB测试集上的成功率分别达到98.23%和90.47%，而在真实场景中，95%以上的时间都能连续误导移动的摄像机。我们还讨论了这种攻击的局限性和防御机制。



## **11. Online Adversarial Attacks**

在线对抗性攻击 cs.LG

ICLR 2022

**SubmitDate**: 2022-03-22    [paper-pdf](http://arxiv.org/pdf/2103.02014v4)

**Authors**: Andjela Mladenovic, Avishek Joey Bose, Hugo Berard, William L. Hamilton, Simon Lacoste-Julien, Pascal Vincent, Gauthier Gidel

**Abstracts**: Adversarial attacks expose important vulnerabilities of deep learning models, yet little attention has been paid to settings where data arrives as a stream. In this paper, we formalize the online adversarial attack problem, emphasizing two key elements found in real-world use-cases: attackers must operate under partial knowledge of the target model, and the decisions made by the attacker are irrevocable since they operate on a transient data stream. We first rigorously analyze a deterministic variant of the online threat model by drawing parallels to the well-studied $k$-secretary problem in theoretical computer science and propose Virtual+, a simple yet practical online algorithm. Our main theoretical result shows Virtual+ yields provably the best competitive ratio over all single-threshold algorithms for $k<5$ -- extending the previous analysis of the $k$-secretary problem. We also introduce the \textit{stochastic $k$-secretary} -- effectively reducing online blackbox transfer attacks to a $k$-secretary problem under noise -- and prove theoretical bounds on the performance of Virtual+ adapted to this setting. Finally, we complement our theoretical results by conducting experiments on MNIST, CIFAR-10, and Imagenet classifiers, revealing the necessity of online algorithms in achieving near-optimal performance and also the rich interplay between attack strategies and online attack selection, enabling simple strategies like FGSM to outperform stronger adversaries.

摘要: 对抗性攻击暴露了深度学习模型的重要漏洞，但很少有人关注数据以流的形式到达的设置。本文对在线敌意攻击问题进行了形式化描述，强调了在实际用例中发现的两个关键因素：攻击者必须在部分了解目标模型的情况下操作，并且攻击者的决策是不可撤销的，因为他们操作的是瞬态数据流。我们首先对在线威胁模型的一个确定性变体进行了严格的分析，将其与理论计算机科学中研究得很好的$k$秘书问题进行了比较，并提出了一种简单实用的在线算法Virtual+。我们的主要理论结果表明，对于$k<5$，Virtual+在所有单阈值算法中可以产生最优的好胜比--扩展了之前对$k$秘书问题的分析。我们还引入了\textit{随机$k$-秘书}--在噪声环境下有效地将在线黑盒传输攻击归结为一个$k$-秘书问题--并证明了适用于此设置的Virtual+性能的理论界限。最后，我们通过在MNIST、CIFAR-10和Imagenet分类器上进行实验来补充我们的理论结果，揭示了在线算法获得接近最优性能的必要性，以及攻击策略和在线攻击选择之间的丰富交互作用，使FGSM等简单策略的性能优于更强大的对手。



## **12. NNReArch: A Tensor Program Scheduling Framework Against Neural Network Architecture Reverse Engineering**

NNReArch：一种抗神经网络结构逆向工程的张量程序调度框架 cs.CR

Accepted by FCCM 2022

**SubmitDate**: 2022-03-22    [paper-pdf](http://arxiv.org/pdf/2203.12046v1)

**Authors**: Yukui Luo, Shijin Duan, Cheng Gongye, Yunsi Fei, Xiaolin Xu

**Abstracts**: Architecture reverse engineering has become an emerging attack against deep neural network (DNN) implementations. Several prior works have utilized side-channel leakage to recover the model architecture while the target is executing on a hardware acceleration platform. In this work, we target an open-source deep-learning accelerator, Versatile Tensor Accelerator (VTA), and utilize electromagnetic (EM) side-channel leakage to comprehensively learn the association between DNN architecture configurations and EM emanations. We also consider the holistic system -- including the low-level tensor program code of the VTA accelerator on a Xilinx FPGA and explore the effect of such low-level configurations on the EM leakage. Our study demonstrates that both the optimization and configuration of tensor programs will affect the EM side-channel leakage.   Gaining knowledge of the association between the low-level tensor program and the EM emanations, we propose NNReArch, a lightweight tensor program scheduling framework against side-channel-based DNN model architecture reverse engineering. Specifically, NNReArch targets reshaping the EM traces of different DNN operators, through scheduling the tensor program execution of the DNN model so as to confuse the adversary. NNReArch is a comprehensive protection framework supporting two modes, a balanced mode that strikes a balance between the DNN model confidentiality and execution performance, and a secure mode where the most secure setting is chosen. We implement and evaluate the proposed framework on the open-source VTA with state-of-the-art DNN architectures. The experimental results demonstrate that NNReArch can efficiently enhance the model architecture security with a small performance overhead. In addition, the proposed obfuscation technique makes reverse engineering of the DNN architecture significantly harder.

摘要: 体系结构逆向工程已经成为对深度神经网络(DNN)实现的一种新兴攻击。已有的几个工作已经利用侧信道泄漏来恢复目标在硬件加速平台上执行时的模型体系结构。在这项工作中，我们以开源的深度学习加速器-通用张量加速器(VTA)为目标，并利用电磁(EM)侧通道泄漏来全面了解DNN架构配置与EM发射之间的关联。我们还考虑了整个系统--包括Xilinx FPGA上VTA加速器的低电平张量程序代码，并探索了这种低电平配置对电磁泄漏的影响。我们的研究表明，张量程序的优化和配置都会影响电磁侧沟道泄漏。通过了解低级张量程序与EM发射之间的关联，我们提出了一种轻量级张量程序调度框架NNReArch，该框架针对基于侧通道的DNN模型体系结构逆向工程。具体地说，NNReArch的目标是通过调度DNN模型的张量程序执行来重塑不同DNN算子的EM轨迹，从而迷惑对手。NNReArch是一个全面的保护框架，支持两种模式，一种是在DNN模型机密性和执行性能之间取得平衡的平衡模式，另一种是选择最安全设置的安全模式。我们在采用最先进的DNN体系结构的开源VTA上实现了该框架，并对其进行了评估。实验结果表明，NNReArch能够以较小的性能开销有效地增强模型体系结构的安全性。此外，所提出的模糊技术使得DNN体系结构的逆向工程变得非常困难。



## **13. Shape-invariant 3D Adversarial Point Clouds**

形状不变的三维对抗性点云 cs.CV

Accepted at CVPR 2022

**SubmitDate**: 2022-03-22    [paper-pdf](http://arxiv.org/pdf/2203.04041v2)

**Authors**: Qidong Huang, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Nenghai Yu

**Abstracts**: Adversary and invisibility are two fundamental but conflict characters of adversarial perturbations. Previous adversarial attacks on 3D point cloud recognition have often been criticized for their noticeable point outliers, since they just involve an "implicit constrain" like global distance loss in the time-consuming optimization to limit the generated noise. While point cloud is a highly structured data format, it is hard to constrain its perturbation with a simple loss or metric properly. In this paper, we propose a novel Point-Cloud Sensitivity Map to boost both the efficiency and imperceptibility of point perturbations. This map reveals the vulnerability of point cloud recognition models when encountering shape-invariant adversarial noises. These noises are designed along the shape surface with an "explicit constrain" instead of extra distance loss. Specifically, we first apply a reversible coordinate transformation on each point of the point cloud input, to reduce one degree of point freedom and limit its movement on the tangent plane. Then we calculate the best attacking direction with the gradients of the transformed point cloud obtained on the white-box model. Finally we assign each point with a non-negative score to construct the sensitivity map, which benefits both white-box adversarial invisibility and black-box query-efficiency extended in our work. Extensive evaluations prove that our method can achieve the superior performance on various point cloud recognition models, with its satisfying adversarial imperceptibility and strong resistance to different point cloud defense settings. Our code is available at: https://github.com/shikiw/SI-Adv.

摘要: 对抗性和隐蔽性是对抗性扰动的两个基本但又相互冲突的特征。以前针对3D点云识别的敌意攻击经常因为其明显的点离群值而受到批评，因为它们只是在耗时的优化过程中涉及诸如全局距离损失这样的“隐式约束”，以限制生成的噪声。虽然点云是一种高度结构化的数据格式，但很难用简单的损失或度量来恰当地约束它的扰动。在本文中，我们提出了一种新的点云敏感度图，以提高点扰动的效率和隐蔽性。这张地图揭示了点云识别模型在遇到形状不变的对抗性噪声时的脆弱性。这些噪波是沿着形状表面设计的，带有“显式约束”，而不是额外的距离损失。具体地说，我们首先对点云输入的每个点应用可逆坐标变换，以减少一个点自由度并限制其在切面上的移动。然后利用白盒模型得到的变换后的点云梯度计算最佳攻击方向。最后，我们给每个点分配一个非负分数来构造敏感度图，这样既有利于白盒对抗不可见性，也有利于提高黑盒查询效率。广泛的评测表明，该方法在各种点云识别模型上都能取得较好的性能，具有令人满意的对抗性和对不同点云防御设置的较强抵抗力。我们的代码可从以下网址获得：https://github.com/shikiw/SI-Adv.



## **14. Semi-Targeted Model Poisoning Attack on Federated Learning via Backward Error Analysis**

基于后向误差分析的联合学习半目标模型中毒攻击 cs.LG

**SubmitDate**: 2022-03-22    [paper-pdf](http://arxiv.org/pdf/2203.11633v1)

**Authors**: Yuwei Sun, Hideya Ochiai, Jun Sakuma

**Abstracts**: Model poisoning attacks on federated learning (FL) intrude in the entire system via compromising an edge model, resulting in malfunctioning of machine learning models. Such compromised models are tampered with to perform adversary-desired behaviors. In particular, we considered a semi-targeted situation where the source class is predetermined however the target class is not. The goal is to cause the global classifier to misclassify data of the source class. Though approaches such as label flipping have been adopted to inject poisoned parameters into FL, it has been shown that their performances are usually class-sensitive varying with different target classes applied. Typically, an attack can become less effective when shifting to a different target class. To overcome this challenge, we propose the Attacking Distance-aware Attack (ADA) to enhance a poisoning attack by finding the optimized target class in the feature space. Moreover, we studied a more challenging situation where an adversary had limited prior knowledge about a client's data. To tackle this problem, ADA deduces pair-wise distances between different classes in the latent feature space from shared model parameters based on the backward error analysis. We performed extensive empirical evaluations on ADA by varying the factor of attacking frequency in three different image classification tasks. As a result, ADA succeeded in increasing the attack performance by 1.8 times in the most challenging case with an attacking frequency of 0.01.

摘要: 针对联邦学习(FL)的模型中毒攻击通过破坏边缘模型侵入整个系统，导致机器学习模型失效。这种被破坏的模型被篡改以执行对手期望的行为。特别地，我们考虑了一种半目标情况，其中源类是预先确定的，而目标类不是。目标是使全局分类器对源类的数据进行错误分类。虽然已经采用了标签翻转等方法向FL注入有毒参数，但研究表明，它们的性能通常是类敏感的，随着所使用的目标类的不同，它们的性能也会有所不同。通常，当转移到不同的目标类别时，攻击可能会变得不那么有效。为了克服这一挑战，我们提出了攻击距离感知攻击(ADA)，通过在特征空间中寻找优化的目标类来增强中毒攻击。此外，我们研究了一种更具挑战性的情况，在这种情况下，对手对客户数据的先验知识有限。针对撞击的这一问题，该算法基于后向误差分析，从共享的模型参数中推导出潜在特征空间中不同类别之间的成对距离。在三种不同的图像分类任务中，通过改变攻击频率因子，我们对ADA进行了广泛的经验评估。结果，在攻击频率为0.01的最具挑战性的情况下，ADA成功地将攻击性能提高了1.8倍。



## **15. Exploring High-Order Structure for Robust Graph Structure Learning**

高阶结构在稳健图结构学习中的探索 cs.LG

**SubmitDate**: 2022-03-22    [paper-pdf](http://arxiv.org/pdf/2203.11492v1)

**Authors**: Guangqian Yang, Yibing Zhan, Jinlong Li, Baosheng Yu, Liu Liu, Fengxiang He

**Abstracts**: Recent studies show that Graph Neural Networks (GNNs) are vulnerable to adversarial attack, i.e., an imperceptible structure perturbation can fool GNNs to make wrong predictions. Some researches explore specific properties of clean graphs such as the feature smoothness to defense the attack, but the analysis of it has not been well-studied. In this paper, we analyze the adversarial attack on graphs from the perspective of feature smoothness which further contributes to an efficient new adversarial defensive algorithm for GNNs. We discover that the effect of the high-order graph structure is a smoother filter for processing graph structures. Intuitively, the high-order graph structure denotes the path number between nodes, where larger number indicates closer connection, so it naturally contributes to defense the adversarial perturbation. Further, we propose a novel algorithm that incorporates the high-order structural information into the graph structure learning. We perform experiments on three popular benchmark datasets, Cora, Citeseer and Polblogs. Extensive experiments demonstrate the effectiveness of our method for defending against graph adversarial attacks.

摘要: 最近的研究表明，图神经网络(GNNs)很容易受到敌意攻击，即潜移默化的结构扰动会欺骗GNNs做出错误的预测。一些研究探索了干净图的一些特殊性质，如特征光滑性来防御攻击，但对它的分析还没有得到很好的研究。本文从特征光滑性的角度对图的对抗性攻击进行了分析，进一步提出了一种新的高效的GNNs对抗性防御算法。我们发现，高阶图结构的影响是处理图结构的更平滑的过滤。直观地说，高阶图结构表示节点之间的路径数，数值越大表示连接越紧密，自然有助于防御对抗性扰动。在此基础上，提出了一种将高阶结构信息融入到图结构学习中的新算法。我们在三个流行的基准数据集CORA、Citeseer和Polblog上进行了实验。大量实验证明了该方法在抵御图攻击方面的有效性。



## **16. Making DeepFakes more spurious: evading deep face forgery detection via trace removal attack**

让DeepFake变得更加虚假：通过痕迹移除攻击来逃避深度人脸伪造检测 cs.CV

**SubmitDate**: 2022-03-22    [paper-pdf](http://arxiv.org/pdf/2203.11433v1)

**Authors**: Chi Liu, Huajie Chen, Tianqing Zhu, Jun Zhang, Wanlei Zhou

**Abstracts**: DeepFakes are raising significant social concerns. Although various DeepFake detectors have been developed as forensic countermeasures, these detectors are still vulnerable to attacks. Recently, a few attacks, principally adversarial attacks, have succeeded in cloaking DeepFake images to evade detection. However, these attacks have typical detector-specific designs, which require prior knowledge about the detector, leading to poor transferability. Moreover, these attacks only consider simple security scenarios. Less is known about how effective they are in high-level scenarios where either the detectors or the attacker's knowledge varies. In this paper, we solve the above challenges with presenting a novel detector-agnostic trace removal attack for DeepFake anti-forensics. Instead of investigating the detector side, our attack looks into the original DeepFake creation pipeline, attempting to remove all detectable natural DeepFake traces to render the fake images more "authentic". To implement this attack, first, we perform a DeepFake trace discovery, identifying three discernible traces. Then a trace removal network (TR-Net) is proposed based on an adversarial learning framework involving one generator and multiple discriminators. Each discriminator is responsible for one individual trace representation to avoid cross-trace interference. These discriminators are arranged in parallel, which prompts the generator to remove various traces simultaneously. To evaluate the attack efficacy, we crafted heterogeneous security scenarios where the detectors were embedded with different levels of defense and the attackers' background knowledge of data varies. The experimental results show that the proposed attack can significantly compromise the detection accuracy of six state-of-the-art DeepFake detectors while causing only a negligible loss in visual quality to the original DeepFake samples.

摘要: DeepFake引起了重大的社会关注。虽然已经开发了各种DeepFake检测器作为取证对策，但这些检测器仍然容易受到攻击。最近，一些攻击，主要是对抗性攻击，成功地伪装DeepFake图像以逃避检测。然而，这些攻击具有典型的特定于检测器的设计，需要事先了解检测器，导致可移植性较差。此外，这些攻击只考虑简单的安全场景。对于它们在检测器或攻击者的知识各不相同的高级场景中的有效性，我们知之甚少。在本文中，我们提出了一种新的针对DeepFake反取证的与检测器无关的痕迹移除攻击，从而解决了上述挑战。我们的攻击不是调查探测器端，而是查看原始的DeepFake创建管道，试图删除所有可检测到的自然DeepFake痕迹，以使虚假图像更“真实”。要实现此攻击，首先，我们执行DeepFake跟踪发现，识别三个可识别的跟踪。然后，基于一个生成器和多个鉴别器的对抗性学习框架，提出了一种痕迹去除网络(TR-Net)。每个鉴别器负责一个单独的轨迹表示，以避免交叉轨迹干扰。这些鉴别器是并行排列的，这会提示发生器同时移除各种痕迹。为了评估攻击效能，我们精心设计了不同的安全场景，其中检测器嵌入了不同级别的防御，攻击者的数据背景知识各不相同。实验结果表明，该攻击可以显著降低现有的6种DeepFake检测器的检测精度，而对原始DeepFake样本的视觉质量损失可以忽略不计。



## **17. Subspace Adversarial Training**

子空间对抗训练 cs.LG

CVPR2022

**SubmitDate**: 2022-03-22    [paper-pdf](http://arxiv.org/pdf/2111.12229v2)

**Authors**: Tao Li, Yingwen Wu, Sizhe Chen, Kun Fang, Xiaolin Huang

**Abstracts**: Single-step adversarial training (AT) has received wide attention as it proved to be both efficient and robust. However, a serious problem of catastrophic overfitting exists, i.e., the robust accuracy against projected gradient descent (PGD) attack suddenly drops to 0% during the training. In this paper, we approach this problem from a novel perspective of optimization and firstly reveal the close link between the fast-growing gradient of each sample and overfitting, which can also be applied to understand robust overfitting in multi-step AT. To control the growth of the gradient, we propose a new AT method, Subspace Adversarial Training (Sub-AT), which constrains AT in a carefully extracted subspace. It successfully resolves both kinds of overfitting and significantly boosts the robustness. In subspace, we also allow single-step AT with larger steps and larger radius, further improving the robustness performance. As a result, we achieve state-of-the-art single-step AT performance. Without any regularization term, our single-step AT can reach over 51% robust accuracy against strong PGD-50 attack of radius 8/255 on CIFAR-10, reaching a competitive performance against standard multi-step PGD-10 AT with huge computational advantages. The code is released at https://github.com/nblt/Sub-AT.

摘要: 单步对抗性训练(AT)因其高效、健壮而受到广泛关注。然而，存在一个严重的灾难性过拟合问题，即在训练过程中，对投影梯度下降(PGD)攻击的鲁棒精度突然下降到0%。本文从一个新的优化角度来研究这一问题，首次揭示了每个样本快速增长的梯度与过拟合之间的密切联系，这也可以用来理解多步AT中的鲁棒过拟合。为了控制梯度的增长，我们提出了一种新的AT方法-子空间对抗训练(Sub-AT)，它将AT约束在一个仔细提取的子空间中。它成功地解决了这两种过拟合问题，并显著提高了鲁棒性。在子空间中，我们还允许步长更大、半径更大的单步AT，进一步提高了算法的鲁棒性。因此，我们实现了最先进的单步AT性能。在没有任何正则化项的情况下，我们的单步AT对CIFAR-10上半径为8/255时的强PGD-50攻击可以达到51%以上的鲁棒准确率，与标准多步PGD-10AT相比达到了好胜的性能，并且具有巨大的计算优势。该代码在https://github.com/nblt/Sub-AT.上发布



## **18. On The Robustness of Offensive Language Classifiers**

论攻击性语言量词的稳健性 cs.CL

9 pages, 2 figures, Accepted at ACL 2022

**SubmitDate**: 2022-03-21    [paper-pdf](http://arxiv.org/pdf/2203.11331v1)

**Authors**: Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan

**Abstracts**: Social media platforms are deploying machine learning based offensive language classification systems to combat hateful, racist, and other forms of offensive speech at scale. However, despite their real-world deployment, we do not yet comprehensively understand the extent to which offensive language classifiers are robust against adversarial attacks. Prior work in this space is limited to studying robustness of offensive language classifiers against primitive attacks such as misspellings and extraneous spaces. To address this gap, we systematically analyze the robustness of state-of-the-art offensive language classifiers against more crafty adversarial attacks that leverage greedy- and attention-based word selection and context-aware embeddings for word replacement. Our results on multiple datasets show that these crafty adversarial attacks can degrade the accuracy of offensive language classifiers by more than 50% while also being able to preserve the readability and meaning of the modified text.

摘要: 社交媒体平台正在部署基于机器学习的攻击性语言分类系统，以大规模打击仇恨、种族主义和其他形式的攻击性言论。然而，尽管在现实世界中部署了攻击性语言分类器，但我们还没有全面了解攻击性语言分类器在多大程度上对对手攻击具有健壮性。以前在这个领域的工作仅限于研究攻击性语言分类器对原始攻击(如拼写错误和无关空格)的健壮性。为了弥补这一差距，我们系统地分析了最先进的攻击性语言分类器对更狡猾的对手攻击的鲁棒性，这些攻击利用基于贪婪和注意力的单词选择和上下文感知嵌入进行单词替换。我们在多个数据集上的实验结果表明，这些狡猾的敌意攻击可以使攻击性语言分类器的准确率降低50%以上，同时还能够保持修改后的文本的可读性和意义。



## **19. FGAN: Federated Generative Adversarial Networks for Anomaly Detection in Network Traffic**

FGAN：用于网络流量异常检测的联合生成对抗网络 cs.CR

8 pages, 2 figures

**SubmitDate**: 2022-03-21    [paper-pdf](http://arxiv.org/pdf/2203.11106v1)

**Authors**: Sankha Das

**Abstracts**: Over the last two decades, a lot of work has been done in improving network security, particularly in intrusion detection systems (IDS) and anomaly detection. Machine learning solutions have also been employed in IDSs to detect known and plausible attacks in incoming traffic. Parameters such as packet contents, sender IP and sender port, connection duration, etc. have been previously used to train these machine learning models to learn to differentiate genuine traffic from malicious ones. Generative Adversarial Networks (GANs) have been significantly successful in detecting such anomalies, mostly attributed to the adversarial training of the generator and discriminator in an attempt to bypass each other and in turn increase their own power and accuracy. However, in large networks having a wide variety of traffic at possibly different regions of the network and susceptible to a large number of potential attacks, training these GANs for a particular kind of anomaly may make it oblivious to other anomalies and attacks. In addition, the dataset required to train these models has to be made centrally available and publicly accessible, posing the obvious question of privacy of the communications of the respective participants of the network. The solution proposed in this work aims at tackling the above two issues by using GANs in a federated architecture in networks of such scale and capacity. In such a setting, different users of the network will be able to train and customize a centrally available adversarial model according to their own frequently faced conditions. Simultaneously, the member users of the network will also able to gain from the experiences of the other users in the network.

摘要: 在过去的二十年里，人们在提高网络安全方面做了大量的工作，特别是在入侵检测系统(IDS)和异常检测方面。机器学习解决方案也已用于IDS中，以检测传入流量中的已知和可能的攻击。诸如分组内容、发送者IP和发送者端口、连接持续时间等参数先前已用于训练这些机器学习模型，以学习区分真实流量和恶意流量。生成性对抗性网络(GANS)在检测这类异常方面取得了很大的成功，这主要归功于生成器和鉴别器的对抗性训练，试图绕过对方，进而提高自己的能力和准确性。但是，在可能在网络的不同区域具有各种通信量并且容易受到大量潜在攻击的大型网络中，针对特定类型的异常对这些GAN进行训练可能会使其容易受到其他异常和攻击的攻击。此外，训练这些模型所需的数据集必须集中提供并向公众开放，这就提出了明显的网络参与者通信隐私问题。本文提出的解决方案旨在解决上述两个问题，即在如此规模和容量的网络中使用联合架构中的GAN来解决上述两个问题。在这样的设置下，网络的不同用户将能够根据他们自己经常面对的情况来训练和定制集中可用的对抗模型。同时，网络的成员用户也将能够从网络中的其他用户的体验中获益。



## **20. An Intermediate-level Attack Framework on The Basis of Linear Regression**

一种基于线性回归的中级攻击框架 cs.CV

**SubmitDate**: 2022-03-21    [paper-pdf](http://arxiv.org/pdf/2203.10723v1)

**Authors**: Yiwen Guo, Qizhang Li, Wangmeng Zuo, Hao Chen

**Abstracts**: This paper substantially extends our work published at ECCV, in which an intermediate-level attack was proposed to improve the transferability of some baseline adversarial examples. We advocate to establish a direct linear mapping from the intermediate-level discrepancies (between adversarial features and benign features) to classification prediction loss of the adversarial example. In this paper, we delve deep into the core components of such a framework by performing comprehensive studies and extensive experiments. We show that 1) a variety of linear regression models can all be considered in order to establish the mapping, 2) the magnitude of the finally obtained intermediate-level discrepancy is linearly correlated with adversarial transferability, 3) further boost of the performance can be achieved by performing multiple runs of the baseline attack with random initialization. By leveraging these findings, we achieve new state-of-the-arts on transfer-based $\ell_\infty$ and $\ell_2$ attacks.

摘要: 本文大大扩展了我们在ECCV上发表的工作，其中提出了一种中级攻击来提高一些基线对抗性示例的可移植性。我们主张建立一个直接的线性映射，从对抗性实例的中间层差异(对抗性特征与良性特征之间)到分类预测损失之间建立一个直接的线性映射。在本文中，我们通过全面的研究和广泛的实验，深入研究了这一框架的核心组成部分。我们表明：1)为了建立映射，可以考虑多种线性回归模型；2)最终得到的中间层差异的大小与对抗可转移性线性相关；3)通过随机初始化执行多次基线攻击，可以进一步提高性能。通过利用这些发现，我们实现了针对基于传输的$\ell_\infty$和$\ell_2$攻击的新技术。



## **21. A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement**

一种基于提示的对抗性实例生成及健壮性增强方法 cs.CL

**SubmitDate**: 2022-03-21    [paper-pdf](http://arxiv.org/pdf/2203.10714v1)

**Authors**: Yuting Yang, Pei Huang, Juan Cao, Jintao Li, Yun Lin, Jin Song Dong, Feifei Ma, Jian Zhang

**Abstracts**: Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.

摘要: 近年来，NLP模型在金融、医疗、新闻媒体等关键领域得到了广泛应用，引起了人们对模型健壮性和脆弱性的担忧。在本文中，我们提出了一种新的基于提示的对抗性攻击来折衷NLP模型和健壮性增强技术。我们首先为每个实例构建恶意提示，并在恶意目的的影响下通过掩码和填充生成敌意实例。我们的攻击技术针对NLP模型的固有漏洞，允许我们生成样本，即使不与受害者NLP模型交互，只要它基于预先训练的语言模型(PLM)。此外，我们还设计了一种基于提示的对抗性训练方法来提高PLM的健壮性。由于我们的训练方法并不实际生成对抗性样本，因此可以有效地应用于大规模训练集。实验结果表明，该攻击方法具有更丰富、更流畅、更自然的对抗性实例，攻击成功率较高。此外，我们的稳健性增强方法可以显著提高模型抵抗对手攻击的稳健性。我们的工作表明，激励范式在探测PLM的一些根本缺陷并为下游任务进行微调方面具有巨大的潜力。



## **22. Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition**

利用专家引导的对抗性增强改进命名实体识别中的泛化 cs.CL

ACL 2022 (Findings)

**SubmitDate**: 2022-03-21    [paper-pdf](http://arxiv.org/pdf/2203.10693v1)

**Authors**: Aaron Reich, Jiaao Chen, Aastha Agrawal, Yanzhe Zhang, Diyi Yang

**Abstracts**: Named Entity Recognition (NER) systems often demonstrate great performance on in-distribution data, but perform poorly on examples drawn from a shifted distribution. One way to evaluate the generalization ability of NER models is to use adversarial examples, on which the specific variations associated with named entities are rarely considered. To this end, we propose leveraging expert-guided heuristics to change the entity tokens and their surrounding contexts thereby altering their entity types as adversarial attacks. Using expert-guided heuristics, we augmented the CoNLL 2003 test set and manually annotated it to construct a high-quality challenging set. We found that state-of-the-art NER systems trained on CoNLL 2003 training data drop performance dramatically on our challenging set. By training on adversarial augmented training examples and using mixup for regularization, we were able to significantly improve the performance on the challenging set as well as improve out-of-domain generalization which we evaluated by using OntoNotes data. We have publicly released our dataset and code at https://github.com/GT-SALT/Guided-Adversarial-Augmentation.

摘要: 命名实体识别(NER)系统通常在分布内数据上表现出很好的性能，但在从平移分布中提取的示例上表现得很差。评估NER模型泛化能力的一种方法是使用对抗性例子，在这些例子上很少考虑与命名实体相关的具体变化。为此，我们建议利用专家指导的启发式方法来更改实体令牌及其周围上下文，从而将其实体类型更改为对抗性攻击。使用专家指导的启发式算法，我们扩充了CoNLL 2003测试集，并手动对其进行注释，以构建高质量的挑战性测试集。我们发现，在我们具有挑战性的集合中，使用CoNLL 2003训练数据训练的最先进的NER系统会显著降低性能。通过对抗性增强训练实例的训练和使用混合正则化，我们能够显著提高在具有挑战性的集合上的性能，以及改善我们使用OntoNotes数据评估的域外泛化。我们已经在https://github.com/GT-SALT/Guided-Adversarial-Augmentation.公开发布了我们的数据集和代码



## **23. RareGAN: Generating Samples for Rare Classes**

RareGan：为稀有类生成样本 cs.LG

Published in AAAI 2022

**SubmitDate**: 2022-03-20    [paper-pdf](http://arxiv.org/pdf/2203.10674v1)

**Authors**: Zinan Lin, Hao Liang, Giulia Fanti, Vyas Sekar

**Abstracts**: We study the problem of learning generative adversarial networks (GANs) for a rare class of an unlabeled dataset subject to a labeling budget. This problem is motivated from practical applications in domains including security (e.g., synthesizing packets for DNS amplification attacks), systems and networking (e.g., synthesizing workloads that trigger high resource usage), and machine learning (e.g., generating images from a rare class). Existing approaches are unsuitable, either requiring fully-labeled datasets or sacrificing the fidelity of the rare class for that of the common classes. We propose RareGAN, a novel synthesis of three key ideas: (1) extending conditional GANs to use labelled and unlabelled data for better generalization; (2) an active learning approach that requests the most useful labels; and (3) a weighted loss function to favor learning the rare class. We show that RareGAN achieves a better fidelity-diversity tradeoff on the rare class than prior work across different applications, budgets, rare class fractions, GAN losses, and architectures.

摘要: 我们研究了一类稀有的未标记数据集的生成性对抗网络(GANS)的学习问题，这类数据集受标记预算的约束。该问题源于领域中的实际应用，包括安全(例如，合成用于DNS放大攻击的分组)、系统和联网(例如，合成触发高资源使用率的工作负载)以及机器学习(例如，从罕见类生成图像)。现有的方法是不合适的，要么需要完全标记的数据集，要么牺牲稀有类的保真度来换取普通类的保真度。我们提出了RareGAN，这是一种新的综合了三个关键思想的方法：(1)扩展条件Gans以使用有标签和无标签的数据进行更好的泛化；(2)要求最有用的标签的主动学习方法；以及(3)有利于学习稀有类的加权损失函数。我们表明，与以前的工作相比，RareGAN在不同的应用、预算、稀有类分数、GaN损耗和架构上在稀有类上实现了更好的保真度-分集折衷。



## **24. Does DQN really learn? Exploring adversarial training schemes in Pong**

DQN真的会学习吗？探索乒乓球对抗性训练方案 cs.LG

RLDM 2022

**SubmitDate**: 2022-03-20    [paper-pdf](http://arxiv.org/pdf/2203.10614v1)

**Authors**: Bowen He, Sreehari Rammohan, Jessica Forde, Michael Littman

**Abstracts**: In this work, we study two self-play training schemes, Chainer and Pool, and show they lead to improved agent performance in Atari Pong compared to a standard DQN agent -- trained against the built-in Atari opponent. To measure agent performance, we define a robustness metric that captures how difficult it is to learn a strategy that beats the agent's learned policy. Through playing past versions of themselves, Chainer and Pool are able to target weaknesses in their policies and improve their resistance to attack. Agents trained using these methods score well on our robustness metric and can easily defeat the standard DQN agent. We conclude by using linear probing to illuminate what internal structures the different agents develop to play the game. We show that training agents with Chainer or Pool leads to richer network activations with greater predictive power to estimate critical game-state features compared to the standard DQN agent.

摘要: 在这项工作中，我们研究了两种自我发挥训练方案，Chainer和Pool，并证明与标准的DQN代理相比，它们在Atari Pong中的代理性能有所提高--针对内置的Atari对手进行训练。为了衡量代理的性能，我们定义了一个健壮性度量，该度量捕获了学习超过代理的学习策略的策略有多难。通过扮演过去版本的自己，Chainer和Pool能够针对他们政策中的弱点，并提高他们对攻击的抵抗力。使用这些方法训练的代理在我们的健壮性度量上得分很好，可以很容易地击败标准的DQN代理。我们最后使用线性探测来说明不同的代理在玩游戏时发展了哪些内部结构。我们表明，与标准的DQN代理相比，用Chainer或Pool训练代理可以导致更丰富的网络激活，并具有更强的预测能力来估计关键的游戏状态特征。



## **25. Improved Semi-Quantum Key Distribution with Two Almost-Classical Users**

改进的两个准经典用户的半量子密钥分配 quant-ph

**SubmitDate**: 2022-03-20    [paper-pdf](http://arxiv.org/pdf/2203.10567v1)

**Authors**: Saachi Mutreja, Walter O. Krawec

**Abstracts**: Semi-quantum key distribution (SQKD) protocols attempt to establish a shared secret key between users, secure against computationally unbounded adversaries. Unlike standard quantum key distribution protocols, SQKD protocols contain at least one user who is limited in their quantum abilities and is almost "classical" in nature. In this paper, we revisit a mediated semi-quantum key distribution protocol, introduced by Massa et al., in 2019, where users need only the ability to detect a qubit, or reflect a qubit; they do not need to perform any other basis measurement; nor do they need to prepare quantum signals. Users require the services of a quantum server which may be controlled by the adversary. In this paper, we show how this protocol may be extended to improve its efficiency and also its noise tolerance. We discuss an extension which allows more communication rounds to be directly usable; we analyze the key-rate of this extension in the asymptotic scenario for a particular class of attacks and compare with prior work. Finally, we evaluate the protocol's performance in a variety of lossy and noisy channels.

摘要: 半量子密钥分发(SQKD)协议试图在用户之间建立共享密钥，该密钥在计算上不受限制的攻击者是安全的。与标准量子密钥分发协议不同，SQKD协议至少包含一个用户，该用户的量子能力是有限的，并且本质上几乎是“经典的”。在这篇文章中，我们回顾了由Massa等人在2019年提出的一个中介半量子密钥分发协议，其中用户只需要检测或反映量子比特的能力，他们不需要执行任何其他的基测量，也不需要准备量子信号。用户需要可能由对手控制的量子服务器的服务。在本文中，我们展示了如何对该协议进行扩展以提高其效率和抗噪性。我们讨论了一种允许更多通信轮次直接使用的扩展，分析了该扩展在一类特定攻击的渐近场景下的键率，并与以前的工作进行了比较。最后，我们对该协议在各种有损和噪声信道中的性能进行了评估。



## **26. Adversarial Parameter Attack on Deep Neural Networks**

基于深度神经网络的对抗性参数攻击 cs.LG

**SubmitDate**: 2022-03-20    [paper-pdf](http://arxiv.org/pdf/2203.10502v1)

**Authors**: Lijia Yu, Yihan Wang, Xiao-Shan Gao

**Abstracts**: In this paper, a new parameter perturbation attack on DNNs, called adversarial parameter attack, is proposed, in which small perturbations to the parameters of the DNN are made such that the accuracy of the attacked DNN does not decrease much, but its robustness becomes much lower. The adversarial parameter attack is stronger than previous parameter perturbation attacks in that the attack is more difficult to be recognized by users and the attacked DNN gives a wrong label for any modified sample input with high probability. The existence of adversarial parameters is proved. For a DNN $F_{\Theta}$ with the parameter set $\Theta$ satisfying certain conditions, it is shown that if the depth of the DNN is sufficiently large, then there exists an adversarial parameter set $\Theta_a$ for $\Theta$ such that the accuracy of $F_{\Theta_a}$ is equal to that of $F_{\Theta}$, but the robustness measure of $F_{\Theta_a}$ is smaller than any given bound. An effective training algorithm is given to compute adversarial parameters and numerical experiments are used to demonstrate that the algorithms are effective to produce high quality adversarial parameters.

摘要: 本文提出了一种新的DNN参数扰动攻击，称为对抗性参数攻击，通过对DNN的参数进行微小的扰动，使得被攻击的DNN的准确率不会降低太多，但鲁棒性却大大降低。对抗性参数攻击比以往的参数扰动攻击更强，因为攻击更难被用户识别，并且被攻击的DNN对任何修改后的样本输入都给出了错误的标签，概率很高。证明了对抗性参数的存在性。对于参数集$\theta$满足一定条件的DNN$F_{\Theta}$，证明了如果DNN的深度足够大，则对于$\Theta$存在对抗参数集$\θ_a$，使得$F_{\theta}$的精度与$F_{\theta}$的精度相等，而$F_{\theta}$的稳健性测度等于$F_{\theta}$给出了一种计算对抗参数的有效训练算法，并通过数值实验证明了该算法能有效地产生高质量的对抗参数。



## **27. Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training**

对比性对抗性训练中认知分离缓解的稳健性 cs.LG

**SubmitDate**: 2022-03-19    [paper-pdf](http://arxiv.org/pdf/2203.08959v2)

**Authors**: Adir Rahamim, Itay Naeh

**Abstracts**: In this paper, we introduce a novel neural network training framework that increases model's adversarial robustness to adversarial attacks while maintaining high clean accuracy by combining contrastive learning (CL) with adversarial training (AT). We propose to improve model robustness to adversarial attacks by learning feature representations that are consistent under both data augmentations and adversarial perturbations. We leverage contrastive learning to improve adversarial robustness by considering an adversarial example as another positive example, and aim to maximize the similarity between random augmentations of data samples and their adversarial example, while constantly updating the classification head in order to avoid a cognitive dissociation between the classification head and the embedding space. This dissociation is caused by the fact that CL updates the network up to the embedding space, while freezing the classification head which is used to generate new positive adversarial examples. We validate our method, Contrastive Learning with Adversarial Features(CLAF), on the CIFAR-10 dataset on which it outperforms both robust accuracy and clean accuracy over alternative supervised and self-supervised adversarial learning methods.

摘要: 本文提出了一种新的神经网络训练框架，将对比学习(CL)和对抗训练(AT)相结合，在保持较高准确率的同时，提高了模型对对手攻击的鲁棒性。我们建议通过学习在数据增加和对抗性扰动下都是一致的特征表示来提高模型对对抗性攻击的鲁棒性。我们利用对比学习通过将一个对抗性示例作为另一个正例来提高对抗性鲁棒性，目标是最大化数据样本的随机增加与其对抗性示例之间的相似度，同时不断更新分类头，以避免分类头和嵌入空间之间的认知分离。这种分离是由于CL将网络更新到嵌入空间，同时冻结用于生成新的正面对抗性示例的分类头。我们在CIFAR-10数据集上验证了我们的方法，即具有对抗性特征的对比学习(CLAF)，在CIFAR-10数据集上，它的性能优于其他监督和自我监督对抗性学习方法的鲁棒准确率和干净准确率。



## **28. On Robust Prefix-Tuning for Text Classification**

面向文本分类的稳健前缀调优方法研究 cs.CL

Accepted in ICLR 2022. We release the code at  https://github.com/minicheshire/Robust-Prefix-Tuning

**SubmitDate**: 2022-03-19    [paper-pdf](http://arxiv.org/pdf/2203.10378v1)

**Authors**: Zonghan Yang, Yang Liu

**Abstracts**: Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness enhancement. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research.

摘要: 近年来，前缀调优作为一种针对大规模预训练语言模型的参数高效精调方法受到越来越多的关注。该方法保持预先训练的模型不变，并且只更新每个下游任务的前缀令牌参数。尽管前缀调优是轻量级和模块化的，但仍然缺乏对文本对手攻击的健壮性。然而，目前开发的大多数防御技术需要辅助模型更新和存储，这不可避免地阻碍了前缀调整的模块化和低存储量。在这项工作中，我们提出了一个健壮的前缀调整框架，它保持了前缀调整的效率和模块性。我们框架的核心思想是通过正确分类训练数据来利用语言模型的LayerWise激活，作为额外前缀优化的标准。在测试阶段，为每个批处理调优一个额外的批级前缀，并将其添加到原始前缀以增强健壮性。在三个文本分类基准上的大量实验表明，我们的框架在保持对干净文本的相对准确率的同时，显著提高了对五种不同类型的文本攻击在几条强基线上的鲁棒性。我们还从最优控制的角度解释了我们的鲁棒前缀调节框架，并对未来的研究提出了几个方向。



## **29. Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense**

荒野中的扰动：利用人类书写的文本扰动进行现实的对抗性攻击和防御 cs.LG

Accepted to the 60th Annual Meeting of the Association for  Computational Linguistics (ACL'22), Findings

**SubmitDate**: 2022-03-19    [paper-pdf](http://arxiv.org/pdf/2203.10346v1)

**Authors**: Thai Le, Jooyoung Lee, Kevin Yen, Yifan Hu, Dongwon Lee

**Abstracts**: We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K human-written text perturbations in the wild and leverages them for realistic adversarial attack. Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts. We find that adversarial texts generated by ANTHRO achieve the best trade-off between (1) attack success rate, (2) semantic preservation of the original text, and (3) stealthiness--i.e. indistinguishable from human writings hence harder to be flagged as suspicious. Specifically, our attacks accomplished around 83% and 91% attack success rates on BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger baseline with an increase of 50% and 40% in terms of semantic preservation and stealthiness when evaluated by both layperson and professional human workers. ANTHRO can further enhance a BERT classifier's performance in understanding different variations of human-written toxic texts via adversarial training when compared to the Perspective API.

摘要: 我们提出了一种新的算法Anthro，该算法在野外归纳提取超过600K个人类书写的文本扰动，并利用它们进行现实的对抗性攻击。与现有的基于字符的攻击经常演绎地假设一组操纵策略不同，我们的工作是基于对真实世界文本的实际观察。我们发现，Anthro生成的敌意文本在(1)攻击成功率、(2)保留原文语义和(3)隐蔽性--即隐蔽性--之间达到了最佳的折衷。与人类的作品难以区分，因此更难被标记为可疑。具体地说，我们对伯特和罗伯塔的攻击成功率分别约为83%和91%。此外，在外行和专业人员的评估中，它在语义保留和隐蔽性方面的表现优于TextBugger基线，分别提高了50%和40%。与透视API相比，Anthro可以通过对抗性训练进一步提高BERT分类器在理解人类书写的有毒文本的不同变体方面的性能。



## **30. Efficient Neural Network Analysis with Sum-of-Infeasibilities**

不可行和条件下的高效神经网络分析 cs.LG

TACAS'22

**SubmitDate**: 2022-03-19    [paper-pdf](http://arxiv.org/pdf/2203.11201v1)

**Authors**: Haoze Wu, Aleksandar Zeljić, Guy Katz, Clark Barrett

**Abstracts**: Inspired by sum-of-infeasibilities methods in convex optimization, we propose a novel procedure for analyzing verification queries on neural networks with piecewise-linear activation functions. Given a convex relaxation which over-approximates the non-convex activation functions, we encode the violations of activation functions as a cost function and optimize it with respect to the convex relaxation. The cost function, referred to as the Sum-of-Infeasibilities (SoI), is designed so that its minimum is zero and achieved only if all the activation functions are satisfied. We propose a stochastic procedure, DeepSoI, to efficiently minimize the SoI. An extension to a canonical case-analysis-based complete search procedure can be achieved by replacing the convex procedure executed at each search state with DeepSoI. Extending the complete search with DeepSoI achieves multiple simultaneous goals: 1) it guides the search towards a counter-example; 2) it enables more informed branching decisions; and 3) it creates additional opportunities for bound derivation. An extensive evaluation across different benchmarks and solvers demonstrates the benefit of the proposed techniques. In particular, we demonstrate that SoI significantly improves the performance of an existing complete search procedure. Moreover, the SoI-based implementation outperforms other state-of-the-art complete verifiers. We also show that our technique can efficiently improve upon the perturbation bound derived by a recent adversarial attack algorithm.

摘要: 受凸优化中不可行和方法的启发，提出了一种分析分段线性激活函数神经网络验证查询的新方法。在给定过逼近非凸激活函数的凸松弛的情况下，将激活函数的违例编码为代价函数，并相对于凸松弛进行优化。成本函数被称为不可行和(SOI)，其最小值被设计为零，并且只有在满足所有激活函数的情况下才能实现。我们提出了一个随机过程，DeepSoI，以有效地最小化SOI。通过用DeepSoI替换在每个搜索状态执行的凸过程，可以实现对基于规范案例分析的完全搜索过程的扩展。使用DeepSoI扩展完整搜索可同时实现多个目标：1)它引导搜索指向反例；2)它支持更知情的分支决策；3)它为界限派生创造更多机会。对不同基准和求解器的广泛评估表明了所提出的技术的好处。特别地，我们证明SOI显著提高了现有完整搜索过程的性能。此外，基于SOI的实现优于其他最先进的完全验证器。我们还表明，我们的技术可以有效地改善最近的对抗性攻击算法得出的扰动界。



## **31. Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model**

用于更健壮的预训练语言模型的非自然和自然对抗性样本的区分 cs.LG

Accepted by findings of ACL 2022

**SubmitDate**: 2022-03-19    [paper-pdf](http://arxiv.org/pdf/2203.11199v1)

**Authors**: Jiayi Wang, Rongzhou Bao, Zhuosheng Zhang, Hai Zhao

**Abstracts**: Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.

摘要: 近年来，预训练语言模型(PrLM)的鲁棒性问题引起了越来越多的研究兴趣。最近关于对抗性攻击的研究取得了对PrLM的高攻击成功率，声称PrLM并不健壮。然而，我们发现PrLM失败的对抗性样本大多是非自然的，在现实中并不出现。我们对目前基于这些非自然对抗性样本的PrLM鲁棒性评估的有效性提出了质疑，并提出了一种异常检测器来评估具有更多自然对抗性样本的PrLM的健壮性。我们还研究了异常检测器的两个应用：(1)在数据增强方面，我们使用异常检测器来强制生成被区分为非自然的扩展数据，从而使PrLM的准确率有了较大的提高。(2)将异常检测器应用到防御框架中，增强了PrLM的健壮性。它可以防御所有类型的攻击，在对抗性样本和顺从性样本上都比其他防御框架获得了更高的准确率。



## **32. Adversarial Defense via Image Denoising with Chaotic Encryption**

基于混沌加密图像去噪的对抗性防御 cs.LG

**SubmitDate**: 2022-03-19    [paper-pdf](http://arxiv.org/pdf/2203.10290v1)

**Authors**: Shi Hu, Eric Nalisnick, Max Welling

**Abstracts**: In the literature on adversarial examples, white box and black box attacks have received the most attention. The adversary is assumed to have either full (white) or no (black) access to the defender's model. In this work, we focus on the equally practical gray box setting, assuming an attacker has partial information. We propose a novel defense that assumes everything but a private key will be made available to the attacker. Our framework uses an image denoising procedure coupled with encryption via a discretized Baker map. Extensive testing against adversarial images (e.g. FGSM, PGD) crafted using various gradients shows that our defense achieves significantly better results on CIFAR-10 and CIFAR-100 than the state-of-the-art gray box defenses in both natural and adversarial accuracy.

摘要: 在对抗性例子的文献中，白盒和黑盒攻击受到的关注最多。假设对手对防御者的模型具有完全(白色)或没有(黑色)访问权限。在这项工作中，我们将重点放在同样实用的灰盒设置上，假设攻击者拥有部分信息。我们提出了一种新的防御方法，它假设除了私钥之外，所有东西都可以提供给攻击者。我们的框架使用图像去噪过程，并通过离散化的贝克图进行加密。对使用各种梯度制作的对抗性图像(例如FGSM、PGD)的广泛测试表明，我们的防御在自然和对抗性准确性方面都比最先进的灰盒防御在CIFAR-10和CIFAR-100上取得了显著更好的结果。



## **33. Synthesis of the Supremal Covert Attacker Against Unknown Supervisors by Using Observations**

利用观测值综合上位隐蔽攻击者对抗未知监督者 eess.SY

arXiv admin note: text overlap with arXiv:2106.12268

**SubmitDate**: 2022-03-19    [paper-pdf](http://arxiv.org/pdf/2203.08360v2)

**Authors**: Ruochen Tai, Liyong Lin, Yuting Zhu, Rong Su

**Abstracts**: In this paper, we consider the problem of synthesizing the supremal covert damage-reachable attacker, in the setup where the model of the supervisor is unknown to the adversary but the adversary has recorded a (prefix-closed) finite set of observations of the runs of the closed-loop system. The synthesized attacker needs to ensure both the damage-reachability and the covertness against all the supervisors which are consistent with the given set of observations. There is a gap between the de facto supremality, assuming the model of the supervisor is known, and the supremality that can be attained with a limited knowledge of the model of the supervisor, from the adversary's point of view. We consider the setup where the attacker can exercise sensor replacement/deletion attacks and actuator enablement/disablement attacks. The solution methodology proposed in this work is to reduce the synthesis of the supremal covert damage-reachable attacker, given the model of the plant and the finite set of observations, to the synthesis of the supremal safe supervisor for certain transformed plant, which shows the decidability of the observation-assisted covert attacker synthesis problem. The effectiveness of our approach is illustrated on a water tank example adapted from the literature.

摘要: 在这篇文章中，我们考虑了在对手未知监督者模型但对手已经记录了闭环系统运行的(前缀闭合的)有限观测集的情况下，合成最高隐蔽损害可达攻击者的问题。合成攻击者需要确保针对所有监督者的损害可达性和隐蔽性，这与给定的观察集合是一致的。在假设监督者的模型已知的情况下，事实上的至高无上与从对手的角度看，通过对监督者的模型的有限了解可以获得的至高无上之间存在差距。我们考虑攻击者可以执行传感器替换/删除攻击和致动器启用/禁用攻击的设置。本文提出的解决方法是，在给定对象模型和有限观测集的情况下，将隐蔽可达攻击者的综合归结为某一变换对象的最高安全监督器的综合，从而证明了观测辅助的隐蔽攻击者综合问题的可判性。从文献中改编的一个水箱例子说明了我们方法的有效性。



## **34. Adversarial Attacks on Deep Learning-based Video Compression and Classification Systems**

基于深度学习的视频压缩和分类系统的敌意攻击 cs.CV

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.10183v1)

**Authors**: Jung-Woo Chang, Mojan Javaheripi, Seira Hidano, Farinaz Koushanfar

**Abstracts**: Video compression plays a crucial role in enabling video streaming and classification systems and maximizing the end-user quality of experience (QoE) at a given bandwidth budget. In this paper, we conduct the first systematic study for adversarial attacks on deep learning based video compression and downstream classification systems. We propose an adaptive adversarial attack that can manipulate the Rate-Distortion (R-D) relationship of a video compression model to achieve two adversarial goals: (1) increasing the network bandwidth or (2) degrading the video quality for end-users. We further devise novel objectives for targeted and untargeted attacks to a downstream video classification service. Finally, we design an input-invariant perturbation that universally disrupts video compression and classification systems in real time. Unlike previously proposed attacks on video classification, our adversarial perturbations are the first to withstand compression. We empirically show the resilience of our attacks against various defenses, i.e., adversarial training, video denoising, and JPEG compression. Our extensive experimental results on various video datasets demonstrate the effectiveness of our attacks. Our video quality and bandwidth attacks deteriorate peak signal-to-noise ratio by up to 5.4dB and the bit-rate by up to 2.4 times on the standard video compression datasets while achieving over 90% attack success rate on a downstream classifier.

摘要: 视频压缩在实现视频流和分类系统以及在给定带宽预算下最大化最终用户体验质量(QoE)方面起着至关重要的作用。本文首次对基于深度学习的视频压缩和下行分类系统的敌意攻击进行了系统的研究。本文提出了一种自适应对抗性攻击方法，可以通过操控视频压缩模型的率失真(R-D)关系来实现两个对抗性目标：(1)增加网络带宽；(2)降低终端用户的视频质量。我们进一步设计了针对下游视频分类服务的定向和非定向攻击的新目标。最后，我们设计了一种输入不变的扰动，该扰动普遍地扰乱了视频压缩和分类系统的实时运行。与之前提出的针对视频分类的攻击不同，我们的对抗性扰动最先经受住了压缩。我们经验性地展示了我们的攻击对各种防御的恢复能力，即对抗性训练、视频去噪和JPEG压缩。我们在各种视频数据集上的大量实验结果证明了我们攻击的有效性。我们的视频质量和带宽攻击在标准视频压缩数据集上使峰值信噪比降低了5.4dB，比特率降低了2.4倍，同时在下游分类器上实现了90%以上的攻击成功率。



## **35. Concept-based Adversarial Attacks: Tricking Humans and Classifiers Alike**

基于概念的对抗性攻击：欺骗人类和分类器 cs.LG

Accepted at IEEE Symposium on Security and Privacy (S&P) Workshop on  Deep Learning and Security, 2022

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.10166v1)

**Authors**: Johannes Schneider, Giovanni Apruzzese

**Abstracts**: We propose to generate adversarial samples by modifying activations of upper layers encoding semantically meaningful concepts. The original sample is shifted towards a target sample, yielding an adversarial sample, by using the modified activations to reconstruct the original sample. A human might (and possibly should) notice differences between the original and the adversarial sample. Depending on the attacker-provided constraints, an adversarial sample can exhibit subtle differences or appear like a "forged" sample from another class. Our approach and goal are in stark contrast to common attacks involving perturbations of single pixels that are not recognizable by humans. Our approach is relevant in, e.g., multi-stage processing of inputs, where both humans and machines are involved in decision-making because invisible perturbations will not fool a human. Our evaluation focuses on deep neural networks. We also show the transferability of our adversarial examples among networks.

摘要: 我们建议通过修改编码有语义意义的概念的上层激活来生成对抗性样本。通过使用修改的激活来重构原始样本，原始样本被移向目标样本，从而产生对抗性样本。人类可能(也可能应该)注意到原始样本和对抗性样本之间的差异。根据攻击者提供的约束，敌意样本可能会显示出细微的差异，或者看起来像是来自另一个类的“伪造”样本。我们的方法和目标与常见的攻击形成鲜明对比，这些攻击涉及人类无法识别的单像素扰动。例如，我们的方法与输入的多阶段处理相关，其中人和机器都参与决策，因为看不见的扰动不会愚弄人。我们的评估集中在深度神经网络上。我们还展示了我们的对抗性例子在网络之间的可转移性。



## **36. All You Need is RAW: Defending Against Adversarial Attacks with Camera Image Pipelines**

您所需要的只是RAW：使用摄像头图像管道防御对手攻击 cs.CV

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2112.09219v2)

**Authors**: Yuxuan Zhang, Bo Dong, Felix Heide

**Abstracts**: Existing neural networks for computer vision tasks are vulnerable to adversarial attacks: adding imperceptible perturbations to the input images can fool these methods to make a false prediction on an image that was correctly predicted without the perturbation. Various defense methods have proposed image-to-image mapping methods, either including these perturbations in the training process or removing them in a preprocessing denoising step. In doing so, existing methods often ignore that the natural RGB images in today's datasets are not captured but, in fact, recovered from RAW color filter array captures that are subject to various degradations in the capture. In this work, we exploit this RAW data distribution as an empirical prior for adversarial defense. Specifically, we proposed a model-agnostic adversarial defensive method, which maps the input RGB images to Bayer RAW space and back to output RGB using a learned camera image signal processing (ISP) pipeline to eliminate potential adversarial patterns. The proposed method acts as an off-the-shelf preprocessing module and, unlike model-specific adversarial training methods, does not require adversarial images to train. As a result, the method generalizes to unseen tasks without additional retraining. Experiments on large-scale datasets (e.g., ImageNet, COCO) for different vision tasks (e.g., classification, semantic segmentation, object detection) validate that the method significantly outperforms existing methods across task domains.

摘要: 现有的用于计算机视觉任务的神经网络容易受到对手的攻击：向输入图像添加不可察觉的扰动可以愚弄这些方法对没有扰动的正确预测的图像进行错误预测。各种防御方法已经提出了图像到图像的映射方法，或者在训练过程中包括这些扰动，或者在预处理去噪步骤中去除它们。在这样做时，现有方法通常忽略了今天数据集中的自然RGB图像没有被捕获，而实际上是从原始滤色器阵列捕获中恢复的，该原始滤色器阵列捕获在捕获中受到各种降级。在这项工作中，我们利用这种原始数据分布作为对抗防御的经验先验。具体地说，我们提出了一种模型不可知的对抗防御方法，该方法将输入的RGB图像映射到拜耳原始空间，然后使用学习的摄像机图像信号处理(ISP)流水线将输入的RGB图像映射回输出RGB，以消除潜在的敌对模式。该方法作为一个现成的预处理模块，与特定模型的对抗性训练方法不同，不需要对对抗性图像进行训练。因此，该方法可以推广到看不见的任务，而不需要额外的再培训。在不同视觉任务(如分类、语义分割、目标检测)的大规模数据集(如ImageNet、CoCo)上的实验验证了该方法在跨任务域的性能上显著优于现有方法。



## **37. Graph-Fraudster: Adversarial Attacks on Graph Neural Network Based Vertical Federated Learning**

图欺诈者：基于图神经网络垂直联合学习的对抗性攻击 cs.LG

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2110.06468v2)

**Authors**: Jinyin Chen, Guohan Huang, Haibin Zheng, Shanqing Yu, Wenrong Jiang, Chen Cui

**Abstracts**: Graph neural network (GNN) has achieved great success on graph representation learning. Challenged by large scale private data collected from user-side, GNN may not be able to reflect the excellent performance, without rich features and complete adjacent relationships. Addressing the problem, vertical federated learning (VFL) is proposed to implement local data protection through training a global model collaboratively. Consequently, for graph-structured data, it is a natural idea to construct a GNN based VFL framework, denoted as GVFL. However, GNN has been proved vulnerable to adversarial attacks. Whether the vulnerability will be brought into the GVFL has not been studied. This is the first study of adversarial attacks on GVFL. A novel adversarial attack method is proposed, named Graph-Fraudster. It generates adversarial perturbations based on the noise-added global node embeddings via the privacy leakage and the gradient of pairwise node. Specifically, first, Graph-Fraudster steals the global node embeddings and sets up a shadow model of the server for the attack generator. Second, noise is added into node embeddings to confuse the shadow model. At last, the gradient of pairwise node is used to generate attacks with the guidance of noise-added node embeddings. Extensive experiments on five benchmark datasets demonstrate that Graph-Fraudster achieves the state-of-the-art attack performance compared with baselines in different GNN based GVFLs. Furthermore, Graph-Fraudster can remain a threat to GVFL even if two possible defense mechanisms are applied. Additionally, some suggestions are put forward for the future work to improve the robustness of GVFL. The code and datasets can be downloaded at https://github.com/hgh0545/Graph-Fraudster.

摘要: 图神经网络(GNN)在图表示学习方面取得了巨大的成功。由于受到来自用户端的大规模隐私数据的挑战，GNN如果没有丰富的功能和完整的邻接关系，可能无法反映出优秀的性能。针对这一问题，提出了垂直联合学习(VFL)，通过协作训练全局模型来实现局部数据保护。因此，对于图结构的数据，构建基于GNN的VFL框架(表示为GVFL)是一个自然的想法。然而，GNN已被证明容易受到敌意攻击。该漏洞是否会被带入GVFL还没有研究。这是首次对GVFL进行对抗性攻击的研究。提出了一种新的对抗性攻击方法--图欺诈器。该算法通过两个结点的隐私泄露和梯度，在加入噪声的全局结点嵌入的基础上产生对抗性扰动。具体地说，首先，Graph欺诈者窃取全局节点嵌入，并为攻击生成器建立服务器的影子模型。其次，在节点嵌入中加入噪声以混淆阴影模型。最后，在加入噪声的节点嵌入指导下，利用成对节点的梯度生成攻击。在五个基准数据集上的大量实验表明，与基线相比，Graph-Fraudster在不同的基于GNN的GVFL上达到了最先进的攻击性能。此外，即使应用了两种可能的防御机制，图形欺诈者仍可能对GVFL构成威胁。此外，对未来的工作提出了一些建议，以提高GVFL的鲁棒性。代码和数据集可从https://github.com/hgh0545/Graph-Fraudster.下载



## **38. Defending Variational Autoencoders from Adversarial Attacks with MCMC**

利用MCMC保护可变自动编码器免受敌意攻击 cs.LG

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.09940v1)

**Authors**: Anna Kuzina, Max Welling, Jakub M. Tomczak

**Abstracts**: Variational autoencoders (VAEs) are deep generative models used in various domains. VAEs can generate complex objects and provide meaningful latent representations, which can be further used in downstream tasks such as classification. As previous work has shown, one can easily fool VAEs to produce unexpected latent representations and reconstructions for a visually slightly modified input. Here, we examine several objective functions for adversarial attacks construction, suggest metrics assess the model robustness, and propose a solution to alleviate the effect of an attack. Our method utilizes the Markov Chain Monte Carlo (MCMC) technique in the inference step and is motivated by our theoretical analysis. Thus, we do not incorporate any additional costs during training or we do not decrease the performance on non-attacked inputs. We validate our approach on a variety of datasets (MNIST, Fashion MNIST, Color MNIST, CelebA) and VAE configurations ($\beta$-VAE, NVAE, TC-VAE) and show that it consistently improves the model robustness to adversarial attacks.

摘要: 变分自动编码器(VAE)是广泛应用于各个领域的深度产生式模型。Vaes可以生成复杂的对象，并提供有意义的潜在表示，这可以进一步用于下游任务，如分类。正如以前的工作所表明的那样，人们可以很容易地欺骗VAE为视觉上稍有修改的输入产生意想不到的潜在表示和重建。在这里，我们检查了几个对抗性攻击构建的目标函数，提出了评估模型健壮性的度量标准，并提出了一种减轻攻击效果的解决方案。我们的方法在推理步骤中使用了马尔可夫链蒙特卡罗(MCMC)技术，并基于我们的理论分析。因此，我们在培训期间不会加入任何额外成本，或者我们不会降低非攻击输入的性能。我们在各种数据集(MNIST，Fashion MNIST，Color MNIST，CelebA)和VAE配置($\beta$-VAE，NVAE，TC-VAE)上验证了我们的方法，并表明它一致地提高了模型对对手攻击的鲁棒性。



## **39. Neural Predictor for Black-Box Adversarial Attacks on Speech Recognition**

语音识别黑盒对抗性攻击的神经预测器 cs.SD

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.09849v1)

**Authors**: Marie Biolková, Bac Nguyen

**Abstracts**: Recent works have revealed the vulnerability of automatic speech recognition (ASR) models to adversarial examples (AEs), i.e., small perturbations that cause an error in the transcription of the audio signal. Studying audio adversarial attacks is therefore the first step towards robust ASR. Despite the significant progress made in attacking audio examples, the black-box attack remains challenging because only the hard-label information of transcriptions is provided. Due to this limited information, existing black-box methods often require an excessive number of queries to attack a single audio example. In this paper, we introduce NP-Attack, a neural predictor-based method, which progressively evolves the search towards a small adversarial perturbation. Given a perturbation direction, our neural predictor directly estimates the smallest perturbation that causes a mistranscription. In particular, it enables NP-Attack to accurately learn promising perturbation directions via gradient-based optimization. Experimental results show that NP-Attack achieves competitive results with other state-of-the-art black-box adversarial attacks while requiring a significantly smaller number of queries. The code of NP-Attack is available online.

摘要: 最近的工作揭示了自动语音识别(ASR)模型对对抗性示例(AE)的脆弱性，即导致音频信号转录错误的微小扰动。因此，研究音频对抗性攻击是迈向健壮ASR的第一步。尽管在攻击音频示例方面取得了重大进展，但黑盒攻击仍然具有挑战性，因为只提供了转录的硬标签信息。由于这些有限的信息，现有的黑盒方法通常需要过多的查询来攻击单个音频示例。在本文中，我们引入了NP-Attack，这是一种基于神经预测器的方法，它将搜索逐步演化为一个小的对抗性扰动。在给定扰动方向的情况下，我们的神经预测器会直接估计导致误译的最小扰动。特别地，它使NP-Attack能够通过基于梯度的优化精确地学习有希望的扰动方向。实验结果表明，NP-Attack与其他黑盒对抗攻击相比，在查询次数明显减少的情况下，达到了好胜攻击的效果。NP-攻击的代码可以在网上找到。



## **40. DTA: Physical Camouflage Attacks using Differentiable Transformation Network**

DTA：基于差分变换网络的物理伪装攻击 cs.CV

Accepted for CVPR 2022

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.09831v1)

**Authors**: Naufal Suryanto, Yongsu Kim, Hyoeun Kang, Harashta Tatimma Larasati, Youngyeo Yun, Thi-Thu-Huong Le, Hunmin Yang, Se-Yoon Oh, Howon Kim

**Abstracts**: To perform adversarial attacks in the physical world, many studies have proposed adversarial camouflage, a method to hide a target object by applying camouflage patterns on 3D object surfaces. For obtaining optimal physical adversarial camouflage, previous studies have utilized the so-called neural renderer, as it supports differentiability. However, existing neural renderers cannot fully represent various real-world transformations due to a lack of control of scene parameters compared to the legacy photo-realistic renderers. In this paper, we propose the Differentiable Transformation Attack (DTA), a framework for generating a robust physical adversarial pattern on a target object to camouflage it against object detection models with a wide range of transformations. It utilizes our novel Differentiable Transformation Network (DTN), which learns the expected transformation of a rendered object when the texture is changed while preserving the original properties of the target object. Using our attack framework, an adversary can gain both the advantages of the legacy photo-realistic renderers including various physical-world transformations and the benefit of white-box access by offering differentiability. Our experiments show that our camouflaged 3D vehicles can successfully evade state-of-the-art object detection models in the photo-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our demonstration on a scaled Tesla Model 3 proves the applicability and transferability of our method to the real world.

摘要: 为了在物理世界中进行对抗性攻击，许多研究提出了对抗性伪装，即通过在三维物体表面应用伪装图案来隐藏目标对象的方法。为了获得最佳的物理对抗伪装，以前的研究已经利用了所谓的神经渲染器，因为它支持可分性。然而，与传统的照片真实感渲染器相比，由于缺乏对场景参数的控制，现有的神经渲染器不能完全表示各种真实世界的变换。本文提出了差分变换攻击(Differentiable Transform Attack，DTA)的框架，该框架用于在目标对象上生成一个健壮的物理对抗模式，以针对具有广泛变换的目标检测模型进行伪装。它利用了我们新颖的微分变换网络(DTN)，它在保持目标对象的原始属性的同时，学习绘制对象在纹理改变时的期望变换。使用我们的攻击框架，对手既可以获得传统照片真实感渲染器(包括各种物理世界变换)的优势，也可以通过提供差异化获得白盒访问的好处。我们的实验表明，我们的伪装3D车辆可以在真实感环境(即虚幻引擎上的Carla)中成功地躲避最先进的目标检测模型。此外，我们在一个缩放的特斯拉Model3上的演示证明了我们的方法在现实世界中的适用性和可移植性。



## **41. AdIoTack: Quantifying and Refining Resilience of Decision Tree Ensemble Inference Models against Adversarial Volumetric Attacks on IoT Networks**

AdIoTack：量化和提炼决策树集成推理模型对物联网敌意体积攻击的恢复能力 cs.LG

15 pages, 16 figures, 4 tables

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.09792v1)

**Authors**: Arman Pashamokhtari, Gustavo Batista, Hassan Habibi Gharakheili

**Abstracts**: Machine Learning-based techniques have shown success in cyber intelligence. However, they are increasingly becoming targets of sophisticated data-driven adversarial attacks resulting in misprediction, eroding their ability to detect threats on network devices. In this paper, we present AdIoTack, a system that highlights vulnerabilities of decision trees against adversarial attacks, helping cybersecurity teams quantify and refine the resilience of their trained models for monitoring IoT networks. To assess the model for the worst-case scenario, AdIoTack performs white-box adversarial learning to launch successful volumetric attacks that decision tree ensemble models cannot flag. Our first contribution is to develop a white-box algorithm that takes a trained decision tree ensemble model and the profile of an intended network-based attack on a victim class as inputs. It then automatically generates recipes that specify certain packets on top of the indented attack packets (less than 15% overhead) that together can bypass the inference model unnoticed. We ensure that the generated attack instances are feasible for launching on IP networks and effective in their volumetric impact. Our second contribution develops a method to monitor the network behavior of connected devices actively, inject adversarial traffic (when feasible) on behalf of a victim IoT device, and successfully launch the intended attack. Our third contribution prototypes AdIoTack and validates its efficacy on a testbed consisting of a handful of real IoT devices monitored by a trained inference model. We demonstrate how the model detects all non-adversarial volumetric attacks on IoT devices while missing many adversarial ones. The fourth contribution develops systematic methods for applying patches to trained decision tree ensemble models, improving their resilience against adversarial volumetric attacks.

摘要: 基于机器学习的技术在网络智能领域取得了成功。然而，它们越来越多地成为复杂的数据驱动的敌意攻击的目标，导致预测失误，削弱了它们检测网络设备上的威胁的能力。本文介绍了AdIoTack系统，该系统突出了决策树对对手攻击的脆弱性，帮助网络安全团队量化和提炼他们训练有素的模型对物联网网络进行监控的弹性。为了评估最坏情况下的模型，AdIoTack执行白盒对抗性学习，以发起决策树集成模型无法标记的成功的体积攻击。我们的第一个贡献是开发了一个白盒算法，该算法将经过训练的决策树集成模型和对受害者类别的预期网络攻击的概况作为输入。然后，它会自动生成配方，在缩进的攻击数据包(开销小于15%)之上指定某些数据包，这些数据包加在一起可以绕过推理模型而不会被注意到。我们确保生成的攻击实例可以在IP网络上启动，并且其体积影响是有效的。我们的第二个贡献开发了一种方法，可以主动监控连接设备的网络行为，代表受害者物联网设备注入敌意流量(如果可行)，并成功发起预期攻击。我们的第三个贡献是AdIoTack的原型，并在由经过训练的推理模型监控的少数真实物联网设备组成的试验台上验证了其有效性。我们演示了该模型如何检测到物联网设备上的所有非对抗性体积攻击，同时又遗漏了许多对抗性攻击。第四个贡献开发了系统的方法，用于将补丁应用于训练的决策树集成模型，提高它们对对手体积攻击的弹性。



## **42. Adversarial Texture for Fooling Person Detectors in the Physical World**

物理世界中愚人探测器的对抗性纹理 cs.CV

Accepted by CVPR 2022

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.03373v3)

**Authors**: Zhanhao Hu, Siyuan Huang, Xiaopei Zhu, Xiaolin Hu, Fuchun Sun, Bo Zhang

**Abstracts**: Nowadays, cameras equipped with AI systems can capture and analyze images to detect people automatically. However, the AI system can make mistakes when receiving deliberately designed patterns in the real world, i.e., physical adversarial examples. Prior works have shown that it is possible to print adversarial patches on clothes to evade DNN-based person detectors. However, these adversarial examples could have catastrophic drops in the attack success rate when the viewing angle (i.e., the camera's angle towards the object) changes. To perform a multi-angle attack, we propose Adversarial Texture (AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people wearing such clothes can hide from person detectors from different viewing angles. We propose a generative method, named Toroidal-Cropping-based Expandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive structures. We printed several pieces of cloth with AdvTexure and then made T-shirts, skirts, and dresses in the physical world. Experiments showed that these clothes could fool person detectors in the physical world.

摘要: 如今，配备人工智能系统的摄像头可以捕捉和分析图像，自动检测人。然而，人工智能系统在接收到现实世界中故意设计的模式时可能会出错，即物理对抗性例子。以前的工作已经表明，可以在衣服上打印敌意补丁来躲避基于DNN的个人探测器。然而，当视角(即相机指向对象的角度)改变时，这些对抗性的例子可能会使攻击成功率灾难性地下降。为了进行多角度攻击，我们提出了对抗性纹理(AdvTexture)。AdvTexture可以覆盖任意形状的衣服，这样穿着这种衣服的人就可以从不同的视角隐藏起来，躲避人的探测器。提出了一种基于环形裁剪的可扩展产生式攻击方法(TC-EGA)，用于制作具有重复结构的AdvTexture。我们用AdvTexure打印了几块布，然后在现实世界中制作了T恤、裙子和连衣裙。实验表明，这些衣服可以愚弄物理世界中的人体探测器。



## **43. AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack**

AutoAdversary：一种稀疏对抗性攻击的像素剪枝方法 cs.CV

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.09756v1)

**Authors**: Jinqiao Li, Xiaotao Liu, Jian Zhao, Furao Shen

**Abstracts**: Deep neural networks (DNNs) have been proven to be vulnerable to adversarial examples. A special branch of adversarial examples, namely sparse adversarial examples, can fool the target DNNs by perturbing only a few pixels. However, many existing sparse adversarial attacks use heuristic methods to select the pixels to be perturbed, and regard the pixel selection and the adversarial attack as two separate steps. From the perspective of neural network pruning, we propose a novel end-to-end sparse adversarial attack method, namely AutoAdversary, which can find the most important pixels automatically by integrating the pixel selection into the adversarial attack. Specifically, our method utilizes a trainable neural network to generate a binary mask for the pixel selection. After jointly optimizing the adversarial perturbation and the neural network, only the pixels corresponding to the value 1 in the mask are perturbed. Experiments demonstrate the superiority of our proposed method over several state-of-the-art methods. Furthermore, since AutoAdversary does not require a heuristic pixel selection process, it does not slow down excessively as other methods when the image size increases.

摘要: 深度神经网络(DNNs)已被证明容易受到敌意例子的攻击。对抗性示例的一个特殊分支，即稀疏对抗性示例，可以通过仅扰动几个像素来欺骗目标DNN。然而，现有的许多稀疏对抗性攻击采用启发式方法选择要扰动的像素，将像素选择和对抗性攻击视为两个独立的步骤。从神经网络剪枝的角度出发，提出了一种新的端到端稀疏对抗攻击方法--AutoAdversary，该方法将像素选择融入到对抗攻击中，能够自动找到最重要的像素。具体地说，我们的方法利用一个可训练的神经网络来生成用于像素选择的二值掩码。在联合优化对抗性扰动和神经网络之后，仅对掩码中与值1对应的像素进行扰动。实验表明，我们提出的方法比几种最先进的方法具有更好的性能。此外，由于AutoAdversary不需要启发式像素选择过程，因此当图像大小增加时，它不会像其他方法那样过度减速。



## **44. HDLock: Exploiting Privileged Encoding to Protect Hyperdimensional Computing Models against IP Stealing**

HDLock：利用特权编码保护超维计算模型免受IP窃取 cs.CR

7 pages, 9 figures, accepted by and to be presented at DAC 2022

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.09681v1)

**Authors**: Shijin Duan, Shaolei Ren, Xiaolin Xu

**Abstracts**: Hyperdimensional Computing (HDC) is facing infringement issues due to straightforward computations. This work, for the first time, raises a critical vulnerability of HDC, an attacker can reverse engineer the entire model, only requiring the unindexed hypervector memory. To mitigate this attack, we propose a defense strategy, namely HDLock, which significantly increases the reasoning cost of encoding. Specifically, HDLock adds extra feature hypervector combination and permutation in the encoding module. Compared to the standard HDC model, a two-layer-key HDLock can increase the adversarial reasoning complexity by 10 order of magnitudes without inference accuracy loss, with only 21% latency overhead.

摘要: 由于计算简单，超维计算(HDC)正面临着侵权问题。这项工作首次引发了HDC的一个严重漏洞，攻击者可以对整个模型进行反向工程，只需要无索引的超矢量内存。为了缓解这种攻击，我们提出了一种防御策略，即HDLock，它显著增加了编码的推理成本。具体来说，HDLock在编码模块中增加了额外的特征超向量组合和置换。与标准HDC模型相比，两层密钥HDLock可以在不损失推理精度的情况下将对抗性推理的复杂度提高10个数量级，延迟开销仅为21%。



## **45. Self-Ensemble Adversarial Training for Improved Robustness**

提高健壮性的自我集成对抗性训练 cs.LG

17 pages, 3 figures, ICLR 2022

**SubmitDate**: 2022-03-18    [paper-pdf](http://arxiv.org/pdf/2203.09678v1)

**Authors**: Hongjun Wang, Yisen Wang

**Abstracts**: Due to numerous breakthroughs in real-world applications brought by machine intelligence, deep neural networks (DNNs) are widely employed in critical applications. However, predictions of DNNs are easily manipulated with imperceptible adversarial perturbations, which impedes the further deployment of DNNs and may result in profound security and privacy implications. By incorporating adversarial samples into the training data pool, adversarial training is the strongest principled strategy against various adversarial attacks among all sorts of defense methods. Recent works mainly focus on developing new loss functions or regularizers, attempting to find the unique optimal point in the weight space. But none of them taps the potentials of classifiers obtained from standard adversarial training, especially states on the searching trajectory of training. In this work, we are dedicated to the weight states of models through the training process and devise a simple but powerful \emph{Self-Ensemble Adversarial Training} (SEAT) method for yielding a robust classifier by averaging weights of history models. This considerably improves the robustness of the target model against several well known adversarial attacks, even merely utilizing the naive cross-entropy loss to supervise. We also discuss the relationship between the ensemble of predictions from different adversarially trained models and the prediction of weight-ensembled models, as well as provide theoretical and empirical evidence that the proposed self-ensemble method provides a smoother loss landscape and better robustness than both individual models and the ensemble of predictions from different classifiers. We further analyze a subtle but fatal issue in the general settings for the self-ensemble model, which causes the deterioration of the weight-ensembled method in the late phases.

摘要: 由于机器智能给现实应用带来的诸多突破，深度神经网络(DNNs)在关键应用中得到了广泛的应用。然而，DNNs的预测很容易受到潜移默化的敌意干扰，这阻碍了DNNs的进一步部署，并可能导致深远的安全和隐私影响。通过将对抗性样本纳入训练数据池中，对抗性训练是各种防御方法中对抗各种对抗性攻击的最强原则性策略。最近的工作主要集中在开发新的损失函数或正则化函数，试图在权空间中找到唯一的最优点。但它们都没有挖掘从标准对抗性训练中获得的分类器的潜力，特别是在训练的搜索轨迹上。在这项工作中，我们致力于通过训练过程研究模型的权重状态，并设计了一种简单但强大的自集成对抗性训练(SEAT)方法，通过平均历史模型的权重来产生鲁棒的分类器。这在很大程度上提高了目标模型对几种众所周知的敌意攻击的鲁棒性，即使仅仅利用天真的交叉熵损失来监督也是如此。我们还讨论了不同对手训练模型的预测集成与权重集成模型的预测之间的关系，并提供了理论和经验证据，表明所提出的自集成方法提供了比单个模型和来自不同分类器的预测集成更平滑的损失情况和更好的鲁棒性。我们进一步分析了自集成模型一般设置中的一个微妙而致命的问题，该问题导致了权重集成方法在后期的恶化。



## **46. Provably Robust Adversarial Examples**

可证明可靠的对抗性例子 cs.LG

**SubmitDate**: 2022-03-17    [paper-pdf](http://arxiv.org/pdf/2007.12133v3)

**Authors**: Dimitar I. Dimitrov, Gagandeep Singh, Timon Gehr, Martin Vechev

**Abstracts**: We introduce the concept of provably robust adversarial examples for deep neural networks - connected input regions constructed from standard adversarial examples which are guaranteed to be robust to a set of real-world perturbations (such as changes in pixel intensity and geometric transformations). We present a novel method called PARADE for generating these regions in a scalable manner which works by iteratively refining the region initially obtained via sampling until a refined region is certified to be adversarial with existing state-of-the-art verifiers. At each step, a novel optimization procedure is applied to maximize the region's volume under the constraint that the convex relaxation of the network behavior with respect to the region implies a chosen bound on the certification objective. Our experimental evaluation shows the effectiveness of PARADE: it successfully finds large provably robust regions including ones containing $\approx 10^{573}$ adversarial examples for pixel intensity and $\approx 10^{599}$ for geometric perturbations. The provability enables our robust examples to be significantly more effective against state-of-the-art defenses based on randomized smoothing than the individual attacks used to construct the regions.

摘要: 对于深度神经网络连接的输入区域，我们引入了可证明稳健的对抗性实例的概念，这些输入区域由标准的对抗性实例构造而成，这些对抗性实例被保证对一组真实世界的扰动(例如像素强度和几何变换的变化)具有健壮性。我们提出了一种称为PARADE的新方法，用于以可伸缩的方式生成这些区域，该方法通过迭代细化最初通过采样获得的区域，直到细化的区域被证明与现有的最先进的验证器是对立的。在每个步骤中，在网络行为相对于区域的凸松弛意味着认证目标上的选定界限的约束下，应用一种新的优化过程来最大化区域的体积。我们的实验评估显示了PARADE的有效性：它成功地找到了大的可证明稳健的区域，其中包含大约10^{573}$像素强度的对抗性示例和$\\约10^{599}$的几何扰动示例。这种可证明性使我们的健壮示例能够比用于构建区域的单个攻击显著更有效地对抗基于随机平滑的最先进的防御。



## **47. Bayesian Framework for Gradient Leakage**

梯度泄漏的贝叶斯框架 cs.LG

**SubmitDate**: 2022-03-17    [paper-pdf](http://arxiv.org/pdf/2111.04706v2)

**Authors**: Mislav Balunović, Dimitar I. Dimitrov, Robin Staab, Martin Vechev

**Abstracts**: Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem.

摘要: 联合学习是一种在不共享训练数据的情况下训练机器学习模型的既定方法。然而，最近的研究表明，它不能保证数据隐私，因为共享梯度仍然可能泄露敏感信息。为了形式化梯度泄漏问题，我们提出了一个理论框架，该框架首次能够将贝叶斯最优对手表述为优化问题进行分析。我们证明了现有的泄漏攻击可以看作是对输入数据的概率分布和梯度的不同假设的最优对手的近似。我们的实验证实了贝叶斯最优对手在知道潜在分布的情况下的有效性。此外，我们的实验评估表明，现有的几种启发式防御方法对更强的攻击并不有效，特别是在训练过程的早期。因此，我们的研究结果表明，构建更有效的防御体系及其评估仍然是一个悬而未决的问题。



## **48. SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers**

SPAA：基于投影仪的隐身攻击深度图像分类器 cs.CV

**SubmitDate**: 2022-03-17    [paper-pdf](http://arxiv.org/pdf/2012.05858v3)

**Authors**: Bingyao Huang, Haibin Ling

**Abstracts**: Light-based adversarial attacks use spatial augmented reality (SAR) techniques to fool image classifiers by altering the physical light condition with a controllable light source, e.g., a projector. Compared with physical attacks that place hand-crafted adversarial objects, projector-based ones obviate modifying the physical entities, and can be performed transiently and dynamically by altering the projection pattern. However, subtle light perturbations are insufficient to fool image classifiers, due to the complex environment and project-and-capture process. Thus, existing approaches focus on projecting clearly perceptible adversarial patterns, while the more interesting yet challenging goal, stealthy projector-based attack, remains open. In this paper, for the first time, we formulate this problem as an end-to-end differentiable process and propose a Stealthy Projector-based Adversarial Attack (SPAA) solution. In SPAA, we approximate the real Project-and-Capture process using a deep neural network named PCNet, then we include PCNet in the optimization of projector-based attacks such that the generated adversarial projection is physically plausible. Finally, to generate both robust and stealthy adversarial projections, we propose an algorithm that uses minimum perturbation and adversarial confidence thresholds to alternate between the adversarial loss and stealthiness loss optimization. Our experimental evaluations show that SPAA clearly outperforms other methods by achieving higher attack success rates and meanwhile being stealthier, for both targeted and untargeted attacks.

摘要: 基于光的对抗性攻击使用空间增强现实(SAR)技术，通过用可控光源(例如，投影仪)改变物理光条件来愚弄图像分类器。与放置手工制作的对抗性对象的物理攻击相比，基于投影仪的攻击避免了对物理实体的修改，并且可以通过改变投影模式来瞬时和动态地执行。然而，由于复杂的环境和投影和捕获过程，细微的光线扰动不足以愚弄图像分类器。因此，现有的方法侧重于投射清晰可感知的对抗模式，而更有趣但更具挑战性的目标-基于投影仪的隐形攻击-仍然是开放的。本文首次将该问题描述为端到端可微过程，并提出了一种基于投影器的隐身对抗性攻击(SPAA)解决方案。在SPAA中，我们使用一种名为PCNet的深度神经网络来近似真实的投射和捕获过程，然后将PCNet引入到基于投影仪的攻击的优化中，使得生成的对抗性投影在物理上是可信的。最后，为了生成稳健的和隐蔽的对抗性预测，我们提出了一种算法，该算法使用最小扰动和对抗性置信度阈值在对抗性损失和隐蔽性损失优化之间交替。我们的实验评估表明，SPAA在获得更高的攻击成功率的同时，对目标攻击和非目标攻击都具有更高的隐蔽性，明显优于其他方法。



## **49. PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks**

PIDAN：一种基于一致性优化的深层神经网络后门攻击检测与消除方法 cs.LG

**SubmitDate**: 2022-03-17    [paper-pdf](http://arxiv.org/pdf/2203.09289v1)

**Authors**: Yue Wang, Wenqing Li, Esha Sarkar, Muhammad Shafique, Michail Maniatakos, Saif Eddin Jabari

**Abstracts**: Backdoor attacks impose a new threat in Deep Neural Networks (DNNs), where a backdoor is inserted into the neural network by poisoning the training dataset, misclassifying inputs that contain the adversary trigger. The major challenge for defending against these attacks is that only the attacker knows the secret trigger and the target class. The problem is further exacerbated by the recent introduction of "Hidden Triggers", where the triggers are carefully fused into the input, bypassing detection by human inspection and causing backdoor identification through anomaly detection to fail. To defend against such imperceptible attacks, in this work we systematically analyze how representations, i.e., the set of neuron activations for a given DNN when using the training data as inputs, are affected by backdoor attacks. We propose PiDAn, an algorithm based on coherence optimization purifying the poisoned data. Our analysis shows that representations of poisoned data and authentic data in the target class are still embedded in different linear subspaces, which implies that they show different coherence with some latent spaces. Based on this observation, the proposed PiDAn algorithm learns a sample-wise weight vector to maximize the projected coherence of weighted samples, where we demonstrate that the learned weight vector has a natural "grouping effect" and is distinguishable between authentic data and poisoned data. This enables the systematic detection and mitigation of backdoor attacks. Based on our theoretical analysis and experimental results, we demonstrate the effectiveness of PiDAn in defending against backdoor attacks that use different settings of poisoned samples on GTSRB and ILSVRC2012 datasets. Our PiDAn algorithm can detect more than 90% infected classes and identify 95% poisoned samples.

摘要: 后门攻击给深度神经网络(DNNS)带来了新的威胁，通过毒化训练数据集，错误分类包含对手触发器的输入，将后门插入到神经网络中。防御这些攻击的主要挑战是只有攻击者知道秘密触发器和目标类别。最近引入的“隐藏触发器”进一步加剧了这一问题，即触发器被小心地融合到输入中，绕过人工检查的检测，并导致通过异常检测进行的后门识别失败。为了防御这种不可察觉的攻击，我们系统地分析了后门攻击如何影响表示，即使用训练数据作为输入时给定DNN的神经元激活集。提出了一种基于相干优化的毒化数据净化算法PIDAN。我们的分析表明，有毒数据和真实数据在目标类中的表示仍然嵌入不同的线性子空间，这意味着它们与一些潜在空间表现出不同的一致性。基于这一观察，提出的PIDAN算法学习样本权重向量来最大化加权样本的投影一致性，其中我们证明了学习的权重向量具有自然的分组效应，并且可以区分真实数据和有毒数据。这使得能够系统地检测和缓解后门攻击。在理论分析和实验结果的基础上，我们在GTSRB和ILSVRC2012数据集上验证了PIDAN对使用不同设置的有毒样本的后门攻击的有效性。我们的Pidan算法可以检测到90%以上的感染类别，并识别出95%的中毒样本。



## **50. On the Properties of Adversarially-Trained CNNs**

关于对抗性训练的CNN的性质 cs.CV

**SubmitDate**: 2022-03-17    [paper-pdf](http://arxiv.org/pdf/2203.09243v1)

**Authors**: Mattia Carletti, Matteo Terzi, Gian Antonio Susto

**Abstracts**: Adversarial Training has proved to be an effective training paradigm to enforce robustness against adversarial examples in modern neural network architectures. Despite many efforts, explanations of the foundational principles underpinning the effectiveness of Adversarial Training are limited and far from being widely accepted by the Deep Learning community. In this paper, we describe surprising properties of adversarially-trained models, shedding light on mechanisms through which robustness against adversarial attacks is implemented. Moreover, we highlight limitations and failure modes affecting these models that were not discussed by prior works. We conduct extensive analyses on a wide range of architectures and datasets, performing a deep comparison between robust and natural models.

摘要: 对抗性训练已被证明是一种有效的训练范例，可以在现代神经网络结构中增强对抗对抗性示例的鲁棒性。尽管做了许多努力，但对支持对抗性训练有效性的基本原则的解释是有限的，远未被深度学习社区广泛接受。在这篇文章中，我们描述了敌方训练模型的令人惊讶的性质，揭示了实现对敌方攻击的鲁棒性的机制。此外，我们强调了影响这些模型的限制和故障模式，这些都是以前的工作没有讨论过的。我们对广泛的体系结构和数据集进行了广泛的分析，并对健壮模型和自然模型进行了深入的比较。



