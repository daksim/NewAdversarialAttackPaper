# Latest Adversarial Attack Papers
**update at 2022-01-24 11:15:42**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Robust and Fully-Dynamic Coreset for Continuous-and-Bounded Learning (With Outliers) Problems**

连续有界学习(带离群点)问题的鲁棒全动态重置 cs.LG

23 pages

**SubmitDate**: 2022-01-21    [paper-pdf](http://arxiv.org/pdf/2107.00068v2)

**Authors**: Zixiu Wang, Yiwen Guo, Hu Ding

**Abstracts**: In many machine learning tasks, a common approach for dealing with large-scale data is to build a small summary, {\em e.g.,} coreset, that can efficiently represent the original input. However, real-world datasets usually contain outliers and most existing coreset construction methods are not resilient against outliers (in particular, an outlier can be located arbitrarily in the space by an adversarial attacker). In this paper, we propose a novel robust coreset method for the {\em continuous-and-bounded learning} problems (with outliers) which includes a broad range of popular optimization objectives in machine learning, {\em e.g.,} logistic regression and $ k $-means clustering. Moreover, our robust coreset can be efficiently maintained in fully-dynamic environment. To the best of our knowledge, this is the first robust and fully-dynamic coreset construction method for these optimization problems. Another highlight is that our coreset size can depend on the doubling dimension of the parameter space, rather than the VC dimension of the objective function which could be very large or even challenging to compute. Finally, we conduct the experiments on real-world datasets to evaluate the effectiveness of our proposed robust coreset method.

摘要: 在许多机器学习任务中，处理大规模数据的一种常见方法是构建一个能够有效表示原始输入的小摘要，{\em，例如}coset。然而，现实世界的数据集通常包含离群值，并且大多数现有的核心重置构造方法对离群值没有弹性(尤其是，敌意攻击者可以在空间中任意定位离群值)。本文提出了一种新的鲁棒重置方法来解决{em连续有界学习}问题(带有离群值)，该方法包括机器学习中广泛流行的优化目标，{em例如}Logistic回归和$k$-均值聚类。此外，在全动态环境下，我们的鲁棒CoReset能够有效地保持。据我们所知，这是解决这些优化问题的第一种鲁棒的、全动态的CoReset构造方法。另一个亮点是，我们的核心大小可以取决于参数空间的倍增维数，而不是目标函数的VC维数，后者可能非常大，甚至很难计算。最后，我们在真实数据集上进行了实验，以评估我们提出的鲁棒重置方法的有效性。



## **2. Natural Attack for Pre-trained Models of Code**

针对预先训练的代码模型的自然攻击 cs.SE

Accepted to the Technical Track of ICSE 2022

**SubmitDate**: 2022-01-21    [paper-pdf](http://arxiv.org/pdf/2201.08698v1)

**Authors**: Zhou Yang, Jieke Shi, Junda He, David Lo

**Abstracts**: Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement.   In this paper, we propose ALERT (nAturaLnEss AwaRe ATtack), a black-box attack that adversarially transforms inputs to make victim models produce wrong outputs. Different from prior works, this paper considers the natural semantic of generated examples at the same time as preserving the operational semantic of original inputs. Our user study demonstrates that human developers consistently consider that adversarial examples generated by ALERT are more natural than those generated by the state-of-the-art work by Zhang et al. that ignores the naturalness requirement. On attacking CodeBERT, our approach can achieve attack success rates of 53.62%, 27.79%, and 35.78% across three downstream tasks: vulnerability prediction, clone detection and code authorship attribution. On GraphCodeBERT, our approach can achieve average success rates of 76.95%, 7.96% and 61.47% on the three tasks. The above outperforms the baseline by 14.07% and 18.56% on the two pre-trained models on average. Finally, we investigated the value of the generated adversarial examples to harden victim models through an adversarial fine-tuning procedure and demonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated adversarial examples increased by 87.59% and 92.32%, respectively.

摘要: 预先训练的代码模型在许多重要的软件工程任务中取得了成功。然而，这些强大的模型容易受到对抗性攻击，这些攻击略微扰动模型输入，使受害者模型产生错误的输出。目前的工作主要是用保持操作程序语义的示例攻击代码模型，而忽略了生成对抗性示例的一个基本要求：扰动对于人类判断来说应该是自然的，我们称之为自然性要求。在本文中，我们提出了ALERT(自然度感知攻击)，这是一种黑盒攻击，它对输入进行恶意转换，使受害者模型产生错误的输出。与以往的工作不同，本文在保留原始输入操作语义的同时，考虑了生成示例的自然语义。我们的用户研究表明，人类开发人员一致认为，ALERT生成的对抗性示例比由Zhang等人的最新工作生成的示例更自然。这忽略了自然度的要求。在攻击CodeBERT上，我们的方法可以在漏洞预测、克隆检测和代码作者归属三个下游任务上获得53.62%、27.79%和35.78%的攻击成功率。在GraphCodeBERT上，我们的方法在三个任务上的平均成功率分别为76.95%、7.96%和61.47%。在两个预先训练的模型上，上述两个模型的平均性能分别比基线高14.07%和18.56%。最后，我们通过对抗性微调过程考察了生成的对抗性实例对硬化受害者模型的价值，并证明了CodeBERT和GraphCodeBERT对警报生成的对抗性实例的准确率分别提高了87.59%和92.32%。结果表明，CodeBERT和GraphCodeBERT对警报生成的对抗性实例的准确率分别提高了87.59%和92.32%。



## **3. A Comprehensive Study of Vision Transformers on Dense Prediction Tasks**

视觉变形器在密集预测任务中的综合研究 cs.CV

17th International Conference on Computer Vision Theory and  Applications (VISAP, 2022)

**SubmitDate**: 2022-01-21    [paper-pdf](http://arxiv.org/pdf/2201.08683v1)

**Authors**: Kishaan Jeeveswaran, Senthilkumar Kathiresan, Arnav Varma, Omar Magdy, Bahram Zonooz, Elahe Arani

**Abstracts**: Convolutional Neural Networks (CNNs), architectures consisting of convolutional layers, have been the standard choice in vision tasks. Recent studies have shown that Vision Transformers (VTs), architectures based on self-attention modules, achieve comparable performance in challenging tasks such as object detection and semantic segmentation. However, the image processing mechanism of VTs is different from that of conventional CNNs. This poses several questions about their generalizability, robustness, reliability, and texture bias when used to extract features for complex tasks. To address these questions, we study and compare VT and CNN architectures as feature extractors in object detection and semantic segmentation. Our extensive empirical results show that the features generated by VTs are more robust to distribution shifts, natural corruptions, and adversarial attacks in both tasks, whereas CNNs perform better at higher image resolutions in object detection. Furthermore, our results demonstrate that VTs in dense prediction tasks produce more reliable and less texture-biased predictions.

摘要: 卷积神经网络(CNNs)是由卷积层组成的结构，已经成为视觉任务的标准选择。最近的研究表明，基于自我注意模块的视觉转换器(VTS)在具有挑战性的任务(如目标检测和语义分割)中取得了与之相当的性能。然而，VTS的图像处理机制不同于传统的CNN。这对它们在用于复杂任务的特征提取时的概括性、健壮性、可靠性和纹理偏差提出了几个问题。为了解决这些问题，我们研究和比较了VT和CNN结构作为目标检测和语义分割的特征提取器。我们广泛的实验结果表明，在这两个任务中，VT生成的特征对分布漂移、自然破坏和敌意攻击具有更强的鲁棒性，而CNN在高图像分辨率的目标检测中表现得更好。此外，我们的结果表明，密集预测任务中的VT可以产生更可靠、更少纹理偏差的预测。



## **4. The Security of Deep Learning Defences for Medical Imaging**

医学影像深度学习防御的安全性研究 cs.CR

**SubmitDate**: 2022-01-21    [paper-pdf](http://arxiv.org/pdf/2201.08661v1)

**Authors**: Moshe Levy, Guy Amit, Yuval Elovici, Yisroel Mirsky

**Abstracts**: Deep learning has shown great promise in the domain of medical image analysis. Medical professionals and healthcare providers have been adopting the technology to speed up and enhance their work. These systems use deep neural networks (DNN) which are vulnerable to adversarial samples; images with imperceivable changes that can alter the model's prediction. Researchers have proposed defences which either make a DNN more robust or detect the adversarial samples before they do harm. However, none of these works consider an informed attacker which can adapt to the defence mechanism. We show that an informed attacker can evade five of the current state of the art defences while successfully fooling the victim's deep learning model, rendering these defences useless. We then suggest better alternatives for securing healthcare DNNs from such attacks: (1) harden the system's security and (2) use digital signatures.

摘要: 深度学习在医学图像分析领域显示出巨大的应用前景。医疗专业人员和医疗保健提供者一直在采用这项技术来加快和加强他们的工作。这些系统使用深度神经网络(DNN)，它容易受到敌意样本的攻击；图像具有不可察觉的变化，可能会改变模型的预测。研究人员已经提出了防御措施，这些防御措施要么使DNN更加健壮，要么在对手样本造成伤害之前将其检测出来。然而，这些工作都没有考虑到能够适应防御机制的知情攻击者。我们表明，知情的攻击者可以躲避五个当前最先进的防御措施，同时成功欺骗受害者的深度学习模型，使这些防御措施变得毫无用处。然后，我们提出了保护医疗DNN免受此类攻击的更好的替代方案：(1)加强系统的安全性，(2)使用数字签名。



## **5. Robust Unsupervised Graph Representation Learning via Mutual Information Maximization**

基于互信息最大化的鲁棒无监督图表示学习 cs.LG

**SubmitDate**: 2022-01-21    [paper-pdf](http://arxiv.org/pdf/2201.08557v1)

**Authors**: Jihong Wang, Minnan Luo, Jundong Li, Ziqi Liu, Jun Zhou, Qinghua Zheng

**Abstracts**: Recent studies have shown that GNNs are vulnerable to adversarial attack. Thus, many approaches are proposed to improve the robustness of GNNs against adversarial attacks. Nevertheless, most of these methods measure the model robustness based on label information and thus become infeasible when labels information is not available. Therefore, this paper focuses on robust unsupervised graph representation learning. In particular, to quantify the robustness of GNNs without label information, we propose a robustness measure, named graph representation robustness (GRR), to evaluate the mutual information between adversarially perturbed node representations and the original graph. There are mainly two challenges to estimate GRR: 1) mutual information estimation upon adversarially attacked graphs; 2) high complexity of adversarial attack to perturb node features and graph structure jointly in the training procedure. To tackle these problems, we further propose an effective mutual information estimator with subgraph-level summary and an efficient adversarial training strategy with only feature perturbations. Moreover, we theoretically establish a connection between our proposed GRR measure and the robustness of downstream classifiers, which reveals that GRR can provide a lower bound to the adversarial risk of downstream classifiers. Extensive experiments over several benchmarks demonstrate the effectiveness and superiority of our proposed method.

摘要: 最近的研究表明，GNN很容易受到敌意攻击。因此，人们提出了许多方法来提高GNNs抵抗敌意攻击的健壮性。然而，这些方法大多基于标签信息来衡量模型的稳健性，因此当标签信息不可用时，这些方法变得不可行。因此，本文重点研究鲁棒无监督图表示学习。特别地，为了量化没有标签信息的GNN的健壮性，我们提出了一种健壮性度量，称为图表示健壮性(GRR)，用于评估恶意扰动的节点表示与原始图之间的互信息。GRR估计主要有两个挑战：1)对抗性攻击图的互信息估计；2)对抗性攻击在训练过程中联合扰动节点特征和图结构的高复杂度。针对这些问题，我们进一步提出了一种有效的子图级摘要互信息估计器和一种只进行特征扰动的高效对抗性训练策略。此外，我们在理论上建立了我们提出的GRR度量与下游分类器的鲁棒性之间的联系，这表明GRR可以为下游分类器的对抗风险提供一个下界。在多个基准上的大量实验证明了该方法的有效性和优越性。



## **6. Identifying Adversarial Attacks on Text Classifiers**

识别对文本分类器的敌意攻击 cs.CL

**SubmitDate**: 2022-01-21    [paper-pdf](http://arxiv.org/pdf/2201.08555v1)

**Authors**: Zhouhang Xie, Jonathan Brophy, Adam Noack, Wencong You, Kalyani Asthana, Carter Perkins, Sabrina Reis, Sameer Singh, Daniel Lowd

**Abstracts**: The landscape of adversarial attacks against text classifiers continues to grow, with new attacks developed every year and many of them available in standard toolkits, such as TextAttack and OpenAttack. In response, there is a growing body of work on robust learning, which reduces vulnerability to these attacks, though sometimes at a high cost in compute time or accuracy. In this paper, we take an alternate approach -- we attempt to understand the attacker by analyzing adversarial text to determine which methods were used to create it. Our first contribution is an extensive dataset for attack detection and labeling: 1.5~million attack instances, generated by twelve adversarial attacks targeting three classifiers trained on six source datasets for sentiment analysis and abuse detection in English. As our second contribution, we use this dataset to develop and benchmark a number of classifiers for attack identification -- determining if a given text has been adversarially manipulated and by which attack. As a third contribution, we demonstrate the effectiveness of three classes of features for these tasks: text properties, capturing content and presentation of text; language model properties, determining which tokens are more or less probable throughout the input; and target model properties, representing how the text classifier is influenced by the attack, including internal node activations. Overall, this represents a first step towards forensics for adversarial attacks against text classifiers.

摘要: 针对文本分类器的敌意攻击持续增长，每年都会出现新的攻击，其中许多都可以在标准工具包(如TextAttack和OpenAttack)中使用。作为回应，有越来越多关于健壮学习的工作，它降低了对这些攻击的脆弱性，尽管有时会以很高的计算时间或准确性为代价。在本文中，我们采取另一种方法--我们试图通过分析敌意文本来了解攻击者，以确定使用了哪些方法来创建它。我们的第一个贡献是为攻击检测和标记提供了一个广泛的数据集：150万个攻击实例，由12个针对3个分类器的对抗性攻击生成，这些分类器在6个源数据集上进行训练，用于情感分析和滥用检测。作为我们的第二个贡献，我们使用这个数据集来开发和基准测试许多用于攻击识别的分类器--确定给定的文本是否被恶意篡改以及被哪种攻击篡改。作为第三个贡献，我们展示了这些任务的三类特征的有效性：文本属性，捕获内容和文本的呈现；语言模型属性，确定哪些标记在整个输入中或多或少是可能的；以及目标模型属性，表示文本分类器如何受到攻击的影响，包括内部节点激活。总体而言，这代表着针对文本分类器的对抗性攻击的取证迈出了第一步。



## **7. Blockchain-based Collaborated Federated Learning for Improved Security, Privacy and Reliability**

基于区块链的协作联合学习提高安全性、隐私性和可靠性 cs.CR

Preliminary work

**SubmitDate**: 2022-01-21    [paper-pdf](http://arxiv.org/pdf/2201.08551v1)

**Authors**: Amir Afaq, Zeeshan Ahmed, Noman Haider, Muhammad Imran

**Abstracts**: Federated Learning (FL) provides privacy preservation by allowing the model training at edge devices without the need of sending the data from edge to a centralized server. FL has distributed the implementation of ML. Another variant of FL which is well suited for the Internet of Things (IoT) is known as Collaborated Federated Learning (CFL), which does not require an edge device to have a direct link to the model aggregator. Instead, the devices can connect to the central model aggregator via other devices using them as relays. Although, FL and CFL protect the privacy of edge devices but raises security challenges for a centralized server that performs model aggregation. The centralized server is prone to malfunction, backdoor attacks, model corruption, adversarial attacks and external attacks. Moreover, edge device to centralized server data exchange is not required in FL and CFL, but model parameters are sent from the model aggregator (global model) to edge devices (local model), which is still prone to cyber-attacks. These security and privacy concerns can be potentially addressed by Blockchain technology. The blockchain is a decentralized and consensus-based chain where devices can share consensus ledgers with increased reliability and security, thus significantly reducing the cyberattacks on an exchange of information. In this work, we will investigate the efficacy of blockchain-based decentralized exchange of model parameters and relevant information among edge devices and from a centralized server to edge devices. Moreover, we will be conducting the feasibility analysis for blockchain-based CFL models for different application scenarios like the internet of vehicles, and the internet of things. The proposed study aims to improve the security, reliability and privacy preservation by the use of blockchain-powered CFL.

摘要: 联合学习(FL)允许在边缘设备上进行模型训练，而无需将数据从边缘发送到中央服务器，从而提供隐私保护。FL已经发布了ML的实现。另一种非常适合物联网(IoT)的FL变体称为协作联合学习(CFL)，它不需要边缘设备直接链接到模型聚合器。相反，这些设备可以通过使用它们作为继电器的其他设备连接到中心模型聚合器。虽然FL和CFL保护边缘设备的隐私，但会给执行模型聚合的集中式服务器带来安全挑战。集中式服务器容易发生故障、后门攻击、模型损坏、对抗性攻击和外部攻击。此外，FL和CFL不需要边缘设备到集中式服务器的数据交换，而是将模型参数从模型聚合器(全局模型)发送到边缘设备(本地模型)，这仍然容易受到网络攻击。区块链技术可以潜在地解决这些安全和隐私问题。区块链是一种分散的、基于共识的链，其中设备可以更高的可靠性和安全性共享共识分类账，从而显著减少对信息交换的网络攻击。在这项工作中，我们将调查基于区块链的边缘设备之间以及从集中式服务器到边缘设备的模型参数和相关信息的分散交换的有效性。此外，我们还将对基于区块链的CFL模型在车联网、物联网等不同应用场景下进行可行性分析。拟议的研究旨在通过使用区块链驱动的CFL来提高安全性、可靠性和隐私保护。



## **8. RoboMal: Malware Detection for Robot Network Systems**

RoboMal：面向机器人网络系统的恶意软件检测 cs.RO

Published in the proceedings of 2021 5th IEEE International  Conference on Robotic Computing (IRC)

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.08470v1)

**Authors**: Upinder Kaur, Haozhe Zhou, Xiaxin Shen, Byung-Cheol Min, Richard M. Voyles

**Abstracts**: Robot systems are increasingly integrating into numerous avenues of modern life. From cleaning houses to providing guidance and emotional support, robots now work directly with humans. Due to their far-reaching applications and progressively complex architecture, they are being targeted by adversarial attacks such as sensor-actuator attacks, data spoofing, malware, and network intrusion. Therefore, security for robotic systems has become crucial. In this paper, we address the underserved area of malware detection in robotic software. Since robots work in close proximity to humans, often with direct interactions, malware could have life-threatening impacts. Hence, we propose the RoboMal framework of static malware detection on binary executables to detect malware before it gets a chance to execute. Additionally, we address the great paucity of data in this space by providing the RoboMal dataset comprising controller executables of a small-scale autonomous car. The performance of the framework is compared against widely used supervised learning models: GRU, CNN, and ANN. Notably, the LSTM-based RoboMal model outperforms the other models with an accuracy of 85% and precision of 87% in 10-fold cross-validation, hence proving the effectiveness of the proposed framework.

摘要: 机器人系统正越来越多地融入现代生活的众多领域。从打扫房屋到提供指导和情感支持，机器人现在直接与人类合作。由于其广泛的应用和日益复杂的体系结构，它们正成为传感器致动器攻击、数据欺骗、恶意软件和网络入侵等敌意攻击的目标。因此，机器人系统的安全性变得至关重要。在这篇文章中，我们解决了机器人软件中恶意软件检测服务不足的领域。由于机器人工作时离人类很近，通常会进行直接交互，因此恶意软件可能会产生危及生命的影响。因此，我们提出了静电恶意软件检测的RoboMal框架，对二进制可执行文件进行检测，以便在恶意软件有机会执行之前将其检测出来。此外，我们通过提供包含小型自动驾驶汽车的控制器可执行文件的RoboMal数据集来解决该空间中数据的巨大匮乏问题。将该框架的性能与广泛使用的监督学习模型GRU、CNN和ANN进行了比较。值得注意的是，基于LSTM的RoboMal模型在10次交叉验证中的准确率分别为85%和87%，证明了该框架的有效性。



## **9. Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs**

作弊自动简答评分--论形容词和副词的对抗性用法 cs.CL

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.08318v1)

**Authors**: Anna Filighera, Sebastian Ochs, Tim Steuer, Thomas Tregel

**Abstracts**: Automatic grading models are valued for the time and effort saved during the instruction of large student bodies. Especially with the increasing digitization of education and interest in large-scale standardized testing, the popularity of automatic grading has risen to the point where commercial solutions are widely available and used. However, for short answer formats, automatic grading is challenging due to natural language ambiguity and versatility. While automatic short answer grading models are beginning to compare to human performance on some datasets, their robustness, especially to adversarially manipulated data, is questionable. Exploitable vulnerabilities in grading models can have far-reaching consequences ranging from cheating students receiving undeserved credit to undermining automatic grading altogether - even when most predictions are valid. In this paper, we devise a black-box adversarial attack tailored to the educational short answer grading scenario to investigate the grading models' robustness. In our attack, we insert adjectives and adverbs into natural places of incorrect student answers, fooling the model into predicting them as correct. We observed a loss of prediction accuracy between 10 and 22 percentage points using the state-of-the-art models BERT and T5. While our attack made answers appear less natural to humans in our experiments, it did not significantly increase the graders' suspicions of cheating. Based on our experiments, we provide recommendations for utilizing automatic grading systems more safely in practice.

摘要: 自动评分模型的价值在于在教学过程中节省了大量学生的时间和精力。特别是随着教育数字化程度的不断提高和人们对大规模标准化考试的兴趣，自动评分的普及程度已经上升到了商业解决方案被广泛获得和使用的地步。然而，对于简答题格式，由于自然语言的歧义性和多功能性，自动评分是具有挑战性的。虽然自动简答评分模型开始与人类在某些数据集上的表现进行比较，但它们的健壮性，特别是对被恶意篡改的数据的健壮性，是值得怀疑的。评分模型中可利用的漏洞可能会产生深远的后果，从欺骗学生获得不应得的学分，到完全破坏自动评分-即使大多数预测是正确的。在本文中，我们设计了一个针对教育简答题评分场景的黑盒对抗性攻击，以考察评分模型的稳健性。在我们的攻击中，我们在错误的学生答案的自然位置插入形容词和副词，愚弄模型预测它们是正确的。我们使用最先进的模型BERT和T5观察到预测精度的损失在10到22个百分点之间。虽然在我们的实验中，我们的攻击使答案对人类来说显得不那么自然，但它并没有显著增加评分员作弊的怀疑。在实验的基础上，我们提出了在实践中更安全地使用自动评分系统的建议。



## **10. Optimization of a Reed-Solomon code-based protocol against blockchain data availability attacks**

一种基于Reed-Solomon码的抗区块链数据可用性攻击协议的优化 cs.IT

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.08261v1)

**Authors**: Paolo Santini, Giulia Rafaiani, Massimo Battaglioni, Franco Chiaraluce, Marco Baldi

**Abstracts**: ASBK (named after the authors' initials) is a recent blockchain protocol tackling data availability attacks against light nodes, employing two-dimensional Reed-Solomon codes to encode the list of transactions and a random sampling phase where adversaries are forced to reveal information. In its original formulation, only codes with rate $1/4$ are considered, and a theoretical analysis requiring computationally demanding formulas is provided. This makes ASBK difficult to optimize in situations of practical interest. In this paper, we introduce a much simpler model for such a protocol, which additionally supports the use of codes with arbitrary rate. This makes blockchains implementing ASBK much easier to design and optimize. Furthermore, disposing of a clearer view of the protocol, some general features and considerations can be derived (e.g., nodes behaviour in largely participated networks). As a concrete application of our analysis, we consider relevant blockchain parameters and find network settings that minimize the amount of data downloaded by light nodes. Our results show that the protocol benefits from the use of codes defined over large finite fields, with code rates that may be even significantly different from the originally proposed ones.

摘要: ASBK(以作者姓名首字母命名)是最近推出的区块链协议，用于应对针对轻节点的数据可用性攻击，使用二维里德-所罗门码对交易列表进行编码，并在随机采样阶段迫使攻击者泄露信息。在其最初的公式中，只考虑了码率为$1/4$的码，并给出了需要计算量很大的公式的理论分析。这使得ASBK很难在有实际意义的情况下进行优化。在本文中，我们介绍了一种更简单的协议模型，该模型还支持使用任意速率的码。这使得实现ASBK的区块链更容易设计和优化。此外，处理协议的更清晰视图后，可以导出一些一般特征和考虑因素(例如，大量参与的网络中的节点行为)。作为我们分析的一个具体应用，我们考虑了相关的区块链参数，并找到了将光节点下载的数据量降至最低的网络设置。我们的结果表明，该协议受益于定义在大有限域上的码的使用，其码率甚至可能与最初提出的显著不同。



## **11. Learning-based Hybrid Local Search for the Hard-label Textual Attack**

基于学习的混合局部搜索在硬标签文本攻击中的应用 cs.CL

8 pages

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.08193v1)

**Authors**: Zhen Yu, Xiaosen Wang, Wanxiang Che, Kun He

**Abstracts**: Deep neural networks are vulnerable to adversarial examples in Natural Language Processing. However, existing textual adversarial attacks usually utilize the gradient or prediction confidence to generate adversarial examples, making it hard to be deployed in real-world applications. To this end, we consider a rarely investigated but more rigorous setting, namely hard-label attack, in which the attacker could only access the prediction label. In particular, we find that the changes on prediction label caused by word substitutions on the adversarial example could precisely reflect the importance of different words. Based on this observation, we propose a novel hard-label attack, called Learning-based Hybrid Local Search (LHLS) algorithm, which effectively estimates word importance with the prediction label from the attack history and integrate such information into hybrid local search algorithm to optimize the adversarial perturbation. Extensive evaluations for text classification and textual entailment using various datasets and models show that our LHLS significantly outperforms existing hard-label attacks regarding the attack performance as well as adversary quality.

摘要: 深度神经网络在自然语言处理中容易受到敌意示例的影响。然而，现有的文本对抗性攻击通常利用梯度或预测置信度来生成对抗性示例，这使得它很难部署到现实世界的应用中。为此，我们考虑了一种很少被研究但更严格的环境，即硬标签攻击，在这种情况下，攻击者只能访问预测标签。特别地，我们发现对抗性例子中的词语替换所引起的预测标签上的变化可以准确地反映不同词语的重要性。基于此，我们提出了一种新的硬标签攻击，称为基于学习的混合局部搜索(LHLS)算法，该算法利用攻击历史中的预测标签有效地估计词的重要性，并将这些信息整合到混合局部搜索算法中，以优化对手的扰动。使用不同的数据集和模型对文本分类和文本蕴涵进行了广泛的评估，结果表明，我们的LHLS在攻击性能和对手质量方面都明显优于现有的硬标签攻击。



## **12. Survey on Federated Learning Threats: concepts, taxonomy on attacks and defences, experimental study and challenges**

联合学习威胁调查：概念、攻防分类、实验研究和挑战 cs.CR

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.08135v1)

**Authors**: Nuria Rodríguez-Barroso, Daniel Jiménez López, M. Victoria Luzón, Francisco Herrera, Eugenio Martínez-Cámara

**Abstracts**: Federated learning is a machine learning paradigm that emerges as a solution to the privacy-preservation demands in artificial intelligence. As machine learning, federated learning is threatened by adversarial attacks against the integrity of the learning model and the privacy of data via a distributed approach to tackle local and global learning. This weak point is exacerbated by the inaccessibility of data in federated learning, which makes harder the protection against adversarial attacks and evidences the need to furtherance the research on defence methods to make federated learning a real solution for safeguarding data privacy. In this paper, we present an extensive review of the threats of federated learning, as well as as their corresponding countermeasures, attacks versus defences. This survey provides a taxonomy of adversarial attacks and a taxonomy of defence methods that depict a general picture of this vulnerability of federated learning and how to overcome it. Likewise, we expound guidelines for selecting the most adequate defence method according to the category of the adversarial attack. Besides, we carry out an extensive experimental study from which we draw further conclusions about the behaviour of attacks and defences and the guidelines for selecting the most adequate defence method according to the category of the adversarial attack. This study is finished leading to meditated learned lessons and challenges.

摘要: 联合学习是一种机器学习范式，作为人工智能中隐私保护需求的解决方案而出现。与机器学习一样，联合学习也面临着针对学习模型完整性和数据隐私的敌意攻击，这些攻击是通过分布式的撞击本地和全球学习方法实现的。联邦学习中数据的不可访问性加剧了这一弱点，这使得抵御对手攻击的保护变得更加困难，并证明有必要进一步研究防御方法，使联邦学习成为保护数据隐私的真正解决方案。在这篇文章中，我们广泛回顾了联合学习的威胁，以及它们相应的对策，攻击与防御。这项调查提供了对抗性攻击的分类和防御方法的分类，描述了联邦学习的这种脆弱性以及如何克服它的大致情况。同样，我们根据对抗性攻击的类别阐述了选择最合适的防御方法的指导原则。此外，我们还进行了广泛的实验研究，从中我们得出了关于攻击和防御行为的进一步结论，以及根据对抗性攻击的类别选择最合适的防御方法的指导方针。这项研究已经完成，引出了沉思的经验教训和挑战。



## **13. Adversarial Jamming for a More Effective Constellation Attack**

一种更有效星座攻击的对抗性干扰 cs.CR

3 pages, 2 figures, published in The 13th International Symposium on  Antennas, Propagation and EM Theory (ISAPE 2021)

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.08052v1)

**Authors**: Haidong Xie, Yizhou Xu, Yuanqing Chen, Nan Ji, Shuai Yuan, Naijin Liu, Xueshuang Xiang

**Abstracts**: The common jamming mode in wireless communication is band barrage jamming, which is controllable and difficult to resist. Although this method is simple to implement, it is obviously not the best jamming waveform. Therefore, based on the idea of adversarial examples, we propose the adversarial jamming waveform, which can independently optimize and find the best jamming waveform. We attack QAM with adversarial jamming and find that the optimal jamming waveform is equivalent to the amplitude and phase between the nearest constellation points. Furthermore, by verifying the jamming performance on a hardware platform, it is shown that our method significantly improves the bit error rate compared to other methods.

摘要: 无线通信中常见的干扰方式是频带阻塞干扰，这种干扰具有可控性和难以抵抗的特点。虽然该方法实现简单，但显然不是最佳的干扰波形。因此，基于对抗性实例的思想，我们提出了对抗性干扰波形，该波形可以自主优化并找到最佳干扰波形。采用对抗性干扰对QAM进行攻击，发现最佳干扰波形相当于最近星座点之间的幅度和相位。此外，在硬件平台上验证了该方法的干扰性能，结果表明，与其他方法相比，该方法显著提高了误码率。



## **14. Steerable Pyramid Transform Enables Robust Left Ventricle Quantification**

可控金字塔变换实现健壮的左心室定量 eess.IV

10 pages, 13 figures, journal paper

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.08388v1)

**Authors**: Xiangyang Zhu, Kede Ma, Wufeng Xue

**Abstracts**: Although multifarious variants of convolutional neural networks (CNNs) have proved successful in cardiac index quantification, they seem vulnerable to mild input perturbations, e.g., spatial transformations, image distortions, and adversarial attacks. Such brittleness erodes our trust in CNN-based automated diagnosis of various cardiovascular diseases. In this work, we describe a simple and effective method to learn robust CNNs for left ventricle (LV) quantification, including cavity and myocardium areas, directional dimensions, and regional wall thicknesses. The key to the success of our approach is the use of the biologically-inspired steerable pyramid transform (SPT) as fixed front-end processing, which brings three computational advantages to LV quantification. First, the basis functions of SPT match the anatomical structure of the LV as well as the geometric characteristics of the estimated indices. Second, SPT enables sharing a CNN across different orientations as a form of parameter regularization, and explicitly captures the scale variations of the LV in a natural way. Third, the residual highpass subband can be conveniently discarded to further encourage robust feature learning. A concise and effective metric, named Robustness Ratio, is proposed to evaluate the robustness under various input perturbations. Extensive experiments on 145 cardiac sequences show that our SPT-augmented method performs favorably against state-of-the-art algorithms in terms of prediction accuracy, but is significantly more robust under input perturbations.

摘要: 虽然卷积神经网络(CNNs)的各种变体在心脏指数量化方面已经被证明是成功的，但它们似乎容易受到轻微的输入扰动，例如空间变换、图像失真和敌意攻击。这种脆弱性侵蚀了我们对基于CNN的各种心血管疾病自动诊断的信任。在这项工作中，我们描述了一种简单而有效的方法来学习用于左心室(LV)定量的健壮CNN，包括空洞和心肌面积、方向尺寸和局部室壁厚度。我们的方法成功的关键是使用生物启发的可控金字塔变换(SPT)作为固定的前端处理，这给LV量化带来了三个计算优势。首先，SPT的基函数与左心室的解剖结构以及估计指标的几何特征相匹配。其次，SPT允许在不同方向上共享CNN作为参数正则化的一种形式，并以自然的方式显式地捕捉LV的尺度变化。第三，可以方便地丢弃剩余的高通子带以进一步鼓励稳健的特征学习。提出了一种简明有效的评价系统在各种输入扰动下鲁棒性的指标--鲁棒性比(Robust Ratio)。在145个心脏序列上的大量实验表明，我们的SPT增强方法在预测精度方面优于最先进的算法，但在输入扰动下的鲁棒性要强得多。



## **15. Unsupervised Graph Poisoning Attack via Contrastive Loss Back-propagation**

基于对比损失反向传播的无监督图中毒攻击 cs.LG

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.07986v1)

**Authors**: Sixiao Zhang, Hongxu Chen, Xiangguo Sun, Yicong Li, Guandong Xu

**Abstracts**: Graph contrastive learning is the state-of-the-art unsupervised graph representation learning framework and has shown comparable performance with supervised approaches. However, evaluating whether the graph contrastive learning is robust to adversarial attacks is still an open problem because most existing graph adversarial attacks are supervised models, which means they heavily rely on labels and can only be used to evaluate the graph contrastive learning in a specific scenario. For unsupervised graph representation methods such as graph contrastive learning, it is difficult to acquire labels in real-world scenarios, making traditional supervised graph attack methods difficult to be applied to test their robustness. In this paper, we propose a novel unsupervised gradient-based adversarial attack that does not rely on labels for graph contrastive learning. We compute the gradients of the adjacency matrices of the two views and flip the edges with gradient ascent to maximize the contrastive loss. In this way, we can fully use multiple views generated by the graph contrastive learning models and pick the most informative edges without knowing their labels, and therefore can promisingly support our model adapted to more kinds of downstream tasks. Extensive experiments show that our attack outperforms unsupervised baseline attacks and has comparable performance with supervised attacks in multiple downstream tasks including node classification and link prediction. We further show that our attack can be transferred to other graph representation models as well.

摘要: 图对比学习是目前最先进的无监督图表示学习框架，表现出与有监督方法相当的性能。然而，由于现有的图对抗性攻击大多是有监督的模型，这意味着它们严重依赖于标签，并且只能在特定场景下用来评价图对比学习，因此评价图对比学习对敌意攻击的健壮性仍然是一个有待解决的问题。对于图对比学习等无监督图表示方法，在现实场景中很难获得标签，这使得传统的有监督图攻击方法很难应用于测试其鲁棒性。在本文中，我们提出了一种新的无监督的基于梯度的对抗性攻击，该攻击不依赖于图的对比学习。我们计算两个视图的邻接矩阵的梯度，并用梯度上升来翻转边以最大化对比损失。这样，我们可以充分利用图对比学习模型生成的多个视图，在不知道其标签的情况下挑选出信息最丰富的边，从而有希望支持我们的模型适应更多类型的下游任务。大量实验表明，我们的攻击性能优于无监督基线攻击，在节点分类和链路预测等多个下游任务中的性能与监督攻击相当。我们进一步证明了我们的攻击也可以转移到其他图表示模型上。



## **16. Get your Foes Fooled: Proximal Gradient Split Learning for Defense against Model Inversion Attacks on IoMT data**

愚弄你的敌人：用于防御IoMT数据模型反转攻击的近梯度分裂学习 cs.CR

9 pages, 5 figures, 2 tables

**SubmitDate**: 2022-01-20    [paper-pdf](http://arxiv.org/pdf/2201.04569v2)

**Authors**: Sunder Ali Khowaja, Ik Hyun Lee, Kapal Dev, Muhammad Aslam Jarwar, Nawab Muhammad Faseeh Qureshi

**Abstracts**: The past decade has seen a rapid adoption of Artificial Intelligence (AI), specifically the deep learning networks, in Internet of Medical Things (IoMT) ecosystem. However, it has been shown recently that the deep learning networks can be exploited by adversarial attacks that not only make IoMT vulnerable to the data theft but also to the manipulation of medical diagnosis. The existing studies consider adding noise to the raw IoMT data or model parameters which not only reduces the overall performance concerning medical inferences but also is ineffective to the likes of deep leakage from gradients method. In this work, we propose proximal gradient split learning (PSGL) method for defense against the model inversion attacks. The proposed method intentionally attacks the IoMT data when undergoing the deep neural network training process at client side. We propose the use of proximal gradient method to recover gradient maps and a decision-level fusion strategy to improve the recognition performance. Extensive analysis show that the PGSL not only provides effective defense mechanism against the model inversion attacks but also helps in improving the recognition performance on publicly available datasets. We report 17.9$\%$ and 36.9$\%$ gains in accuracy over reconstructed and adversarial attacked images, respectively.

摘要: 在过去的十年中，人工智能(AI)，特别是深度学习网络，在医疗物联网(IoMT)生态系统中得到了迅速的采用。然而，最近的研究表明，深度学习网络可以被敌意攻击所利用，这些攻击不仅使物联网容易受到数据窃取的攻击，而且还容易受到医疗诊断的篡改。已有的研究都是考虑在原始IoMT数据或模型参数中加入噪声，这不仅降低了医学推断的整体性能，而且对梯度法等深度渗漏方法效果不佳。在这项工作中，我们提出了近邻梯度分裂学习(PSGL)方法来防御模型反转攻击。该方法在客户端对IoMT数据进行深度神经网络训练时，对IoMT数据进行故意攻击。提出了利用近邻梯度法恢复梯度图，并提出了决策层融合策略来提高识别性能。大量分析表明，PGSL不仅提供了对模型反转攻击的有效防御机制，而且有助于提高对公开数据集的识别性能。我们报告的准确率分别比重建图像和对抗性攻击图像提高了17.9美元和36.9美元。



## **17. Enhancing the Security & Privacy of Wearable Brain-Computer Interfaces**

提高可穿戴脑机接口的安全性和保密性 cs.CR

**SubmitDate**: 2022-01-19    [paper-pdf](http://arxiv.org/pdf/2201.07711v1)

**Authors**: Zahra Tarkhani, Lorena Qendro, Malachy O'Connor Brown, Oscar Hill, Cecilia Mascolo, Anil Madhavapeddy

**Abstracts**: Brain computing interfaces (BCI) are used in a plethora of safety/privacy-critical applications, ranging from healthcare to smart communication and control. Wearable BCI setups typically involve a head-mounted sensor connected to a mobile device, combined with ML-based data processing. Consequently, they are susceptible to a multiplicity of attacks across the hardware, software, and networking stacks used that can leak users' brainwave data or at worst relinquish control of BCI-assisted devices to remote attackers. In this paper, we: (i) analyse the whole-system security and privacy threats to existing wearable BCI products from an operating system and adversarial machine learning perspective; and (ii) introduce Argus, the first information flow control system for wearable BCI applications that mitigates these attacks. Argus' domain-specific design leads to a lightweight implementation on Linux ARM platforms suitable for existing BCI use-cases. Our proof of concept attacks on real-world BCI devices (Muse, NeuroSky, and OpenBCI) led us to discover more than 300 vulnerabilities across the stacks of six major attack vectors. Our evaluation shows Argus is highly effective in tracking sensitive dataflows and restricting these attacks with an acceptable memory and performance overhead (<15%).

摘要: 脑计算接口(BCI)用于大量安全/隐私关键型应用，从医疗保健到智能通信和控制，应有尽有。可穿戴BCI设置通常包括连接到移动设备的头盔传感器，以及基于ML的数据处理。因此，它们很容易受到所使用的硬件、软件和网络堆栈的多种攻击，这些攻击可能会泄露用户的脑电波数据，最坏的情况是会将BCI辅助设备的控制权拱手让给远程攻击者。本文从操作系统和对抗性机器学习的角度分析了现有可穿戴BCI产品面临的全系统安全和隐私威胁，并介绍了第一个针对可穿戴BCI应用的信息流控制系统Argus。Argus的特定于域的设计在Linux ARM平台上实现了适用于现有BCI用例的轻量级实现。我们对现实世界的BCI设备(Muse、NeuroSky和OpenBCI)进行的概念验证攻击使我们在六个主要攻击载体的堆栈中发现了300多个漏洞。我们的评估表明，Argus在跟踪敏感数据流并以可接受的内存和性能开销(<15%)限制这些攻击方面非常有效。



## **18. Privacy and Robustness in Federated Learning: Attacks and Defenses**

联合学习中的隐私与健壮性：攻击与防御 cs.CR

**SubmitDate**: 2022-01-19    [paper-pdf](http://arxiv.org/pdf/2012.06337v3)

**Authors**: Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, Philip S. Yu

**Abstracts**: As data are increasingly being stored in different silos and societies becoming more aware of data privacy issues, the traditional centralized training of artificial intelligence (AI) models is facing efficiency and privacy challenges. Recently, federated learning (FL) has emerged as an alternative solution and continue to thrive in this new reality. Existing FL protocol design has been shown to be vulnerable to adversaries within or outside of the system, compromising data privacy and system robustness. Besides training powerful global models, it is of paramount importance to design FL systems that have privacy guarantees and are resistant to different types of adversaries. In this paper, we conduct the first comprehensive survey on this topic. Through a concise introduction to the concept of FL, and a unique taxonomy covering: 1) threat models; 2) poisoning attacks and defenses against robustness; 3) inference attacks and defenses against privacy, we provide an accessible review of this important topic. We highlight the intuitions, key techniques as well as fundamental assumptions adopted by various attacks and defenses. Finally, we discuss promising future research directions towards robust and privacy-preserving federated learning.

摘要: 随着数据越来越多地存储在不同的竖井中，社会越来越多地意识到数据隐私问题，传统的人工智能(AI)模型的集中训练正面临效率和隐私方面的挑战。最近，联合学习(FL)已经作为一种替代解决方案出现，并在这一新的现实中继续蓬勃发展。现有的FL协议设计已被证明容易受到系统内外的攻击，从而危及数据隐私和系统健壮性。除了训练强大的全球模型，设计具有隐私保障和抵抗不同类型对手的FL系统是至关重要的。在本文中，我们对这一主题进行了第一次全面的调查。通过对FL概念的简要介绍和一个独特的分类，涵盖：1)威胁模型；2)中毒攻击和对健壮性的防御；3)推理攻击和对隐私的防御，我们提供了对这一重要主题的易于理解的回顾。我们强调各种攻击和防御所采用的直觉、关键技术以及基本假设。最后，我们讨论了鲁棒和隐私保护的联邦学习的未来研究方向。



## **19. Deperturbation of Online Social Networks via Bayesian Label Transition**

基于贝叶斯标签转移的在线社交网络去扰动 cs.LG

TL;DR: GraphLT is the first model that adapts the Bayesian label  transition method on GCNs for deperturbation in online social networks. Our  work is accepted by SDM 2022

**SubmitDate**: 2022-01-18    [paper-pdf](http://arxiv.org/pdf/2010.14121v3)

**Authors**: Jun Zhuang, Mohammad Al Hasan

**Abstracts**: Online social networks (OSNs) classify users into different categories based on their online activities and interests, a task which is referred as a node classification task. Such a task can be solved effectively using Graph Convolutional Networks (GCNs). However, a small number of users, so-called perturbators, may perform random activities on an OSN, which significantly deteriorate the performance of a GCN-based node classification task. Existing works in this direction defend GCNs either by adversarial training or by identifying the attacker nodes followed by their removal. However, both of these approaches require that the attack patterns or attacker nodes be identified first, which is difficult in the scenario when the number of perturbator nodes is very small. In this work, we develop a GCN defense model, namely GraphLT, which uses the concept of label transition. GraphLT assumes that perturbators' random activities deteriorate GCN's performance. To overcome this issue, GraphLT subsequently uses a novel Bayesian label transition model, which takes GCN's predicted labels and applies label transitions by Gibbs-sampling-based inference and thus repairs GCN's prediction to achieve better node classification. Extensive experiments on seven benchmark datasets show that GraphLT considerably enhances the performance of the node classifier in an unperturbed environment; furthermore, it validates that GraphLT can successfully repair a GCN-based node classifier with superior performance than several competing methods.

摘要: 在线社交网络(OSN)根据用户的在线活动和兴趣将用户分类为不同的类别，这一任务称为节点分类任务。利用图卷积网络(GCNS)可以有效地解决这一问题。然而，少数用户，即所谓的微扰器，可能在OSN上执行随机活动，这显著降低了基于GCN的节点分类任务的性能。这方面的现有工作要么是通过对抗性训练，要么是通过识别攻击者节点并随后删除它们来保护GCNS。然而，这两种方法都需要首先识别攻击模式或攻击者节点，这在扰动节点数量很少的情况下是很困难的。在这项工作中，我们开发了一个GCN防御模型，即GraphLT，它使用了标签转换的概念。GraphLT假设微扰器的随机活动会恶化GCN的性能。为了克服这个问题，GraphLT随后使用了一种新的贝叶斯标签转换模型，该模型采用GCN的预测标签，并通过基于Gibbs抽样的推理应用标签转换，从而修正GCN的预测，以实现更好的节点分类。在7个基准数据集上的大量实验表明，GraphLT在不受干扰的环境下显著提高了节点分类器的性能；此外，它还验证了GraphLT可以成功修复基于GCN的节点分类器，其性能优于几种竞争的方法。



## **20. Planning Not to Talk: Multiagent Systems that are Robust to Communication Loss**

计划不说话：对通信丢失具有健壮性的多代理系统 cs.MA

Accepted at AAMAS 2022

**SubmitDate**: 2022-01-17    [paper-pdf](http://arxiv.org/pdf/2201.06619v1)

**Authors**: Mustafa O. Karabag, Cyrus Neary, Ufuk Topcu

**Abstracts**: In a cooperative multiagent system, a collection of agents executes a joint policy in order to achieve some common objective. The successful deployment of such systems hinges on the availability of reliable inter-agent communication. However, many sources of potential disruption to communication exist in practice, such as radio interference, hardware failure, and adversarial attacks. In this work, we develop joint policies for cooperative multiagent systems that are robust to potential losses in communication. More specifically, we develop joint policies for cooperative Markov games with reach-avoid objectives. First, we propose an algorithm for the decentralized execution of joint policies during periods of communication loss. Next, we use the total correlation of the state-action process induced by a joint policy as a measure of the intrinsic dependencies between the agents. We then use this measure to lower-bound the performance of a joint policy when communication is lost. Finally, we present an algorithm that maximizes a proxy to this lower bound in order to synthesize minimum-dependency joint policies that are robust to communication loss. Numerical experiments show that the proposed minimum-dependency policies require minimal coordination between the agents while incurring little to no loss in performance; the total correlation value of the synthesized policy is one fifth of the total correlation value of the baseline policy which does not take potential communication losses into account. As a result, the performance of the minimum-dependency policies remains consistently high regardless of whether or not communication is available. By contrast, the performance of the baseline policy decreases by twenty percent when communication is lost.

摘要: 在协作多Agent系统中，一组Agent执行联合策略以实现某些共同目标。此类系统的成功部署取决于可靠的代理间通信的可用性。然而，在实践中存在许多潜在的通信中断来源，例如无线电干扰、硬件故障和敌意攻击。在这项工作中，我们为协作多Agent系统开发了联合策略，这些策略对通信中的潜在丢失具有鲁棒性。更具体地说，我们给出了具有到达-回避目标的合作马尔可夫对策的联合策略。首先，我们提出了一种在通信丢失期间分散执行联合策略的算法。接下来，我们使用联合策略引起的状态-动作过程的总相关性作为代理之间内在依赖的度量。然后，当通信中断时，我们使用此度量来降低联合策略的性能。最后，我们给出了一个算法，使代理最大化到这个下界，以合成对通信丢失具有鲁棒性的最小依赖联合策略。数值实验表明，所提出的最小依赖策略在性能损失很小甚至没有损失的情况下，对Agent之间的协调要求最低；合成策略的总相关值是基线策略总相关值的五分之一，没有考虑潜在的通信损失。结果，无论通信是否可用，最小依赖策略的性能始终保持较高。相比之下，当通信中断时，基线策略的性能会下降20%。



## **21. SPoTKD: A Protocol for Symmetric Key Distribution over Public Channels Using Self-Powered Timekeeping Devices**

SPoTKD：一种利用自供电计时装置在公共信道上进行对称密钥分发的协议 cs.CR

14 pages, 12 figures

**SubmitDate**: 2022-01-17    [paper-pdf](http://arxiv.org/pdf/2104.04553v3)

**Authors**: Mustafizur Rahman, Liang Zhou, Shantanu Chakrabartty

**Abstracts**: In this paper, we propose a novel class of symmetric key distribution protocols that leverages basic security primitives offered by low-cost, hardware chipsets containing millions of synchronized self-powered timers. The keys are derived from the temporal dynamics of a physical, micro-scale time-keeping device which makes the keys immune to any potential side-channel attacks, malicious tampering, or snooping. Using the behavioral model of the self-powered timers, we first show that the derived key-strings can pass the randomness test as defined by the National Institute of Standards and Technology (NIST) suite. The key-strings are then used in two SPoTKD (Self-Powered Timer Key Distribution) protocols that exploit the timer's dynamics as one-way functions: (a) protocol 1 facilitates secure communications between a user and a remote Server, and (b) protocol 2 facilitates secure communications between two users. In this paper, we investigate the security of these protocols under standard model and against different adversarial attacks. Using Monte-Carlo simulations, we also investigate the robustness of these protocols in the presence of real-world operating conditions and propose error-correcting SPoTKD protocols to mitigate these noise-related artifacts.

摘要: 在本文中，我们提出了一类新的对称密钥分发协议，它利用了包含数百万同步自供电定时器的低成本硬件芯片组提供的基本安全原语。密钥是从物理的微尺度计时设备的时间动态中导出的，这使得密钥不受任何潜在的旁路攻击、恶意篡改或窥探。利用自供电定时器的行为模型，我们首先证明了导出的密钥串可以通过美国国家标准与技术研究所(NIST)套件定义的随机性测试。然后在利用定时器的动态作为单向函数的两个SPoTKD(自供电定时器密钥分发)协议中使用密钥串：(A)协议1促进用户和远程服务器之间的安全通信，以及(B)协议2促进两个用户之间的安全通信。在本文中，我们研究了这些协议在标准模型下的安全性以及对不同对手攻击的安全性。通过Monte-Carlo仿真，我们还研究了这些协议在真实操作条件下的鲁棒性，并提出了纠错的SPoTKD协议来缓解这些噪声相关的伪像。



## **22. AugLy: Data Augmentations for Robustness**

AugLy：增强数据以增强健壮性 cs.AI

**SubmitDate**: 2022-01-17    [paper-pdf](http://arxiv.org/pdf/2201.06494v1)

**Authors**: Zoe Papakipos, Joanna Bitton

**Abstracts**: We introduce AugLy, a data augmentation library with a focus on adversarial robustness. AugLy provides a wide array of augmentations for multiple modalities (audio, image, text, & video). These augmentations were inspired by those that real users perform on social media platforms, some of which were not already supported by existing data augmentation libraries. AugLy can be used for any purpose where data augmentations are useful, but it is particularly well-suited for evaluating robustness and systematically generating adversarial attacks. In this paper we present how AugLy works, benchmark it compared against existing libraries, and use it to evaluate the robustness of various state-of-the-art models to showcase AugLy's utility. The AugLy repository can be found at https://github.com/facebookresearch/AugLy.

摘要: 我们介绍了AugLy，一个关注对手健壮性的数据增强库。AugLy为多种模态(音频、图像、文本和视频)提供了广泛的增强功能。这些增强的灵感来自真实用户在社交媒体平台上的表现，其中一些还没有得到现有数据增强库的支持。AugLy可以用于数据扩充有用的任何目的，但它特别适合于评估健壮性和系统地生成敌意攻击。在本文中，我们介绍了AugLy的工作原理，将其与现有库进行了比较，并使用它来评估各种最先进模型的健壮性，以展示AugLy的实用性。AugLy存储库可在https://github.com/facebookresearch/AugLy.上找到



## **23. Sensitivity of Standard Library Cells to Optical Fault Injection Attacks in IHP 250 nm Technology**

IHP 250 nm技术中标准库细胞对光学故障注入攻击的敏感性 cs.CR

**SubmitDate**: 2022-01-17    [paper-pdf](http://arxiv.org/pdf/2103.12433v2)

**Authors**: Dmytro Petryk, Zoya Dyka, Peter Langendoerfer

**Abstracts**: The IoT consists of a lot of devices such as embedded systems, wireless sensor nodes (WSNs), control systems, etc. It is essential for some of these devices to protect information that they process and transmit. The issue is that an adversary may steal these devices to gain a physical access to the device. There is a variety of ways that allows to reveal cryptographic keys. One of them are optical Fault Injection attacks. We performed successful optical Fault Injections into different type of gates, in particular INV, NAND, NOR, FF. In our work we concentrate on the selection of the parameters configured by an attacker and their influence on the success of the Fault Injections.

摘要: 物联网由许多设备组成，如嵌入式系统、无线传感器节点(WSN)、控制系统等，其中一些设备对其处理和传输的信息进行保护是必不可少的。问题在于，攻击者可能会窃取这些设备以获得对该设备的物理访问权限。有多种方式允许泄露加密密钥。其中之一是光学故障注入攻击。我们成功地将光学故障注入不同类型的门，特别是INV、NAND、NOR、FF。在我们的工作中，我们将重点放在攻击者配置的参数的选择以及它们对故障注入成功的影响上。



## **24. Masked Faces with Faced Masks**

使用面罩的蒙面面孔 cs.CV

8 pages

**SubmitDate**: 2022-01-17    [paper-pdf](http://arxiv.org/pdf/2201.06427v1)

**Authors**: Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu

**Abstracts**: Modern face recognition systems (FRS) still fall short when the subjects are wearing facial masks, a common theme in the age of respiratory pandemics. An intuitive partial remedy is to add a mask detector to flag any masked faces so that the FRS can act accordingly for those low-confidence masked faces. In this work, we set out to investigate the potential vulnerability of such FRS, equipped with a mask detector, on large-scale masked faces. As existing face recognizers and mask detectors have high performance in their respective tasks, it is a challenge to simultaneously fool them and preserve the transferability of the attack. To this end, we devise realistic facial masks that exhibit partial face patterns (i.e., faced masks) and stealthily add adversarial textures that can not only lead to significant performance deterioration of the SOTA deep learning-based FRS, but also remain undetected by the SOTA facial mask detector, thus successfully fooling both systems at the same time. The proposed method unveils the vulnerability of the FRS when dealing with masked faces wearing faced masks.

摘要: 当受试者戴着口罩时，现代人脸识别系统(FRS)仍然不足，这在呼吸道大流行的时代是一个常见的主题。一种直观的部分补救方法是添加一个掩模检测器来标记任何被掩蔽的人脸，以便FRS可以对这些低置信度的被掩蔽的人脸采取相应的行动。在这项工作中，我们开始调查这种配备了面罩检测器的FRS在大规模蒙面面孔上的潜在脆弱性。由于现有的人脸识别器和面罩检测器在各自的任务中都有很高的性能，如何在欺骗它们的同时保持攻击的可转移性是一个挑战。为此，我们设计了具有真实感的人脸面具，展示了部分人脸图案(即人脸面具)，并秘密添加了对抗性纹理，这些纹理不仅会导致基于SOTA深度学习的FRS的性能显著下降，而且仍然不会被SOTA人脸面具检测器检测到，从而成功地同时欺骗了这两个系统。提出的方法揭示了FRS在处理戴面罩的蒙面人脸时的脆弱性。



## **25. Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems**

愚弄自动驾驶车辆的眼睛：针对交通标志识别系统的强有力的身体对抗例子 cs.CV

17 pages, 15 figures

**SubmitDate**: 2022-01-17    [paper-pdf](http://arxiv.org/pdf/2201.06192v1)

**Authors**: Wei Jia, Zhaojun Lu, Haichun Zhang, Zhenglin Liu, Jie Wang, Gang Qu

**Abstracts**: Adversarial Examples (AEs) can deceive Deep Neural Networks (DNNs) and have received a lot of attention recently. However, majority of the research on AEs is in the digital domain and the adversarial patches are static, which is very different from many real-world DNN applications such as Traffic Sign Recognition (TSR) systems in autonomous vehicles. In TSR systems, object detectors use DNNs to process streaming video in real time. From the view of object detectors, the traffic sign`s position and quality of the video are continuously changing, rendering the digital AEs ineffective in the physical world.   In this paper, we propose a systematic pipeline to generate robust physical AEs against real-world object detectors. Robustness is achieved in three ways. First, we simulate the in-vehicle cameras by extending the distribution of image transformations with the blur transformation and the resolution transformation. Second, we design the single and multiple bounding boxes filters to improve the efficiency of the perturbation training. Third, we consider four representative attack vectors, namely Hiding Attack, Appearance Attack, Non-Target Attack and Target Attack.   We perform a comprehensive set of experiments under a variety of environmental conditions, and considering illuminations in sunny and cloudy weather as well as at night. The experimental results show that the physical AEs generated from our pipeline are effective and robust when attacking the YOLO v5 based TSR system. The attacks have good transferability and can deceive other state-of-the-art object detectors. We launched HA and NTA on a brand-new 2021 model vehicle. Both attacks are successful in fooling the TSR system, which could be a life-threatening case for autonomous vehicles. Finally, we discuss three defense mechanisms based on image preprocessing, AEs detection, and model enhancing.

摘要: 对抗性例子(AEs)可以欺骗深度神经网络(DNNs)，近年来受到了广泛的关注。然而，目前对自动驾驶系统的研究大多是在数字领域，敌方补丁是静电，这与自动驾驶车辆中的交通标志识别系统等实际应用有很大的不同。在TSR系统中，对象检测器使用DNN来实时处理流视频。从物体检测器的角度来看，交通标志的位置和视频质量不断变化，使得数字AE在物理世界中无效。在这篇文章中，我们提出了一种系统的流水线来针对真实世界的物体检测器产生健壮的物理声学效果。健壮性通过三种方式实现。首先，通过模糊变换和分辨率变换扩展图像变换的分布，模拟车载摄像机。其次，设计了单包围盒和多包围盒滤波器，提高了扰动训练的效率。第三，我们考虑了四种具有代表性的攻击向量，即隐藏攻击、外观攻击、非目标攻击和目标攻击。我们在各种环境条件下进行了一系列全面的实验，并考虑了晴天和多云天气以及夜间的照明。实验结果表明，在攻击基于YOLOv5的TSR系统时，我们的流水线产生的物理AEs是有效的和健壮的。这些攻击具有很好的可移植性，可以欺骗其他最先进的对象检测器。我们在一款全新的2021年车型上推出了HA和NTA。这两次攻击都成功地愚弄了TSR系统，这可能是一个危及自动驾驶车辆生命的案例。最后，讨论了基于图像预处理、AEs检测和模型增强的三种防御机制。



## **26. Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization**

从Mini到Minimax优化的加速零阶和一阶动量方法 math.OC

Published in Journal of Machine Learning Research (JMLR)

**SubmitDate**: 2022-01-17    [paper-pdf](http://arxiv.org/pdf/2008.08170v7)

**Authors**: Feihu Huang, Shangqian Gao, Jian Pei, Heng Huang

**Abstracts**: In the paper, we propose a class of accelerated zeroth-order and first-order momentum methods for both nonconvex mini-optimization and minimax-optimization. Specifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM) method for black-box mini-optimization where only function values can be obtained. Moreover, we prove that our Acc-ZOM method achieves a lower query complexity of $\tilde{O}(d^{3/4}\epsilon^{-3})$ for finding an $\epsilon$-stationary point, which improves the best known result by a factor of $O(d^{1/4})$ where $d$ denotes the variable dimension. In particular, our Acc-ZOM does not need large batches required in the existing zeroth-order stochastic algorithms. Meanwhile, we propose an accelerated zeroth-order momentum descent ascent (Acc-ZOMDA) method for black-box minimax optimization, where only function values can be obtained. Our Acc-ZOMDA obtains a low query complexity of $\tilde{O}((d_1+d_2)^{3/4}\kappa_y^{4.5}\epsilon^{-3})$ without requiring large batches for finding an $\epsilon$-stationary point, where $d_1$ and $d_2$ denote variable dimensions and $\kappa_y$ is condition number. Moreover, we propose an accelerated first-order momentum descent ascent (Acc-MDA) method for minimax optimization, whose explicit gradients are accessible. Our Acc-MDA achieves a low gradient complexity of $\tilde{O}(\kappa_y^{4.5}\epsilon^{-3})$ without requiring large batches for finding an $\epsilon$-stationary point. In particular, our Acc-MDA can obtain a lower gradient complexity of $\tilde{O}(\kappa_y^{2.5}\epsilon^{-3})$ with a batch size $O(\kappa_y^4)$, which improves the best known result by a factor of $O(\kappa_y^{1/2})$. Extensive experimental results on black-box adversarial attack to deep neural networks and poisoning attack to logistic regression demonstrate efficiency of our algorithms.

摘要: 本文提出了一类求解非凸极小优化和极小极大优化的加速零阶和一阶动量方法。具体地说，我们提出了一种新的加速零阶动量(ACC-ZOM)方法来求解只能得到函数值的黑盒极小优化问题。此外，我们还证明了我们的ACC-zom方法在寻找$\epsilon$-驻点时的查询复杂度为$tilde{O}(d^{3/4}\epsilon^{-3})$，这将最著名的结果改进了$O(d^{1/4})$，其中$d$表示变量维数。特别地，我们的ACC-ZOM不需要现有零阶随机算法所要求的大批量。同时，我们提出了一种求解黑盒极大极小优化问题的加速零阶动量下降上升(ACC-ZOMDA)方法，该方法只能得到函数值。该算法在不需要大批量查询的情况下，获得了较低的$\tilde{O}((d_1+d_2)^{3/4}\kappa_y^{4.5}\epsilon^{-3})$查询复杂度，其中$d_1$和$d_2$表示变量维，$\kappa_y$表示条件数。此外，我们还提出了一种求解极小极大优化问题的加速一阶动量下降上升(ACC-MDA)方法，其显式梯度是可取的。我们的ACC-MDA算法的梯度复杂度为$\tide{O}(\kappa_y^{4.5}\epsilon^{-3})$，而不需要大批量寻找$\epsilon$-固定点。特别地，在批量为$O(\kappa_y^4)$的情况下，我们的ACC-MDA可以获得$tilde{O}(\kappa_y^{2.5}\epsilon^{-3})$的较低梯度复杂度，从而将最著名的结果改进了$O(\kappa_y^{1/2})$。对深层神经网络的黑盒攻击和对Logistic回归的中毒攻击的大量实验结果表明了该算法的有效性。



## **27. Improving Privacy and Security in Unmanned Aerial Vehicles Network using Blockchain**

利用区块链提高无人机网络的保密性和安全性 cs.CR

18 Pages; 14 Figures; 2 Tables

**SubmitDate**: 2022-01-16    [paper-pdf](http://arxiv.org/pdf/2201.06100v1)

**Authors**: Hardik Sachdeva, Shivam Gupta, Anushka Misra, Khushbu Chauhan, Mayank Dave

**Abstracts**: Unmanned Aerial Vehicles (UAVs), also known as drones, have exploded in every segment present in todays business industry. They have scope in reinventing old businesses, and they are even developing new opportunities for various brands and franchisors. UAVs are used in the supply chain, maintaining surveillance and serving as mobile hotspots. Although UAVs have potential applications, they bring several societal concerns and challenges that need addressing in public safety, privacy, and cyber security. UAVs are prone to various cyber-attacks and vulnerabilities; they can also be hacked and misused by malicious entities resulting in cyber-crime. The adversaries can exploit these vulnerabilities, leading to data loss, property, and destruction of life. One can partially detect the attacks like false information dissemination, jamming, gray hole, blackhole, and GPS spoofing by monitoring the UAV behavior, but it may not resolve privacy issues. This paper presents secure communication between UAVs using blockchain technology. Our approach involves building smart contracts and making a secure and reliable UAV adhoc network. This network will be resilient to various network attacks and is secure against malicious intrusions.

摘要: 无人驾驶飞行器(UAV)，也被称为无人机，已经在当今商业行业的每个领域都出现了爆炸式增长。他们有改造旧业务的余地，甚至正在为各种品牌和加盟商开发新的机会。无人机在供应链中使用，保持监视，并作为移动热点。虽然无人机有潜在的应用，但它们带来了一些社会问题和挑战，需要在公共安全、隐私和网络安全方面加以解决。无人机容易受到各种网络攻击和漏洞；它们也可能被恶意实体黑客和滥用，从而导致网络犯罪。攻击者可以利用这些漏洞，导致数据丢失、财产损失和生命损失。通过对无人机行为的监控，可以部分检测到虚假信息传播、干扰、灰洞、黑洞、GPS欺骗等攻击，但不一定能解决隐私问题。本文介绍了利用区块链技术实现无人机之间的安全通信。我们的方法包括建立智能合同和建立安全可靠的无人机临时网络。该网络可抵御各种网络攻击，并能安全地抵御恶意入侵。



## **28. Untargeted Poisoning Attack Detection in Federated Learning via Behavior Attestation**

联合学习中基于行为证明的非目标中毒攻击检测 cs.CR

**SubmitDate**: 2022-01-16    [paper-pdf](http://arxiv.org/pdf/2101.10904v3)

**Authors**: Ranwa Al Mallah, David Lopez, Godwin Badu Marfo, Bilal Farooq

**Abstracts**: Federated Learning (FL) is a paradigm in Machine Learning (ML) that addresses data privacy, security, access rights and access to heterogeneous information issues by training a global model using distributed nodes. Despite its advantages, there is an increased potential for cyberattacks on FL-based ML techniques that can undermine the benefits. Model-poisoning attacks on FL target the availability of the model. The adversarial objective is to disrupt the training. We propose attestedFL, a defense mechanism that monitors the training of individual nodes through state persistence in order to detect a malicious worker. A fine-grained assessment of the history of the worker permits the evaluation of its behavior in time and results in innovative detection strategies. We present three lines of defense that aim at assessing if the worker is reliable by observing if the node is really training, advancing towards a goal. Our defense exposes an attacker's malicious behavior and removes unreliable nodes from the aggregation process so that the FL process converge faster. Through extensive evaluations and against various adversarial settings, attestedFL increased the accuracy of the model between 12% to 58% under different scenarios such as attacks performed at different stages of convergence, attackers colluding and continuous attacks.

摘要: 联合学习(FL)是机器学习(ML)中的一种范例，它通过使用分布式节点训练全局模型来解决数据隐私、安全、访问权限和对异构信息的访问问题。尽管有其优势，但针对基于FL的ML技术的网络攻击的可能性增加，这可能会破坏这些好处。针对FL目标的模型中毒攻击验证了模型的可用性。敌对的目标是破坏训练。我们提出了attedFL，这是一种通过状态持久化来监控单个节点的训练以检测恶意工作者的防御机制。对工人历史的细粒度评估允许及时评估其行为，并在创新的检测策略中得出结果。我们提出了三条防线，旨在通过观察节点是否真的在训练，朝着目标前进，来评估工人是否可靠。我们的防御暴露了攻击者的恶意行为，并从聚合过程中删除了不可靠的节点，以便FL过程更快地收敛。通过广泛的评估和针对各种对抗性环境，attedFL将模型在不同场景下的准确率提高了12%到58%，例如在收敛的不同阶段执行的攻击、攻击者合谋和持续攻击。



## **29. Adversarial Machine Learning Threat Analysis in Open Radio Access Networks**

开放无线接入网络中的对抗性机器学习威胁分析 cs.CR

**SubmitDate**: 2022-01-16    [paper-pdf](http://arxiv.org/pdf/2201.06093v1)

**Authors**: Ron Bitton, Dan Avraham, Eitan Klevansky, Dudu Mimran, Oleg Brodt, Heiko Lehmann, Yuval Elovici, Asaf Shabtai

**Abstracts**: The Open Radio Access Network (O-RAN) is a new, open, adaptive, and intelligent RAN architecture. Motivated by the success of artificial intelligence in other domains, O-RAN strives to leverage machine learning (ML) to automatically and efficiently manage network resources in diverse use cases such as traffic steering, quality of experience prediction, and anomaly detection. Unfortunately, ML-based systems are not free of vulnerabilities; specifically, they suffer from a special type of logical vulnerabilities that stem from the inherent limitations of the learning algorithms. To exploit these vulnerabilities, an adversary can utilize an attack technique referred to as adversarial machine learning (AML). These special type of attacks has already been demonstrated in recent researches. In this paper, we present a systematic AML threat analysis for the O-RAN. We start by reviewing relevant ML use cases and analyzing the different ML workflow deployment scenarios in O-RAN. Then, we define the threat model, identifying potential adversaries, enumerating their adversarial capabilities, and analyzing their main goals. Finally, we explore the various AML threats in the O-RAN and review a large number of attacks that can be performed to materialize these threats and demonstrate an AML attack on a traffic steering model.

摘要: 开放式无线接入网(O-RAN)是一种新型的、开放的、自适应的、智能化的无线接入网体系结构。在人工智能在其他领域取得成功的推动下，O-RAN努力利用机器学习(ML)来自动高效地管理不同使用案例中的网络资源，如流量控制、体验质量预测和异常检测。不幸的是，基于ML的系统并不是没有漏洞；具体地说，它们受到一种特殊类型的逻辑漏洞的影响，这种漏洞源于学习算法的固有限制。为了利用这些漏洞，对手可以利用一种称为对抗性机器学习(AML)的攻击技术。这些特殊类型的攻击已经在最近的研究中得到了证明。本文对O-RAN进行了系统的AML威胁分析。我们首先回顾相关的ML用例，并分析O-RAN中不同的ML工作流部署场景。然后，我们定义了威胁模型，识别潜在的对手，列举他们的对抗能力，并分析他们的主要目标。最后，我们探讨了O-RAN中的各种AML威胁，并回顾了可以执行以实现这些威胁的大量攻击，并演示了针对流量导向模型的AML攻击。



## **30. ALA: Adversarial Lightness Attack via Naturalness-aware Regularizations**

ALA：通过自然性感知规则的对抗性光攻击 cs.CV

8 pages

**SubmitDate**: 2022-01-16    [paper-pdf](http://arxiv.org/pdf/2201.06070v1)

**Authors**: Liangru Sun, Felix Juefei-Xu, Yihao Huang, Qing Guo, Jiayi Zhu, Jincao Feng, Yang Liu, Geguang Pu

**Abstracts**: Most researchers have tried to enhance the robustness of deep neural networks (DNNs) by revealing and repairing the vulnerability of DNNs with specialized adversarial examples. Parts of the attack examples have imperceptible perturbations restricted by Lp norm. However, due to their high-frequency property, the adversarial examples usually have poor transferability and can be defensed by denoising methods. To avoid the defects, some works make the perturbations unrestricted to gain better robustness and transferability. However, these examples usually look unnatural and alert the guards. To generate unrestricted adversarial examples with high image quality and good transferability, in this paper, we propose Adversarial Lightness Attack (ALA), a white-box unrestricted adversarial attack that focuses on modifying the lightness of the images. The shape and color of the samples, which are crucial to human perception, are barely influenced. To obtain adversarial examples with high image quality, we craft a naturalness-aware regularization. To achieve stronger transferability, we propose random initialization and non-stop attack strategy in the attack procedure. We verify the effectiveness of ALA on two popular datasets for different tasks (i.e., ImageNet for image classification and Places-365 for scene recognition). The experiments show that the generated adversarial examples have both strong transferability and high image quality. Besides, the adversarial examples can also help to improve the standard trained ResNet50 on defending lightness corruption.

摘要: 大多数研究人员试图通过专门的对抗性例子来揭示和修复深度神经网络(DNNs)的脆弱性，以增强DNNs的鲁棒性。部分攻击实例具有Lp范数约束下的不可察觉扰动。然而，由于对抗性样本的高频特性，通常可移植性较差，可以通过去噪方法进行防御。为了避免这些缺陷，一些工作使扰动不受限制，以获得更好的鲁棒性和可移植性。然而，这些例子通常看起来不自然，会引起警卫的警觉。为了生成图像质量高、可移植性好的无限制对抗性示例，本文提出了对抗性亮度攻击(ALA)，这是一种白盒无限制对抗性攻击，其重点是改变图像的亮度。样本的形状和颜色对人类的感知至关重要，几乎没有受到影响。为了获得高质量的对抗性样本，我们设计了一种自然度感知的正则化方法。为了实现更强的可移植性，我们在攻击过程中提出了随机初始化和不间断攻击策略。我们在两个用于不同任务的流行数据集(即用于图像分类的ImageNet和用于场景识别的Places-365)上验证了ALA的有效性。实验表明，生成的对抗性实例具有较强的可移植性和较高的图像质量。此外，对抗性的例子也有助于提高标准训练的ResNet50在防御轻量级腐败方面的能力。



## **31. Submodularity-based False Data Injection Attack Scheme in Multi-agent Dynamical Systems**

多智能体动态系统中基于子模块性的虚假数据注入攻击方案 math.DS

**SubmitDate**: 2022-01-16    [paper-pdf](http://arxiv.org/pdf/2201.06017v1)

**Authors**: Xiaoyu Luo, Chengcheng Zhao, Chongrong Fang, Jianping He

**Abstracts**: Consensus in multi-agent dynamical systems is prone to be sabotaged by the adversary, which has attracted much attention due to its key role in broad applications. In this paper, we study a new false data injection (FDI) attack design problem, where the adversary with limited capability aims to select a subset of agents and manipulate their local multi-dimensional states to maximize the consensus convergence error. We first formulate the FDI attack design problem as a combinatorial optimization problem and prove it is NP-hard. Then, based on the submodularity optimization theory, we show the convergence error is a submodular function of the set of the compromised agents, which satisfies the property of diminishing marginal returns. In other words, the benefit of adding an extra agent to the compromised set decreases as that set becomes larger. With this property, we exploit the greedy scheme to find the optimal compromised agent set that can produce the maximum convergence error when adding one extra agent to that set each time. Thus, the FDI attack set selection algorithms are developed to obtain the near-optimal subset of the compromised agents. Furthermore, we derive the analytical suboptimality bounds and the worst-case running time under the proposed algorithms. Extensive simulation results are conducted to show the effectiveness of the proposed algorithm.

摘要: 多智能体动态系统中的共识容易受到对手的破坏，因其在广泛应用中的关键作用而备受关注。本文研究了一类新的虚假数据注入(FDI)攻击设计问题，其中能力有限的敌手的目标是选择一个Agent子集并操纵其局部多维状态以最大化共识收敛误差。我们首先将FDI攻击设计问题描述为一个组合优化问题，并证明了它是NP难的。然后，基于子模优化理论，证明了收敛误差是折衷智能体集合的子模函数，满足边际收益递减的性质。换句话说，向受危害的集合添加额外代理的好处随着该集合变得更大而降低。利用这一性质，我们利用贪婪方案来寻找最优的折衷智能体集合，该集合在每次额外增加一个智能体的情况下可以产生最大的收敛误差。因此，开发了FDI攻击集选择算法，以获得受攻击代理的近优子集。此外，我们还给出了所提出算法的分析次优界和最坏情况下的运行时间。大量的仿真结果表明了该算法的有效性。



## **32. Quickest Bayesian and non-Bayesian detection of false data injection attack in remote state estimation**

远程状态估计中虚假数据注入攻击的最快贝叶斯和非贝叶斯检测 eess.SY

**SubmitDate**: 2022-01-16    [paper-pdf](http://arxiv.org/pdf/2010.15785v3)

**Authors**: Akanshu Gupta, Abhinava Sikdar, Arpan Chattopadhyay

**Abstracts**: In this paper, quickest detection of false data injection attack on remote state estimation is considered. A set of $N$ sensors make noisy linear observations of a discrete-time linear process with Gaussian noise, and report the observations to a remote estimator. The challenge is the presence of a few potentially malicious sensors which can start strategically manipulating their observations at a random time in order to skew the estimates. The quickest attack detection problem for a known {\em linear} attack scheme in the Bayesian setting with a Geometric prior on the attack initiation instant is posed as a constrained Markov decision process (MDP), in order to minimize the expected detection delay subject to a false alarm constraint, with the state involving the probability belief at the estimator that the system is under attack. State transition probabilities are derived in terms of system parameters, and the structure of the optimal policy is derived analytically. It turns out that the optimal policy amounts to checking whether the probability belief exceeds a threshold. Next, generalized CUSUM based attack detection algorithm is proposed for the non-Bayesian setting where the attacker chooses the attack initiation instant in a particularly adversarial manner. It turns out that computing the statistic for the generalised CUSUM test in this setting relies on the same techniques developed to compute the state transition probabilities of the MDP. Numerical results demonstrate significant performance gain under the proposed algorithms against competing algorithms.

摘要: 本文研究了远程状态估计中虚假数据注入攻击的最快检测问题。一组$N$传感器对带有高斯噪声的离散时间线性过程进行噪声线性观测，并将观测结果报告给远程估计器。挑战在于少数潜在的恶意传感器的存在，这些传感器可以在随机时间开始战略性地操纵它们的观测，以便歪曲估计。将已知{em线性}攻击方案在攻击起始时刻具有几何先验的贝叶斯环境下的最快攻击检测问题假设为一个约束马尔可夫决策过程(MDP)，以最小化虚警约束下的期望检测延迟，且状态包含估计器对系统受到攻击的概率信念.根据系统参数推导了状态转移概率，并对最优策略的结构进行了解析推导。结果表明，最优策略相当于检查概率信念是否超过阈值。其次，针对攻击者以一种特别对抗性的方式选择攻击发起时刻的非贝叶斯环境，提出了基于累积和的广义攻击检测算法。事实证明，在此设置中计算广义CUSUM测试的统计数据依赖于为计算MDP的状态转移概率而开发的相同技术。数值结果表明，与竞争算法相比，提出的算法具有显著的性能提升。



## **33. Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks**

寻找最优切点以减少硬标签攻击的失真 cs.CV

Accepted at NeurIPS 2021. In the previous versions (v1 and v2), the  experimental results of Table 10 are incorrect and have been corrected. We  have also corrected some typos and a co-author's institution in this version

**SubmitDate**: 2022-01-16    [paper-pdf](http://arxiv.org/pdf/2111.07492v4)

**Authors**: Chen Ma, Xiangyu Guo, Li Chen, Jun-Hai Yong, Yisen Wang

**Abstracts**: One major problem in black-box adversarial attacks is the high query complexity in the hard-label attack setting, where only the top-1 predicted label is available. In this paper, we propose a novel geometric-based approach called Tangent Attack (TA), which identifies an optimal tangent point of a virtual hemisphere located on the decision boundary to reduce the distortion of the attack. Assuming the decision boundary is locally flat, we theoretically prove that the minimum $\ell_2$ distortion can be obtained by reaching the decision boundary along the tangent line passing through such tangent point in each iteration. To improve the robustness of our method, we further propose a generalized method which replaces the hemisphere with a semi-ellipsoid to adapt to curved decision boundaries. Our approach is free of pre-training. Extensive experiments conducted on the ImageNet and CIFAR-10 datasets demonstrate that our approach can consume only a small number of queries to achieve the low-magnitude distortion. The implementation source code is released online at https://github.com/machanic/TangentAttack.

摘要: 黑盒对抗性攻击的一个主要问题是硬标签攻击设置中的高查询复杂度，在硬标签攻击设置中，只有前1个预测标签可用。本文提出了一种新的基于几何的切线攻击方法(TA)，该方法识别位于决策边界上的虚拟半球的最佳切点，以减少攻击的失真。假设决策边界是局部平坦的，我们从理论上证明了在每一次迭代中，沿着通过该切点的切线到达决策边界可以获得最小的$\\ell2$失真。为了提高方法的鲁棒性，我们进一步提出了一种广义方法，用半椭球代替半球，以适应弯曲的决策边界。我们的方法是免费的前期培训。在ImageNet和CIFAR-10数据集上进行的大量实验表明，我们的方法可以只消耗少量的查询来实现低幅度的失真。实现源代码在https://github.com/machanic/TangentAttack.上在线发布



## **34. Explaining and Measuring Functionalities of Malware Detectors**

解释和测量恶意软件检测器的功能 cs.CR

**SubmitDate**: 2022-01-16    [paper-pdf](http://arxiv.org/pdf/2111.10085v2)

**Authors**: Wei Wang, Ruoxi Sun, Tian Dong, Shaofeng Li, Minhui Xue, Gareth Tyson, Haojin Zhu

**Abstracts**: Numerous open-source and commercial malware detectors are available. However, their efficacy is threatened by new adversarial attacks, whereby malware attempts to evade detection, e.g., by performing feature-space manipulation. In this work, we propose an explainability-guided and model-agnostic framework for measuring the ability of malware to evade detection. The framework introduces the concept of Accrued Malicious Magnitude (AMM) to identify which malware features should be manipulated to maximize the likelihood of evading detection. We then use this framework to test several state-of-the-art malware detectors ability to detect manipulated malware. We find that (i) commercial antivirus engines are vulnerable to AMM-guided manipulated samples; (ii) the ability of a manipulated malware generated using one detector to evade detection by another detector (i.e., transferability) depends on the overlap of features with large AMM values between the different detectors; and (iii) AMM values effectively measure the importance of features and explain the ability to evade detection. Our findings shed light on the weaknesses of current malware detectors, as well as how they can be improved.

摘要: 有许多开源和商业恶意软件检测器可用。然而，它们的有效性受到新的敌意攻击的威胁，由此恶意软件试图例如通过执行特征空间操纵来逃避检测。在这项工作中，我们提出了一个可解释性指导和模型不可知的框架来衡量恶意软件逃避检测的能力。该框架引入了累计恶意量级(AMM)的概念，以确定应操纵哪些恶意软件功能以最大限度地提高逃避检测的可能性。然后，我们使用这个框架来测试几个最先进的恶意软件检测器检测被操纵的恶意软件的能力。我们发现(I)商业反病毒引擎容易受到AMM引导的操纵样本的攻击；(Ii)使用一个检测器生成的操纵恶意软件逃避另一个检测器检测的能力(即可移植性)取决于不同检测器之间AMM值较大的特征的重叠；以及(Iii)AMM值有效地度量了特征的重要性并解释了逃避检测的能力。我们的发现揭示了当前恶意软件检测器的弱点，以及如何改进它们。



## **35. Jamming Attacks on Federated Learning in Wireless Networks**

无线网络中对联合学习的干扰攻击 cs.LG

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/2201.05172v1)

**Authors**: Yi Shi, Yalin E. Sagduyu

**Abstracts**: Federated learning (FL) offers a decentralized learning environment so that a group of clients can collaborate to train a global model at the server, while keeping their training data confidential. This paper studies how to launch over-the-air jamming attacks to disrupt the FL process when it is executed over a wireless network. As a wireless example, FL is applied to learn how to classify wireless signals collected by clients (spectrum sensors) at different locations (such as in cooperative sensing). An adversary can jam the transmissions for the local model updates from clients to the server (uplink attack), or the transmissions for the global model updates the server to clients (downlink attack), or both. Given a budget imposed on the number of clients that can be attacked per FL round, clients for the (uplink/downlink) attack are selected according to their local model accuracies that would be expected without an attack or ranked via spectrum observations. This novel attack is extended to general settings by accounting different processing speeds and attack success probabilities for clients. Compared to benchmark attack schemes, this attack approach degrades the FL performance significantly, thereby revealing new vulnerabilities of FL to jamming attacks in wireless networks.

摘要: 联合学习(FL)提供了一个分散的学习环境，以便一组客户可以协作在服务器上训练全局模型，同时保护他们的训练数据的机密性。本文研究了在无线网络上执行FL过程时，如何发起空中干扰攻击来中断FL过程。作为无线示例，FL被应用于学习如何对由不同位置的客户端(频谱传感器)收集的无线信号进行分类(例如在协作侦听中)。敌手可以将本地模型更新的传输从客户端阻塞到服务器(上行链路攻击)，或者将全局模型的传输更新到客户端(下行链路攻击)，或者两者兼而有之。给定对每FL轮可被攻击的客户端数量强加的预算，根据其本地模型精度来选择(上行链路/下行链路)攻击的客户端，所述本地模型精度是在没有攻击的情况下预期的，或者通过频谱观察来排序。通过计算客户端的不同处理速度和攻击成功概率，将这种新型攻击扩展到一般设置。与基准攻击方案相比，该攻击方案显著降低了FL的性能，从而揭示了FL在无线网络中对干扰攻击的新脆弱性。



## **36. Unlabeled Data Improves Adversarial Robustness**

未标记的数据提高了对手的健壮性 stat.ML

Corrected some math typos in the proof of Lemma 1

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/1905.13736v4)

**Authors**: Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, John C. Duchi

**Abstracts**: We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) $\ell_\infty$ robustness against several strong attacks via adversarial training and (ii) certified $\ell_2$ and $\ell_\infty$ robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.

摘要: 我们从理论上和经验上证明，半监督学习可以显著提高对手的稳健性。理论上，我们重温了Schmidt等人的简单高斯模型。这显示了标准分类和健壮分类之间的样本复杂性差距。我们证明了无标签数据弥合了这一差距：一个简单的半监督学习过程(自我训练)使用相同数量的标签来实现高标准精度所需的高鲁棒精度。经验上，我们用来自8000万幅微小图像的500K未标记图像来增强CIFAR-10，并使用稳健的自我训练在(I)通过对抗性训练对几种强攻击的鲁棒性和(Ii)通过随机平滑认证的$\ell_2和$\ell_\infty$鲁棒性方面超过最先进的鲁棒准确率5个点。在SVHN上，添加删除了标签的数据集自己的额外训练集可以提供4到10个点的增益，与使用额外标签的增益相差1个点。



## **37. Attention-Guided Black-box Adversarial Attacks with Large-Scale Multiobjective Evolutionary Optimization**

基于大规模多目标进化优化的注意力引导黑盒对抗攻击 cs.CV

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/2101.07512v3)

**Authors**: Jie Wang, Zhaoxia Yin, Jing Jiang, Yang Du

**Abstracts**: Fooling deep neural networks (DNNs) with the black-box optimization has become a popular adversarial attack fashion, as the structural prior knowledge of DNNs is always unknown. Nevertheless, recent black-box adversarial attacks may struggle to balance their attack ability and visual quality of the generated adversarial examples (AEs) in tackling high-resolution images. In this paper, we propose an attention-guided black-box adversarial attack based on the large-scale multiobjective evolutionary optimization, termed as LMOA. By considering the spatial semantic information of images, we firstly take advantage of the attention map to determine the perturbed pixels. Instead of attacking the entire image, reducing the perturbed pixels with the attention mechanism can help to avoid the notorious curse of dimensionality and thereby improves the performance of attacking. Secondly, a large-scale multiobjective evolutionary algorithm is employed to traverse the reduced pixels in the salient region. Benefiting from its characteristics, the generated AEs have the potential to fool target DNNs while being imperceptible by the human vision. Extensive experimental results have verified the effectiveness of the proposed LMOA on the ImageNet dataset. More importantly, it is more competitive to generate high-resolution AEs with better visual quality compared with the existing black-box adversarial attacks.

摘要: 由于深度神经网络(DNNs)的结构先验知识总是未知的，用黑盒优化来愚弄DNNs已经成为一种流行的对抗性攻击方式。然而，最近的黑盒对抗性攻击可能难以平衡它们的攻击能力和生成的对抗性示例(AE)在处理高分辨率图像时的视觉质量。本文提出了一种基于大规模多目标进化优化的注意力引导的黑盒对抗攻击，称为LMOA。考虑到图像的空间语义信息，首先利用注意力图确定扰动像素。与攻击整幅图像不同的是，利用注意力机制减少扰动像素可以帮助避免臭名昭著的维数诅咒，从而提高攻击性能。其次，采用大规模多目标进化算法遍历显著区域内的缩减像素。得益于它的特性，生成的AE有可能愚弄目标DNN，同时又是人眼看不见的。大量的实验结果验证了所提出的LMOA在ImageNet数据集上的有效性。更重要的是，与现有的黑盒对抗性攻击相比，更多的是好胜能够生成视觉质量更好的高分辨率音效。



## **38. On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles**

自主车辆轨迹预测的对抗鲁棒性研究 cs.CV

11 pages, 11 figures

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/2201.05057v1)

**Authors**: Qingzhao Zhang, Shengtuo Hu, Jiachen Sun, Qi Alfred Chen, Z. Morley Mao

**Abstracts**: Trajectory prediction is a critical component for autonomous vehicles (AVs) to perform safe planning and navigation. However, few studies have analyzed the adversarial robustness of trajectory prediction or investigated whether the worst-case prediction can still lead to safe planning. To bridge this gap, we study the adversarial robustness of trajectory prediction models by proposing a new adversarial attack that perturbs normal vehicle trajectories to maximize the prediction error. Our experiments on three models and three datasets show that the adversarial prediction increases the prediction error by more than 150%. Our case studies show that if an adversary drives a vehicle close to the target AV following the adversarial trajectory, the AV may make an inaccurate prediction and even make unsafe driving decisions. We also explore possible mitigation techniques via data augmentation and trajectory smoothing.

摘要: 轨迹预测是自动驾驶车辆进行安全规划和导航的重要组成部分。然而，很少有研究分析弹道预测的对抗稳健性，或研究最坏情况的预测是否仍能导致安全规划。为了弥补这一差距，我们研究了轨迹预测模型的对抗性，提出了一种新的对抗性攻击，通过扰动正常的车辆轨迹来最大化预测误差。在三个模型和三个数据集上的实验表明，对抗性预测使预测误差增加了150%以上。我们的案例研究表明，如果对手沿着敌对的轨迹驾驶车辆接近目标AV，AV可能会做出不准确的预测，甚至做出不安全的驾驶决策。我们还通过数据增强和轨迹平滑来探索可能的缓解技术。



## **39. Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash**

学习打破深度感知散列：用例NeuralHash cs.LG

24 pages, 16 figures, 5 tables

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/2111.06628v3)

**Authors**: Lukas Struppek, Dominik Hintersdorf, Daniel Neider, Kristian Kersting

**Abstracts**: Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system's reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.

摘要: 苹果最近公布了其深度感知散列系统NeuralHash，用于在文件上传到其iCloud服务之前检测用户设备上的儿童性虐待材料(CSAM)。公众很快就对保护用户隐私和系统的可靠性提出了批评。本文首次提出了基于NeuralHash的深度感知散列的综合实证分析。具体地说，我们表明当前的深度感知散列可能并不健壮。攻击者可以通过在图像中应用微小的改变来操纵散列值，这些改变要么是由基于梯度的方法引起的，要么是简单地通过执行标准图像转换来强制或防止散列冲突。这样的攻击让恶意行为者很容易利用检测系统：从隐藏滥用材料到陷害无辜用户，一切皆有可能。此外，使用散列值，仍然可以对存储在用户设备上的数据进行推断。在我们看来，根据我们的结果，当前形式的深度感知散列通常不能用于健壮的客户端扫描，不应该从隐私的角度使用。



## **40. Evaluation of Four Black-box Adversarial Attacks and Some Query-efficient Improvement Analysis**

四种黑盒对抗性攻击的评测及查询效率改进分析 cs.CR

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/2201.05001v1)

**Authors**: Rui Wang

**Abstracts**: With the fast development of machine learning technologies, deep learning models have been deployed in almost every aspect of everyday life. However, the privacy and security of these models are threatened by adversarial attacks. Among which black-box attack is closer to reality, where limited knowledge can be acquired from the model. In this paper, we provided basic background knowledge about adversarial attack and analyzed four black-box attack algorithms: Bandits, NES, Square Attack and ZOsignSGD comprehensively. We also explored the newly proposed Square Attack method with respect to square size, hoping to improve its query efficiency.

摘要: 随着机器学习技术的快速发展，深度学习模型几乎已经部署到日常生活的方方面面。然而，这些模型的隐私和安全受到对抗性攻击的威胁。其中黑箱攻击更接近实际，可以从模型中获取有限的知识。本文介绍了对抗性攻击的基本背景知识，并对四种黑盒攻击算法：强盗算法、NES算法、Square攻击算法和ZOsignSGD算法进行了全面的分析。我们还对新提出的关于正方形大小的正方形攻击方法进行了探索，希望能提高其查询效率。



## **41. Captcha Attack: Turning Captchas Against Humanity**

验证码攻击：将验证码变成反人类的 cs.CR

Currently under submission

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/2201.04014v2)

**Authors**: Mauro Conti, Luca Pajola, Pier Paolo Tricomi

**Abstracts**: Nowadays, people generate and share massive content on online platforms (e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook users posted around 150 thousand photos every minute. Content moderators constantly monitor these online platforms to prevent the spreading of inappropriate content (e.g., hate speech, nudity images). Based on deep learning (DL) advances, Automatic Content Moderators (ACM) help human moderators handle high data volume. Despite their advantages, attackers can exploit weaknesses of DL components (e.g., preprocessing, model) to affect their performance. Therefore, an attacker can leverage such techniques to spread inappropriate content by evading ACM.   In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that allows users to spread inappropriate text online by evading ACM controls. CAPA, by generating custom textual CAPTCHAs, exploits ACM's careless design implementations and internal procedures vulnerabilities. We test our attack on real-world ACM, and the results confirm the ferocity of our simple yet effective attack, reaching up to a 100% evasion success in most cases. At the same time, we demonstrate the difficulties in designing CAPA mitigations, opening new challenges in CAPTCHAs research area.

摘要: 如今，人们在在线平台(如社交网络、博客)上生成和共享大量内容。2021年，Facebook的19亿日活跃用户每分钟发布约15万张照片。内容版主不断监控这些在线平台，以防止不恰当内容(如仇恨言论、裸照)的传播。基于深度学习(DL)的进步，自动内容版主(ACM)帮助人工版主处理大量数据。尽管具有优势，攻击者仍可以利用DL组件的弱点(例如，预处理、模型)来影响其性能。因此，攻击者可以利用这些技术通过规避ACM来传播不适当的内容。在这项工作中，我们提出了验证码攻击(CAPA)，这是一种敌意技术，允许用户通过逃避ACM控制来在线传播不恰当的文本。通过生成自定义文本验证码，CAPA可以利用ACM粗心的设计实现和内部过程漏洞。我们在真实的ACM上测试了我们的攻击，结果证实了我们简单而有效的攻击的凶猛程度，在大多数情况下达到了100%的规避成功。同时，我们论证了设计CAPA缓解措施的困难，为CAPTCHAS研究领域带来了新的挑战。



## **42. Reconstructing Training Data with Informed Adversaries**

利用知情对手重建训练数据 cs.CR

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/2201.04845v1)

**Authors**: Borja Balle, Giovanni Cherubin, Jamie Hayes

**Abstracts**: Given access to a machine learning model, can an adversary reconstruct the model's training data? This work studies this question from the lens of a powerful informed adversary who knows all the training data points except one. By instantiating concrete attacks, we show it is feasible to reconstruct the remaining data point in this stringent threat model. For convex models (e.g. logistic regression), reconstruction attacks are simple and can be derived in closed-form. For more general models (e.g. neural networks), we propose an attack strategy based on training a reconstructor network that receives as input the weights of the model under attack and produces as output the target data point. We demonstrate the effectiveness of our attack on image classifiers trained on MNIST and CIFAR-10, and systematically investigate which factors of standard machine learning pipelines affect reconstruction success. Finally, we theoretically investigate what amount of differential privacy suffices to mitigate reconstruction attacks by informed adversaries. Our work provides an effective reconstruction attack that model developers can use to assess memorization of individual points in general settings beyond those considered in previous works (e.g. generative language models or access to training gradients); it shows that standard models have the capacity to store enough information to enable high-fidelity reconstruction of training data points; and it demonstrates that differential privacy can successfully mitigate such attacks in a parameter regime where utility degradation is minimal.

摘要: 如果可以访问机器学习模型，对手可以重建模型的训练数据吗？这项工作从一个强大的、消息灵通的对手的角度来研究这个问题，他知道除一个以外的所有训练数据点。通过对具体攻击的实例化，我们证明了在这种严格的威胁模型中重构剩余数据点是可行的。对于凸模型(例如Logistic回归)，重构攻击是简单的，并且可以以封闭形式导出。对于更一般的模型(如神经网络)，我们提出了一种基于训练重构器网络的攻击策略，该重构器网络接收被攻击模型的权重作为输入，并产生目标数据点作为输出。我们在MNIST和CIFAR-10上训练的图像分类器上验证了我们的攻击的有效性，并系统地研究了标准机器学习管道中哪些因素影响重建成功。最后，我们从理论上研究了多少差异隐私足以减轻知情攻击者的重构攻击。我们的工作提供了一个有效的重建攻击，模型开发者可以用它来评估在一般环境下对单个点的记忆，而不是以前的工作中考虑的那些(例如，生成性语言模型或对训练梯度的访问)；它表明标准模型有能力存储足够的信息来实现训练数据点的高保真重建；并且它证明了在效用降级最小的参数机制中，差分隐私可以成功地缓解这样的攻击。



## **43. Towards Adversarially Robust Deep Image Denoising**

面向对抗性鲁棒的深部图像去噪研究 eess.IV

**SubmitDate**: 2022-01-13    [paper-pdf](http://arxiv.org/pdf/2201.04397v2)

**Authors**: Hanshu Yan, Jingfeng Zhang, Jiashi Feng, Masashi Sugiyama, Vincent Y. F. Tan

**Abstracts**: This work systematically investigates the adversarial robustness of deep image denoisers (DIDs), i.e, how well DIDs can recover the ground truth from noisy observations degraded by adversarial perturbations. Firstly, to evaluate DIDs' robustness, we propose a novel adversarial attack, namely Observation-based Zero-mean Attack ({\sc ObsAtk}), to craft adversarial zero-mean perturbations on given noisy images. We find that existing DIDs are vulnerable to the adversarial noise generated by {\sc ObsAtk}. Secondly, to robustify DIDs, we propose an adversarial training strategy, hybrid adversarial training ({\sc HAT}), that jointly trains DIDs with adversarial and non-adversarial noisy data to ensure that the reconstruction quality is high and the denoisers around non-adversarial data are locally smooth. The resultant DIDs can effectively remove various types of synthetic and adversarial noise. We also uncover that the robustness of DIDs benefits their generalization capability on unseen real-world noise. Indeed, {\sc HAT}-trained DIDs can recover high-quality clean images from real-world noise even without training on real noisy data. Extensive experiments on benchmark datasets, including Set68, PolyU, and SIDD, corroborate the effectiveness of {\sc ObsAtk} and {\sc HAT}.

摘要: 本文系统地研究了深层图像去噪器(DIDS)的对抗鲁棒性，即DIDS从被对抗扰动降级的噪声观测中恢复地面真实信息的能力。首先，为了评估DIDS的鲁棒性，我们提出了一种新的敌意攻击方法，即基于观测的零均值攻击({\sc ObsAtk})，对给定的噪声图像进行敌意零均值扰动。我们发现现有的DID容易受到{\sc ObsAtk}产生的对抗性噪声的影响。其次，为了增强DIDs的鲁棒性，我们提出了一种对抗性训练策略--混合对抗性训练({\sc HAT})，用对抗性和非对抗性噪声数据联合训练DIDs，以确保重建质量高，非对抗性数据周围的去噪器局部光滑。由此产生的DID可以有效地去除各种类型的合成噪声和对抗性噪声。我们还发现，DID的健壮性有利于它们对不可见的真实世界噪声的泛化能力。事实上，经过{\sc HAT}训练的DID可以从真实世界的噪声中恢复高质量的干净图像，即使不需要对真实的噪声数据进行培训。在包括Set68、PolyU和SIDD在内的基准数据集上的大量实验证实了{\sc ObsAtk}和{\sc HAT}的有效性。



## **44. Security for Machine Learning-based Software Systems: a survey of threats, practices and challenges**

基于机器学习的软件系统安全：威胁、实践和挑战综述 cs.CR

**SubmitDate**: 2022-01-12    [paper-pdf](http://arxiv.org/pdf/2201.04736v1)

**Authors**: Huaming Chen, M. Ali Babar

**Abstracts**: The rapid development of Machine Learning (ML) has demonstrated superior performance in many areas, such as computer vision, video and speech recognition. It has now been increasingly leveraged in software systems to automate the core tasks. However, how to securely develop the machine learning-based modern software systems (MLBSS) remains a big challenge, for which the insufficient consideration will largely limit its application in safety-critical domains. One concern is that the present MLBSS development tends to be rush, and the latent vulnerabilities and privacy issues exposed to external users and attackers will be largely neglected and hard to be identified. Additionally, machine learning-based software systems exhibit different liabilities towards novel vulnerabilities at different development stages from requirement analysis to system maintenance, due to its inherent limitations from the model and data and the external adversary capabilities. In this work, we consider that security for machine learning-based software systems may arise by inherent system defects or external adversarial attacks, and the secure development practices should be taken throughout the whole lifecycle. While machine learning has become a new threat domain for existing software engineering practices, there is no such review work covering the topic. Overall, we present a holistic review regarding the security for MLBSS, which covers a systematic understanding from a structure review of three distinct aspects in terms of security threats. Moreover, it provides a thorough state-of-the-practice for MLBSS secure development. Finally, we summarise the literature for system security assurance, and motivate the future research directions with open challenges. We anticipate this work provides sufficient discussion and novel insights to incorporate system security engineering for future exploration.

摘要: 机器学习(ML)的快速发展在计算机视觉、视频和语音识别等领域表现出了优异的性能。现在，在软件系统中越来越多地利用它来自动执行核心任务。然而，如何安全地开发基于机器学习的现代软件系统(MLBSS)仍然是一个巨大的挑战，考虑不足将在很大程度上限制其在安全关键领域的应用。一个令人担忧的问题是，目前MLBSS的开发趋于仓促，暴露给外部用户和攻击者的潜在漏洞和隐私问题将在很大程度上被忽视，难以识别。此外，基于机器学习的软件系统在从需求分析到系统维护的不同开发阶段，由于其模型和数据的固有局限性以及外部对手能力的限制，对新的漏洞表现出不同的易感性。在这项工作中，我们认为基于机器学习的软件系统的安全性可能是由固有的系统缺陷或外部的对抗性攻击引起的，安全的开发实践应该贯穿于整个生命周期。虽然机器学习已经成为现有软件工程实践的一个新的威胁领域，但还没有覆盖这个主题的这样的审查工作。总体而言，我们提出了关于MLBSS安全的整体审查，其中包括从安全威胁的三个不同方面的结构审查来系统地理解MLBSS的安全。为MLBSS的安全开发提供了全面的实践基础。最后，对系统安全保障方面的文献进行了总结，并对未来的研究方向进行了展望。我们期望这项工作能为将来的探索提供充分的讨论和新的见解，将系统安全工程纳入其中。



## **45. Adversarially Robust Classification by Conditional Generative Model Inversion**

基于条件生成模型反演的对抗性鲁棒分类 cs.LG

**SubmitDate**: 2022-01-12    [paper-pdf](http://arxiv.org/pdf/2201.04733v1)

**Authors**: Mitra Alirezaei, Tolga Tasdizen

**Abstracts**: Most adversarial attack defense methods rely on obfuscating gradients. These methods are successful in defending against gradient-based attacks; however, they are easily circumvented by attacks which either do not use the gradient or by attacks which approximate and use the corrected gradient. Defenses that do not obfuscate gradients such as adversarial training exist, but these approaches generally make assumptions about the attack such as its magnitude. We propose a classification model that does not obfuscate gradients and is robust by construction without assuming prior knowledge about the attack. Our method casts classification as an optimization problem where we "invert" a conditional generator trained on unperturbed, natural images to find the class that generates the closest sample to the query image. We hypothesize that a potential source of brittleness against adversarial attacks is the high-to-low-dimensional nature of feed-forward classifiers which allows an adversary to find small perturbations in the input space that lead to large changes in the output space. On the other hand, a generative model is typically a low-to-high-dimensional mapping. While the method is related to Defense-GAN, the use of a conditional generative model and inversion in our model instead of the feed-forward classifier is a critical difference. Unlike Defense-GAN, which was shown to generate obfuscated gradients that are easily circumvented, we show that our method does not obfuscate gradients. We demonstrate that our model is extremely robust against black-box attacks and has improved robustness against white-box attacks compared to naturally trained, feed-forward classifiers.

摘要: 大多数对抗性攻击防御方法都依赖于模糊梯度。这些方法在防御基于梯度的攻击方面是成功的；但是，它们很容易被不使用梯度的攻击或近似并使用校正的梯度的攻击所绕过。存在不会混淆诸如对抗性训练等梯度的防御措施，但这些方法通常会对攻击进行假设，例如其强度。我们提出了一种分类模型，它不会混淆梯度，并且在不假设攻击先验知识的情况下，通过构造具有健壮性。我们的方法将分类转换为一个优化问题，在这个问题中，我们“反转”在未受干扰的自然图像上训练的条件生成器，以找到生成与查询图像最接近的样本的类。我们假设一个潜在的抵抗敌意攻击的脆性来源是前馈分类器从高维到低维的性质，它允许敌手在输入空间中发现导致输出空间大变化的小扰动。另一方面，生成模型通常是低维到高维的映射。虽然该方法与防御性GaN有关，但在我们的模型中使用条件生成模型和反演来代替前馈分类器是一个关键的区别。与Defense-GAN不同的是，它可以生成容易规避的模糊梯度，而我们的方法不会模糊梯度。我们证明，与自然训练的前馈分类器相比，我们的模型对黑盒攻击具有极强的鲁棒性，并且对白盒攻击具有更好的鲁棒性。



## **46. Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL**

谁是最强大的敌人？基于Deep RL的最优高效规避攻击研究 cs.LG

**SubmitDate**: 2022-01-12    [paper-pdf](http://arxiv.org/pdf/2106.05087v3)

**Authors**: Yanchao Sun, Ruijie Zheng, Yongyuan Liang, Furong Huang

**Abstracts**: Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. This paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named "actor" and an RL-based learner named "director". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically optimal and significantly more efficient than prior RL-based works in environments with large state spaces. Empirical results show that our proposed PA-AD universally outperforms state-of-the-art attacking methods in various Atari and MuJoCo environments. By applying PA-AD to adversarial training, we achieve state-of-the-art empirical robustness in multiple tasks under strong adversaries.

摘要: 评估强化学习(RL)Agent在状态观测(在一定约束范围内)的最强/最优对抗扰动下的最坏情况下的性能，对于理解RL Agent的鲁棒性是至关重要的。然而，无论是从我们是否能找到最佳攻击，还是从我们找到最佳攻击的效率来看，找到最佳对手都是具有挑战性的。现有的对抗性RL研究要么使用基于启发式的方法，可能找不到最强的对手，要么将Agent视为环境的一部分，直接训练基于RL的对手，这可以找到最优的对手，但在大的状态空间中可能会变得难以处理。本文提出了一种新的攻击方法，通过设计一个名为“参与者”的函数和一个名为“导演”的基于RL的学习器之间的协作来寻找最优攻击。参与者为给定的政策扰动方向制作状态扰动，导演学习提出最佳政策扰动方向。我们提出的算法PA-AD在理论上是最优的，并且在具有大状态空间的环境中比以前的基于RL的工作效率要高得多。实验结果表明，在不同的Atari和MuJoCo环境下，我们提出的PA-AD攻击方法普遍优于最新的攻击方法。通过将PA-AD应用于对抗性训练，我们在强对手下的多任务中获得了最先进的经验鲁棒性。



## **47. Complete Traceability Multimedia Fingerprinting Codes Resistant to Averaging Attack and Adversarial Noise with Optimal Rate**

具有最佳速率的抗平均攻击和对抗噪声的完全可追溯性多媒体指纹码 cs.IT

**SubmitDate**: 2022-01-12    [paper-pdf](http://arxiv.org/pdf/2108.09015v3)

**Authors**: Ilya Vorobyev

**Abstracts**: In this paper we consider complete traceability multimedia fingerprinting codes resistant to averaging attacks and adversarial noise. Recently it was shown that there are no such codes for the case of an arbitrary linear attack. However, for the case of averaging attacks complete traceability multimedia fingerprinting codes of exponential cardinality resistant to constant adversarial noise were constructed in 2020 by Egorova et al. We continue this work and provide an improved lower bound on the rate of these codes.

摘要: 在本文中，我们考虑了完全可追溯性多媒体指纹码，它能抵抗平均攻击和对抗噪声。最近有研究表明，在任意线性攻击的情况下，不存在这样的码。然而，对于平均攻击的情况，Egorova等人于2020年构造了指数基数抵抗恒定对抗噪声的完全可追溯性多媒体指纹码。我们继续这项工作，并提供了这些码率的一个改进的下界。



## **48. Game Theory for Adversarial Attacks and Defenses**

对抗性攻防的博弈论 cs.LG

With the agreement of my coauthors, I would like to withdraw the  manuscript "Game Theory for Adversarial Attacks and Defenses". Some  experimental procedures were not included in the manuscript, which makes a  part of important claims not meaningful

**SubmitDate**: 2022-01-12    [paper-pdf](http://arxiv.org/pdf/2110.06166v3)

**Authors**: Shorya Sharma

**Abstracts**: Adversarial attacks can generate adversarial inputs by applying small but intentionally worst-case perturbations to samples from the dataset, which leads to even state-of-the-art deep neural networks outputting incorrect answers with high confidence. Hence, some adversarial defense techniques are developed to improve the security and robustness of the models and avoid them being attacked. Gradually, a game-like competition between attackers and defenders formed, in which both players would attempt to play their best strategies against each other while maximizing their own payoffs. To solve the game, each player would choose an optimal strategy against the opponent based on the prediction of the opponent's strategy choice. In this work, we are on the defensive side to apply game-theoretic approaches on defending against attacks. We use two randomization methods, random initialization and stochastic activation pruning, to create diversity of networks. Furthermore, we use one denoising technique, super resolution, to improve models' robustness by preprocessing images before attacks. Our experimental results indicate that those three methods can effectively improve the robustness of deep-learning neural networks.

摘要: 对抗性攻击可以通过对数据集中的样本应用小但有意的最坏情况扰动来生成对抗性输入，这甚至导致最先进的深度神经网络以高置信度输出不正确的答案。因此，为了提高模型的安全性和健壮性，避免模型受到攻击，一些对抗性防御技术应运而生。逐渐地，攻击者和后卫之间形成了一种游戏般的竞争，双方都会试图在最大化自己收益的同时，发挥自己最好的策略。为了解决博弈，每个玩家都会根据对手的策略选择预测来选择一个最优策略来对抗对手。在这项工作中，我们处于守势，应用博弈论的方法来防御攻击。我们使用随机初始化和随机激活剪枝两种随机化方法来创建网络多样性。此外，我们还使用了超分辨率去噪技术，通过在攻击前对图像进行预处理来提高模型的鲁棒性。实验结果表明，这三种方法均能有效提高深度学习神经网络的鲁棒性。



## **49. Similarity-based Gray-box Adversarial Attack Against Deep Face Recognition**

基于相似度的灰度盒对抗深度人脸识别攻击 cs.CV

ACCEPTED in IEEE International Conference on Automatic Face and  Gesture Recognition (FG 2021)

**SubmitDate**: 2022-01-12    [paper-pdf](http://arxiv.org/pdf/2201.04011v2)

**Authors**: Hanrui Wang, Shuo Wang, Zhe Jin, Yandan Wang, Cunjian Chen, Massimo Tistarell

**Abstracts**: The majority of adversarial attack techniques perform well against deep face recognition when the full knowledge of the system is revealed (\emph{white-box}). However, such techniques act unsuccessfully in the gray-box setting where the face templates are unknown to the attackers. In this work, we propose a similarity-based gray-box adversarial attack (SGADV) technique with a newly developed objective function. SGADV utilizes the dissimilarity score to produce the optimized adversarial example, i.e., similarity-based adversarial attack. This technique applies to both white-box and gray-box attacks against authentication systems that determine genuine or imposter users using the dissimilarity score. To validate the effectiveness of SGADV, we conduct extensive experiments on face datasets of LFW, CelebA, and CelebA-HQ against deep face recognition models of FaceNet and InsightFace in both white-box and gray-box settings. The results suggest that the proposed method significantly outperforms the existing adversarial attack techniques in the gray-box setting. We hence summarize that the similarity-base approaches to develop the adversarial example could satisfactorily cater to the gray-box attack scenarios for de-authentication.

摘要: 当系统的全部知识被揭示时，大多数对抗性攻击技术对深度人脸识别表现良好(\emph{白盒})。然而，这样的技术在人脸模板对攻击者未知的灰盒设置中不起作用。在这项工作中，我们提出了一种基于相似度的灰盒对抗攻击(SGADV)技术，并引入了一个新的目标函数。SGADV利用相异度来产生优化的对抗性实例，即基于相似度的对抗性攻击。该技术既适用于针对身份验证系统的白盒和灰盒攻击，也适用于使用相异分数来确定真实用户或冒牌用户的身份验证系统。为了验证SGADV的有效性，我们在LFW、CelebA和CelebA-HQ的人脸数据集上与FaceNet和InsightFace的深度人脸识别模型在白盒和灰盒环境下进行了广泛的实验。结果表明，在灰箱环境下，该方法的性能明显优于现有的对抗性攻击技术。因此，我们总结出基于相似性的方法来开发敌意示例可以令人满意地迎合取消认证的灰盒攻击场景。



## **50. Tor circuit fingerprinting defenses using adaptive padding**

使用自适应填充的ToR电路指纹防御 cs.CR

17 pages

**SubmitDate**: 2022-01-11    [paper-pdf](http://arxiv.org/pdf/2103.03831v2)

**Authors**: George Kadianakis, Theodoros Polyzos, Mike Perry, Kostas Chatzikokolakis

**Abstracts**: Online anonymity and privacy has been based on confusing the adversary by creating indistinguishable network elements. Tor is the largest and most widely deployed anonymity system, designed against realistic modern adversaries. Recently, researchers have managed to fingerprint Tor's circuits -- and hence the type of underlying traffic -- simply by capturing and analyzing traffic traces. In this work, we study the circuit fingerprinting problem, isolating it from website fingerprinting, and revisit previous findings in this model, showing that accurate attacks are possible even when the application-layer traffic is identical. We then proceed to incrementally create defenses against circuit fingerprinting, using a generic adaptive padding framework for Tor based on WTF-PAD. We present a simple defense which delays a fraction of the traffic, as well as a more advanced one which can effectively hide onion service circuits with zero delays. We thoroughly evaluate both defenses, both analytically and experimentally, discovering new subtle fingerprints, but also showing the effectiveness of our defenses.

摘要: 在线匿名和隐私一直基于通过创建难以区分的网络元素来迷惑对手。Tor是最大和部署最广泛的匿名系统，专为现实的现代对手而设计。最近，研究人员仅通过捕捉和分析交通痕迹就成功地提取了Tor电路的指纹--因此也就是潜在的交通类型。在这项工作中，我们研究了电路指纹问题，将其从网站指纹中分离出来，并重新审视了该模型中以前的发现，表明即使在应用层流量相同的情况下，准确的攻击也是可能的。然后，我们使用基于WTF-PAD的Tor通用自适应填充框架，逐步创建对电路指纹的防御。我们提出了一个简单的防御方案，它可以延迟一小部分流量，以及一个更高级的防御方案，它可以有效地隐藏洋葱服务电路，而不会造成延迟。我们通过分析和实验对这两种防御措施进行了彻底评估，发现了新的微妙指纹，但也显示了我们防御措施的有效性。



