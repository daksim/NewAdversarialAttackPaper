# Latest Adversarial Attack Papers
**update at 2023-03-09 11:08:29**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. On the Risks of Stealing the Decoding Algorithms of Language Models**

论窃取语言模型译码算法的风险 cs.LG

**SubmitDate**: 2023-03-08    [abs](http://arxiv.org/abs/2303.04729v1) [paper-pdf](http://arxiv.org/pdf/2303.04729v1)

**Authors**: Ali Naseh, Kalpesh Krishna, Mohit Iyyer, Amir Houmansadr

**Abstract**: A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.

摘要: 从现代语言模型(LM)生成文本的一个关键组件是解码算法的选择和调整。这些算法确定如何从LM生成的内部概率分布生成文本。选择解码算法和调整其超参数的过程需要大量的时间、人工和计算，还需要广泛的人工评估。因此，这种译码算法的恒等式和超参数被认为对它们的所有者非常有价值。在这项工作中，我们首次证明，具有典型API访问权限的攻击者可以以非常低的金钱成本窃取其解码算法的类型和超参数。我们的攻击对文本生成API中使用的流行LMS有效，包括GPT-2和GPT-3。我们证明了只需几美元即可窃取此类信息的可行性，例如，对于GPT-3的四个版本，仅需$0.8$、$1$、$4$和$40$。



## **2. Immune Defense: A Novel Adversarial Defense Mechanism for Preventing the Generation of Adversarial Examples**

免疫防御：一种防止对抗性事例产生的新型对抗性防御机制 cs.CV

**SubmitDate**: 2023-03-08    [abs](http://arxiv.org/abs/2303.04502v1) [paper-pdf](http://arxiv.org/pdf/2303.04502v1)

**Authors**: Jinwei Wang, Hao Wu, Haihua Wang, Jiawei Zhang, Xiangyang Luo, Bin Ma

**Abstract**: The vulnerability of Deep Neural Networks (DNNs) to adversarial examples has been confirmed. Existing adversarial defenses primarily aim at preventing adversarial examples from attacking DNNs successfully, rather than preventing their generation. If the generation of adversarial examples is unregulated, images within reach are no longer secure and pose a threat to non-robust DNNs. Although gradient obfuscation attempts to address this issue, it has been shown to be circumventable. Therefore, we propose a novel adversarial defense mechanism, which is referred to as immune defense and is the example-based pre-defense. This mechanism applies carefully designed quasi-imperceptible perturbations to the raw images to prevent the generation of adversarial examples for the raw images, and thereby protecting both images and DNNs. These perturbed images are referred to as Immune Examples (IEs). In the white-box immune defense, we provide a gradient-based and an optimization-based approach, respectively. Additionally, the more complex black-box immune defense is taken into consideration. We propose Masked Gradient Sign Descent (MGSD) to reduce approximation error and stabilize the update to improve the transferability of IEs and thereby ensure their effectiveness against black-box adversarial attacks. The experimental results demonstrate that the optimization-based approach has superior performance and better visual quality in white-box immune defense. In contrast, the gradient-based approach has stronger transferability and the proposed MGSD significantly improve the transferability of baselines.

摘要: 深度神经网络(DNN)对敌意例子的脆弱性已被证实。现有的对抗性防御主要是为了防止敌意实例成功攻击DNN，而不是阻止它们的生成。如果敌意示例的生成不受控制，则触手可及的图像不再安全，并对非健壮的DNN构成威胁。虽然渐变模糊试图解决这个问题，但它已被证明是可以规避的。因此，我们提出了一种新颖的对抗性防御机制，称为免疫防御，是基于实例的预防御。该机制将精心设计的准不可感知扰动应用于原始图像，以防止原始图像产生对抗性示例，从而保护图像和DNN。这些受干扰的图像被称为免疫样例(IE)。在白盒免疫防御中，我们分别提出了基于梯度的方法和基于优化的方法。此外，还考虑了更复杂的黑盒免疫防御。我们提出了掩蔽梯度符号下降算法(MGSD)来减少逼近误差，稳定更新，从而提高IES的可转移性，从而确保其对抗黑盒攻击的有效性。实验结果表明，基于优化的方法在白盒免疫防御中具有更好的性能和更好的视觉质量。相比之下，基于梯度的方法具有更强的可转移性，所提出的MGSD显著提高了基线的可转移性。



## **3. Dishing Out DoS: How to Disable and Secure the Starlink User Terminal**

拒绝服务：如何禁用和保护Starlink用户终端 cs.CR

6 pages, 2 figures; the first two authors contributed equally to this  paper

**SubmitDate**: 2023-03-08    [abs](http://arxiv.org/abs/2303.00582v2) [paper-pdf](http://arxiv.org/pdf/2303.00582v2)

**Authors**: Joshua Smailes, Edd Salkield, Sebastian Köhler, Simon Birnbach, Ivan Martinovic

**Abstract**: Satellite user terminals are a promising target for adversaries seeking to target satellite communication networks. Despite this, many protections commonly found in terrestrial routers are not present in some user terminals.   As a case study we audit the attack surface presented by the Starlink router's admin interface, using fuzzing to uncover a denial of service attack on the Starlink user terminal. We explore the attack's impact, particularly in the cases of drive-by attackers, and attackers that are able to maintain a continuous presence on the network. Finally, we discuss wider implications, looking at lessons learned in terrestrial router security, and how to properly implement them in this new context.

摘要: 卫星用户终端是寻求以卫星通信网络为目标的敌人的一个有希望的目标。尽管如此，地面路由器中常见的许多保护在一些用户终端中并不存在。作为一个案例研究，我们审计了Starlink路由器的管理界面呈现的攻击面，使用Fuzze发现了对Starlink用户终端的拒绝服务攻击。我们探讨了攻击的影响，特别是在路过攻击者和能够在网络上持续存在的攻击者的情况下。最后，我们将讨论更广泛的影响，总结地面路由器安全方面的经验教训，以及如何在新的环境中正确实施这些经验教训。



## **4. GLOW: Global Layout Aware Attacks on Object Detection**

Glow：针对对象检测的全局布局感知攻击 cs.CV

ICCV

**SubmitDate**: 2023-03-08    [abs](http://arxiv.org/abs/2302.14166v2) [paper-pdf](http://arxiv.org/pdf/2302.14166v2)

**Authors**: Buyu Liu, BaoJun, Jianping Fan, Xi Peng, Kui Ren, Jun Yu

**Abstract**: Adversarial attacks aim to perturb images such that a predictor outputs incorrect results. Due to the limited research in structured attacks, imposing consistency checks on natural multi-object scenes is a promising yet practical defense against conventional adversarial attacks. More desired attacks, to this end, should be able to fool defenses with such consistency checks. Therefore, we present the first approach GLOW that copes with various attack requests by generating global layout-aware adversarial attacks, in which both categorical and geometric layout constraints are explicitly established. Specifically, we focus on object detection task and given a victim image, GLOW first localizes victim objects according to target labels. And then it generates multiple attack plans, together with their context-consistency scores. Our proposed GLOW, on the one hand, is capable of handling various types of requests, including single or multiple victim objects, with or without specified victim objects. On the other hand, it produces a consistency score for each attack plan, reflecting the overall contextual consistency that both semantic category and global scene layout are considered. In experiment, we design multiple types of attack requests and validate our ideas on MS COCO and Pascal. Extensive experimental results demonstrate that we can achieve about 30$\%$ average relative improvement compared to state-of-the-art methods in conventional single object attack request; Moreover, our method outperforms SOTAs significantly on more generic attack requests by about 20$\%$ in average; Finally, our method produces superior performance under challenging zero-query black-box setting, or 20$\%$ better than SOTAs. Our code, model and attack requests would be made available.

摘要: 敌意攻击的目的是扰乱图像，使预测器输出错误的结果。由于结构化攻击的研究有限，对自然多目标场景进行一致性检查是对抗传统对手攻击的一种很有前途的实用防御方法。为了达到这个目的，更多想要的攻击应该能够通过这样的一致性检查来愚弄防御。因此，我们提出了第一种方法GLOW，它通过生成全局布局感知的对抗性攻击来应对各种攻击请求，其中明确地建立了分类布局约束和几何布局约束。具体地说，我们针对目标检测任务，在给定受害者图像的情况下，GLOW首先根据目标标签定位受害者对象。然后，它生成多个攻击计划，以及它们的上下文一致性分数。一方面，我们提出的Glow能够处理各种类型的请求，包括单个或多个受害者对象，具有或不具有指定的受害者对象。另一方面，它为每个攻击计划生成一个一致性分数，反映了语义类别和全局场景布局都被考虑的整体上下文一致性。在实验中，我们设计了多种类型的攻击请求，并在MS Coco和Pascal上验证了我们的想法。大量的实验结果表明，在传统的单对象攻击请求中，我们的方法可以比现有的方法平均提高约30美元；此外，在更一般的攻击请求上，我们的方法比Sotas显著提高约20美元；最后，在挑战零查询黑盒设置的情况下，我们的方法获得了更好的性能，比Sotas高出20美元。我们的代码、模型和攻击请求将可用。



## **5. Sampling Attacks on Meta Reinforcement Learning: A Minimax Formulation and Complexity Analysis**

元强化学习中的抽样攻击：一种极小极大公式及其复杂性分析 cs.LG

updates: github repo posted

**SubmitDate**: 2023-03-08    [abs](http://arxiv.org/abs/2208.00081v2) [paper-pdf](http://arxiv.org/pdf/2208.00081v2)

**Authors**: Tao Li, Haozhe Lei, Quanyan Zhu

**Abstract**: Meta reinforcement learning (meta RL), as a combination of meta-learning ideas and reinforcement learning (RL), enables the agent to adapt to different tasks using a few samples. However, this sampling-based adaptation also makes meta RL vulnerable to adversarial attacks. By manipulating the reward feedback from sampling processes in meta RL, an attacker can mislead the agent into building wrong knowledge from training experience, which deteriorates the agent's performance when dealing with different tasks after adaptation. This paper provides a game-theoretical underpinning for understanding this type of security risk. In particular, we formally define the sampling attack model as a Stackelberg game between the attacker and the agent, which yields a minimax formulation. It leads to two online attack schemes: Intermittent Attack and Persistent Attack, which enable the attacker to learn an optimal sampling attack, defined by an $\epsilon$-first-order stationary point, within $\mathcal{O}(\epsilon^{-2})$ iterations. These attack schemes freeride the learning progress concurrently without extra interactions with the environment. By corroborating the convergence results with numerical experiments, we observe that a minor effort of the attacker can significantly deteriorate the learning performance, and the minimax approach can also help robustify the meta RL algorithms.

摘要: 元强化学习作为元学习思想和强化学习的结合，使智能体能够利用少量的样本来适应不同的任务。然而，这种基于采样的自适应也使得Meta RL容易受到对手攻击。通过操纵Meta RL中采样过程的奖励反馈，攻击者可以误导代理从训练经验中建立错误的知识，从而降低代理在适应后处理不同任务的性能。本文为理解这种类型的安全风险提供了博弈论基础。特别地，我们将抽样攻击模型正式定义为攻击者和代理之间的Stackelberg博弈，从而产生极小极大公式。它导致了两种在线攻击方案：间歇攻击和持续攻击，使攻击者能够在$\mathcal{O}(\epsilon^{-2})$迭代内学习由$\epsilon$-一阶固定点定义的最优抽样攻击。这些攻击方案同时加快了学习过程，而无需与环境进行额外的交互。通过数值实验证实了收敛结果，我们观察到攻击者的微小努力会显著降低学习性能，并且极小极大方法也有助于增强Meta RL算法的健壮性。



## **6. Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors**

隐形补丁：对物体探测器的自然主义黑箱对抗性攻击 cs.CV

**SubmitDate**: 2023-03-07    [abs](http://arxiv.org/abs/2303.04238v1) [paper-pdf](http://arxiv.org/pdf/2303.04238v1)

**Authors**: Raz Lapid, Moshe Sipper

**Abstract**: Adversarial attacks on deep-learning models have been receiving increased attention in recent years. Work in this area has mostly focused on gradient-based techniques, so-called white-box attacks, wherein the attacker has access to the targeted model's internal parameters; such an assumption is usually unrealistic in the real world. Some attacks additionally use the entire pixel space to fool a given model, which is neither practical nor physical (i.e., real-world). On the contrary, we propose herein a gradient-free method that uses the learned image manifold of a pretrained generative adversarial network (GAN) to generate naturalistic physical adversarial patches for object detectors. We show that our proposed method works both digitally and physically.

摘要: 近年来，针对深度学习模型的对抗性攻击受到越来越多的关注。这一领域的工作主要集中在基于梯度的技术，即所谓的白盒攻击，即攻击者可以访问目标模型的内部参数；这种假设在现实世界中通常是不现实的。一些攻击还使用整个像素空间来愚弄给定的模型，这既不实用也不物理(即，现实世界)。相反，我们在这里提出了一种无梯度的方法，它使用预先训练的生成性对抗性网络(GAN)的学习图像流形来为目标检测器生成自然的物理对抗性斑块。我们证明了我们提出的方法在数字和物理上都是有效的。



## **7. Robustness-preserving Lifelong Learning via Dataset Condensation**

基于数据集压缩的保持健壮性的终身学习 cs.LG

Accepted by ICASSP2023 Main Track: Machine Learning for Signal  Processing

**SubmitDate**: 2023-03-07    [abs](http://arxiv.org/abs/2303.04183v1) [paper-pdf](http://arxiv.org/pdf/2303.04183v1)

**Authors**: Jinghan Jia, Yihua Zhang, Dogyoon Song, Sijia Liu, Alfred Hero

**Abstract**: Lifelong learning (LL) aims to improve a predictive model as the data source evolves continuously. Most work in this learning paradigm has focused on resolving the problem of 'catastrophic forgetting,' which refers to a notorious dilemma between improving model accuracy over new data and retaining accuracy over previous data. Yet, it is also known that machine learning (ML) models can be vulnerable in the sense that tiny, adversarial input perturbations can deceive the models into producing erroneous predictions. This motivates the research objective of this paper - specification of a new LL framework that can salvage model robustness (against adversarial attacks) from catastrophic forgetting. Specifically, we propose a new memory-replay LL strategy that leverages modern bi-level optimization techniques to determine the 'coreset' of the current data (i.e., a small amount of data to be memorized) for ease of preserving adversarial robustness over time. We term the resulting LL framework 'Data-Efficient Robustness-Preserving LL' (DERPLL). The effectiveness of DERPLL is evaluated for class-incremental image classification using ResNet-18 over the CIFAR-10 dataset. Experimental results show that DERPLL outperforms the conventional coreset-guided LL baseline and achieves a substantial improvement in both standard accuracy and robust accuracy.

摘要: 终身学习的目标是随着数据源的不断发展而改进预测模型。这种学习范式的大部分工作都集中在解决“灾难性遗忘”的问题上，“灾难性遗忘”指的是在提高模型相对于新数据的准确性和保持相对于先前数据的准确性之间的两难境地。然而，众所周知，机器学习(ML)模型可能是脆弱的，因为微小的对抗性输入扰动可能会欺骗模型产生错误的预测。这促使了本文的研究目标-规范一个新的LL框架，该框架可以从灾难性遗忘中挽救模型的健壮性(对抗对手攻击)。具体地说，我们提出了一种新的记忆重放L1策略，该策略利用现代双层优化技术来确定当前数据(即需要记忆的少量数据)的核心重置，从而易于随着时间的推移保持对手的健壮性。我们将所得到的LL框架称为数据高效的保持健壮性的LL(DERPLL)。在CIFAR-10数据集上，使用ResNet-18对DERPLL算法进行了类增量图像分类的有效性评估。实验结果表明，DERPLL的性能优于传统的CoReset制导的LL基线，在标准精度和稳健精度方面都有很大的提高。



## **8. Exploiting Trust for Resilient Hypothesis Testing with Malicious Robots (evolved version)**

利用信任对恶意机器人进行弹性假设测试(进化版) cs.RO

21 pages, 5 figures, 1 table. arXiv admin note: substantial text  overlap with arXiv:2209.12285

**SubmitDate**: 2023-03-07    [abs](http://arxiv.org/abs/2303.04075v1) [paper-pdf](http://arxiv.org/pdf/2303.04075v1)

**Authors**: Matthew Cavorsi, Orhan Eren Akgün, Michal Yemini, Andrea Goldsmith, Stephanie Gil

**Abstract**: We develop a resilient binary hypothesis testing framework for decision making in adversarial multi-robot crowdsensing tasks. This framework exploits stochastic trust observations between robots to arrive at tractable, resilient decision making at a centralized Fusion Center (FC) even when i) there exist malicious robots in the network and their number may be larger than the number of legitimate robots, and ii) the FC uses one-shot noisy measurements from all robots. We derive two algorithms to achieve this. The first is the Two Stage Approach (2SA) that estimates the legitimacy of robots based on received trust observations, and provably minimizes the probability of detection error in the worst-case malicious attack. Here, the proportion of malicious robots is known but arbitrary. For the case of an unknown proportion of malicious robots, we develop the Adversarial Generalized Likelihood Ratio Test (A-GLRT) that uses both the reported robot measurements and trust observations to estimate the trustworthiness of robots, their reporting strategy, and the correct hypothesis simultaneously. We exploit special problem structure to show that this approach remains computationally tractable despite several unknown problem parameters. We deploy both algorithms in a hardware experiment where a group of robots conducts crowdsensing of traffic conditions on a mock-up road network similar in spirit to Google Maps, subject to a Sybil attack. We extract the trust observations for each robot from actual communication signals which provide statistical information on the uniqueness of the sender. We show that even when the malicious robots are in the majority, the FC can reduce the probability of detection error to 30.5% and 29% for the 2SA and the A-GLRT respectively.

摘要: 提出了一种用于对抗性多机器人群体感知任务决策的弹性二元假设检验框架。该框架利用机器人之间的随机信任观察，即使在i)网络中存在恶意机器人并且它们的数量可能大于合法机器人的数量，以及ii)FC使用来自所有机器人的一次噪声测量的情况下，也可以在集中式融合中心(FC)获得易于处理的、有弹性的决策。我们推导了两个算法来实现这一点。第一种是两阶段方法(2SA)，它根据接收到的信任观察来估计机器人的合法性，并证明在最坏情况下恶意攻击的检测错误概率最小。在这里，恶意机器人的比例是已知的，但是随意的。对于恶意机器人比例未知的情况，我们提出了对抗性广义似然比检验(A-GLRT)，它同时使用报告的机器人测量值和信任观察来估计机器人的可信性、报告策略和正确的假设。我们利用特殊的问题结构表明，尽管有几个未知的问题参数，该方法在计算上仍然是容易处理的。我们在硬件实验中部署了这两种算法，在硬件实验中，一组机器人在一个类似于谷歌地图的模拟道路网络上对交通状况进行众感，受到Sybil攻击。我们从提供关于发送者唯一性的统计信息的实际通信信号中提取每个机器人的信任观察。实验结果表明，即使恶意机器人占多数，FC算法也能将2SA和A-GLRT的误检率分别降低到30.5%和29%。



## **9. Bounding Information Leakage in Machine Learning**

机器学习中的有界信息泄漏 cs.LG

Published in [Elsevier  Neurocomputing](https://doi.org/10.1016/j.neucom.2023.02.058)

**SubmitDate**: 2023-03-07    [abs](http://arxiv.org/abs/2105.03875v2) [paper-pdf](http://arxiv.org/pdf/2105.03875v2)

**Authors**: Ganesh Del Grosso, Georg Pichler, Catuscia Palamidessi, Pablo Piantanida

**Abstract**: Recently, it has been shown that Machine Learning models can leak sensitive information about their training data. This information leakage is exposed through membership and attribute inference attacks. Although many attack strategies have been proposed, little effort has been made to formalize these problems. We present a novel formalism, generalizing membership and attribute inference attack setups previously studied in the literature and connecting them to memorization and generalization. First, we derive a universal bound on the success rate of inference attacks and connect it to the generalization gap of the target model. Second, we study the question of how much sensitive information is stored by the algorithm about its training set and we derive bounds on the mutual information between the sensitive attributes and model parameters. Experimentally, we illustrate the potential of our approach by applying it to both synthetic data and classification tasks on natural images. Finally, we apply our formalism to different attribute inference strategies, with which an adversary is able to recover the identity of writers in the PenDigits dataset.

摘要: 最近，有研究表明，机器学习模型会泄露有关其训练数据的敏感信息。这种信息泄露通过成员身份和属性推理攻击暴露出来。虽然已经提出了许多攻击策略，但几乎没有努力将这些问题形式化。我们提出了一种新的形式主义，推广了以前在文献中研究的成员资格和属性推理攻击设置，并将它们与记忆和泛化联系起来。首先，我们推导出推理攻击成功率的一个普适界，并将其与目标模型的泛化差距联系起来。其次，研究了该算法对其训练集存储了多少敏感信息的问题，得到了敏感属性与模型参数之间互信息量的界。在实验上，我们通过将该方法应用于合成数据和自然图像上的分类任务来说明该方法的潜力。最后，我们将我们的形式化应用于不同的属性推理策略，通过这些策略，对手能够恢复PenDigits数据集中作者的身份。



## **10. SCRAMBLE-CFI: Mitigating Fault-Induced Control-Flow Attacks on OpenTitan**

SCRIBLE-CFI：缓解OpenTitan上的错误引起的控制流攻击 cs.CR

**SubmitDate**: 2023-03-07    [abs](http://arxiv.org/abs/2303.03711v1) [paper-pdf](http://arxiv.org/pdf/2303.03711v1)

**Authors**: Pascal Nasahl, Stefan Mangard

**Abstract**: Secure elements physically exposed to adversaries are frequently targeted by fault attacks. These attacks can be utilized to hijack the control-flow of software allowing the attacker to bypass security measures, extract sensitive data, or gain full code execution. In this paper, we systematically analyze the threat vector of fault-induced control-flow manipulations on the open-source OpenTitan secure element. Our thorough analysis reveals that current countermeasures of this chip either induce large area overheads or still cannot prevent the attacker from exploiting the identified threats. In this context, we introduce SCRAMBLE-CFI, an encryption-based control-flow integrity scheme utilizing existing hardware features of OpenTitan. SCRAMBLE-CFI confines, with minimal hardware overhead, the impact of fault-induced control-flow attacks by encrypting each function with a different encryption tweak at load-time. At runtime, code only can be successfully decrypted when the correct decryption tweak is active. We open-source our hardware changes and release our LLVM toolchain automatically protecting programs. Our analysis shows that SCRAMBLE-CFI complementarily enhances security guarantees of OpenTitan with a negligible hardware overhead of less than 3.97 % and a runtime overhead of 7.02 % for the Embench-IoT benchmarks.

摘要: 物理上暴露在对手面前的安全元素经常成为故障攻击的目标。这些攻击可用于劫持软件的控制流，从而允许攻击者绕过安全措施、提取敏感数据或获得完整的代码执行。在本文中，我们系统地分析了开源OpenTitan安全元素上由错误引起的控制流操作的威胁向量。我们的深入分析表明，目前该芯片的应对措施要么导致大面积开销，要么仍然无法阻止攻击者利用已识别的威胁。在此背景下，我们介绍了一种基于加密的控制流完整性方案SCRIBLE-CFI，该方案利用了OpenTitan现有的硬件特性。置乱-CFI通过在加载时使用不同的加密调整对每个函数进行加密，以最小的硬件开销限制了故障引发的控制流攻击的影响。在运行时，只有当正确的解密调整处于活动状态时，才能成功解密代码。我们将我们的硬件更改开源，并发布我们的LLVM工具链自动保护程序。我们的分析表明，在硬件开销小于3.97%、运行时开销为7.02%的情况下，SCRIBLE-CFI互补地增强了OpenTitan的安全保证。



## **11. Logit Margin Matters: Improving Transferable Targeted Adversarial Attack by Logit Calibration**

Logit边际重要：通过Logit校准提高可转移的目标对抗性攻击 cs.CV

**SubmitDate**: 2023-03-07    [abs](http://arxiv.org/abs/2303.03680v1) [paper-pdf](http://arxiv.org/pdf/2303.03680v1)

**Authors**: Juanjuan Weng, Zhiming Luo, Zhun Zhong, Shaozi Li, Nicu Sebe

**Abstract**: Previous works have extensively studied the transferability of adversarial samples in untargeted black-box scenarios. However, it still remains challenging to craft targeted adversarial examples with higher transferability than non-targeted ones. Recent studies reveal that the traditional Cross-Entropy (CE) loss function is insufficient to learn transferable targeted adversarial examples due to the issue of vanishing gradient. In this work, we provide a comprehensive investigation of the CE loss function and find that the logit margin between the targeted and untargeted classes will quickly obtain saturation in CE, which largely limits the transferability. Therefore, in this paper, we devote to the goal of continually increasing the logit margin along the optimization to deal with the saturation issue and propose two simple and effective logit calibration methods, which are achieved by downscaling the logits with a temperature factor and an adaptive margin, respectively. Both of them can effectively encourage optimization to produce a larger logit margin and lead to higher transferability. Besides, we show that minimizing the cosine distance between the adversarial examples and the classifier weights of the target class can further improve the transferability, which is benefited from downscaling logits via L2-normalization. Experiments conducted on the ImageNet dataset validate the effectiveness of the proposed methods, which outperform the state-of-the-art methods in black-box targeted attacks. The source code is available at \href{https://github.com/WJJLL/Target-Attack/}{Link}

摘要: 以前的工作已经广泛地研究了对抗性样本在非目标黑盒场景中的可转移性。然而，制作具有比非目标例子更高可转移性的有针对性的对抗性例子仍然具有挑战性。最近的研究表明，由于存在梯度消失的问题，传统的交叉熵损失函数不足以学习可转移的目标对抗性样本。在这项工作中，我们对CE损失函数进行了全面的研究，发现目标类和非目标类之间的Logit差值在CE中会很快达到饱和，这在很大程度上限制了CE的可转移性。因此，在本文中，我们致力于沿着优化不断增加Logit裕度的目标来处理饱和问题，并提出了两种简单有效的Logit校准方法，分别是通过降低Logit的温度因子和自适应裕度来实现的。两者都能有效地鼓励优化，以产生更大的Logit利润率，并导致更高的可转移性。此外，我们还证明了最小化对抗性样本和目标类的分类器权重之间的余弦距离可以进一步提高可转移性，这得益于通过L2归一化来降低Logits的尺度。在ImageNet数据集上进行的实验验证了提出的方法的有效性，在黑盒定向攻击中的性能优于最新的方法。源代码可在\href{https://github.com/WJJLL/Target-Attack/}{Link}上找到



## **12. Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations**

基于非凸低阶半正定松弛的对抗性训练神经网络的紧认证 cs.LG

**SubmitDate**: 2023-03-07    [abs](http://arxiv.org/abs/2211.17244v2) [paper-pdf](http://arxiv.org/pdf/2211.17244v2)

**Authors**: Hong-Ming Chiu, Richard Y. Zhang

**Abstract**: Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming (LP) techniques in particular face a "convex relaxation barrier" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming (MILP) techniques, and even when using state-of-the-art computational facilities. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming (SDP) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive SDP methods, while optimizing over dramatically fewer variables comparable to much weaker LP methods. Despite nonconvexity, we show how off-the-shelf local optimization algorithms can be used to achieve and to certify global optimality in polynomial time. Our experiments find that the nonconvex relaxation almost completely closes the gap towards exact certification of adversarially trained models.

摘要: 众所周知，对抗性训练可以产生高质量的神经网络模型，这些模型对对抗性扰动具有经验上的健壮性。然而，一旦一个模型经过对抗性的训练，人们往往希望得到一个证明，证明该模型对未来的所有攻击都是真正健壮的。不幸的是，当面对对手训练的模型时，所有现有的方法都在制作强大到足以实用的证书方面存在重大问题。线性规划(LP)技术尤其面临着“凸松弛障碍”，即使在使用混合整数线性规划(MILP)技术进行改进之后，甚至在使用最先进的计算设施时，也无法进行高质量的认证。本文提出了一种基于半定规划(SDP)松弛的低阶限制的非凸证明技术。非凸松弛使强认证可与昂贵得多的SDP方法相媲美，同时优化的变量可比弱得多的线性规划方法少得多。尽管非凸性，我们展示了如何使用现成的局部优化算法来在多项式时间内实现和证明全局最优性。我们的实验发现，非凸松弛几乎完全弥合了对对抗性训练模型进行精确验证的差距。



## **13. Securing Autonomous Vehicles Under Partial-Information Cyber Attacks on LiDAR Data**

激光雷达数据部分信息网络攻击下自主车辆的安全 cs.CR

**SubmitDate**: 2023-03-06    [abs](http://arxiv.org/abs/2303.03470v1) [paper-pdf](http://arxiv.org/pdf/2303.03470v1)

**Authors**: R. Spencer Hallyburton, Miroslav Pajic

**Abstract**: Safety is paramount in autonomous vehicles (AVs). Auto manufacturers have spent millions of dollars and driven billions of miles to prove AVs are safe. However, this is ill-suited to answer: what happens to an AV if its data are adversarially compromised? We design a framework built on security-relevant metrics to benchmark AVs on longitudinal datasets. We establish the capabilities of a cyber-level attacker with only access to LiDAR datagrams and from them derive novel attacks on LiDAR. We demonstrate that even though the attacker has minimal knowledge and only access to raw datagrams, the attacks compromise perception and tracking in multi-sensor AVs and lead to objectively unsafe scenarios. To mitigate vulnerabilities and advance secure architectures in AVs, we present two improvements for security-aware fusion -- a data-asymmetry monitor and a scalable track-to-track fusion of 3D LiDAR and monocular detections (T2T-3DLM); we demonstrate that the approaches significantly reduce the attack effectiveness.

摘要: 自动驾驶汽车(AVs)的安全性是最重要的。汽车制造商已经花费了数百万美元，开了数十亿英里来证明自动驾驶汽车是安全的。然而，这并不适合回答：如果反病毒软件的数据遭到恶意泄露，会发生什么？我们设计了一个基于安全相关度量的框架来对纵向数据集上的AVS进行基准测试。我们建立了只能访问LiDAR数据报的网络级攻击者的能力，并从这些能力中衍生出对LiDAR的新型攻击。我们证明，即使攻击者只有最少的知识和访问原始数据报，攻击损害了多传感器AVs的感知和跟踪，并导致客观上不安全的场景。为了缓解AVS中的漏洞和推进安全体系结构，我们提出了两种安全感知融合的改进--数据不对称监控器和3D LiDAR和单目检测的可扩展航迹到航迹融合(T2T-3DLM)；我们证明这两种方法显著降低了攻击效率。



## **14. ALMOST: Adversarial Learning to Mitigate Oracle-less ML Attacks via Synthesis Tuning**

几乎：通过综合调整减少Oracle-Less ML攻击的对抗性学习 cs.CR

Accepted at Design Automation Conference (DAC 2023)

**SubmitDate**: 2023-03-06    [abs](http://arxiv.org/abs/2303.03372v1) [paper-pdf](http://arxiv.org/pdf/2303.03372v1)

**Authors**: Animesh Basak Chowdhury, Lilas Alrahis, Luca Collini, Johann Knechtel, Ramesh Karri, Siddharth Garg, Ozgur Sinanoglu, Benjamin Tan

**Abstract**: Oracle-less machine learning (ML) attacks have broken various logic locking schemes. Regular synthesis, which is tailored for area-power-delay optimization, yields netlists where key-gate localities are vulnerable to learning. Thus, we call for security-aware logic synthesis. We propose ALMOST, a framework for adversarial learning to mitigate oracle-less ML attacks via synthesis tuning. ALMOST uses a simulated-annealing-based synthesis recipe generator, employing adversarially trained models that can predict state-of-the-art attacks' accuracies over wide ranges of recipes and key-gate localities. Experiments on ISCAS benchmarks confirm the attacks' accuracies drops to around 50\% for ALMOST-synthesized circuits, all while not undermining design optimization.

摘要: 无甲骨文机器学习(ML)攻击破坏了各种逻辑锁定方案。规则综合是为面积-功率-延迟优化量身定做的，它产生了关键门位置容易学习的网表。因此，我们呼吁安全感知逻辑综合。我们提出了一个对抗性学习框架，通过综合调整来减轻无预言机的ML攻击。几乎使用基于模拟退火法的合成配方生成器，采用反向训练的模型，可以在大范围的配方和关键门位置预测最先进的攻击精度。在ISCAS基准上的实验证实，对于几乎综合的电路，攻击的准确率下降到50%左右，而所有这些都不会破坏设计优化。



## **15. Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations**

基于最大流形容量表示的自然图像学习高效编码 cs.CV

**SubmitDate**: 2023-03-06    [abs](http://arxiv.org/abs/2303.03307v1) [paper-pdf](http://arxiv.org/pdf/2303.03307v1)

**Authors**: Thomas Yerxa, Yilun Kuang, Eero Simoncelli, SueYeon Chung

**Abstract**: Self-supervised Learning (SSL) provides a strategy for constructing useful representations of images without relying on hand-assigned labels. Many such methods aim to map distinct views of the same scene or object to nearby points in the representation space, while employing some constraint to prevent representational collapse. Here we recast the problem in terms of efficient coding by adopting manifold capacity, a measure that quantifies the quality of a representation based on the number of linearly separable object manifolds it can support, as the efficiency metric to optimize. Specifically, we adapt the manifold capacity for use as an objective function in a contrastive learning framework, yielding a Maximum Manifold Capacity Representation (MMCR). We apply this method to unlabeled images, each augmented by a set of basic transformations, and find that it learns meaningful features using the standard linear evaluation protocol. Specifically, we find that MMCRs support performance on object recognition comparable to or surpassing that of recently developed SSL frameworks, while providing more robustness to adversarial attacks. Empirical analyses reveal differences between MMCRs and representations learned by other SSL frameworks, and suggest a mechanism by which manifold compression gives rise to class separability.

摘要: 自监督学习(SSL)提供了一种构建有用的图像表示的策略，而不依赖于手动分配的标签。许多这样的方法旨在将同一场景或对象的不同视图映射到表示空间中的邻近点，同时使用一些约束来防止表示崩溃。在这里，我们通过采用流形容量来从高效编码的角度来重塑问题，流形容量是基于表示可以支持的线性可分对象流形的数量来量化表示的质量的度量，作为优化的效率度量。具体地说，我们将流形容量作为对比学习框架中的目标函数，产生最大流形容量表示(MMCR)。我们将这种方法应用于未标记的图像，每个图像都通过一组基本变换进行扩充，并发现它使用标准的线性评估协议学习有意义的特征。具体地说，我们发现MMCR在对象识别方面的性能与最近开发的SSL框架相当或超过，同时提供了更好的对抗攻击的健壮性。经验分析揭示了MMCR和其他SSL框架学习的表示之间的差异，并提出了一种流形压缩导致类可分性的机制。



## **16. A Unified Algebraic Perspective on Lipschitz Neural Networks**

关于Lipschitz神经网络的统一代数观点 cs.LG

ICLR 2023. Spotlight paper

**SubmitDate**: 2023-03-06    [abs](http://arxiv.org/abs/2303.03169v1) [paper-pdf](http://arxiv.org/pdf/2303.03169v1)

**Authors**: Alexandre Araujo, Aaron Havens, Blaise Delattre, Alexandre Allauzen, Bin Hu

**Abstract**: Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical solutions of a common semidefinite programming (SDP) condition. We also prove that AOL biases the scaled weight to the ones which are close to the set of orthogonal matrices in a certain mathematical manner. Moreover, our algebraic condition, combined with the Gershgorin circle theorem, readily leads to new and diverse parameterizations for 1-Lipschitz network layers. Our approach, called SDP-based Lipschitz Layers (SLL), allows us to design non-trivial yet efficient generalization of convex potential layers. Finally, the comprehensive set of experiments on image classification shows that SLLs outperform previous approaches on certified robust accuracy. Code is available at https://github.com/araujoalexandre/Lipschitz-SLL-Networks.

摘要: 重要的研究工作集中在设计和训练具有受控Lipschitz常数的神经网络。其目标是增加有时甚至保证对对手攻击的健壮性。最近很有前途的技术从不同的背景中获得了设计1-Lipschitz神经网络的灵感，仅举几个例子：凸势层源于连续动力系统的离散化，几乎正交层提出了一种定制的矩阵重标度方法。然而，今天，重要的是在共同的理论视角下考虑该领域最近和有希望的贡献，以更好地设计新的和改进的层。本文介绍了一种新的代数观点，统一了各种类型的1-Lipschitz神经网络，包括前面提到的那些，以及基于正交性和谱方法的方法。有趣的是，我们证明了许多现有的技术可以通过寻找一个常见的半定规划(SDP)条件的解析解来推导和推广。我们还证明了AOL以一定的数学方式将标度权重偏向于接近正交矩阵集的权重。此外，我们的代数条件与Gershgorin圆定理相结合，很容易得到新的和不同的1-Lipschitz网络层的参数。我们的方法称为基于SDP的Lipschitz层(SLL)，它允许我们设计非平凡但有效的凸势层的推广。最后，一组综合的图像分类实验表明，SLLS在证明的稳健性精度方面优于以往的方法。代码可在https://github.com/araujoalexandre/Lipschitz-SLL-Networks.上找到



## **17. On the Feasibility of Specialized Ability Stealing for Large Language Code Models**

论大型语言代码模型专业能力窃取的可行性 cs.SE

11 pages

**SubmitDate**: 2023-03-06    [abs](http://arxiv.org/abs/2303.03012v1) [paper-pdf](http://arxiv.org/pdf/2303.03012v1)

**Authors**: Zongjie Li, Chaozheng Wang, Pingchuan Ma, Chaowei Liu, Shuai Wang, Daoyuan Wu, Cuiyun Gao

**Abstract**: Recent progress in large language code models (LLCMs) has led to a dramatic surge in the use of software development. Nevertheless, it is widely known that training a well-performed LLCM requires a plethora of workforce for collecting the data and high quality annotation. Additionally, the training dataset may be proprietary (or partially open source to the public), and the training process is often conducted on a large-scale cluster of GPUs with high costs. Inspired by the recent success of imitation attacks in stealing computer vision and natural language models, this work launches the first imitation attack on LLCMs: by querying a target LLCM with carefully-designed queries and collecting the outputs, the adversary can train an imitation model that manifests close behavior with the target LLCM. We systematically investigate the effectiveness of launching imitation attacks under different query schemes and different LLCM tasks. We also design novel methods to polish the LLCM outputs, resulting in an effective imitation training process. We summarize our findings and provide lessons harvested in this study that can help better depict the attack surface of LLCMs. Our research contributes to the growing body of knowledge on imitation attacks and defenses in deep neural models, particularly in the domain of code related tasks.

摘要: 大型语言代码模型(LLCM)的最新进展导致软件开发的使用激增。然而，众所周知，培训一个表现良好的LLCM需要大量的劳动力来收集数据和高质量的注释。此外，训练数据集可能是专有的(或部分向公众开放源代码)，并且训练过程通常在成本较高的大规模GPU集群上进行。受最近成功窃取计算机视觉和自然语言模型的模仿攻击的启发，该工作对LLCM发起了第一次模仿攻击：通过使用精心设计的查询来查询目标LLCM并收集输出，对手可以训练出与目标LLCM表现出密切行为的模仿模型。系统地研究了在不同的查询方案和不同的LLCM任务下发起模仿攻击的有效性。我们还设计了新的方法来完善LLCM的输出，从而产生了一个有效的模拟训练过程。我们总结了我们的发现，并提供了在这项研究中获得的教训，有助于更好地描述LLCM的攻击面。我们的研究有助于在深层神经模型中，特别是在与代码相关的任务领域中，关于模仿攻击和防御的知识不断增长。



## **18. Adversarial Sampling for Fairness Testing in Deep Neural Network**

基于对抗性抽样的深度神经网络公平性检验 cs.LG

7 pages, 5 figures, International Journal of Advanced Computer  Science and Application

**SubmitDate**: 2023-03-06    [abs](http://arxiv.org/abs/2303.02874v1) [paper-pdf](http://arxiv.org/pdf/2303.02874v1)

**Authors**: Tosin Ige, William Marfo, Justin Tonkinson, Sikiru Adewale, Bolanle Hafiz Matti

**Abstract**: In this research, we focus on the usage of adversarial sampling to test for the fairness in the prediction of deep neural network model across different classes of image in a given dataset. While several framework had been proposed to ensure robustness of machine learning model against adversarial attack, some of which includes adversarial training algorithm. There is still the pitfall that adversarial training algorithm tends to cause disparity in accuracy and robustness among different group. Our research is aimed at using adversarial sampling to test for fairness in the prediction of deep neural network model across different classes or categories of image in a given dataset. We successfully demonstrated a new method of ensuring fairness across various group of input in deep neural network classifier. We trained our neural network model on the original image, and without training our model on the perturbed or attacked image. When we feed the adversarial samplings to our model, it was able to predict the original category/ class of the image the adversarial sample belongs to. We also introduced and used the separation of concern concept from software engineering whereby there is an additional standalone filter layer that filters perturbed image by heavily removing the noise or attack before automatically passing it to the network for classification, we were able to have accuracy of 93.3%. Cifar-10 dataset have ten categories of dataset, and so, in order to account for fairness, we applied our hypothesis across each categories of dataset and were able to get a consistent result and accuracy.

摘要: 在这项研究中，我们重点使用对抗性抽样来检验深度神经网络模型对给定数据集中不同类别图像的预测的公平性。为了保证机器学习模型对对抗性攻击的健壮性，已经提出了几种框架，其中一些框架包括对抗性训练算法。对抗性训练算法往往会导致不同群体之间在准确性和稳健性方面的差异，这一缺陷仍然存在。我们的研究目的是使用对抗性抽样来检验深度神经网络模型在给定数据集中的不同类别或类别的图像预测中的公平性。我们成功地展示了一种在深度神经网络分类器中确保各类输入公平的新方法。我们在原始图像上训练神经网络模型，而在受干扰或攻击的图像上不训练我们的模型。当我们将对抗性样本提供给我们的模型时，它能够预测对抗性样本所属的图像的原始类别/类别。我们还引入并使用了软件工程的关注点分离概念，即增加了一个独立的滤波层，通过大量去除噪声或攻击来过滤受干扰的图像，然后自动将其传递到网络进行分类，我们能够获得93.3%的准确率。CIFAR-10数据集有十个类别的数据集，因此，为了说明公平性，我们将我们的假设应用于每一类数据集，并能够获得一致的结果和准确性。



## **19. Visual Analytics of Neuron Vulnerability to Adversarial Attacks on Convolutional Neural Networks**

卷积神经网络神经元对抗攻击脆弱性的可视化分析 cs.CV

Accepted by the Special Issue on Human-Centered Explainable AI, ACM  Transactions on Interactive Intelligent Systems

**SubmitDate**: 2023-03-06    [abs](http://arxiv.org/abs/2303.02814v1) [paper-pdf](http://arxiv.org/pdf/2303.02814v1)

**Authors**: Yiran Li, Junpeng Wang, Takanori Fujiwara, Kwan-Liu Ma

**Abstract**: Adversarial attacks on a convolutional neural network (CNN) -- injecting human-imperceptible perturbations into an input image -- could fool a high-performance CNN into making incorrect predictions. The success of adversarial attacks raises serious concerns about the robustness of CNNs, and prevents them from being used in safety-critical applications, such as medical diagnosis and autonomous driving. Our work introduces a visual analytics approach to understanding adversarial attacks by answering two questions: (1) which neurons are more vulnerable to attacks and (2) which image features do these vulnerable neurons capture during the prediction? For the first question, we introduce multiple perturbation-based measures to break down the attacking magnitude into individual CNN neurons and rank the neurons by their vulnerability levels. For the second, we identify image features (e.g., cat ears) that highly stimulate a user-selected neuron to augment and validate the neuron's responsibility. Furthermore, we support an interactive exploration of a large number of neurons by aiding with hierarchical clustering based on the neurons' roles in the prediction. To this end, a visual analytics system is designed to incorporate visual reasoning for interpreting adversarial attacks. We validate the effectiveness of our system through multiple case studies as well as feedback from domain experts.

摘要: 对卷积神经网络(CNN)的对抗性攻击--在输入图像中注入人类无法察觉的扰动--可能会愚弄高性能的CNN做出错误的预测。对抗性攻击的成功引起了人们对CNN健壮性的严重担忧，并阻止了它们被用于安全关键应用，如医疗诊断和自动驾驶。我们的工作引入了一种视觉分析方法来理解对抗性攻击，通过回答两个问题：(1)哪些神经元更容易受到攻击，以及(2)这些脆弱的神经元在预测过程中捕获了哪些图像特征？对于第一个问题，我们引入了多个基于扰动的措施，将攻击的大小分解为单个CNN神经元，并根据神经元的脆弱性级别对神经元进行排序。对于第二个，我们识别图像特征(例如，猫耳)，这些图像特征高度刺激用户选择的神经元以增强和验证神经元的责任。此外，我们通过基于神经元在预测中的角色的分层聚类来支持对大量神经元的交互探索。为此，设计了一个视觉分析系统，以结合视觉推理来解释对抗性攻击。我们通过多个案例研究以及来自领域专家的反馈来验证我们的系统的有效性。



## **20. Consistent Valid Physically-Realizable Adversarial Attack against Crowd-flow Prediction Models**

针对人流预测模型的一致、有效、物理可实现的对抗性攻击 cs.LG

**SubmitDate**: 2023-03-05    [abs](http://arxiv.org/abs/2303.02669v1) [paper-pdf](http://arxiv.org/pdf/2303.02669v1)

**Authors**: Hassan Ali, Muhammad Atif Butt, Fethi Filali, Ala Al-Fuqaha, Junaid Qadir

**Abstract**: Recent works have shown that deep learning (DL) models can effectively learn city-wide crowd-flow patterns, which can be used for more effective urban planning and smart city management. However, DL models have been known to perform poorly on inconspicuous adversarial perturbations. Although many works have studied these adversarial perturbations in general, the adversarial vulnerabilities of deep crowd-flow prediction models in particular have remained largely unexplored. In this paper, we perform a rigorous analysis of the adversarial vulnerabilities of DL-based crowd-flow prediction models under multiple threat settings, making three-fold contributions. (1) We propose CaV-detect by formally identifying two novel properties - Consistency and Validity - of the crowd-flow prediction inputs that enable the detection of standard adversarial inputs with 0% false acceptance rate (FAR). (2) We leverage universal adversarial perturbations and an adaptive adversarial loss to present adaptive adversarial attacks to evade CaV-detect defense. (3) We propose CVPR, a Consistent, Valid and Physically-Realizable adversarial attack, that explicitly inducts the consistency and validity priors in the perturbation generation mechanism. We find out that although the crowd-flow models are vulnerable to adversarial perturbations, it is extremely challenging to simulate these perturbations in physical settings, notably when CaV-detect is in place. We also show that CVPR attack considerably outperforms the adaptively modified standard attacks in FAR and adversarial loss metrics. We conclude with useful insights emerging from our work and highlight promising future research directions.

摘要: 最近的研究表明，深度学习模型可以有效地学习城市范围内的人群流动模式，可以用于更有效的城市规划和智能城市管理。然而，众所周知，在不明显的对抗性扰动下，DL模型的表现很差。虽然已有许多工作对这些对抗性扰动进行了总体研究，但特别是深度人群流预测模型的对抗性脆弱性在很大程度上仍未被探索。本文对基于动态链接库的人群流预测模型在多种威胁环境下的对抗脆弱性进行了严格的分析，取得了三方面的贡献。(1)通过形式化地识别人群流预测输入的一致性和有效性这两个新的性质，我们提出了CAV-DETECT，这两个属性使得对标准对抗性输入的检测具有0%的错误接受率(FAR)。(2)利用普遍的对抗性扰动和自适应对抗性损失来提出自适应对抗性攻击，以逃避CAV检测防御。(3)提出了一种在扰动生成机制中显式引入一致性先验和有效性先验的一致、有效、物理可实现的敌意攻击方法CVPR。我们发现，尽管人群流模型容易受到对抗性扰动的影响，但在物理环境中模拟这些扰动是非常具有挑战性的，特别是在CAV-Detect到位的情况下。我们还表明，CVPR攻击在FAR和对抗性损失指标上明显优于自适应修改的标准攻击。我们以我们工作中出现的有用的见解结束，并强调了有前途的未来研究方向。



## **21. Cyber Vaccine for Deepfake Immunity**

用于深伪免疫的网络疫苗 cs.CR

**SubmitDate**: 2023-03-05    [abs](http://arxiv.org/abs/2303.02659v1) [paper-pdf](http://arxiv.org/pdf/2303.02659v1)

**Authors**: Ching-Chun Chang, Huy Hong Nguyen, Junichi Yamagishi, Isao Echizen

**Abstract**: Deepfakes pose an evolving threat to cybersecurity, which calls for the development of automated countermeasures. While considerable forensic research has been devoted to the detection and localisation of deepfakes, solutions for reversing fake to real are yet to be developed. In this study, we introduce cyber vaccination for conferring immunity to deepfakes. Analogous to biological vaccination that injects antigens to induce immunity prior to infection by an actual pathogen, cyber vaccination simulates deepfakes and performs adversarial training to build a defensive immune system. Aiming at building up attack-agnostic immunity with limited computational resources, we propose to simulate various deepfakes with one single overpowered attack: face masking. The proposed immune system consists of a vaccinator for inducing immunity and a neutraliser for recovering facial content. Experimental evaluations demonstrate effective immunity to face replacement, face reenactment and various types of corruptions.

摘要: Deepfake对网络安全构成了不断演变的威胁，这要求开发自动对策。虽然已经有相当多的法医研究致力于深度假货的检测和定位，但尚未开发出将假货反转为真品的解决方案。在这项研究中，我们引入了网络疫苗来增强对深度假货的免疫力。类似于在被实际病原体感染之前注射抗原以诱导免疫的生物疫苗，网络疫苗模拟深度假冒并执行对抗性训练以建立防御性免疫系统。为了在有限的计算资源下建立攻击不可知的免疫力，我们提出了用一个强大的攻击：人脸掩蔽来模拟各种深度伪装。拟议的免疫系统包括一个用于诱导免疫的疫苗接种装置和一个用于恢复面部内容的中和器。实验评估表明，对人脸替换、人脸重演和各种类型的腐败具有有效的免疫力。



## **22. Improved Robustness Against Adaptive Attacks With Ensembles and Error-Correcting Output Codes**

利用集成和纠错输出码提高了对自适应攻击的稳健性 cs.LG

**SubmitDate**: 2023-03-04    [abs](http://arxiv.org/abs/2303.02322v1) [paper-pdf](http://arxiv.org/pdf/2303.02322v1)

**Authors**: Thomas Philippon, Christian Gagné

**Abstract**: Neural network ensembles have been studied extensively in the context of adversarial robustness and most ensemble-based approaches remain vulnerable to adaptive attacks. In this paper, we investigate the robustness of Error-Correcting Output Codes (ECOC) ensembles through architectural improvements and ensemble diversity promotion. We perform a comprehensive robustness assessment against adaptive attacks and investigate the relationship between ensemble diversity and robustness. Our results demonstrate the benefits of ECOC ensembles for adversarial robustness compared to regular ensembles of convolutional neural networks (CNNs) and show why the robustness of previous implementations is limited. We also propose an adversarial training method specific to ECOC ensembles that allows to further improve robustness to adaptive attacks.

摘要: 神经网络集成已经在对抗性鲁棒性的背景下进行了广泛的研究，并且大多数基于集成的方法仍然容易受到自适应攻击。本文从结构改进和集成多样性提升两个方面研究了纠错输出码集成的稳健性。我们对自适应攻击进行了全面的稳健性评估，并研究了集成多样性和稳健性之间的关系。我们的结果证明了与卷积神经网络(CNN)的常规集成相比，ECOC集成在对抗健壮性方面的优势，并说明了为什么以前的实现的健壮性是有限的。我们还提出了一种针对ECOC集成的对抗性训练方法，允许进一步提高对自适应攻击的健壮性。



## **23. Certified Robust Neural Networks: Generalization and Corruption Resistance**

认证的稳健神经网络：泛化和抗腐蚀性 stat.ML

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2303.02251v1) [paper-pdf](http://arxiv.org/pdf/2303.02251v1)

**Authors**: Amine Bennouna, Ryan Lucas, Bart Van Parys

**Abstract**: Adversarial training aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training of neural networks despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar ``robust overfitting'' phenomenon. Subsequently, we advance a novel loss function which we show both theoretically as well as empirically to enjoy a certified level of robustness against data evasion and poisoning attacks while ensuring guaranteed generalization. We indicate through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance in terms of adversarial error loss. Finally, we indicate that HR training can be interpreted as a direct extension of adversarial training and comes with a negligible additional computational burden.

摘要: 对抗性训练旨在降低现代神经网络对小数据扰动的问题敏感性。令人惊讶的是，过度适应是神经网络对抗性训练中的一个主要问题，尽管在标准训练中大多没有。我们在这里为这一特殊的“稳健过拟合”现象提供了理论证据。随后，我们提出了一种新的损失函数，我们在理论上和经验上都证明了它在保证泛化的同时，对数据逃避和中毒攻击具有经过认证的健壮性。我们通过仔细的数值实验表明，我们产生的整体稳健(HR)训练过程在对手错误损失方面产生了SOTA性能。最后，我们指出，HR训练可以被解释为对抗性训练的直接扩展，并且伴随着可以忽略的额外计算负担。



## **24. Adversarial Attacks on Machine Learning in Embedded and IoT Platforms**

嵌入式和物联网平台中机器学习的对抗性攻击 cs.LG

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2303.02214v1) [paper-pdf](http://arxiv.org/pdf/2303.02214v1)

**Authors**: Christian Westbrook, Sudeep Pasricha

**Abstract**: Machine learning (ML) algorithms are increasingly being integrated into embedded and IoT systems that surround us, and they are vulnerable to adversarial attacks. The deployment of these ML algorithms on resource-limited embedded platforms also requires the use of model compression techniques. The impact of such model compression techniques on adversarial robustness in ML is an important and emerging area of research. This article provides an overview of the landscape of adversarial attacks and ML model compression techniques relevant to embedded systems. We then describe efforts that seek to understand the relationship between adversarial attacks and ML model compression before discussing open problems in this area.

摘要: 机器学习(ML)算法越来越多地集成到我们周围的嵌入式和物联网系统中，它们很容易受到对手的攻击。在资源受限的嵌入式平台上部署这些ML算法还需要使用模型压缩技术。这种模型压缩技术对ML中对手健壮性的影响是一个重要的新兴研究领域。本文概述了与嵌入式系统相关的敌意攻击和ML模型压缩技术。然后，我们描述在讨论这一领域的公开问题之前，试图理解敌意攻击和ML模型压缩之间的关系的努力。



## **25. Towards Adversarial Realism and Robust Learning for IoT Intrusion Detection and Classification**

物联网入侵检测与分类的对抗性真实感和稳健学习 cs.CR

19 pages, 5 tables, 7 figures, Annals of Telecommunications journal

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2301.13122v3) [paper-pdf](http://arxiv.org/pdf/2301.13122v3)

**Authors**: João Vitorino, Isabel Praça, Eva Maia

**Abstract**: The Internet of Things (IoT) faces tremendous security challenges. Machine learning models can be used to tackle the growing number of cyber-attack variations targeting IoT systems, but the increasing threat posed by adversarial attacks restates the need for reliable defense strategies. This work describes the types of constraints required for a realistic adversarial cyber-attack example and proposes a methodology for a trustworthy adversarial robustness analysis with a realistic adversarial evasion attack vector. The proposed methodology was used to evaluate three supervised algorithms, Random Forest (RF), Extreme Gradient Boosting (XGB), and Light Gradient Boosting Machine (LGBM), and one unsupervised algorithm, Isolation Forest (IFOR). Constrained adversarial examples were generated with the Adaptative Perturbation Pattern Method (A2PM), and evasion attacks were performed against models created with regular and adversarial training. Even though RF was the least affected in binary classification, XGB consistently achieved the highest accuracy in multi-class classification. The obtained results evidence the inherent susceptibility of tree-based algorithms and ensembles to adversarial evasion attacks and demonstrates the benefits of adversarial training and a security by design approach for a more robust IoT network intrusion detection and cyber-attack classification.

摘要: 物联网(IoT)面临巨大的安全挑战。机器学习模型可以用来应对越来越多的针对物联网系统的网络攻击变体，但对抗性攻击构成的日益增长的威胁重申了对可靠防御策略的需求。这项工作描述了一个现实的对抗性网络攻击实例所需的约束类型，并提出了一种使用现实的对抗性逃避攻击向量进行可信对抗性健壮性分析的方法。使用该方法对随机森林(RF)、极端梯度增强(XGB)和光梯度增强机器(LGBM)三种有监督算法和一种无监督算法隔离森林(IFOR)进行了评估。使用自适应扰动模式方法(A2PM)生成受限对抗性样本，并对通过常规和对抗性训练创建的模型进行逃避攻击。尽管RF在二进制分类中受影响最小，但XGB在多类分类中始终达到最高的准确率。得到的结果证明了基于树的算法和集成对对抗性逃避攻击的固有敏感性，并通过设计方法证明了对抗性训练和安全性的好处，从而实现了更健壮的物联网网络入侵检测和网络攻击分类。



## **26. Fool SHAP with Stealthily Biased Sampling**

偷偷有偏抽样的愚弄Shap cs.LG

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2205.15419v3) [paper-pdf](http://arxiv.org/pdf/2205.15419v3)

**Authors**: Gabriel Laberge, Ulrich Aïvodji, Satoshi Hara, Mario Marchand., Foutse Khomh

**Abstract**: SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups while remaining undetected. More precisely, experiments performed on real-world datasets showed that our attack could yield up to a 90\% relative decrease in amplitude of the sensitive feature attribution. These results highlight the manipulability of SHAP explanations and encourage auditors to treat them with skepticism.

摘要: Shap解释旨在确定在特定输入与背景分布下，哪些特征对模型预测的差异贡献最大。最近的研究表明，它们可以被恶意攻击者操纵，以产生任意想要的解释。然而，现有的攻击仅仅集中在改变黑盒模型本身。在本文中，我们提出了一类互补的攻击，这些攻击保持模型不变，并通过对用于近似预期的背景分布的数据点的秘密有偏采样来操纵Shap解释。在公平审计的背景下，我们证明了我们的攻击可以在解释组之间结果差异时降低敏感特征的重要性，同时保持未被检测到。更准确地说，在真实数据集上进行的实验表明，我们的攻击可以产生高达90%的敏感特征属性幅度的相对下降。这些结果突显了Shap解释的可操作性，并鼓励审计师以怀疑的态度对待它们。



## **27. Multi-Agent Adversarial Training Using Diffusion Learning**

基于扩散学习的多智能体对抗训练 cs.LG

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2303.01936v1) [paper-pdf](http://arxiv.org/pdf/2303.01936v1)

**Authors**: Ying Cao, Elsa Rizk, Stefan Vlaski, Ali H. Sayed

**Abstract**: This work focuses on adversarial learning over graphs. We propose a general adversarial training framework for multi-agent systems using diffusion learning. We analyze the convergence properties of the proposed scheme for convex optimization problems, and illustrate its enhanced robustness to adversarial attacks.

摘要: 这项工作集中在图上的对抗性学习。提出了一种基于扩散学习的多智能体系统对抗性训练框架。分析了该方案在求解凸优化问题时的收敛性质，并证明了该方案具有较强的抗敌意攻击能力。



## **28. Visually Adversarial Attacks and Defenses in the physical world: A Survey**

物理世界中的视觉对抗性攻击和防御：综述 cs.CV

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2211.01671v2) [paper-pdf](http://arxiv.org/pdf/2211.01671v2)

**Authors**: Xingxing Wei, Bangzheng Pu, Jiefan Lu, Baoyuan Wu

**Abstract**: Although Deep Neural Networks (DNNs) have been widely applied in various real-world scenarios, they are vulnerable to adversarial examples. The current adversarial attacks in computer vision can be divided into digital attacks and physical attacks according to their different attack forms. Compared with digital attacks, which generate perturbations in the digital pixels, physical attacks are more practical in the real world. Owing to the serious security problem caused by physically adversarial examples, many works have been proposed to evaluate the physically adversarial robustness of DNNs in the past years. In this paper, we summarize a survey versus the current physically adversarial attacks and physically adversarial defenses in computer vision. To establish a taxonomy, we organize the current physical attacks from attack tasks, attack forms, and attack methods, respectively. Thus, readers can have a systematic knowledge of this topic from different aspects. For the physical defenses, we establish the taxonomy from pre-processing, in-processing, and post-processing for the DNN models to achieve full coverage of the adversarial defenses. Based on the above survey, we finally discuss the challenges of this research field and further outlook on the future direction.

摘要: 尽管深度神经网络(DNN)已被广泛应用于各种现实场景中，但它们很容易受到对手例子的影响。根据攻击形式的不同，目前计算机视觉中的对抗性攻击可分为数字攻击和物理攻击。与在数字像素中产生扰动的数字攻击相比，物理攻击在现实世界中更实用。由于物理对抗实例带来了严重的安全问题，在过去的几年里，人们已经提出了许多工作来评估DNN的物理对抗健壮性。本文对当前计算机视觉中的身体对抗攻击和身体对抗防御进行了综述。为了建立分类，我们分别从攻击任务、攻击形式和攻击方法三个方面对当前的物理攻击进行了组织。因此，读者可以从不同的方面对这一主题有一个系统的了解。对于物理防御，我们从DNN模型的前处理、内处理和后处理三个方面建立了分类，以实现对抗性防御的全覆盖。在上述调查的基础上，我们最后讨论了该研究领域面临的挑战和对未来方向的进一步展望。



## **29. NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning**

NCL：基于噪声增强对比学习的文本后门防御 cs.CR

6 pages, 5 figures. To appear in ICASSP 2023

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2303.01742v1) [paper-pdf](http://arxiv.org/pdf/2303.01742v1)

**Authors**: Shengfang Zhai, Qingni Shen, Xiaoyi Chen, Weilong Wang, Cong Li, Yuejian Fang, Zhonghai Wu

**Abstract**: At present, backdoor attacks attract attention as they do great harm to deep learning models. The adversary poisons the training data making the model being injected with a backdoor after being trained unconsciously by victims using the poisoned dataset. In the field of text, however, existing works do not provide sufficient defense against backdoor attacks. In this paper, we propose a Noise-augmented Contrastive Learning (NCL) framework to defend against textual backdoor attacks when training models with untrustworthy data. With the aim of mitigating the mapping between triggers and the target label, we add appropriate noise perturbing possible backdoor triggers, augment the training dataset, and then pull homology samples in the feature space utilizing contrastive learning objective. Experiments demonstrate the effectiveness of our method in defending three types of textual backdoor attacks, outperforming the prior works.

摘要: 目前，后门攻击因对深度学习模型危害较大而备受关注。敌手毒化训练数据，使模型在受害者使用有毒数据集无意识地训练后被注入后门。然而，在文本领域，现有的作品不能提供足够的防御后门攻击。本文提出了一种噪声增强的对比学习(NCL)框架，用于在训练含有不可信数据的模型时抵御文本后门攻击。为了缓解触发器和目标标签之间的映射，我们在可能的后门触发器中添加适当的噪声扰动，扩大训练数据集，然后利用对比学习目标在特征空间中拉取同源样本。实验证明，该方法对三种类型的文本后门攻击具有较好的防御效果，优于以往的工作。



## **30. AdvART: Adversarial Art for Camouflaged Object Detection Attacks**

AdvART：伪装目标检测攻击的对抗艺术 cs.CV

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2303.01734v1) [paper-pdf](http://arxiv.org/pdf/2303.01734v1)

**Authors**: Amira Guesmi, Ioan Marius Bilasco, Muhammad Shafique, Ihsen Alouani

**Abstract**: A majority of existing physical attacks in the real world result in conspicuous and eye-catching patterns for generated patches, which made them identifiable/detectable by humans. To overcome this limitation, recent work has proposed several approaches that aim at generating naturalistic patches using generative adversarial networks (GANs), which may not catch human's attention. However, these approaches are computationally intensive and do not always converge to natural looking patterns. In this paper, we propose a novel lightweight framework that systematically generates naturalistic adversarial patches without using GANs. To illustrate the proposed approach, we generate adversarial art (AdvART), which are patches generated to look like artistic paintings while maintaining high attack efficiency. In fact, we redefine the optimization problem by introducing a new similarity objective. Specifically, we leverage similarity metrics to construct a similarity loss that is added to the optimized objective function. This component guides the patch to follow a predefined artistic patterns while maximizing the victim model's loss function. Our patch achieves high success rates with $12.53\%$ mean average precision (mAP) on YOLOv4tiny for INRIA dataset.

摘要: 现实世界中现有的大多数物理攻击都会导致生成的补丁具有明显和醒目的模式，这使得它们可以被人类识别/检测。为了克服这一局限性，最近的工作提出了几种方法，旨在使用生成性对手网络(GANS)来生成自然主义斑块，这可能不会引起人类的注意。然而，这些方法是计算密集型的，并且不总是收敛到看起来自然的模式。在本文中，我们提出了一个新的轻量级框架，它系统地生成自然主义的对抗性补丁，而不使用GANS。为了说明所提出的方法，我们生成了对抗性ART(AdvART)，它是在保持高攻击效率的同时生成的看起来像艺术绘画的补丁。实际上，我们通过引入一个新的相似性目标来重新定义优化问题。具体地说，我们利用相似性度量来构建相似性损失，并将其添加到优化的目标函数中。该组件引导补丁遵循预定义的艺术模式，同时最大化受害者模型的损失函数。我们的补丁在YOLOv4TINY for INRIA数据集上取得了很高的成功率，平均精度(MAP)为12.53美元。



## **31. Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning**

数据增强在对抗性对比学习中作用的再思考 cs.LG

ICLR 2023

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2303.01289v2) [paper-pdf](http://arxiv.org/pdf/2303.01289v2)

**Authors**: Rundong Luo, Yifei Wang, Yisen Wang

**Abstract**: Recent works have shown that self-supervised learning can achieve remarkable robustness when integrated with adversarial training (AT). However, the robustness gap between supervised AT (sup-AT) and self-supervised AT (self-AT) remains significant. Motivated by this observation, we revisit existing self-AT methods and discover an inherent dilemma that affects self-AT robustness: either strong or weak data augmentations are harmful to self-AT, and a medium strength is insufficient to bridge the gap. To resolve this dilemma, we propose a simple remedy named DYNACL (Dynamic Adversarial Contrastive Learning). In particular, we propose an augmentation schedule that gradually anneals from a strong augmentation to a weak one to benefit from both extreme cases. Besides, we adopt a fast post-processing stage for adapting it to downstream tasks. Through extensive experiments, we show that DYNACL can improve state-of-the-art self-AT robustness by 8.84% under Auto-Attack on the CIFAR-10 dataset, and can even outperform vanilla supervised adversarial training for the first time. Our code is available at \url{https://github.com/PKU-ML/DYNACL}.

摘要: 最近的研究表明，当自监督学习与对抗性训练(AT)相结合时，可以获得显著的鲁棒性。然而，监督AT(sup-AT)和自我监督AT(self-AT)之间的稳健性差距仍然很大。在这一观察的基础上，我们重新审视了现有的自我测试方法，并发现了影响自我测试稳健性的内在困境：无论是强的或弱的数据增加都对自我测试有害，中等强度不足以弥合差距。为了解决这一困境，我们提出了一种简单的补救方法--动态对抗性对比学习(DYNACL)。特别地，我们提出了一个从强增强逐渐退火到弱增强的增强调度，以从这两种极端情况中受益。此外，我们还采用了快速后处理阶段，使其适应于下游任务。通过大量的实验表明，在CIFAR-10数据集上的自动攻击下，DYNACL算法的健壮性提高了8.84%，甚至首次超过了普通监督的对抗性训练。我们的代码可在\url{https://github.com/PKU-ML/DYNACL}.



## **32. Semantic-Preserving Adversarial Text Attacks**

保留语义的对抗性文本攻击 cs.CL

12 pages, 3 figures, 10 tables

**SubmitDate**: 2023-03-03    [abs](http://arxiv.org/abs/2108.10015v2) [paper-pdf](http://arxiv.org/pdf/2108.10015v2)

**Authors**: Xinghao Yang, Weifeng Liu, James Bailey, Dacheng Tao, Wei Liu

**Abstract**: Deep neural networks (DNNs) are known to be vulnerable to adversarial images, while their robustness in text classification is rarely studied. Several lines of text attack methods have been proposed in the literature, including character-level, word-level, and sentence-level attacks. However, it is still a challenge to minimize the number of word changes necessary to induce misclassification, while simultaneously ensuring lexical correctness, syntactic soundness, and semantic similarity. In this paper, we propose a Bigram and Unigram based adaptive Semantic Preservation Optimization (BU-SPO) method to examine the vulnerability of deep models. Our method has four major merits. Firstly, we propose to attack text documents not only at the unigram word level but also at the bigram level which better keeps semantics and avoids producing meaningless outputs. Secondly, we propose a hybrid method to replace the input words with options among both their synonyms candidates and sememe candidates, which greatly enriches the potential substitutions compared to only using synonyms. Thirdly, we design an optimization algorithm, i.e., Semantic Preservation Optimization (SPO), to determine the priority of word replacements, aiming to reduce the modification cost. Finally, we further improve the SPO with a semantic Filter (named SPOF) to find the adversarial example with the highest semantic similarity. We evaluate the effectiveness of our BU-SPO and BU-SPOF on IMDB, AG's News, and Yahoo! Answers text datasets by attacking four popular DNNs models. Results show that our methods achieve the highest attack success rates and semantics rates by changing the smallest number of words compared with existing methods.

摘要: 深度神经网络(DNN)容易受到敌意图像的影响，而其在文本分类中的稳健性却鲜有研究。在文献中已经提出了几种文本攻击方法，包括字符级别、单词级别和句子级别的攻击。然而，在确保词汇正确性、句法可靠性和语义相似性的同时，将导致错误分类所需的单词更改数量降至最低仍然是一个挑战。本文提出了一种基于Bigram和Unigram的自适应语义保存优化方法(BU-SPO)来检测深层模型的脆弱性。我们的方法有四个主要优点。首先，我们提出了不仅在单文法单词层面上攻击文本文档，而且在二元文法层面上攻击文本文档，这样更好地保持了语义，避免了产生无意义的输出。其次，我们提出了一种在同义词候选词和义素候选词之间选择替换输入词的混合方法，与只使用同义词相比，极大地丰富了潜在的替换。再次，我们设计了一种优化算法，即语义保留优化算法(SPO)来确定词语替换的优先级，以降低修改成本。最后，我们使用语义过滤器(称为SPOF)对SPO进行了进一步的改进，以找到语义相似度最高的对抗性实例。我们评估了我们的BU-SPO和BU-SPOF在IMDB、AG‘s News和Yahoo！通过攻击四个流行的DNN模型来回答文本数据集。结果表明，与现有方法相比，通过改变最少的词数，我们的方法获得了最高的攻击成功率和语义率。



## **33. Certified Randomness from Quantum Supremacy**

来自量子至上的认证随机性 quant-ph

84 pages

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2303.01625v1) [paper-pdf](http://arxiv.org/pdf/2303.01625v1)

**Authors**: Scott Aaronson, Shih-Han Hung

**Abstract**: We propose an application for near-term quantum devices: namely, generating cryptographically certified random bits, to use (for example) in proof-of-stake cryptocurrencies. Our protocol repurposes the existing "quantum supremacy" experiments, based on random circuit sampling, that Google and USTC have successfully carried out starting in 2019. We show that, whenever the outputs of these experiments pass the now-standard Linear Cross-Entropy Benchmark (LXEB), under plausible hardness assumptions they necessarily contain $\Omega(n)$ min-entropy, where $n$ is the number of qubits. To achieve a net gain in randomness, we use a small random seed to produce pseudorandom challenge circuits. In response to the challenge circuits, the quantum computer generates output strings that, after verification, can then be fed into a randomness extractor to produce certified nearly-uniform bits -- thereby "bootstrapping" from pseudorandomness to genuine randomness. We prove our protocol sound in two senses: (i) under a hardness assumption called Long List Quantum Supremacy Verification, which we justify in the random oracle model, and (ii) unconditionally in the random oracle model against an eavesdropper who could share arbitrary entanglement with the device. (Note that our protocol's output is unpredictable even to a computationally unbounded adversary who can see the random oracle.) Currently, the central drawback of our protocol is the exponential cost of verification, which in practice will limit its implementation to at most $n\sim 60$ qubits, a regime where attacks are expensive but not impossible. Modulo that drawback, our protocol appears to be the only practical application of quantum computing that both requires a QC and is physically realizable today.

摘要: 我们提出了一种用于近期量子设备的应用：即，生成经密码认证的随机比特，以用于(例如)股权证明加密货币。我们的协议改变了现有的基于随机电路抽样的“量子至上”实验的用途，谷歌和科大已经从2019年开始成功地进行了这些实验。我们证明，当这些实验的输出通过现在标准的线性交叉熵基准(LXEB)时，在看似合理的硬度假设下，它们必然包含$Omega(N)$min-熵，其中$n$是量子比特的数量。为了实现随机性的净收益，我们使用一个小的随机种子来产生伪随机挑战电路。作为对挑战电路的响应，量子计算机产生输出串，经过验证后，这些输出串可以被送入随机性抽取器，以产生经过认证的几乎一致的比特--从而从伪随机性变成真正的随机性。我们在两种意义上证明了我们的协议是安全的：(I)在称为长列表量子至上验证的困难假设下，我们在随机预言模型中证明了这一点；(Ii)在随机预言模型中无条件地对抗可能与设备共享任意纠缠的窃听者。(请注意，我们的协议的输出是不可预测的，即使对于可以看到随机预言的计算无限的对手也是如此。)目前，我们的协议的主要缺陷是验证的指数成本，这将在实践中将其实施限制在最多$n\sim 60美元的量子比特，这是一个攻击代价高昂但并不是不可能的制度。模这个缺点，我们的协议似乎是量子计算的唯一实际应用，既需要QC，也是今天物理上可以实现的。



## **34. APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation**

APARATE：基于CNN的自主导航单眼深度估计的自适应对抗性补丁 cs.CV

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2303.01351v1) [paper-pdf](http://arxiv.org/pdf/2303.01351v1)

**Authors**: Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique

**Abstract**: In recent years, monocular depth estimation (MDE) has witnessed a substantial performance improvement due to convolutional neural networks (CNNs). However, CNNs are vulnerable to adversarial attacks, which pose serious concerns for safety-critical and security-sensitive systems. Specifically, adversarial attacks can have catastrophic impact on MDE given its importance for scene understanding in applications like autonomous driving and robotic navigation. To physically assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE. However, these methods are not powerful enough to fully fool the vision system in a systemically threatening manner. In fact, their impact is partial and locally limited; they mislead the depth prediction of only the overlapping region with the input image regardless of the target object size, shape and location. In this paper, we investigate MDE vulnerability to adversarial patches in a more comprehensive manner. We propose a novel adaptive adversarial patch (APARATE) that is able to selectively jeopardize MDE by either corrupting the estimated distance, or simply manifesting an object as disappeared for the autonomous system. Specifically, APARATE is optimized to be shape and scale-aware, and its impact adapts to the target object instead of being limited to the immediate neighborhood. Our proposed patch achieves more than $14~meters$ mean depth estimation error, with $99\%$ of the target region being affected. We believe this work highlights the threat of adversarial attacks in the context of MDE, and we hope it would alert the community to the real-life potential harm of this attack and motivate investigating more robust and adaptive defenses for autonomous robots.

摘要: 近年来，由于卷积神经网络(CNN)的引入，单目深度估计(MDE)的性能有了很大的提高。然而，CNN很容易受到敌意攻击，这对安全关键和安全敏感的系统构成了严重的担忧。具体地说，对抗性攻击可能会对MDE产生灾难性的影响，因为它对于自动驾驶和机器人导航等应用中的场景理解非常重要。为了物理评估基于CNN的深度预测方法的脆弱性，最近的工作试图设计对抗MDE的对抗性补丁。然而，这些方法还不够强大，不足以以具有系统性威胁的方式完全愚弄视觉系统。事实上，它们的影响是局部的和局部有限的；它们误导了仅与输入图像重叠的区域的深度预测，而不考虑目标对象的大小、形状和位置。在本文中，我们更全面地研究了MDE对敌意补丁的脆弱性。我们提出了一种新的自适应对抗补丁(APARATE)，它能够通过破坏估计距离或简单地将自治系统中的对象显示为消失来选择性地危害MDE。具体地说，APARATE被优化为形状和比例感知，其影响适应目标对象，而不是局限于紧邻的邻居。我们提出的补丁获得了超过$14~m$的平均深度估计误差，目标区域的$99\$受到影响。我们相信这项工作突出了MDE背景下对抗性攻击的威胁，我们希望它能提醒社区注意这种攻击的现实潜在危害，并激励研究更健壮和自适应的自主机器人防御措施。



## **35. AdvRain: Adversarial Raindrops to Attack Camera-based Smart Vision Systems**

AdvRain：敌意雨滴攻击基于摄像头的智能视觉系统 cs.CV

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2303.01338v1) [paper-pdf](http://arxiv.org/pdf/2303.01338v1)

**Authors**: Amira Guesmi, Muhammad Abdullah Hanif, Muhammad Shafique

**Abstract**: Vision-based perception modules are increasingly deployed in many applications, especially autonomous vehicles and intelligent robots. These modules are being used to acquire information about the surroundings and identify obstacles. Hence, accurate detection and classification are essential to reach appropriate decisions and take appropriate and safe actions at all times. Current studies have demonstrated that "printed adversarial attacks", known as physical adversarial attacks, can successfully mislead perception models such as object detectors and image classifiers. However, most of these physical attacks are based on noticeable and eye-catching patterns for generated perturbations making them identifiable/detectable by human eye or in test drives. In this paper, we propose a camera-based inconspicuous adversarial attack (\textbf{AdvRain}) capable of fooling camera-based perception systems over all objects of the same class. Unlike mask based fake-weather attacks that require access to the underlying computing hardware or image memory, our attack is based on emulating the effects of a natural weather condition (i.e., Raindrops) that can be printed on a translucent sticker, which is externally placed over the lens of a camera. To accomplish this, we provide an iterative process based on performing a random search aiming to identify critical positions to make sure that the performed transformation is adversarial for a target classifier. Our transformation is based on blurring predefined parts of the captured image corresponding to the areas covered by the raindrop. We achieve a drop in average model accuracy of more than $45\%$ and $40\%$ on VGG19 for ImageNet and Resnet34 for Caltech-101, respectively, using only $20$ raindrops.

摘要: 基于视觉的感知模块越来越多地部署在许多应用中，特别是自动驾驶汽车和智能机器人。这些模块被用来获取关于周围环境的信息和识别障碍物。因此，准确的检测和分类对于作出适当的决定并始终采取适当和安全的行动至关重要。目前的研究表明，被称为物理对抗性攻击的“印刷对抗性攻击”可以成功地误导对象检测器和图像分类器等感知模型。然而，大多数这些物理攻击都是基于所产生的扰动的明显和醒目的模式，使得它们可以被人眼或试驾识别/检测。在这篇文章中，我们提出了一种基于摄像机的隐蔽敌意攻击(Textbf{AdvRain})，能够在同一类对象上欺骗基于摄像机的感知系统。与需要访问底层计算硬件或图像内存的基于面具的假天气攻击不同，我们的攻击基于模拟自然天气条件(即雨滴)的影响，可以将其打印在半透明贴纸上，该贴纸外部放置在相机的镜头上。为了实现这一点，我们提供了一种迭代过程，该过程基于执行旨在识别关键位置的随机搜索，以确保所执行的变换对目标分类器是对抗性的。我们的变换是基于模糊与雨滴覆盖的区域相对应的捕获图像的预定义部分。在VGG19(ImageNet)和Resnet34(Caltech-101)上，仅使用$20$雨滴，我们的平均模型精度分别下降了$45$和$40$。



## **36. The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training**

对话必须继续：通过生成性自我训练提高视觉对话 cs.CV

CVPR 2023

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2205.12502v2) [paper-pdf](http://arxiv.org/pdf/2205.12502v2)

**Authors**: Gi-Cheon Kang, Sungdong Kim, Jin-Hwa Kim, Donghyun Kwak, Byoung-Tak Zhang

**Abstract**: Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre-training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually-grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the images via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original VisDial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For robust training of the synthetic dialogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on VisDial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual adversarial attacks. Finally, GST yields strong performance gains in the low-data regime. Code is available at https://github.com/gicheonkang/gst-visdial.

摘要: 可视化对话(VisDial)是一种使用对话历史作为上下文，回答基于图像的一系列问题的任务。先前的工作通过监督学习或利用相关视觉和语言数据集的预培训，仅就VisDial数据对对话代理进行了培训。本文提出了一种基于视觉的对话的半监督学习方法，称为生成性自我训练(GST)，以利用Web上未标记的图像。具体地说，GST首先通过非分布检测来检索域内图像，并通过多模式条件文本生成来生成关于图像的合成对话。GST然后根据合成的和原始的VisDial数据对对话代理进行培训。因此，GST将训练数据量扩展到VisDial的数量级(从120万到1290万QA数据)。为了对合成对话进行稳健的训练，我们还提出了基于困惑的数据选择和多模式一致性正则化。在VisDial v1.0和v0.9数据集上的评估表明，GST在这两个数据集上都取得了新的最先进的结果。我们进一步观察了GST对视觉和文本攻击的稳健性。最后，商品及服务税在低数据区域产生了强劲的性能提升。代码可在https://github.com/gicheonkang/gst-visdial.上找到



## **37. Targeted Adversarial Attacks against Neural Machine Translation**

针对神经机器翻译的针对性对抗性攻击 cs.CL

ICASSP 2023, Code available at:  http://github.com/sssadrizadeh/NMT-targeted-attack

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2303.01068v1) [paper-pdf](http://arxiv.org/pdf/2303.01068v1)

**Authors**: Sahar Sadrizadeh, AmirHossein Dabiri Aghdam, Ljiljana Dolamic, Pascal Frossard

**Abstract**: Neural Machine Translation (NMT) systems are used in various applications. However, it has been shown that they are vulnerable to very small perturbations of their inputs, known as adversarial attacks. In this paper, we propose a new targeted adversarial attack against NMT models. In particular, our goal is to insert a predefined target keyword into the translation of the adversarial sentence while maintaining similarity between the original sentence and the perturbed one in the source domain. To this aim, we propose an optimization problem, including an adversarial loss term and a similarity term. We use gradient projection in the embedding space to craft an adversarial sentence. Experimental results show that our attack outperforms Seq2Sick, the other targeted adversarial attack against NMT models, in terms of success rate and decrease in translation quality. Our attack succeeds in inserting a keyword into the translation for more than 75% of sentences while similarity with the original sentence stays preserved.

摘要: 神经机器翻译(NMT)系统被用于各种应用。然而，已经表明，它们很容易受到其输入的非常小的扰动，即所谓的对抗性攻击。本文提出了一种新的针对NMT模型的定向对抗性攻击。特别是，我们的目标是在对抗性句子的翻译中插入一个预定义的目标关键词，同时保持源域中原始句子和扰动句子之间的相似性。为此，我们提出了一个包含对抗性损失项和相似项的优化问题。我们使用嵌入空间中的梯度投影来构造对抗性句子。实验结果表明，我们的攻击在成功率和翻译质量方面都优于另一种针对NMT模型的对抗性攻击Seq2Sick。我们的攻击成功地在75%以上的句子中插入了一个关键字，同时保持了与原始句子的相似性。



## **38. Defending against Adversarial Audio via Diffusion Model**

利用扩散模型防御恶意音频 cs.SD

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2303.01507v1) [paper-pdf](http://arxiv.org/pdf/2303.01507v1)

**Authors**: Shutong Wu, Jiongxiao Wang, Wei Ping, Weili Nie, Chaowei Xiao

**Abstract**: Deep learning models have been widely used in commercial acoustic systems in recent years. However, adversarial audio examples can cause abnormal behaviors for those acoustic systems, while being hard for humans to perceive. Various methods, such as transformation-based defenses and adversarial training, have been proposed to protect acoustic systems from adversarial attacks, but they are less effective against adaptive attacks. Furthermore, directly applying the methods from the image domain can lead to suboptimal results because of the unique properties of audio data. In this paper, we propose an adversarial purification-based defense pipeline, AudioPure, for acoustic systems via off-the-shelf diffusion models. Taking advantage of the strong generation ability of diffusion models, AudioPure first adds a small amount of noise to the adversarial audio and then runs the reverse sampling step to purify the noisy audio and recover clean audio. AudioPure is a plug-and-play method that can be directly applied to any pretrained classifier without any fine-tuning or re-training. We conduct extensive experiments on speech command recognition task to evaluate the robustness of AudioPure. Our method is effective against diverse adversarial attacks (e.g. $\mathcal{L}_2$ or $\mathcal{L}_\infty$-norm). It outperforms the existing methods under both strong adaptive white-box and black-box attacks bounded by $\mathcal{L}_2$ or $\mathcal{L}_\infty$-norm (up to +20\% in robust accuracy). Besides, we also evaluate the certified robustness for perturbations bounded by $\mathcal{L}_2$-norm via randomized smoothing. Our pipeline achieves a higher certified accuracy than baselines.

摘要: 近年来，深度学习模型在商业声学系统中得到了广泛的应用。然而，敌意音频示例会导致这些声学系统的异常行为，同时人类很难感知。人们提出了各种方法，如基于变换的防御和对抗性训练，以保护声学系统免受对抗性攻击，但它们对自适应攻击的效果较差。此外，由于音频数据的独特性质，直接应用图像域的方法可能会导致次优结果。在本文中，我们提出了一种基于对抗性净化的防御管道，AudioPure，用于声学系统，通过现成的扩散模型。利用扩散模型强大的生成能力，AudioPure先将少量噪声添加到对抗性音频中，然后运行反向采样步骤来净化噪声音频，恢复干净的音频。AudioPure是一种即插即用的方法，可以直接应用于任何预先训练的分类器，而无需任何微调或重新训练。我们在语音命令识别任务上进行了大量的实验，以评估AudioPure的健壮性。我们的方法对不同的对手攻击(如$\mathcal{L}_2$或$\mathcal{L}_\inty$-Norm)是有效的。它在强自适应白盒和黑盒攻击下的性能都优于现有的方法，这些攻击以$\mathcal{L}_2$或$\mathcal{L}_inty$-范数为界(鲁棒性精度高达+20\%)。此外，我们还通过随机光滑化评估了对有界于$\数学{L}_2$-范数的扰动的证明的稳健性。我们的管道达到了比基线更高的认证精度。



## **39. Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples**

使替代模型更具贝叶斯性质可以增强对抗性例子的可转移性 cs.LG

Accepted by ICLR 2023, fix typos

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2302.05086v2) [paper-pdf](http://arxiv.org/pdf/2302.05086v2)

**Authors**: Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen

**Abstract**: The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https://github.com/qizhangli/MoreBayesian-attack.

摘要: 恶意例子在深度神经网络(DNN)之间的可转移性是许多黑盒攻击的症结所在。先前的许多努力都致力于通过增加一些替代模型的投入的多样性来提高可转移性。相反，在本文中，我们选择了替代模型的多样性，并主张攻击贝叶斯模型，以实现理想的可转移性。从贝叶斯公式出发，我们开发了一种可能的精调的原则性策略，该策略可以与许多关于DNN参数的现成的高斯后验近似相结合。在常见的基准数据集上进行了广泛的实验来验证我们的方法的有效性，结果表明我们的方法的性能比最近的最新技术有很大的差距(在ImageNet上的平均攻击成功率大约绝对提高了19%)，并且通过结合这些最新的方法，可以获得进一步的性能提升。我们的代码：https://github.com/qizhangli/MoreBayesian-attack.



## **40. Robust Ranking Explanations**

稳健的排名解释 cs.LG

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2212.14106v2) [paper-pdf](http://arxiv.org/pdf/2212.14106v2)

**Authors**: Chao Chen, Chenghua Guo, Guixiang Ma, Ming Zeng, Xi Zhang, Sihong Xie

**Abstract**: Gradient-based explanation is the cornerstone of explainable deep networks, but it has been shown to be vulnerable to adversarial attacks. However, existing works measure the explanation robustness based on $\ell_p$-norm, which can be counter-intuitive to humans, who only pay attention to the top few salient features. We propose explanation ranking thickness as a more suitable explanation robustness metric. We then present a new practical adversarial attacking goal for manipulating explanation rankings. To mitigate the ranking-based attacks while maintaining computational feasibility, we derive surrogate bounds of the thickness that involve expensive sampling and integration. We use a multi-objective approach to analyze the convergence of a gradient-based attack to confirm that the explanation robustness can be measured by the thickness metric. We conduct experiments on various network architectures and diverse datasets to prove the superiority of the proposed methods, while the widely accepted Hessian-based curvature smoothing approaches are not as robust as our method.

摘要: 基于梯度的解释是可解释深度网络的基石，但已被证明容易受到对手攻击。然而，现有的工作基于$\ell_p$-范数来衡量解释的稳健性，这对于只关注最显著的几个特征的人类来说可能是违反直觉的。我们提出解释排序厚度作为一种更合适的解释稳健性度量。然后，我们提出了一种新的实用的对抗性攻击目标，用于操纵解释排名。为了在保持计算可行性的同时减轻基于排名的攻击，我们推导了包含昂贵的采样和积分的厚度的代理界限。我们使用多目标方法对基于梯度的攻击的收敛进行了分析，以确认解释的稳健性可以通过厚度度量来度量。我们在不同的网络结构和不同的数据集上进行了实验，以证明所提出的方法的优越性，而被广泛接受的基于Hessian的曲率平滑方法不如我们的方法健壮。



## **41. On the Robustness of Safe Reinforcement Learning under Observational Perturbations**

安全强化学习在观测摄动下的稳健性 cs.LG

Published at the 11th International Conference on Learning  Representations (ICLR 2023). 30 pages, 5 figures, 8 tables

**SubmitDate**: 2023-03-02    [abs](http://arxiv.org/abs/2205.14691v3) [paper-pdf](http://arxiv.org/pdf/2205.14691v3)

**Authors**: Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo Li, Ding Zhao

**Abstract**: Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting. We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward. One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \url{https://github.com/liuzuxin/safe-rl-robustness}

摘要: 安全强化学习(RL)训练一种策略，在满足安全约束的同时最大化任务奖励。虽然以前的工作主要集中在性能最优性上，但我们发现许多安全RL问题的最优解对于精心设计的观测扰动并不是健壮的和安全的。我们形式化地分析了在安全RL环境下设计有效的观察型对抗攻击者的独特性质。我们证明了标准RL任务的基线对抗性攻击技术对于安全RL并不总是有效的，并提出了两种新的方法-一种最大化成本，另一种最大化回报。一个有趣和违反直觉的发现是，最大奖励攻击是强大的，因为它既可以诱导不安全的行为，又可以通过保持奖励来使攻击隐形。我们进一步提出了一个健壮的安全RL训练框架，并通过全面的实验对其进行了评估。本文为进一步研究RL在观察性攻击下的安全性和健壮性提供了开拓性的工作。代码可从以下网址获得：\url{https://github.com/liuzuxin/safe-rl-robustness}



## **42. Planning for Attacker Entrapment in Adversarial Settings**

对抗性环境下攻击者诱捕的计划 cs.AI

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2303.00822v1) [paper-pdf](http://arxiv.org/pdf/2303.00822v1)

**Authors**: Brittany Cates, Anagha Kulkarni, Sarath Sreedharan

**Abstract**: In this paper, we propose a planning framework to generate a defense strategy against an attacker who is working in an environment where a defender can operate without the attacker's knowledge. The objective of the defender is to covertly guide the attacker to a trap state from which the attacker cannot achieve their goal. Further, the defender is constrained to achieve its goal within K number of steps, where K is calculated as a pessimistic lower bound within which the attacker is unlikely to suspect a threat in the environment. Such a defense strategy is highly useful in real world systems like honeypots or honeynets, where an unsuspecting attacker interacts with a simulated production system while assuming it is the actual production system. Typically, the interaction between an attacker and a defender is captured using game theoretic frameworks. Our problem formulation allows us to capture it as a much simpler infinite horizon discounted MDP, in which the optimal policy for the MDP gives the defender's strategy against the actions of the attacker. Through empirical evaluation, we show the merits of our problem formulation.

摘要: 在本文中，我们提出了一个计划框架来生成针对攻击者的防御策略，在这种环境中，防御者可以在攻击者不知情的情况下操作。防御者的目标是秘密地引导攻击者进入陷阱状态，使攻击者无法实现他们的目标。此外，防御者被限制在K个步骤内实现其目标，其中K被计算为悲观的下限，在该下限内攻击者不太可能怀疑环境中的威胁。这样的防御策略在蜜罐或蜜网等现实世界系统中非常有用，在这些系统中，毫无戒心的攻击者与模拟生产系统交互，同时假设它是实际的生产系统。通常，攻击者和防御者之间的交互是使用博弈论框架来捕捉的。我们的问题公式允许我们将其捕获为一个更简单的无限地平线折扣MDP，其中MDP的最优策略给出了防御者针对攻击者的行为的策略。通过实证评估，我们展示了我们的问题描述的优点。



## **43. CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning**

CANIFE：为联合学习中的经验隐私测量制作金丝雀 cs.LG

Accepted to ICLR 2023

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2210.02912v2) [paper-pdf](http://arxiv.org/pdf/2210.02912v2)

**Authors**: Samuel Maddock, Alexandre Sablayrolles, Pierre Stock

**Abstract**: Federated Learning (FL) is a setting for training machine learning models in distributed environments where the clients do not share their raw data but instead send model updates to a server. However, model updates can be subject to attacks and leak private information. Differential Privacy (DP) is a leading mitigation strategy which involves adding noise to clipped model updates, trading off performance for strong theoretical privacy guarantees. Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice. In this paper, we aim to achieve a tighter measurement of the model exposure by considering a realistic threat model. We propose a novel method, CANIFE, that uses canaries - carefully crafted samples by a strong adversary to evaluate the empirical privacy of a training round. We apply this attack to vision models trained on CIFAR-10 and CelebA and to language models trained on Sent140 and Shakespeare. In particular, in realistic FL scenarios, we demonstrate that the empirical per-round epsilon obtained with CANIFE is 4-5x lower than the theoretical bound.

摘要: 联合学习(FL)是一种用于在分布式环境中训练机器学习模型的设置，在分布式环境中，客户端不共享其原始数据，而是将模型更新发送到服务器。然而，模型更新可能会受到攻击并泄露私人信息。差分隐私(DP)是一种领先的缓解策略，它涉及到向剪辑的模型更新添加噪声，以牺牲性能来换取强大的理论隐私保证。以往的工作表明，DP的威胁模型是保守的，所获得的保证可能是空洞的，或者可能高估了实际中的信息泄漏。在本文中，我们的目标是通过考虑一个真实的威胁模型来实现对模型暴露的更严格的测量。我们提出了一种新的方法CANIFE，它使用金丝雀精心制作的样本来评估训练轮的经验隐私。我们将这种攻击应用于在CIFAR-10和CelebA上训练的视觉模型，以及在Sent140和莎士比亚上训练的语言模型。特别是，在现实的FL场景中，我们证明了用CANIFE得到的每轮经验epsilon比理论值低4-5倍。



## **44. Poster: Sponge ML Model Attacks of Mobile Apps**

发信人：移动应用的海绵ML模型攻击 cs.LG

2 pages, 6 figures. Proceedings of the 24th International Workshop on  Mobile Computing Systems and Applications (HotMobile). Feb. 2023

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2303.01243v1) [paper-pdf](http://arxiv.org/pdf/2303.01243v1)

**Authors**: Souvik Paul, Nicolas Kourtellis

**Abstract**: Machine Learning (ML)-powered apps are used in pervasive devices such as phones, tablets, smartwatches and IoT devices. Recent advances in collaborative, distributed ML such as Federated Learning (FL) attempt to solve privacy concerns of users and data owners, and thus used by tech industry leaders such as Google, Facebook and Apple. However, FL systems and models are still vulnerable to adversarial membership and attribute inferences and model poisoning attacks, especially in FL-as-a-Service ecosystems recently proposed, which can enable attackers to access multiple ML-powered apps. In this work, we focus on the recently proposed Sponge attack: It is designed to soak up energy consumed while executing inference (not training) of ML model, without hampering the classifier's performance. Recent work has shown sponge attacks on ASCI-enabled GPUs can potentially escalate the power consumption and inference time. For the first time, in this work, we investigate this attack in the mobile setting and measure the effect it can have on ML models running inside apps on mobile devices.

摘要: 机器学习(ML)支持的应用程序用于手机、平板电脑、智能手表和物联网设备等普及设备中。协作、分布式ML的最新进展，如联合学习(FL)，试图解决用户和数据所有者对隐私的担忧，因此被谷歌、Facebook和苹果等科技行业领先者使用。然而，FL系统和模型仍然容易受到敌意成员身份、属性推理和模型中毒攻击，特别是在最近提出的FL即服务生态系统中，这可以使攻击者访问多个ML支持的应用程序。在这项工作中，我们重点研究了最近提出的海绵攻击：它旨在吸收在执行ML模型的推理(而不是训练)时消耗的能量，而不会影响分类器的性能。最近的工作表明，对启用ASCI的GPU进行海绵攻击可能会增加功耗和推理时间。在这项工作中，我们第一次在移动环境中研究了这种攻击，并测量了它对在移动设备上的应用程序中运行的ML模型的影响。



## **45. Measuring the Transferability of $\ell_\infty$ Attacks by the $\ell_2$ Norm**

用$\ELL_2$范数度量$\ELL_INFTY$攻击的可转移性 cs.LG

ICASSP 2023

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2102.10343v4) [paper-pdf](http://arxiv.org/pdf/2102.10343v4)

**Authors**: Sizhe Chen, Qinghua Tao, Zhixing Ye, Xiaolin Huang

**Abstract**: Deep neural networks could be fooled by adversarial examples with trivial differences to original samples. To keep the difference imperceptible in human eyes, researchers bound the adversarial perturbations by the $\ell_\infty$ norm, which is now commonly served as the standard to align the strength of different attacks for a fair comparison. However, we propose that using the $\ell_\infty$ norm alone is not sufficient in measuring the attack strength, because even with a fixed $\ell_\infty$ distance, the $\ell_2$ distance also greatly affects the attack transferability between models. Through the discovery, we reach more in-depth understandings towards the attack mechanism, i.e., several existing methods attack black-box models better partly because they craft perturbations with 70% to 130% larger $\ell_2$ distances. Since larger perturbations naturally lead to better transferability, we thereby advocate that the strength of attacks should be simultaneously measured by both the $\ell_\infty$ and $\ell_2$ norm. Our proposal is firmly supported by extensive experiments on ImageNet dataset from 7 attacks, 4 white-box models, and 9 black-box models.

摘要: 深度神经网络可能会被与原始样本有微小差异的对抗性例子所愚弄。为了让这种差异在人眼中看不到，研究人员用$\ell_\inty$范数来约束对抗性扰动，这现在通常被用作对不同攻击强度进行公平比较的标准。然而，我们认为，单独使用$\ell_inty$范数来度量攻击强度是不够的，因为即使在固定的$\ell_\inty$距离的情况下，$\ell_2$距离也会对攻击在模型之间的可转移性产生很大影响。通过这一发现，我们对攻击机制得到了更深入的理解，即现有的几种方法对黑盒模型的攻击效果更好，部分原因是它们制造了比$\ell_2距离大70%到130%的扰动。由于更大的扰动自然会导致更好的可转移性，因此我们主张攻击的强度应该同时用$\ell_inty$和$\ell_2$范数来衡量。我们的建议得到了来自7个攻击、4个白盒模型和9个黑盒模型的ImageNet数据集的广泛实验的坚定支持。



## **46. DOLOS: A Novel Architecture for Moving Target Defense**

Dolos：一种新型的动目标防御体系结构 cs.CR

13 pages, 5 figures

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2303.00387v1) [paper-pdf](http://arxiv.org/pdf/2303.00387v1)

**Authors**: Giulio Pagnotta, Fabio De Gaspari, Dorjan Hitaj, Mauro Andreolini, Michele Colajanni, Luigi V. Mancini

**Abstract**: Moving Target Defense and Cyber Deception emerged in recent years as two key proactive cyber defense approaches, contrasting with the static nature of the traditional reactive cyber defense. The key insight behind these approaches is to impose an asymmetric disadvantage for the attacker by using deception and randomization techniques to create a dynamic attack surface. Moving Target Defense typically relies on system randomization and diversification, while Cyber Deception is based on decoy nodes and fake systems to deceive attackers. However, current Moving Target Defense techniques are complex to manage and can introduce high overheads, while Cyber Deception nodes are easily recognized and avoided by adversaries.   This paper presents DOLOS, a novel architecture that unifies Cyber Deception and Moving Target Defense approaches. DOLOS is motivated by the insight that deceptive techniques are much more powerful when integrated into production systems rather than deployed alongside them. DOLOS combines typical Moving Target Defense techniques, such as randomization, diversity, and redundancy, with cyber deception and seamlessly integrates them into production systems through multiple layers of isolation. We extensively evaluate DOLOS against a wide range of attackers, ranging from automated malware to professional penetration testers, and show that DOLOS is highly effective in slowing down attacks and protecting the integrity of production systems. We also provide valuable insights and considerations for the future development of MTD techniques based on our findings.

摘要: 移动目标防御和网络欺骗是近年来出现的两种关键的主动式网络防御手段，与传统的反应性网络防御的静态性质形成了鲜明对比。这些方法背后的关键洞察是通过使用欺骗和随机化技术来创建动态攻击面，从而对攻击者施加不对称的劣势。动目标防御通常依赖于系统的随机化和多样化，而网络欺骗则是基于诱骗节点和假系统来欺骗攻击者。然而，当前的移动目标防御技术管理复杂，开销高，而网络欺骗节点很容易被对手识别和避开。本文提出了一种将网络欺骗和移动目标防御相结合的新型体系结构Dolos。Dolos的动机是这样一种洞察，即欺骗性技术在集成到生产系统中时比与它们一起部署时要强大得多。Dolos将典型的移动目标防御技术(如随机化、多样性和冗余)与网络欺骗相结合，并通过多层隔离将它们无缝集成到生产系统中。我们针对从自动恶意软件到专业渗透测试仪的广泛攻击者对Dolos进行了广泛的评估，并表明Dolos在减缓攻击和保护生产系统的完整性方面非常有效。我们还根据我们的发现，为MTD技术的未来发展提供了有价值的见解和考虑。



## **47. Competence-Based Analysis of Language Models**

基于能力的语言模型分析 cs.CL

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2303.00333v1) [paper-pdf](http://arxiv.org/pdf/2303.00333v1)

**Authors**: Adam Davies, Jize Jiang, ChengXiang Zhai

**Abstract**: Despite the recent success of large pretrained language models (LMs) on a variety of prompting tasks, these models can be alarmingly brittle to small changes in inputs or application contexts. To better understand such behavior and motivate the design of more robust LMs, we propose a general experimental framework, CALM (Competence-based Analysis of Language Models), where targeted causal interventions are utilized to damage an LM's internal representation of various linguistic properties in order to evaluate its use of each representation in performing a given task. We implement these interventions as gradient-based adversarial attacks, which (in contrast to prior causal probing methodologies) are able to target arbitrarily-encoded representations of relational properties, and carry out a case study of this approach to analyze how BERT-like LMs use representations of several relational properties in performing associated relation prompting tasks. We find that, while the representations LMs leverage in performing each task are highly entangled, they may be meaningfully interpreted in terms of the tasks where they are most utilized; and more broadly, that CALM enables an expanded scope of inquiry in LM analysis that may be useful in predicting and explaining weaknesses of existing LMs.

摘要: 尽管大型预先训练的语言模型(LMS)最近在各种提示任务上取得了成功，但这些模型对输入或应用程序上下文的微小变化可能会非常脆弱。为了更好地理解这类行为，并激励设计更健壮的LMS，我们提出了一个通用的实验框架，CAMPE(基于能力的语言模型分析)，其中有针对性的因果干预被用来破坏语言模型对各种语言属性的内部表征，以便评估它在执行给定任务时对每个表征的使用。我们将这些干预实现为基于梯度的对抗性攻击，与以往的因果探测方法不同，它能够针对关系属性的任意编码表示，并进行了该方法的案例研究，以分析类似于BERT的LMS如何在执行关联关系提示任务时使用多个关系属性的表示。我们发现，虽然LMS在执行每项任务时所利用的表征是高度纠缠的，但它们可能会被有意义地解释为它们最被利用的任务；更广泛地说，这种平静使LM分析中的调查范围得以扩大，这可能有助于预测和解释现有LMS的弱点。



## **48. Oops..! I Glitched It Again! How to Multi-Glitch the Glitching-Protections on ARM TrustZone-M**

哎呀..！我又出故障了！如何在ARM TrustZone-M上实现多毛刺保护 cs.CR

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2302.06932v2) [paper-pdf](http://arxiv.org/pdf/2302.06932v2)

**Authors**: Marvin Saß, Richard Mitev, Ahmad-Reza Sadeghi

**Abstract**: Voltage Fault Injection (VFI), also known as power glitching, has proven to be a severe threat to real-world systems. In VFI attacks, the adversary disturbs the power-supply of the target-device forcing the device to illegitimate behavior. Various countermeasures have been proposed to address different types of fault injection attacks at different abstraction layers, either requiring to modify the underlying hardware or software/firmware at the machine instruction level. Moreover, only recently, individual chip manufacturers have started to respond to this threat by integrating countermeasures in their products. Generally, these countermeasures aim at protecting against single fault injection (SFI) attacks, since Multiple Fault Injection (MFI) is believed to be challenging and sometimes even impractical. In this paper, we present {\mu}-Glitch, the first Voltage Fault Injection (VFI) platform which is capable of injecting multiple, coordinated voltage faults into a target device, requiring only a single trigger signal. We provide a novel flow for Multiple Voltage Fault Injection (MVFI) attacks to significantly reduce the search complexity for fault parameters, as the search space increases exponentially with each additional fault injection. We evaluate and showcase the effectiveness and practicality of our attack platform on four real-world chips, featuring TrustZone-M: The first two have interdependent backchecking mechanisms, while the second two have additionally integrated countermeasures against fault injection. Our evaluation revealed that {\mu}-Glitch can successfully inject four consecutive faults within an average time of one day. Finally, we discuss potential countermeasures to mitigate VFI attacks and additionally propose two novel attack scenarios for MVFI.

摘要: 电压故障注入(VFI)，也称为电源毛刺，已被证明是对实际系统的严重威胁。在VFI攻击中，对手干扰目标设备的电源，迫使设备进行非法行为。已经提出了各种对策来解决在不同抽象层的不同类型的故障注入攻击，或者需要在机器指令级修改底层硬件或软件/固件。此外，直到最近，个别芯片制造商才开始通过将对策整合到他们的产品中来应对这种威胁。通常，这些对策旨在防御单故障注入(SFI)攻击，因为多故障注入(MFI)被认为具有挑战性，有时甚至不切实际。在本文中，我们介绍了第一个电压故障注入(VFI)平台，它能够将多个协调的电压故障注入到目标设备中，只需要一个触发信号。我们提出了一种新的多电压故障注入(MVFI)攻击流程，以显著降低故障参数的搜索复杂度，因为每增加一次故障注入，搜索空间就会成倍增加。我们在四个真实芯片上评估和展示了我们的攻击平台的有效性和实用性：前两个芯片具有相互依赖的回溯检查机制，而后两个芯片具有针对故障注入的额外集成对策。我们的评估显示，{\MU}-GLITCH可以在平均一天的时间内成功注入四个连续故障。最后，我们讨论了缓解VFI攻击的潜在对策，并提出了两种新的MVFI攻击方案。



## **49. Robust Prototypical Few-Shot Organ Segmentation with Regularized Neural-ODEs**

基于正则化神经节点的典型少发器官分割 cs.CV

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2208.12428v3) [paper-pdf](http://arxiv.org/pdf/2208.12428v3)

**Authors**: Prashant Pandey, Mustafa Chasmai, Tanuj Sur, Brejesh Lall

**Abstract**: Despite the tremendous progress made by deep learning models in image semantic segmentation, they typically require large annotated examples, and increasing attention is being diverted to problem settings like Few-Shot Learning (FSL) where only a small amount of annotation is needed for generalisation to novel classes. This is especially seen in medical domains where dense pixel-level annotations are expensive to obtain. In this paper, we propose Regularized Prototypical Neural Ordinary Differential Equation (R-PNODE), a method that leverages intrinsic properties of Neural-ODEs, assisted and enhanced by additional cluster and consistency losses to perform Few-Shot Segmentation (FSS) of organs. R-PNODE constrains support and query features from the same classes to lie closer in the representation space thereby improving the performance over the existing Convolutional Neural Network (CNN) based FSS methods. We further demonstrate that while many existing Deep CNN based methods tend to be extremely vulnerable to adversarial attacks, R-PNODE exhibits increased adversarial robustness for a wide array of these attacks. We experiment with three publicly available multi-organ segmentation datasets in both in-domain and cross-domain FSS settings to demonstrate the efficacy of our method. In addition, we perform experiments with seven commonly used adversarial attacks in various settings to demonstrate R-PNODE's robustness. R-PNODE outperforms the baselines for FSS by significant margins and also shows superior performance for a wide array of attacks varying in intensity and design.

摘要: 尽管深度学习模型在图像语义分割方面取得了巨大的进步，但它们通常需要大量的注释示例，并且越来越多的注意力被转移到像少镜头学习(FSL)这样的问题环境中，其中只需要少量的注释就可以概括到新的类。这在医学领域中尤其常见，在医学领域中，密集像素级注释的获取成本很高。在本文中，我们提出了正则化的原型神经常微分方程(R-PNODE)，该方法利用神经节点的固有特性，通过额外的聚类和一致性损失来辅助和增强器官的少镜头分割(FSS)。R-PNODE约束支持和查询来自同一类的特征在表示空间中更接近，从而提高了现有基于卷积神经网络(CNN)的FSS方法的性能。我们进一步证明，虽然许多现有的基于Deep CNN的方法往往非常容易受到对抗性攻击，但R-PNODE对一系列此类攻击表现出更强的对抗性。我们用三个公开可用的多器官分割数据集在域内和跨域的FSS环境中进行了实验，以证明我们方法的有效性。此外，我们在不同的环境下对七种常用的对抗性攻击进行了实验，以验证R-PNODE的健壮性。R-PNODE的表现远远超过FSS的基线，并在各种强度和设计的攻击中显示出卓越的性能。



## **50. To Make Yourself Invisible with Adversarial Semantic Contours**

用对抗性的语义轮廓让自己看不见 cs.CV

11 pages, 7 figures, published in Computer Vision and Image  Understanding in 2023

**SubmitDate**: 2023-03-01    [abs](http://arxiv.org/abs/2303.00284v1) [paper-pdf](http://arxiv.org/pdf/2303.00284v1)

**Authors**: Yichi Zhang, Zijian Zhu, Hang Su, Jun Zhu, Shibao Zheng, Yuan He, Hui Xue

**Abstract**: Modern object detectors are vulnerable to adversarial examples, which may bring risks to real-world applications. The sparse attack is an important task which, compared with the popular adversarial perturbation on the whole image, needs to select the potential pixels that is generally regularized by an $\ell_0$-norm constraint, and simultaneously optimize the corresponding texture. The non-differentiability of $\ell_0$ norm brings challenges and many works on attacking object detection adopted manually-designed patterns to address them, which are meaningless and independent of objects, and therefore lead to relatively poor attack performance.   In this paper, we propose Adversarial Semantic Contour (ASC), an MAP estimate of a Bayesian formulation of sparse attack with a deceived prior of object contour. The object contour prior effectively reduces the search space of pixel selection and improves the attack by introducing more semantic bias. Extensive experiments demonstrate that ASC can corrupt the prediction of 9 modern detectors with different architectures (\e.g., one-stage, two-stage and Transformer) by modifying fewer than 5\% of the pixels of the object area in COCO in white-box scenario and around 10\% of those in black-box scenario. We further extend the attack to datasets for autonomous driving systems to verify the effectiveness. We conclude with cautions about contour being the common weakness of object detectors with various architecture and the care needed in applying them in safety-sensitive scenarios.

摘要: 现代的目标检测器容易受到敌意例子的攻击，这可能会给现实世界的应用带来风险。稀疏攻击是一项重要的任务，与常见的对整幅图像的对抗性扰动相比，它需要选择通常由$\ell_0$-范数约束正则化的潜在像素，并同时优化相应的纹理。HELL_0范数的不可微性给攻击目标检测带来了挑战，许多攻击目标检测工作都采用手工设计的模式来解决这些问题，这些模式没有意义且与对象无关，从而导致攻击性能相对较差。在本文中，我们提出了对抗性语义轮廓(ASC)，这是一种稀疏攻击的贝叶斯公式的MAP估计，该公式具有对象轮廓的欺骗先验。目标轮廓先验有效地缩小了像素选择的搜索空间，并通过引入更多的语义偏向来改善攻击。大量实验表明，在白盒场景和黑盒场景中，ASC分别修改不到5个和10个左右的目标区域像素，从而破坏了9种不同结构(如一级、两级和Transformer)的现代探测器的预测能力。我们进一步将攻击扩展到自动驾驶系统的数据集，以验证攻击的有效性。最后，我们警告说，轮廓是具有各种体系结构的对象探测器的共同弱点，以及在安全敏感场景中应用它们时需要注意的事项。



