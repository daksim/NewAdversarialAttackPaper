# Latest Adversarial Attack Papers
**update at 2023-02-01 11:59:07**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Reverse engineering adversarial attacks with fingerprints from adversarial examples**

利用对抗性例子的指纹进行对抗性攻击的逆向工程 cs.AI

8 pages, 6 figures

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2301.13869v1) [paper-pdf](http://arxiv.org/pdf/2301.13869v1)

**Authors**: David Aaron Nicholson, Vincent Emanuele

**Abstract**: In spite of intense research efforts, deep neural networks remain vulnerable to adversarial examples: an input that forces the network to confidently produce incorrect outputs. Adversarial examples are typically generated by an attack algorithm that optimizes a perturbation added to a benign input. Many such algorithms have been developed. If it were possible to reverse engineer attack algorithms from adversarial examples, this could deter bad actors because of the possibility of attribution. Here we formulate reverse engineering as a supervised learning problem where the goal is to assign an adversarial example to a class that represents the algorithm and parameters used. To our knowledge it has not been previously shown whether this is even possible. We first test whether we can classify the perturbations added to images by attacks on undefended single-label image classification models. Taking a ``fight fire with fire'' approach, we leverage the sensitivity of deep neural networks to adversarial examples, training them to classify these perturbations. On a 17-class dataset (5 attacks, 4 bounded with 4 epsilon values each), we achieve an accuracy of 99.4\% with a ResNet50 model trained on the perturbations. We then ask whether we can perform this task without access to the perturbations, obtaining an estimate of them with signal processing algorithms, an approach we call ``fingerprinting''. We find the JPEG algorithm serves as a simple yet effective fingerprinter (85.05\% accuracy), providing a strong baseline for future work. We discuss how our approach can be extended to attack agnostic, learnable fingerprints, and to open-world scenarios with unknown attacks.

摘要: 尽管进行了密集的研究工作，深度神经网络仍然容易受到敌意例子的攻击：一种迫使网络自信地产生错误输出的输入。对抗性示例通常由攻击算法生成，该攻击算法优化添加到良性输入的扰动。已经开发了许多这样的算法。如果有可能从敌对的例子中逆向设计攻击算法，这可能会阻止不良行为者，因为可能会归因于他们。在这里，我们将逆向工程描述为一个有监督的学习问题，其中的目标是将一个对抗性示例分配给一个表示所用算法和参数的类。据我们所知，此前甚至没有证据表明这是否可能。我们首先测试是否可以通过对无防御的单标签图像分类模型的攻击来分类添加到图像中的扰动。采取“以牙还牙”的方法，我们利用深层神经网络对敌意例子的敏感性，训练它们对这些扰动进行分类。在一个17类数据集(5次攻击，4次攻击，每个攻击有4个epsilon值)上，我们用一个关于扰动的ResNet50模型训练，获得了99.4\%的准确率。然后我们问，我们是否可以在不接触扰动的情况下执行这项任务，通过信号处理算法获得对它们的估计，我们将这种方法称为“指纹识别”。我们发现，JPEG算法是一种简单而有效的指纹图谱(85.05精度)，为下一步的工作提供了强有力的基线。我们讨论了如何将我们的方法扩展到攻击不可知的、可学习的指纹，以及具有未知攻击的开放世界场景。



## **2. EC-CFI: Control-Flow Integrity via Code Encryption Counteracting Fault Attacks**

EC-CFI：通过代码加密对抗错误攻击的控制流完整性 cs.CR

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2301.13760v1) [paper-pdf](http://arxiv.org/pdf/2301.13760v1)

**Authors**: Pascal Nasahl, Salmin Sultana, Hans Liljestrand, Karanvir Grewal, Michael LeMay, David M. Durham, David Schrammel, Stefan Mangard

**Abstract**: Fault attacks enable adversaries to manipulate the control-flow of security-critical applications. By inducing targeted faults into the CPU, the software's call graph can be escaped and the control-flow can be redirected to arbitrary functions inside the program. To protect the control-flow from these attacks, dedicated fault control-flow integrity (CFI) countermeasures are commonly deployed. However, these schemes either have high detection latencies or require intrusive hardware changes.   In this paper, we present EC-CFI, a software-based cryptographically enforced CFI scheme with no detection latency utilizing hardware features of recent Intel platforms. Our EC-CFI prototype is designed to prevent an adversary from escaping the program's call graph using faults by encrypting each function with a different key before execution. At runtime, the instrumented program dynamically derives the decryption key, ensuring that the code only can be successfully decrypted when the program follows the intended call graph. To enable this level of protection on Intel commodity systems, we introduce extended page table (EPT) aliasing allowing us to achieve function-granular encryption by combing Intel's TME-MK and virtualization technology. We open-source our custom LLVM-based toolchain automatically protecting arbitrary programs with EC-CFI. Furthermore, we evaluate our EPT aliasing approach with the SPEC CPU2017 and Embench-IoT benchmarks and discuss and evaluate potential TME-MK hardware changes minimizing runtime overheads.

摘要: 故障攻击使攻击者能够操纵安全关键型应用程序的控制流。通过在CPU中引入有针对性的错误，可以避开软件的调用图，并将控制流重定向到程序内的任意函数。为了保护控制流免受这些攻击，通常部署专用的故障控制流完整性(CFI)对策。然而，这些方案要么具有很高的检测延迟，要么需要侵入性的硬件改变。在本文中，我们提出了EC-CFI，这是一种基于软件的密码强制CFI方案，利用最近Intel平台的硬件特性，没有检测延迟。我们的EC-CFI原型旨在通过在执行前使用不同的密钥加密每个函数，防止对手使用错误逃离程序的调用图。在运行时，插入指令的程序动态地派生解密密钥，确保只有当程序遵循预期的调用图时才能成功解密代码。为了在英特尔商用系统上实现这种级别的保护，我们引入了扩展页表(EPT)别名，使我们能够通过结合英特尔的TME-MK和虚拟化技术来实现函数级加密。我们将基于LLVM的定制工具链开源，使用EC-CFI自动保护任意程序。此外，我们使用SPEC CPU2017和Embase-IoT基准评估了我们的EPT混叠方法，并讨论和评估了潜在的TME-MK硬件更改，以最大限度地减少运行时开销。



## **3. PINCH: An Adversarial Extraction Attack Framework for Deep Learning Models**

PINCH：一种面向深度学习模型的对抗性抽取攻击框架 cs.CR

19 pages, 13 figures, 5 tables

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2209.06300v2) [paper-pdf](http://arxiv.org/pdf/2209.06300v2)

**Authors**: William Hackett, Stefan Trawicki, Zhengxin Yu, Neeraj Suri, Peter Garraghan

**Abstract**: Adversarial extraction attacks constitute an insidious threat against Deep Learning (DL) models in-which an adversary aims to steal the architecture, parameters, and hyper-parameters of a targeted DL model. Existing extraction attack literature have observed varying levels of attack success for different DL models and datasets, yet the underlying cause(s) behind their susceptibility often remain unclear, and would help facilitate creating secure DL systems. In this paper we present PINCH: an efficient and automated extraction attack framework capable of designing, deploying, and analyzing extraction attack scenarios across heterogeneous hardware platforms. Using PINCH, we perform extensive experimental evaluation of extraction attacks against 21 model architectures to explore new extraction attack scenarios and further attack staging. Our findings show (1) key extraction characteristics whereby particular model configurations exhibit strong resilience against specific attacks, (2) even partial extraction success enables further staging for other adversarial attacks, and (3) equivalent stolen models uncover differences in expressive power, yet exhibit similar captured knowledge.

摘要: 对抗性提取攻击构成了对深度学习(DL)模型的潜在威胁-在该模型中，攻击者的目标是窃取目标DL模型的体系结构、参数和超参数。现有的提取攻击文献已经针对不同的DL模型和数据集观察到不同程度的攻击成功，但其易感性背后的潜在原因往往尚不清楚，这将有助于促进创建安全的DL系统。本文介绍了PINCH：一种高效、自动化的抽取攻击框架，能够设计、部署和分析跨异类硬件平台的抽取攻击场景。使用Pinch，我们对21种模型架构的抽取攻击进行了广泛的实验评估，以探索新的抽取攻击场景和进一步的攻击阶段。我们的发现表明(1)关键提取特征，其中特定的模型配置表现出对特定攻击的强大弹性，(2)即使部分提取成功也能够为其他对抗性攻击提供进一步的准备，以及(3)等价的被盗模型揭示了表达能力的差异，但表现出相似的捕获知识。



## **4. Are Defenses for Graph Neural Networks Robust?**

图神经网络的防御是健壮的吗？ cs.LG

34 pages, 36th Conference on Neural Information Processing Systems  (NeurIPS 2022)

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2301.13694v1) [paper-pdf](http://arxiv.org/pdf/2301.13694v1)

**Authors**: Felix Mujkanovic, Simon Geisler, Stephan Günnemann, Aleksandar Bojchevski

**Abstract**: A cursory reading of the literature suggests that we have made a lot of progress in designing effective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard methodology has a serious flaw - virtually all of the defenses are evaluated against non-adaptive attacks leading to overly optimistic robustness estimates. We perform a thorough robustness analysis of 7 of the most popular defenses spanning the entire spectrum of strategies, i.e., aimed at improving the graph, the architecture, or the training. The results are sobering - most defenses show no or only marginal improvement compared to an undefended baseline. We advocate using custom adaptive attacks as a gold standard and we outline the lessons we learned from successfully designing such attacks. Moreover, our diverse collection of perturbed graphs forms a (black-box) unit test offering a first glance at a model's robustness.

摘要: 粗略地阅读文献就会发现，我们在为图神经网络(GNN)设计有效的对抗防御方面已经取得了很大进展。然而，标准的方法有一个严重的缺陷--几乎所有的防御都是针对非适应性攻击进行评估的，这导致了过于乐观的健壮性估计。我们对跨越整个战略范围的7个最受欢迎的防御进行了彻底的健壮性分析，即旨在改进图表、体系结构或培训。结果是发人深省的-与没有防御的基线相比，大多数防御没有或只有轻微的改善。我们主张使用自定义自适应攻击作为黄金标准，并概述了我们从成功设计此类攻击中学到的教训。此外，我们的各种受干扰的图形集合形成了一个(黑盒)单元测试，让您可以初步了解模型的健壮性。



## **5. Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks**

对抗物理世界攻击的自监督单目深度估计的对抗性训练 cs.CV

Accepted to ICLR2023 (Spotlight)

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2301.13487v1) [paper-pdf](http://arxiv.org/pdf/2301.13487v1)

**Authors**: Zhiyuan Cheng, James Liang, Guanhong Tao, Dongfang Liu, Xiangyu Zhang

**Abstract**: Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems. Traditional adversarial training method requires ground-truth labels hence cannot be directly applied to self-supervised MDE that does not have ground-truth depth. Some self-supervised model hardening techniques (e.g., contrastive learning) ignore the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using ground-truth depth. We improve adversarial robustness against physical-world attacks using L0-norm-bounded perturbation in training. We compare our method with supervised learning based and contrastive learning based methods that are tailored for MDE. Results on two representative MDE networks show that we achieve better robustness against various adversarial attacks with nearly no benign performance degradation.

摘要: 单目深度估计(MDE)是自动驾驶等应用中的重要组成部分。针对MDE网络的攻击有多种。这些攻击，尤其是物理攻击，对这类系统的安全构成了极大的威胁。传统的对抗性训练方法需要地面真相标签，因此不能直接应用于没有地面真相深度的自监督MDE。一些自监督模型强化技术(如对比学习)忽略了MDE的领域知识，很难达到最优性能。在这项工作中，我们提出了一种新的基于视图合成的自监督MDE模型的对抗性训练方法，而不使用地面真实深度。我们在训练中使用L0范数有界扰动来提高对手对物理世界攻击的健壮性。我们将我们的方法与基于监督学习和基于对比学习的方法进行了比较，这些方法都是为MDE量身定制的。在两个具有代表性的MDE网络上的实验结果表明，该算法在几乎不降低性能的情况下，对各种敌意攻击具有更好的鲁棒性。



## **6. Robust Linear Regression: Gradient-descent, Early-stopping, and Beyond**

稳健线性回归：梯度下降、提前停止和超越 stat.ML

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2301.13486v1) [paper-pdf](http://arxiv.org/pdf/2301.13486v1)

**Authors**: Meyer Scetbon, Elvis Dohmatob

**Abstract**: In this work we study the robustness to adversarial attacks, of early-stopping strategies on gradient-descent (GD) methods for linear regression. More precisely, we show that early-stopped GD is optimally robust (up to an absolute constant) against Euclidean-norm adversarial attacks. However, we show that this strategy can be arbitrarily sub-optimal in the case of general Mahalanobis attacks. This observation is compatible with recent findings in the case of classification~\cite{Vardi2022GradientMP} that show that GD provably converges to non-robust models. To alleviate this issue, we propose to apply instead a GD scheme on a transformation of the data adapted to the attack. This data transformation amounts to apply feature-depending learning rates and we show that this modified GD is able to handle any Mahalanobis attack, as well as more general attacks under some conditions. Unfortunately, choosing such adapted transformations can be hard for general attacks. To the rescue, we design a simple and tractable estimator whose adversarial risk is optimal up to within a multiplicative constant of 1.1124 in the population regime, and works for any norm.

摘要: 在这项工作中，我们研究了线性回归的梯度下降(GD)方法的提前停止策略对对手攻击的稳健性。更准确地说，我们证明了提前停止的GD对欧几里得范数对手攻击具有最优的健壮性(直到一个绝对常数)。然而，我们证明了在一般马氏攻击的情况下，该策略可以是任意次优的。这一观察结果与最近在分类{Vardi2022GRadientMP}的情况下的发现是一致的，该发现表明GD可证明地收敛到非稳健模型。为了缓解这个问题，我们建议将GD方案应用于适应攻击的数据转换。这种数据变换相当于应用了依赖于特征的学习率，我们证明了这种改进的GD能够应对任何马氏攻击，以及在某些条件下的更一般的攻击。不幸的是，对于一般攻击来说，选择这样的适应转换可能很困难。为了救援，我们设计了一个简单易处理的估计器，其对抗风险在人口制度下的乘法常数为1.1124的范围内是最优的，并且适用于任何范数。



## **7. Can we achieve robustness from data alone?**

我们能仅从数据中获得稳健性吗？ cs.LG

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2207.11727v2) [paper-pdf](http://arxiv.org/pdf/2207.11727v2)

**Authors**: Nikolaos Tsilivis, Jingtong Su, Julia Kempe

**Abstract**: We introduce a meta-learning algorithm for adversarially robust classification. The proposed method tries to be as model agnostic as possible and optimizes a dataset prior to its deployment in a machine learning system, aiming to effectively erase its non-robust features. Once the dataset has been created, in principle no specialized algorithm (besides standard gradient descent) is needed to train a robust model. We formulate the data optimization procedure as a bi-level optimization problem on kernel regression, with a class of kernels that describe infinitely wide neural nets (Neural Tangent Kernels). We present extensive experiments on standard computer vision benchmarks using a variety of different models, demonstrating the effectiveness of our method, while also pointing out its current shortcomings. In parallel, we revisit prior work that also focused on the problem of data optimization for robust classification \citep{Ily+19}, and show that being robust to adversarial attacks after standard (gradient descent) training on a suitable dataset is more challenging than previously thought.

摘要: 我们介绍了一种元学习算法，用于恶意鲁棒分类。该方法试图尽可能地与模型无关，并在将数据集部署到机器学习系统之前对其进行优化，旨在有效地消除其非健壮性特征。一旦创建了数据集，原则上不需要专门的算法(除了标准的梯度下降)来训练稳健的模型。我们将数据优化过程描述为一个关于核回归的双层优化问题，并给出了一类描述无限宽神经网络的核(即神经切核)。我们使用各种不同的模型在标准的计算机视觉基准上进行了大量的实验，证明了我们方法的有效性，同时也指出了它目前的不足。同时，我们回顾了以前的工作，这些工作也集中在健壮分类的数据优化问题上，并表明在合适的数据集上进行标准(梯度下降)训练后，对对手攻击具有健壮性比先前认为的更具挑战性。



## **8. Inference Time Evidences of Adversarial Attacks for Forensic on Transformers**

变压器法对抗性攻击的推理时间证据 cs.CV

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2301.13356v1) [paper-pdf](http://arxiv.org/pdf/2301.13356v1)

**Authors**: Hugo Lemarchant, Liangzi Li, Yiming Qian, Yuta Nakashima, Hajime Nagahara

**Abstract**: Vision Transformers (ViTs) are becoming a very popular paradigm for vision tasks as they achieve state-of-the-art performance on image classification. However, although early works implied that this network structure had increased robustness against adversarial attacks, some works argue ViTs are still vulnerable. This paper presents our first attempt toward detecting adversarial attacks during inference time using the network's input and outputs as well as latent features. We design four quantifications (or derivatives) of input, output, and latent vectors of ViT-based models that provide a signature of the inference, which could be beneficial for the attack detection, and empirically study their behavior over clean samples and adversarial samples. The results demonstrate that the quantifications from input (images) and output (posterior probabilities) are promising for distinguishing clean and adversarial samples, while latent vectors offer less discriminative power, though they give some insights on how adversarial perturbations work.

摘要: 视觉转换器(VITS)正成为视觉任务的一种非常流行的范例，因为它们在图像分类方面实现了最先进的性能。然而，尽管早期的工作表明这种网络结构增强了对对手攻击的健壮性，但一些工作认为VITS仍然是脆弱的。本文首次尝试利用网络的输入、输出和潜在特征来检测推理过程中的敌意攻击。我们设计了基于VIT的模型的输入、输出和潜在向量的四个量化(或导数)，这些量化(或导数)提供了有助于攻击检测的推理的特征，并实证研究了它们在干净样本和对手样本上的行为。结果表明，输入(图像)和输出(后验概率)的量化在区分干净样本和对抗性样本方面是有希望的，而潜在向量提供的区分能力较弱，尽管它们提供了一些关于对抗性扰动如何工作的见解。



## **9. Affinity Uncertainty-based Hard Negative Mining in Graph Contrastive Learning**

图对比学习中基于亲和力不确定性的硬否定挖掘 cs.LG

11 pages, 4 figures

**SubmitDate**: 2023-01-31    [abs](http://arxiv.org/abs/2301.13340v1) [paper-pdf](http://arxiv.org/pdf/2301.13340v1)

**Authors**: Chaoxi Niu, Guansong Pang, Ling Chen

**Abstract**: Hard negative mining has shown effective in enhancing self-supervised contrastive learning (CL) on diverse data types, including graph contrastive learning (GCL). Existing hardness-aware CL methods typically treat negative instances that are most similar to the anchor instance as hard negatives, which helps improve the CL performance, especially on image data. However, this approach often fails to identify the hard negatives but leads to many false negatives on graph data. This is mainly due to that the learned graph representations are not sufficiently discriminative due to over-smooth representations and/or non-i.i.d. issues in graph data. To tackle this problem, this paper proposes a novel approach that builds a discriminative model on collective affinity information (i.e, two sets of pairwise affinities between the negative instances and the anchor instance) to mine hard negatives in GCL. In particular, the proposed approach evaluates how confident/uncertain the discriminative model is about the affinity of each negative instance to an anchor instance to determine its hardness weight relative to the anchor instance. This uncertainty information is then incorporated into existing GCL loss functions via a weighting term to enhance their performance. The enhanced GCL is theoretically grounded that the resulting GCL loss is equivalent to a triplet loss with an adaptive margin being exponentially proportional to the learned uncertainty of each negative instance. Extensive experiments on 10 graph datasets show that our approach i) consistently enhances different state-of-the-art GCL methods in both graph and node classification tasks, and ii) significantly improves their robustness against adversarial attacks.

摘要: 硬负挖掘在增强包括图对比学习(GCL)在内的不同数据类型上的自我监督对比学习(CL)方面表现出了有效的效果。现有的硬度感知CL方法通常将与锚实例最相似的否定实例视为硬否定，这有助于提高CL的性能，特别是在图像数据上。然而，这种方法往往不能识别硬否定，而是导致图数据上的许多假否定。这主要是由于学习的图形表示由于过度平滑的表示和/或非I.I.D.而不具有足够的区分性。图形数据中的问题。针对这一问题，本文提出了一种基于群体亲和力信息(即否定实例和锚实例之间的两组成对亲和度)的判别模型来挖掘GCL中的硬否定。特别地，该方法评估判别模型关于每个否定实例对锚实例的亲和度的置信度/不确定性，以确定其相对于锚实例的硬度权重。然后，这种不确定性信息通过加权项被合并到现有的GCL损失函数中，以增强其性能。增强型GCL的理论基础是，由此产生的GCL损失等同于三重损失，其自适应裕度与每个负实例的学习不确定性成指数正比。在10个图数据集上的大量实验表明，我们的方法在图和节点分类任务中一致地提高了不同的GCL方法，并且显著地提高了它们对对手攻击的健壮性。



## **10. E-DPNCT: An Enhanced Attack Resilient Differential Privacy Model For Smart Grids Using Split Noise Cancellation**

E-DPNCT：一种基于分裂噪声抵消的智能电网抗攻击增强差分隐私模型 cs.CR

13 pages, 7 figues, 1 tables

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2110.11091v3) [paper-pdf](http://arxiv.org/pdf/2110.11091v3)

**Authors**: Khadija Hafeez, Donna OShea, Thomas Newe, Mubashir Husain Rehmani

**Abstract**: High frequency reporting of energy consumption data in smart grids can be used to infer sensitive information regarding the consumers life style and poses serious security and privacy threats. Differential privacy (DP) based privacy models for smart grids ensure privacy when analysing energy consumption data for billing and load monitoring. However, DP models for smart grids are vulnerable to collusion attack where an adversary colludes with malicious smart meters and un-trusted aggregator in order to get private information from other smart meters. We propose an Enhanced Differential Private Noise Cancellation Model for Load Monitoring and Billing for Smart Meters (E-DPNCT) to protect the privacy of the smart grid data using a split noise cancellation protocol with multiple master smart meters (MSMs) to provide accurate billing and load monitoring and resistance against collusion attacks. We did extensive comparison of our E-DPNCT model with state of the art attack resistant privacy preserving models such as EPIC for collusion attack. We simulate our E-DPNCT model with real time data which shows significant improvement in privacy attack scenarios. Further, we analyze the impact of selecting different sensitivity parameters for calibrating DP noise over the privacy of customer electricity profile and accuracy of electricity data aggregation such as load monitoring and billing.

摘要: 智能电网中能源消耗数据的高频报告可用于推断有关用户生活方式的敏感信息，并构成严重的安全和隐私威胁。基于差分隐私(DP)的智能电网隐私模型可在分析用于计费和负荷监控的能耗数据时确保隐私。然而，智能电网的DP模型容易受到合谋攻击，即对手与恶意智能电表和不可信的聚合器串通，以便从其他智能电表获取私人信息。提出了一种用于智能电表负荷监测和计费的增强型差分专用噪声抵消模型(E-DPNCT)，该模型采用多个主智能电表(MSM)的分离噪声抵消协议来保护智能电网数据的隐私，以提供准确的计费和负荷监测并抵抗合谋攻击。我们对我们的E-DPNCT模型和最新的抗攻击隐私保护模型进行了广泛的比较，例如EPIC用于合谋攻击。我们用实时数据模拟了我们的E-DPNCT模型，结果表明该模型在隐私攻击场景中有明显的改善。在此基础上，分析了不同灵敏度参数的选取对用户用电信息的隐私性和负荷监测、计费等用电数据汇总的准确性的影响。



## **11. Towards Adversarial Realism and Robust Learning for IoT Intrusion Detection and Classification**

物联网入侵检测与分类的对抗性真实感和稳健学习 cs.CR

19 pages, 5 tables, 7 figures, Internet of Things journal

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.13122v1) [paper-pdf](http://arxiv.org/pdf/2301.13122v1)

**Authors**: João Vitorino, Isabel Praça, Eva Maia

**Abstract**: The Internet of Things (IoT) faces tremendous security challenges. Machine learning models can be used to tackle the growing number of cyber-attack variations targeting IoT systems, but the increasing threat posed by adversarial attacks restates the need for reliable defense strategies. This work describes the types of constraints required for an adversarial cyber-attack example to be realistic and proposes a methodology for a trustworthy adversarial robustness analysis with a realistic adversarial evasion attack vector. The proposed methodology was used to evaluate three supervised algorithms, Random Forest (RF), Extreme Gradient Boosting (XGB), and Light Gradient Boosting Machine (LGBM), and one unsupervised algorithm, Isolation Forest (IFOR). Constrained adversarial examples were generated with the Adaptative Perturbation Pattern Method (A2PM), and evasion attacks were performed against models created with regular and adversarial training. Even though RF was the least affected in binary classification, XGB consistently achieved the highest accuracy in multi-class classification. The obtained results evidence the inherent susceptibility of tree-based algorithms and ensembles to adversarial evasion attacks and demonstrates the benefits of adversarial training and a security by design approach for a more robust IoT network intrusion detection.

摘要: 物联网(IoT)面临巨大的安全挑战。机器学习模型可以用来应对越来越多的针对物联网系统的网络攻击变体，但对抗性攻击构成的日益增长的威胁重申了对可靠防御策略的需求。这项工作描述了使敌意网络攻击实例成为现实所需的约束类型，并提出了一种使用现实的对抗性逃避攻击向量进行可信对抗健壮性分析的方法。使用该方法对随机森林(RF)、极端梯度增强(XGB)和光梯度增强机器(LGBM)三种有监督算法和一种无监督算法隔离森林(IFOR)进行了评估。使用自适应扰动模式方法(A2PM)生成受限对抗性样本，并对通过常规和对抗性训练创建的模型进行逃避攻击。尽管RF在二进制分类中受影响最小，但XGB在多类分类中始终达到最高的准确率。得到的结果证明了基于树的算法和集成对对抗性逃避攻击的固有敏感性，并通过设计方法证明了对抗性训练和安全性的好处，从而实现了更健壮的物联网网络入侵检测。



## **12. Anchor-Based Adversarially Robust Zero-Shot Learning Driven by Language**

基于锚点的语言驱动的对抗性健壮零射学习 cs.CV

11 pages

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.13096v1) [paper-pdf](http://arxiv.org/pdf/2301.13096v1)

**Authors**: Xiao Li, Wei Zhang, Yining Liu, Zhanhao Hu, Bo Zhang, Xiaolin Hu

**Abstract**: Deep neural networks are vulnerable to adversarial attacks. We consider adversarial defense in the case of zero-shot image classification setting, which has rarely been explored because both adversarial defense and zero-shot learning are challenging. We propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy, to improve the adversarial robustness in a zero-shot setting. LAAT uses a text encoder to obtain fixed anchors (normalized feature embeddings) of each category, then uses these anchors to perform adversarial training. The text encoder has the property that semantically similar categories can be mapped to neighboring anchors in the feature space. By leveraging this property, LAAT can make the image model adversarially robust on novel categories without any extra examples. Experimental results show that our method achieves impressive zero-shot adversarial performance, even surpassing the previous state-of-the-art adversarially robust one-shot methods in most attacking settings. When models are trained with LAAT on large datasets like ImageNet-1K, they can have substantial zero-shot adversarial robustness across several downstream datasets.

摘要: 深度神经网络很容易受到敌意攻击。在零镜头图像分类的情况下，我们考虑了对抗性防御，这是很少被研究的，因为对抗性防御和零镜头学习都是具有挑战性的。我们提出了一种新颖的语言驱动的、基于锚的对抗性训练策略LAAT，以提高零射击环境下的对抗性。LAAT使用文本编码器获取每个类别的固定锚(归一化特征嵌入)，然后使用这些锚进行对抗性训练。文本编码器具有语义相似的类别可以被映射到特征空间中的相邻锚的特性。通过利用这一特性，LAAT可以使图像模型在新的类别上具有相反的健壮性，而不需要任何额外的示例。实验结果表明，在大多数攻击环境下，我们的方法取得了令人印象深刻的零射击对抗性能，甚至超过了以前最先进的对抗健壮的一次射击方法。当使用LAAT在像ImageNet-1K这样的大型数据集上训练模型时，它们可以在几个下游数据集上具有实质性的零命中对抗性健壮性。



## **13. On the Efficacy of Metrics to Describe Adversarial Attacks**

论度量方法描述对抗性攻击的有效性 cs.LG

7 pages, selected for presentation at AICS

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.13028v1) [paper-pdf](http://arxiv.org/pdf/2301.13028v1)

**Authors**: Tommaso Puccetti, Tommaso Zoppi, Andrea Ceccarelli

**Abstract**: Adversarial defenses are naturally evaluated on their ability to tolerate adversarial attacks. To test defenses, diverse adversarial attacks are crafted, that are usually described in terms of their evading capability and the L0, L1, L2, and Linf norms. We question if the evading capability and L-norms are the most effective information to claim that defenses have been tested against a representative attack set. To this extent, we select image quality metrics from the state of the art and search correlations between image perturbation and detectability. We observe that computing L-norms alone is rarely the preferable solution. We observe a strong correlation between the identified metrics computed on an adversarial image and the output of a detector on such an image, to the extent that they can predict the response of a detector with approximately 0.94 accuracy. Further, we observe that metrics can classify attacks based on similar perturbations and similar detectability. This suggests a possible review of the approach to evaluate detectors, where additional metrics are included to assure that a representative attack dataset is selected.

摘要: 对抗性防御自然是根据其容忍对抗性攻击的能力进行评估的。为了测试防御，设计了不同的对抗性攻击，通常根据它们的躲避能力和L0、L1、L2和LINF规范进行描述。我们质疑规避能力和L-范数是否是声称防御已经针对典型攻击集进行了测试的最有效信息。为此，我们从最先进的图像质量度量中选择图像质量度量，并寻找图像扰动和可检测性之间的相关性。我们观察到，单独计算L-范数很少是更好的解决方案。我们观察到在敌方图像上计算的识别度量与检测器在这样的图像上的输出之间存在很强的相关性，以至于它们可以以大约0.94的精度预测检测器的响应。此外，我们观察到，度量可以基于相似的扰动和相似的可检测性来对攻击进行分类。这建议对评估检测器的方法进行可能的审查，其中包括其他指标，以确保选择具有代表性的攻击数据集。



## **14. PCV: A Point Cloud-Based Network Verifier**

PCV：一种基于点云的网络验证器 cs.CV

11 pages, 12 figures

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.11806v2) [paper-pdf](http://arxiv.org/pdf/2301.11806v2)

**Authors**: Arup Kumar Sarker, Farzana Yasmin Ahmad, Matthew B. Dwyer

**Abstract**: 3D vision with real-time LiDAR-based point cloud data became a vital part of autonomous system research, especially perception and prediction modules use for object classification, segmentation, and detection. Despite their success, point cloud-based network models are vulnerable to multiple adversarial attacks, where the certain factor of changes in the validation set causes significant performance drop in well-trained networks. Most of the existing verifiers work perfectly on 2D convolution. Due to complex architecture, dimension of hyper-parameter, and 3D convolution, no verifiers can perform the basic layer-wise verification. It is difficult to conclude the robustness of a 3D vision model without performing the verification. Because there will be always corner cases and adversarial input that can compromise the model's effectiveness.   In this project, we describe a point cloud-based network verifier that successfully deals state of the art 3D classifier PointNet verifies the robustness by generating adversarial inputs. We have used extracted properties from the trained PointNet and changed certain factors for perturbation input. We calculate the impact on model accuracy versus property factor and can test PointNet network's robustness against a small collection of perturbing input states resulting from adversarial attacks like the suggested hybrid reverse signed attack. The experimental results reveal that the resilience property of PointNet is affected by our hybrid reverse signed perturbation strategy

摘要: 基于激光雷达的实时点云数据的三维视觉成为自主系统研究的重要组成部分，特别是用于目标分类、分割和检测的感知和预测模块。尽管基于点云的网络模型取得了成功，但它们仍然容易受到多个对抗性攻击，在这些攻击中，验证集中的某些因素变化会导致训练有素的网络的性能显著下降。现有的大多数验证器在2D卷积上都能很好地工作。由于复杂的结构、超参数的维度和三维卷积，没有验证器可以执行基本的层级验证。在不进行验证的情况下很难得出3D视觉模型的稳健性结论。因为总是会有可能损害模型有效性的转折点和对抗性输入。在这个项目中，我们描述了一个基于点云的网络验证器，它成功地处理了最先进的3D分类器PointNet通过生成敌意输入来验证其健壮性。我们使用了从训练点网络中提取的属性，并改变了某些因素作为扰动输入。我们计算了模型精度随属性因素的影响，并可以测试PointNet网络对来自对抗性攻击(如建议的混合反向签名攻击)的少量扰动输入状态的稳健性。实验结果表明，混合反向符号扰动策略对PointNet的恢复性能有一定的影响



## **15. Improving Adversarial Transferability with Scheduled Step Size and Dual Example**

利用预定步长和对偶样本提高对抗性转移能力 cs.LG

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12968v1) [paper-pdf](http://arxiv.org/pdf/2301.12968v1)

**Authors**: Zeliang Zhang, Peihan Liu, Xiaosen Wang, Chenliang Xu

**Abstract**: Deep neural networks are widely known to be vulnerable to adversarial examples, especially showing significantly poor performance on adversarial examples generated under the white-box setting. However, most white-box attack methods rely heavily on the target model and quickly get stuck in local optima, resulting in poor adversarial transferability. The momentum-based methods and their variants are proposed to escape the local optima for better transferability. In this work, we notice that the transferability of adversarial examples generated by the iterative fast gradient sign method (I-FGSM) exhibits a decreasing trend when increasing the number of iterations. Motivated by this finding, we argue that the information of adversarial perturbations near the benign sample, especially the direction, benefits more on the transferability. Thus, we propose a novel strategy, which uses the Scheduled step size and the Dual example (SD), to fully utilize the adversarial information near the benign sample. Our proposed strategy can be easily integrated with existing adversarial attack methods for better adversarial transferability. Empirical evaluations on the standard ImageNet dataset demonstrate that our proposed method can significantly enhance the transferability of existing adversarial attacks.

摘要: 众所周知，深度神经网络很容易受到对抗性实例的影响，特别是在白盒设置下生成的对抗性实例上表现出显著的低性能。然而，大多数白盒攻击方法严重依赖于目标模型，并很快陷入局部最优，导致对手可转换性差。提出了基于动量的方法及其变种，以避免陷入局部最优，以获得更好的可转移性。在这项工作中，我们注意到迭代快速梯度符号方法(I-FGSM)生成的对抗性样本的可转移性随着迭代次数的增加而呈现下降的趋势。基于这一发现，我们认为，良性样本附近的对抗性扰动信息，特别是方向，对转移更有利。因此，我们提出了一种新的策略，该策略使用预定步长和对偶样本(SD)来充分利用良性样本附近的敌意信息。我们提出的策略可以很容易地与现有的对抗性攻击方法相结合，以获得更好的对抗性可转移性。在标准ImageNet数据集上的实验结果表明，该方法能够显著提高现有对抗性攻击的可转移性。



## **16. Identifying Adversarially Attackable and Robust Samples**

识别恶意攻击和健壮样本 cs.LG

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12896v1) [paper-pdf](http://arxiv.org/pdf/2301.12896v1)

**Authors**: Vyas Raina, Mark Gales

**Abstract**: This work proposes a novel perspective on adversarial attacks by introducing the concept of sample attackability and robustness. Adversarial attacks insert small, imperceptible perturbations to the input that cause large, undesired changes to the output of deep learning models. Despite extensive research on generating adversarial attacks and building defense systems, there has been limited research on understanding adversarial attacks from an input-data perspective. We propose a deep-learning-based method for detecting the most attackable and robust samples in an unseen dataset for an unseen target model. The proposed method is based on a neural network architecture that takes as input a sample and outputs a measure of attackability or robustness. The proposed method is evaluated using a range of different models and different attack methods, and the results demonstrate its effectiveness in detecting the samples that are most likely to be affected by adversarial attacks. Understanding sample attackability can have important implications for future work in sample-selection tasks. For example in active learning, the acquisition function can be designed to select the most attackable samples, or in adversarial training, only the most attackable samples are selected for augmentation.

摘要: 该工作通过引入样本可攻击性和健壮性的概念，为对抗性攻击提出了一个新的视角。对抗性攻击在输入中插入微小的、不可察觉的扰动，从而导致深度学习模型的输出发生巨大的、不希望看到的变化。尽管对生成对抗性攻击和建立防御系统进行了广泛的研究，但从输入数据的角度理解对抗性攻击的研究有限。我们提出了一种基于深度学习的方法，用于从不可见目标模型的不可见数据集中检测最具攻击性和健壮性的样本。该方法基于神经网络结构，以样本为输入，输出可攻击性或稳健性的度量。使用一系列不同的模型和不同的攻击方法对该方法进行了评估，结果表明该方法在检测最容易受到对抗性攻击影响的样本方面是有效的。了解样本可攻击性对样本选择任务中的未来工作具有重要意义。例如，在主动学习中，获取函数可以被设计为选择最可攻击的样本，或者在对抗性训练中，仅选择最可攻击的样本进行增强。



## **17. On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex**

基于大预训练语言模型的基于提示的语义分析的稳健性研究--基于CODEX的实证研究 cs.CL

Accepted at EACL2023 (main)

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12868v1) [paper-pdf](http://arxiv.org/pdf/2301.12868v1)

**Authors**: Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, Fatemeh Shiri

**Abstract**: Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advancements in few-shot language models trained on code have demonstrated superior performance in generating these representations compared to traditional unimodal language models, which are trained on downstream tasks. Despite these advancements, existing fine-tuned neural semantic parsers are susceptible to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a large prompt-based language model of code, \codex. Our results demonstrate that the state-of-the-art (SOTA) code-language models are vulnerable to carefully crafted adversarial examples. To address this challenge, we propose methods for improving robustness without the need for significant amounts of labeled data or heavy computational resources.

摘要: 语义分析是一种旨在构建自然语言问题意义的结构化表示的技术。与在下游任务上训练的传统单峰语言模型相比，在生成这些表示方面，少数几次语言模型的最新进展已经显示出更好的性能。尽管有这些进步，但现有的微调神经语义解析器很容易受到自然语言输入的对抗性攻击。虽然已经确定可以通过对抗性训练来增强较小语义解析器的稳健性，但这种方法对于真实世界场景中的大型语言模型是不可行的，因为它需要大量的计算资源和昂贵的人工对域内语义解析数据的注释。本文首次对基于提示的大型代码语言模型CODEX的对手健壮性进行了实证研究。我们的结果表明，最先进的(SOTA)代码语言模型容易受到精心设计的敌意示例的攻击。为了应对这一挑战，我们提出了在不需要大量标记数据或大量计算资源的情况下提高稳健性的方法。



## **18. The PartialSpoof Database and Countermeasures for the Detection of Short Fake Speech Segments Embedded in an Utterance**

PartialSpoof数据库及其在语音中嵌入短伪语段检测的对策 eess.AS

Published in IEEE/ACM Transactions on Audio, Speech, and Language  Processing (DOI: 10.1109/TASLP.2022.3233236)

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2204.05177v3) [paper-pdf](http://arxiv.org/pdf/2204.05177v3)

**Authors**: Lin Zhang, Xin Wang, Erica Cooper, Nicholas Evans, Junichi Yamagishi

**Abstract**: Automatic speaker verification is susceptible to various manipulations and spoofing, such as text-to-speech synthesis, voice conversion, replay, tampering, adversarial attacks, and so on. We consider a new spoofing scenario called "Partial Spoof" (PS) in which synthesized or transformed speech segments are embedded into a bona fide utterance. While existing countermeasures (CMs) can detect fully spoofed utterances, there is a need for their adaptation or extension to the PS scenario. We propose various improvements to construct a significantly more accurate CM that can detect and locate short-generated spoofed speech segments at finer temporal resolutions. First, we introduce newly developed self-supervised pre-trained models as enhanced feature extractors. Second, we extend our PartialSpoof database by adding segment labels for various temporal resolutions. Since the short spoofed speech segments to be embedded by attackers are of variable length, six different temporal resolutions are considered, ranging from as short as 20 ms to as large as 640 ms. Third, we propose a new CM that enables the simultaneous use of the segment-level labels at different temporal resolutions as well as utterance-level labels to execute utterance- and segment-level detection at the same time. We also show that the proposed CM is capable of detecting spoofing at the utterance level with low error rates in the PS scenario as well as in a related logical access (LA) scenario. The equal error rates of utterance-level detection on the PartialSpoof database and ASVspoof 2019 LA database were 0.77 and 0.90%, respectively.

摘要: 自动说话人确认容易受到各种操纵和欺骗，如文语转换、语音转换、重放、篡改、敌意攻击等。我们考虑了一种称为“部分欺骗”(PS)的新的欺骗场景，在该场景中，合成或转换的语音片段被嵌入到真实话语中。虽然现有的对策(CMS)可以检测完全欺骗的话语，但它们需要适应或扩展到PS场景。我们提出了各种改进来构造一个明显更准确的CM，它可以在更精细的时间分辨率上检测和定位短生成的欺骗语音片段。首先，我们引入了新发展的自监督预训练模型作为增强的特征提取工具。其次，我们通过为不同的时间分辨率添加分段标签来扩展我们的PartialSpoof数据库。由于攻击者要嵌入的短欺骗语音段的长度是可变的，因此考虑了六种不同的时间分辨率，从短到20ms到大到640ms。第三，我们提出了一种新的CM，它能够同时使用不同时间分辨率的段级标签以及语音级标签来同时执行语音级和段级检测。在PS场景和相关的逻辑接入(LA)场景中，我们还证明了所提出的CM能够以较低的错误率检测发声级别的欺骗。在PartialSpoof数据库和ASVspoof 2019 LA数据库上，语音级检测的等同错误率分别为0.77和0.90%。



## **19. GPS-Spoofing Attack Detection Mechanism for UAV Swarms**

无人机群的GPS欺骗攻击检测机制 cs.CR

8 pages, 3 figures

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12766v1) [paper-pdf](http://arxiv.org/pdf/2301.12766v1)

**Authors**: Pavlo Mykytyn, Marcin Brzozowski, Zoya Dyka, Peter Langendoerfer

**Abstract**: Recently autonomous and semi-autonomous Unmanned Aerial Vehicle (UAV) swarms started to receive a lot of research interest and demand from various civil application fields. However, for successful mission execution, UAV swarms require Global navigation satellite system signals and in particular, Global Positioning System (GPS) signals for navigation. Unfortunately, civil GPS signals are unencrypted and unauthenticated, which facilitates the execution of GPS spoofing attacks. During these attacks, adversaries mimic the authentic GPS signal and broadcast it to the targeted UAV in order to change its course, and force it to land or crash. In this study, we propose a GPS spoofing detection mechanism capable of detecting single-transmitter and multi-transmitter GPS spoofing attacks to prevent the outcomes mentioned above. Our detection mechanism is based on comparing the distance between each two swarm members calculated from their GPS coordinates to the distance acquired from Impulse Radio Ultra-Wideband ranging between the same swarm members. If the difference in distances is larger than a chosen threshold the GPS spoofing attack is declared detected.

摘要: 近年来，自主和半自主无人机群体开始受到各个民用应用领域的研究兴趣和需求。然而，为了成功执行任务，无人机群需要全球导航卫星系统信号，特别是导航全球定位系统(GPS)信号。不幸的是，民用GPS信号是未加密和未认证的，这为GPS欺骗攻击的执行提供了便利。在这些攻击中，对手模仿真实的GPS信号，将其广播到目标无人机，以改变其航线，并迫使其着陆或坠毁。在这项研究中，我们提出了一种GPS欺骗检测机制，能够检测到单发送器和多发送器的GPS欺骗攻击，以防止上述结果。我们的检测机制是基于比较两个蜂群成员之间的距离，该距离是根据它们的GPS坐标计算的，与通过ImPulse Radio超宽带获得的相同蜂群成员之间的距离进行比较。如果距离之差大于选定的阈值，则宣布检测到GPS欺骗攻击。



## **20. Private Node Selection in Personalized Decentralized Learning**

个性化分散学习中的私有节点选择 cs.LG

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12755v1) [paper-pdf](http://arxiv.org/pdf/2301.12755v1)

**Authors**: Edvin Listo Zec, Johan Östman, Olof Mogren, Daniel Gillblad

**Abstract**: In this paper, we propose a novel approach for privacy-preserving node selection in personalized decentralized learning, which we refer to as Private Personalized Decentralized Learning (PPDL). Our method mitigates the risk of inference attacks through the use of secure aggregation while simultaneously enabling efficient identification of collaborators. This is achieved by leveraging adversarial multi-armed bandit optimization that exploits dependencies between the different arms. Through comprehensive experimentation on various benchmarks under label and covariate shift, we demonstrate that our privacy-preserving approach outperforms previous non-private methods in terms of model performance.

摘要: 本文提出了一种个性化分散学习中隐私保护节点选择的新方法，称为私有个性化分散学习(PPDL)。我们的方法通过使用安全聚合来降低推理攻击的风险，同时能够有效地识别协作者。这是通过利用对抗性多臂强盗优化来实现的，该优化利用了不同武器之间的依赖关系。通过在标号和协变量平移下的各种基准测试的综合实验，我们证明了我们的隐私保护方法在模型性能方面优于以往的非私有方法。



## **21. Robust Stochastic Linear Contextual Bandits Under Adversarial Attacks**

对抗性攻击下的稳健随机线性关联带 stat.ML

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2106.02978v3) [paper-pdf](http://arxiv.org/pdf/2106.02978v3)

**Authors**: Qin Ding, Cho-Jui Hsieh, James Sharpnack

**Abstract**: Stochastic linear contextual bandit algorithms have substantial applications in practice, such as recommender systems, online advertising, clinical trials, etc. Recent works show that optimal bandit algorithms are vulnerable to adversarial attacks and can fail completely in the presence of attacks. Existing robust bandit algorithms only work for the non-contextual setting under the attack of rewards and cannot improve the robustness in the general and popular contextual bandit environment. In addition, none of the existing methods can defend against attacked context. In this work, we provide the first robust bandit algorithm for stochastic linear contextual bandit setting under a fully adaptive and omniscient attack with sub-linear regret. Our algorithm not only works under the attack of rewards, but also under attacked context. Moreover, it does not need any information about the attack budget or the particular form of the attack. We provide theoretical guarantees for our proposed algorithm and show by experiments that our proposed algorithm improves the robustness against various kinds of popular attacks.

摘要: 随机线性上下文盗贼算法在推荐系统、在线广告、临床试验等方面有着广泛的应用。最近的研究表明，最优盗贼算法很容易受到对手攻击，并且在攻击存在的情况下可能完全失败。现有的稳健盗贼算法只适用于奖赏攻击下的非上下文环境，不能提高在一般流行的上下文盗贼环境下的鲁棒性。此外，现有的方法都不能防御受攻击的上下文。在这项工作中，我们给出了在完全自适应的、具有子线性后悔的全知攻击下，随机线性上下文盗贼环境下的第一个稳健的盗贼算法。我们的算法不仅能在奖励攻击下工作，而且能在受攻击的环境下工作。此外，它不需要关于攻击预算或特定攻击形式的任何信息。我们为我们提出的算法提供了理论上的保证，实验表明，我们提出的算法提高了对各种流行攻击的稳健性。



## **22. Sparse Oblique Decision Trees: A Tool to Understand and Manipulate Neural Net Features**

稀疏斜决策树：一种理解和操纵神经网络特征的工具 cs.LG

Appears in Data Mining and Knowledge Discovery (2023), Special Issue  on Explainable and Interpretable Machine Learning and Data Mining

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2104.02922v2) [paper-pdf](http://arxiv.org/pdf/2104.02922v2)

**Authors**: Suryabhan Singh Hada, Miguel Á. Carreira-Perpiñán, Arman Zharmagambetov

**Abstract**: The widespread deployment of deep nets in practical applications has lead to a growing desire to understand how and why such black-box methods perform prediction. Much work has focused on understanding what part of the input pattern (an image, say) is responsible for a particular class being predicted, and how the input may be manipulated to predict a different class. We focus instead on understanding which of the internal features computed by the neural net are responsible for a particular class. We achieve this by mimicking part of the neural net with an oblique decision tree having sparse weight vectors at the decision nodes. Using the recently proposed Tree Alternating Optimization (TAO) algorithm, we are able to learn trees that are both highly accurate and interpretable. Such trees can faithfully mimic the part of the neural net they replaced, and hence they can provide insights into the deep net black box. Further, we show we can easily manipulate the neural net features in order to make the net predict, or not predict, a given class, thus showing that it is possible to carry out adversarial attacks at the level of the features. These insights and manipulations apply globally to the entire training and test set, not just at a local (single-instance) level. We demonstrate this robustly in the MNIST and ImageNet datasets with LeNet5 and VGG networks.

摘要: 随着深度网络在实际应用中的广泛应用，人们越来越希望了解这种黑盒方法如何以及为什么执行预测。很多工作都集中在理解输入模式(比如图像)的哪一部分负责预测特定的类，以及如何操作输入来预测不同的类。相反，我们将重点放在了解神经网络计算出的哪些内部特征对特定类负责。我们通过在决策节点处具有稀疏权向量的倾斜决策树来模仿神经网络的一部分来实现这一点。使用最近提出的树交替优化(TAO)算法，我们能够学习既高精度又可解释的树。这种树可以忠实地模仿它们所取代的神经网络的一部分，因此它们可以提供对深层网络黑匣子的洞察。此外，我们还表明，我们可以很容易地操纵神经网络的特征，以使网络预测或不预测给定的类别，从而表明在特征级别上进行对抗性攻击是可能的。这些见解和操作适用于整个培训和测试集，而不仅仅是局部(单实例)级别。我们在使用LeNet5和VGG网络的MNIST和ImageNet数据集中稳健地演示了这一点。



## **23. Attack Impact Evaluation for Stochastic Control Systems through Alarm Flag State Augmentation**

基于报警标志状态增强的随机控制系统攻击影响评估 math.OC

8 pages. arXiv admin note: substantial text overlap with  arXiv:2203.16803

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12684v1) [paper-pdf](http://arxiv.org/pdf/2301.12684v1)

**Authors**: Hampei Sasahara, Takashi Tanaka, Henrik Sandberg

**Abstract**: This note addresses the problem of evaluating the impact of an attack on discrete-time nonlinear stochastic control systems. The problem is formulated as an optimal control problem with a joint chance constraint that forces the adversary to avoid detection throughout a given time period. Due to the joint constraint, the optimal control policy depends not only on the current state, but also on the entire history, leading to an explosion of the search space and making the problem generally intractable. However, we discover that the current state and whether an alarm has been triggered, or not, is sufficient for specifying the optimal decision at each time step. This information, which we refer to as the alarm flag, can be added to the state space to create an equivalent optimal control problem that can be solved with existing numerical approaches using a Markov policy. Additionally, we note that the formulation results in a policy that does not avoid detection once an alarm has been triggered. We extend the formulation to handle multi-alarm avoidance policies for more reasonable attack impact evaluations, and show that the idea of augmenting the state space with an alarm flag is valid in this extended formulation as well.

摘要: 本文讨论了评估攻击对离散时间非线性随机控制系统的影响的问题。该问题被描述为一个带有联合机会约束的最优控制问题，迫使对手在给定的时间段内避免被发现。由于联合约束，最优控制策略不仅取决于当前状态，还取决于整个历史，导致搜索空间爆炸，使问题总体上难以解决。然而，我们发现，当前状态以及是否触发了警报，足以指定每个时间步的最优决策。这种信息，我们称为警报标志，可以被添加到状态空间，以创建等价的最优控制问题，该问题可以使用使用马尔可夫策略的现有数值方法来解决。此外，我们注意到，这一提法产生了一种一旦触发警报就不会逃避检测的策略。我们将该公式扩展到处理多个警报避免策略，以更合理地评估攻击影响，并证明了在该扩展公式中使用警报标志来扩展状态空间的思想也是有效的。



## **24. Feature-Space Bayesian Adversarial Learning Improved Malware Detector Robustness**

特征空间贝叶斯对抗学习提高恶意软件检测器的稳健性 cs.CR

Accepted to AAAI 2023 conference

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12680v1) [paper-pdf](http://arxiv.org/pdf/2301.12680v1)

**Authors**: Bao Gia Doan, Shuiqiao Yang, Paul Montague, Olivier De Vel, Tamas Abraham, Seyit Camtepe, Salil S. Kanhere, Ehsan Abbasnejad, Damith C. Ranasinghe

**Abstract**: We present a new algorithm to train a robust malware detector. Modern malware detectors rely on machine learning algorithms. Now, the adversarial objective is to devise alterations to the malware code to decrease the chance of being detected whilst preserving the functionality and realism of the malware. Adversarial learning is effective in improving robustness but generating functional and realistic adversarial malware samples is non-trivial. Because: i) in contrast to tasks capable of using gradient-based feedback, adversarial learning in a domain without a differentiable mapping function from the problem space (malware code inputs) to the feature space is hard; and ii) it is difficult to ensure the adversarial malware is realistic and functional. This presents a challenge for developing scalable adversarial machine learning algorithms for large datasets at a production or commercial scale to realize robust malware detectors. We propose an alternative; perform adversarial learning in the feature space in contrast to the problem space. We prove the projection of perturbed, yet valid malware, in the problem space into feature space will always be a subset of adversarials generated in the feature space. Hence, by generating a robust network against feature-space adversarial examples, we inherently achieve robustness against problem-space adversarial examples. We formulate a Bayesian adversarial learning objective that captures the distribution of models for improved robustness. We prove that our learning method bounds the difference between the adversarial risk and empirical risk explaining the improved robustness. We show that adversarially trained BNNs achieve state-of-the-art robustness. Notably, adversarially trained BNNs are robust against stronger attacks with larger attack budgets by a margin of up to 15% on a recent production-scale malware dataset of more than 20 million samples.

摘要: 我们提出了一种新的算法来训练一个健壮的恶意软件检测器。现代恶意软件检测器依赖于机器学习算法。现在，敌意的目标是设计对恶意软件代码的更改，以减少被检测到的机会，同时保留恶意软件的功能和真实性。对抗性学习在提高健壮性方面是有效的，但生成功能性和真实性的对抗性恶意软件样本并不是一件容易的事情。这是因为：i)与能够使用基于梯度的反馈的任务相比，在没有从问题空间(恶意软件代码输入)到特征空间的可微映射函数的域中的对抗性学习是困难的；以及ii)很难确保对抗性恶意软件是真实的和起作用的。这对在生产或商业规模的大数据集上开发可扩展的对抗性机器学习算法以实现健壮的恶意软件检测器提出了挑战。我们提出了一种替代方案：在特征空间中进行对抗性学习，而不是在问题空间中。我们证明了问题空间中被扰动但有效的恶意软件在特征空间中的投影总是在特征空间中生成的敌意的子集。因此，通过生成针对特征空间敌意示例的健壮网络，我们内在地实现了针对问题空间敌意示例的健壮性。我们制定了一个贝叶斯对抗性学习目标，该目标捕捉模型的分布以提高稳健性。我们证明了我们的学习方法限定了对手风险和经验风险之间的差异，解释了改进的稳健性。我们证明了反向训练的BNN达到了最先进的稳健性。值得注意的是，经过敌意训练的BNN对更强大的攻击具有健壮性，在最近超过2000万个样本的生产规模恶意软件数据集上，攻击预算更大，幅度高达15%。



## **25. Lateralized Learning for Multi-Class Visual Classification Tasks**

多类视觉分类任务的偏侧化学习 cs.CV

13 pages, 5 figures

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12637v1) [paper-pdf](http://arxiv.org/pdf/2301.12637v1)

**Authors**: Abubakar Siddique, Will N. Browne, Gina M. Grimshaw

**Abstract**: The majority of computer vision algorithms fail to find higher-order (abstract) patterns in an image so are not robust against adversarial attacks, unlike human lateralized vision. Deep learning considers each input pixel in a homogeneous manner such that different parts of a ``locality-sensitive hashing table'' are often not connected, meaning higher-order patterns are not discovered. Hence these systems are not robust against noisy, irrelevant, and redundant data, resulting in the wrong prediction being made with high confidence. Conversely, vertebrate brains afford heterogeneous knowledge representation through lateralization, enabling modular learning at different levels of abstraction. This work aims to verify the effectiveness, scalability, and robustness of a lateralized approach to real-world problems that contain noisy, irrelevant, and redundant data. The experimental results of multi-class (200 classes) image classification show that the novel system effectively learns knowledge representation at multiple levels of abstraction making it more robust than other state-of-the-art techniques. Crucially, the novel lateralized system outperformed all the state-of-the-art deep learning-based systems for the classification of normal and adversarial images by 19.05% - 41.02% and 1.36% - 49.22%, respectively. Findings demonstrate the value of heterogeneous and lateralized learning for computer vision applications.

摘要: 大多数计算机视觉算法无法在图像中发现更高阶(抽象)的模式，因此不像人类偏侧化的视觉那样，对对手攻击没有健壮性。深度学习以一种同类的方式考虑每个输入像素，从而使“对位置敏感的哈希表”的不同部分通常不连接，这意味着不会发现更高阶的模式。因此，这些系统对噪声、无关和冗余的数据不具有健壮性，从而导致错误的预测被高置信度地做出。相反，脊椎动物的大脑通过偏侧化提供不同的知识表征，使不同抽象水平的模块化学习成为可能。这项工作旨在验证针对包含噪声、无关和冗余数据的真实世界问题的偏侧化方法的有效性、可扩展性和健壮性。对多类(200类)图像分类的实验结果表明，该系统能够有效地在多个抽象层次上学习知识表示，具有比其他先进技术更好的鲁棒性。关键是，新的侧化系统在正常图像和对抗性图像的分类上分别比所有最先进的基于深度学习的系统高19.05%-41.02%和1.36%-49.22%。研究结果证明了异质和侧化学习对计算机视觉应用的价值。



## **26. Adapting Step-size: A Unified Perspective to Analyze and Improve Gradient-based Methods for Adversarial Attacks**

调整步长：分析和改进基于梯度的对抗性攻击方法的统一视角 cs.LG

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.11546v2) [paper-pdf](http://arxiv.org/pdf/2301.11546v2)

**Authors**: Wei Tao, Lei Bao, Sheng Long, Gaowei Wu, Qing Tao

**Abstract**: Learning adversarial examples can be formulated as an optimization problem of maximizing the loss function with some box-constraints. However, for solving this induced optimization problem, the state-of-the-art gradient-based methods such as FGSM, I-FGSM and MI-FGSM look different from their original methods especially in updating the direction, which makes it difficult to understand them and then leaves some theoretical issues to be addressed in viewpoint of optimization. In this paper, from the perspective of adapting step-size, we provide a unified theoretical interpretation of these gradient-based adversarial learning methods. We show that each of these algorithms is in fact a specific reformulation of their original gradient methods but using the step-size rules with only current gradient information. Motivated by such analysis, we present a broad class of adaptive gradient-based algorithms based on the regular gradient methods, in which the step-size strategy utilizing information of the accumulated gradients is integrated. Such adaptive step-size strategies directly normalize the scale of the gradients rather than use some empirical operations. The important benefit is that convergence for the iterative algorithms is guaranteed and then the whole optimization process can be stabilized. The experiments demonstrate that our AdaI-FGM consistently outperforms I-FGSM and AdaMI-FGM remains competitive with MI-FGSM for black-box attacks.

摘要: 对抗性例子的学习可以归结为在一定的盒子约束下最大化损失函数的最优化问题。然而，为了解决这一诱导优化问题，现有的基于梯度的方法，如FGSM、I-FGSM和MI-FGSM，在方向更新方面与原来的方法有很大的不同，这使得人们很难理解它们，从而留下了一些理论问题需要从优化的角度来解决。本文从自适应步长的角度，对这些基于梯度的对抗性学习方法进行了统一的理论解释。我们证明了这些算法中的每一个实际上都是其原始梯度方法的具体改写，但使用了仅具有当前梯度信息的步长规则。在这种分析的基础上，我们提出了一类基于规则梯度方法的自适应梯度算法，其中集成了利用累积梯度信息的步长策略。这种自适应步长策略直接对梯度的规模进行标准化，而不是使用一些经验操作。重要的好处是保证了迭代算法的收敛，从而稳定了整个优化过程。实验表明，我们的ADAI-FGM在黑盒攻击方面始终优于I-FGSM，并且在黑盒攻击方面仍具有与MI-FGSM的竞争力。



## **27. Adversarial Attacks on Adversarial Bandits**

对抗性强盗的对抗性攻击 cs.LG

Accepted by ICLR 2023

**SubmitDate**: 2023-01-30    [abs](http://arxiv.org/abs/2301.12595v1) [paper-pdf](http://arxiv.org/pdf/2301.12595v1)

**Authors**: Yuzhe Ma, Zhijin Zhou

**Abstract**: We study a security threat to adversarial multi-armed bandits, in which an attacker perturbs the loss or reward signal to control the behavior of the victim bandit player. We show that the attacker is able to mislead any no-regret adversarial bandit algorithm into selecting a suboptimal target arm in every but sublinear (T-o(T)) number of rounds, while incurring only sublinear (o(T)) cumulative attack cost. This result implies critical security concern in real-world bandit-based systems, e.g., in online recommendation, an attacker might be able to hijack the recommender system and promote a desired product. Our proposed attack algorithms require knowledge of only the regret rate, thus are agnostic to the concrete bandit algorithm employed by the victim player. We also derived a theoretical lower bound on the cumulative attack cost that any victim-agnostic attack algorithm must incur. The lower bound matches the upper bound achieved by our attack, which shows that our attack is asymptotically optimal.

摘要: 研究了对抗性多臂强盗的安全威胁，其中攻击者干扰损失或奖励信号以控制受害者强盗玩家的行为。我们证明了攻击者能够在次线性(T-o(T))次轮次中误导任何无悔意的对抗性盗贼算法选择次优的目标臂，而仅招致次线性(o(T))累积攻击代价。这一结果意味着在现实世界中基于强盗的系统中存在严重的安全问题，例如在在线推荐中，攻击者可能能够劫持推荐系统并推广所需的产品。我们提出的攻击算法只需要知道悔过率，因此与受害者玩家使用的具体盗贼算法无关。我们还推导出了任何受害者不可知攻击算法必须引起的累积攻击成本的理论下界。下界与我们的攻击所达到的上界相匹配，这表明我们的攻击是渐近最优的。



## **28. Uncovering Adversarial Risks of Test-Time Adaptation**

揭示考试时间调整的对抗性风险 cs.LG

**SubmitDate**: 2023-01-29    [abs](http://arxiv.org/abs/2301.12576v1) [paper-pdf](http://arxiv.org/pdf/2301.12576v1)

**Authors**: Tong Wu, Feiran Jia, Xiangyu Qi, Jiachen T. Wang, Vikash Sehwag, Saeed Mahloujifar, Prateek Mittal

**Abstract**: Recently, test-time adaptation (TTA) has been proposed as a promising solution for addressing distribution shifts. It allows a base model to adapt to an unforeseen distribution during inference by leveraging the information from the batch of (unlabeled) test data. However, we uncover a novel security vulnerability of TTA based on the insight that predictions on benign samples can be impacted by malicious samples in the same batch. To exploit this vulnerability, we propose Distribution Invading Attack (DIA), which injects a small fraction of malicious data into the test batch. DIA causes models using TTA to misclassify benign and unperturbed test data, providing an entirely new capability for adversaries that is infeasible in canonical machine learning pipelines. Through comprehensive evaluations, we demonstrate the high effectiveness of our attack on multiple benchmarks across six TTA methods. In response, we investigate two countermeasures to robustify the existing insecure TTA implementations, following the principle of "security by design". Together, we hope our findings can make the community aware of the utility-security tradeoffs in deploying TTA and provide valuable insights for developing robust TTA approaches.

摘要: 最近，测试时间适应(TTA)被提出作为解决分布偏移的一种有前途的解决方案。它允许基本模型通过利用来自一批(未标记的)测试数据的信息来适应推理过程中的意外分布。然而，我们发现了TTA的一个新的安全漏洞，这是基于对良性样本的预测可能会受到同一批恶意样本的影响的洞察。为了利用这个漏洞，我们提出了分布式入侵攻击(DIA)，它将一小部分恶意数据注入到测试批次中。DIA导致使用TTA的模型对良性和未受干扰的测试数据进行错误分类，为对手提供了一种全新的能力，这在规范的机器学习管道中是不可行的。通过综合评估，我们证明了我们在六种TTA方法上对多个基准的攻击的高效性。作为回应，我们研究了两种对策，以增强现有不安全的TTA实现的健壮性，遵循“设计安全”的原则。总之，我们希望我们的发现能够让社区意识到部署TTA时的效用和安全权衡，并为开发强大的TTA方法提供有价值的见解。



## **29. Improving the Accuracy-Robustness Trade-off of Classifiers via Adaptive Smoothing**

用自适应平滑提高分类器的精度和稳健性 cs.LG

**SubmitDate**: 2023-01-29    [abs](http://arxiv.org/abs/2301.12554v1) [paper-pdf](http://arxiv.org/pdf/2301.12554v1)

**Authors**: Yatong Bai, Brendon G. Anderson, Aerin Kim, Somayeh Sojoudi

**Abstract**: While it is shown in the literature that simultaneously accurate and robust classifiers exist for common datasets, previous methods that improve the adversarial robustness of classifiers often manifest an accuracy-robustness trade-off. We build upon recent advancements in data-driven ``locally biased smoothing'' to develop classifiers that treat benign and adversarial test data differently. Specifically, we tailor the smoothing operation to the usage of a robust neural network as the source of robustness. We then extend the smoothing procedure to the multi-class setting and adapt an adversarial input detector into a policy network. The policy adaptively adjusts the mixture of the robust base classifier and a standard network, where the standard network is optimized for clean accuracy and is not robust in general. We provide theoretical analyses to motivate the use of the adaptive smoothing procedure, certify the robustness of the smoothed classifier under realistic assumptions, and justify the introduction of the policy network. We use various attack methods, including AutoAttack and adaptive attack, to empirically verify that the smoothed model noticeably improves the accuracy-robustness trade-off. On the CIFAR-100 dataset, our method simultaneously achieves an 80.09\% clean accuracy and a 32.94\% AutoAttacked accuracy. The code that implements adaptive smoothing is available at https://github.com/Bai-YT/AdaptiveSmoothing.

摘要: 虽然文献表明，对于常见的数据集，同时存在准确和健壮的分类器，但以前提高分类器的对抗性健壮性的方法往往表现出精度和健壮性之间的权衡。我们在数据驱动的“局部偏向平滑”方面的最新进展的基础上，开发出以不同方式对待良性和对抗性测试数据的分类器。具体地说，我们定制了平滑操作，以使用健壮的神经网络作为健壮性的来源。然后，我们将平滑过程扩展到多类设置，并将敌意输入检测器适应于策略网络。该策略自适应地调整稳健的基分类器和标准网络的混合，其中标准网络针对干净的精度进行了优化，并且总体上是不稳健的。我们提供了理论分析来激励自适应平滑过程的使用，证明了平滑分类器在现实假设下的稳健性，并证明了引入策略网络的合理性。我们使用了各种攻击方法，包括AutoAttack和自适应攻击，实证验证了平滑后的模型显著改善了准确性和稳健性之间的权衡。在CIFAR-100数据集上，我们的方法同时达到了80.09的清洁精度和32.94的自动附加精度。实现自适应平滑的代码可从https://github.com/Bai-YT/AdaptiveSmoothing.获得



## **30. Mitigating Adversarial Effects of False Data Injection Attacks in Power Grid**

缓解电网虚假数据注入攻击的敌意影响 cs.CR

**SubmitDate**: 2023-01-29    [abs](http://arxiv.org/abs/2301.12487v1) [paper-pdf](http://arxiv.org/pdf/2301.12487v1)

**Authors**: Farhin Farhad Riya, Shahinul Hoque, Jinyuan Stella Sun, Jiangnan Li

**Abstract**: Deep Neural Networks have proven to be highly accurate at a variety of tasks in recent years. The benefits of Deep Neural Networks have also been embraced in power grids to detect False Data Injection Attacks (FDIA) while conducting critical tasks like state estimation. However, the vulnerabilities of DNNs along with the distinct infrastructure of cyber-physical-system (CPS) can favor the attackers to bypass the detection mechanism. Moreover, the divergent nature of CPS engenders limitations to the conventional defense mechanisms for False Data Injection Attacks. In this paper, we propose a DNN framework with additional layer which utilizes randomization to mitigate the adversarial effect by padding the inputs. The primary advantage of our method is when deployed to a DNN model it has trivial impact on the models performance even with larger padding sizes. We demonstrate the favorable outcome of the framework through simulation using the IEEE 14-bus, 30-bus, 118-bus and 300-bus systems. Furthermore to justify the framework we select attack techniques that generate subtle adversarial examples that can bypass the detection mechanism effortlessly.

摘要: 近年来，深度神经网络已被证明在各种任务中具有很高的准确性。深度神经网络的优点也在电网中得到了应用，可以在执行状态估计等关键任务时检测虚假数据注入攻击(FDIA)。然而，DNN的漏洞以及网络物理系统(CPS)独特的基础设施可以帮助攻击者绕过检测机制。此外，CPS的发散性给传统的虚假数据注入攻击防御机制带来了限制。在本文中，我们提出了一种具有附加层的DNN框架，该框架利用随机化来通过填充输入来缓解对抗效果。我们方法的主要优势是当部署到DNN模型时，即使在较大填充大小的情况下，它对模型性能的影响也很小。通过对IEEE14节点、30节点、118节点和300节点系统的仿真，验证了该框架的有效性。此外，为了证明框架的合理性，我们选择了生成微妙的对抗性示例的攻击技术，这些示例可以毫不费力地绕过检测机制。



## **31. PrivHAR: Recognizing Human Actions From Privacy-preserving Lens**

PrivHAR：从隐私保护镜头识别人类行为 cs.CV

Oral paper presented at European Conference on Computer Vision (ECCV)  2022, in Tel Aviv, Israel

**SubmitDate**: 2023-01-29    [abs](http://arxiv.org/abs/2206.03891v2) [paper-pdf](http://arxiv.org/pdf/2206.03891v2)

**Authors**: Carlos Hinojosa, Miguel Marquez, Henry Arguello, Ehsan Adeli, Li Fei-Fei, Juan Carlos Niebles

**Abstract**: The accelerated use of digital cameras prompts an increasing concern about privacy and security, particularly in applications such as action recognition. In this paper, we propose an optimizing framework to provide robust visual privacy protection along the human action recognition pipeline. Our framework parameterizes the camera lens to successfully degrade the quality of the videos to inhibit privacy attributes and protect against adversarial attacks while maintaining relevant features for activity recognition. We validate our approach with extensive simulations and hardware experiments.

摘要: 数码相机的加速使用促使人们越来越关注隐私和安全，特别是在动作识别等应用中。在这篇文章中，我们提出了一个优化的框架，以提供稳健的视觉隐私保护沿人类行为识别管道。我们的框架对摄像机镜头进行了参数化处理，成功地降低了视频的质量，从而抑制了隐私属性并防止了敌意攻击，同时保持了活动识别的相关特征。我们通过大量的仿真和硬件实验来验证我们的方法。



## **32. Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering**

渐变成形：增强对逆向工程的后门攻击 cs.CR

**SubmitDate**: 2023-01-29    [abs](http://arxiv.org/abs/2301.12318v1) [paper-pdf](http://arxiv.org/pdf/2301.12318v1)

**Authors**: Rui Zhu, Di Tang, Siyuan Tang, Guanhong Tao, Shiqing Ma, Xiaofeng Wang, Haixu Tang

**Abstract**: Most existing methods to detect backdoored machine learning (ML) models take one of the two approaches: trigger inversion (aka. reverse engineer) and weight analysis (aka. model diagnosis). In particular, the gradient-based trigger inversion is considered to be among the most effective backdoor detection techniques, as evidenced by the TrojAI competition, Trojan Detection Challenge and backdoorBench. However, little has been done to understand why this technique works so well and, more importantly, whether it raises the bar to the backdoor attack. In this paper, we report the first attempt to answer this question by analyzing the change rate of the backdoored model around its trigger-carrying inputs. Our study shows that existing attacks tend to inject the backdoor characterized by a low change rate around trigger-carrying inputs, which are easy to capture by gradient-based trigger inversion. In the meantime, we found that the low change rate is not necessary for a backdoor attack to succeed: we design a new attack enhancement called \textit{Gradient Shaping} (GRASP), which follows the opposite direction of adversarial training to reduce the change rate of a backdoored model with regard to the trigger, without undermining its backdoor effect. Also, we provide a theoretic analysis to explain the effectiveness of this new technique and the fundamental weakness of gradient-based trigger inversion. Finally, we perform both theoretical and experimental analysis, showing that the GRASP enhancement does not reduce the effectiveness of the stealthy attacks against the backdoor detection methods based on weight analysis, as well as other backdoor mitigation methods without using detection.

摘要: 大多数现有的检测回溯机器学习(ML)模型的方法都采用两种方法之一：触发反转(又名。逆向工程)和权重分析(又名模型诊断)。特别是，基于梯度的触发器反转被认为是最有效的后门检测技术之一，特洛伊木马竞赛、特洛伊木马检测挑战赛和后门B边就是证明。然而，对于这种技术为什么如此有效，以及更重要的是，它是否提高了后门攻击的门槛，人们几乎没有做过什么。在这篇文章中，我们首次尝试通过分析回溯模型围绕其触发输入的变化率来回答这个问题。我们的研究表明，现有的攻击倾向于注入带有触发器的输入周围变化率低的后门，这很容易通过基于梯度的触发器反转来捕获。同时，我们发现低更改率并不是后门攻击成功的必要条件：我们设计了一种新的攻击增强机制，称为文本{梯度整形}(GRAPH)，它遵循对抗性训练的相反方向，在不破坏后门效应的情况下，降低后门模型关于触发的更改率。此外，我们还从理论上分析了这一新技术的有效性以及基于梯度的触发反演的根本缺陷。最后，我们进行了理论和实验分析，结果表明抓取能力的增强不会降低基于权重分析的后门检测方法以及其他不使用检测的后门防御方法的隐身攻击的有效性。



## **33. Node Injection for Class-specific Network Poisoning**

针对特定类别的网络中毒的节点注入 cs.LG

28 pages, 5 figures

**SubmitDate**: 2023-01-28    [abs](http://arxiv.org/abs/2301.12277v1) [paper-pdf](http://arxiv.org/pdf/2301.12277v1)

**Authors**: Ansh Kumar Sharma, Rahul Kukreja, Mayank Kharbanda, Tanmoy Chakraborty

**Abstract**: Graph Neural Networks (GNNs) are powerful in learning rich network representations that aid the performance of downstream tasks. However, recent studies showed that GNNs are vulnerable to adversarial attacks involving node injection and network perturbation. Among these, node injection attacks are more practical as they don't require manipulation in the existing network and can be performed more realistically. In this paper, we propose a novel problem statement - a class-specific poison attack on graphs in which the attacker aims to misclassify specific nodes in the target class into a different class using node injection. Additionally, nodes are injected in such a way that they camouflage as benign nodes. We propose NICKI, a novel attacking strategy that utilizes an optimization-based approach to sabotage the performance of GNN-based node classifiers. NICKI works in two phases - it first learns the node representation and then generates the features and edges of the injected nodes. Extensive experiments and ablation studies on four benchmark networks show that NICKI is consistently better than four baseline attacking strategies for misclassifying nodes in the target class. We also show that the injected nodes are properly camouflaged as benign, thus making the poisoned graph indistinguishable from its clean version w.r.t various topological properties.

摘要: 图形神经网络(GNN)在学习丰富的网络表示方面功能强大，有助于下游任务的执行。然而，最近的研究表明，GNN很容易受到包括节点注入和网络扰动在内的对抗性攻击。其中，节点注入攻击更实用，因为它们不需要在现有网络中进行操作，并且可以更真实地执行。在本文中，我们提出了一种新的问题陈述--针对图的特定类的毒物攻击，攻击者的目的是通过节点注入将目标类中的特定节点错误地分类到不同的类中。此外，注入结节的方式是伪装成良性结节。我们提出了一种新的攻击策略Nicki，它利用基于优化的方法来破坏基于GNN的节点分类器的性能。Nicki分两个阶段工作--它首先学习节点表示，然后生成注入节点的特征和边。在四个基准网络上的大量实验和烧蚀研究表明，对于目标类中的节点误分类，Nicki一致优于四种基线攻击策略。我们还证明了注入的节点被适当伪装成良性的，从而使得中毒的图与其干净的版本无法区分各种拓扑性质。



## **34. Tackling Stackelberg Network Interdiction against a Boundedly Rational Adversary**

对付有限理性对手的Stackelberg网络阻断 math.OC

**SubmitDate**: 2023-01-28    [abs](http://arxiv.org/abs/2301.12232v1) [paper-pdf](http://arxiv.org/pdf/2301.12232v1)

**Authors**: Tien Mai, Avinandan Bose, Arunesh Sinha, Thanh H. Nguyen

**Abstract**: This work studies Stackelberg network interdiction games -- an important class of games in which a defender first allocates (randomized) defense resources to a set of critical nodes on a graph while an adversary chooses its path to attack these nodes accordingly. We consider a boundedly rational adversary in which the adversary's response model is based on a dynamic form of classic logit-based discrete choice models. We show that the problem of finding an optimal interdiction strategy for the defender in the rational setting is NP-hard. The resulting optimization is in fact non-convex and additionally, involves complex terms that sum over exponentially many paths. We tackle these computational challenges by presenting new efficient approximation algorithms with bounded solution guarantees. First, we address the exponentially-many-path challenge by proposing a polynomial-time dynamic programming-based formulation. We then show that the gradient of the non-convex objective can also be computed in polynomial time, which allows us to use a gradient-based method to solve the problem efficiently. Second, we identify a restricted problem that is convex and hence gradient-based methods find the global optimal solution for this restricted problem. We further identify mild conditions under which this restricted problem provides a bounded approximation for the original problem.

摘要: 这项工作研究了Stackelberg网络拦截游戏--这是一类重要的游戏，在这种游戏中，防御者首先将(随机的)防御资源分配给图上的一组关键节点，而对手则选择相应的路径攻击这些节点。我们考虑一个有界理性的对手，其中对手的反应模型是基于经典的基于Logit的离散选择模型的动态形式。我们证明了在理性环境下为防守者寻找最优拦截策略的问题是NP难的。由此产生的优化实际上是非凸的，另外，还涉及到指数级多条路径上求和的复数项。我们通过提出具有有界解保证的新的高效近似算法来解决这些计算挑战。首先，我们提出了一种基于多项式时间动态规划的公式来解决指数多路径问题。然后，我们证明了非凸目标的梯度也可以在多项式时间内计算，这使得我们可以使用基于梯度的方法来有效地解决该问题。其次，我们识别了一个凸的约束问题，因此基于梯度的方法找到了这个约束问题的全局最优解。我们进一步证明了在较温和的条件下，这个受限问题提供了原问题的有界逼近。



## **35. Selecting Models based on the Risk of Damage Caused by Adversarial Attacks**

基于对抗性攻击造成的损害风险选择模型 cs.LG

**SubmitDate**: 2023-01-28    [abs](http://arxiv.org/abs/2301.12151v1) [paper-pdf](http://arxiv.org/pdf/2301.12151v1)

**Authors**: Jona Klemenc, Holger Trittenbach

**Abstract**: Regulation, legal liabilities, and societal concerns challenge the adoption of AI in safety and security-critical applications. One of the key concerns is that adversaries can cause harm by manipulating model predictions without being detected. Regulation hence demands an assessment of the risk of damage caused by adversaries. Yet, there is no method to translate this high-level demand into actionable metrics that quantify the risk of damage.   In this article, we propose a method to model and statistically estimate the probability of damage arising from adversarial attacks. We show that our proposed estimator is statistically consistent and unbiased. In experiments, we demonstrate that the estimation results of our method have a clear and actionable interpretation and outperform conventional metrics. We then show how operators can use the estimation results to reliably select the model with the lowest risk.

摘要: 监管、法律责任和社会关切对人工智能在安全和安保关键应用中的采用提出了挑战。其中一个关键问题是，对手可能会通过操纵模型预测而不被发现来造成伤害。因此，监管要求评估对手造成损害的风险。然而，目前还没有办法将这种高水平的需求转化为量化损害风险的可操作指标。在这篇文章中，我们提出了一种建模和统计估计敌对攻击造成的损害概率的方法。我们证明了我们提出的估计量在统计上是一致的和无偏的。在实验中，我们证明了我们的方法的估计结果具有清晰和可操作的解释，并且优于传统的度量标准。然后，我们展示了运营商如何使用估计结果来可靠地选择风险最低的模型。



## **36. Quantum Man-in-the-middle Attacks: a Game-theoretic Approach with Applications to Radars**

量子中间人攻击：博弈论方法及其在雷达中的应用 eess.SP

**SubmitDate**: 2023-01-28    [abs](http://arxiv.org/abs/2211.02228v2) [paper-pdf](http://arxiv.org/pdf/2211.02228v2)

**Authors**: Yinan Hu, Quanyan Zhu

**Abstract**: The detection and discrimination of quantum states serve a crucial role in quantum signal processing, a discipline that studies methods and techniques to process signals that obey the quantum mechanics frameworks. However, just like classical detection, evasive behaviors also exist in quantum detection. In this paper, we formulate an adversarial quantum detection scenario where the detector is passive and does not know the quantum states have been distorted by an attacker. We compare the performance of a passive detector with the one of a non-adversarial detector to demonstrate how evasive behaviors can undermine the performance of quantum detection. We use a case study of target detection with quantum radars to corroborate our analytical results.

摘要: 量子态的检测和识别在量子信号处理中起着至关重要的作用，量子信号处理是一门研究遵循量子力学框架处理信号的方法和技术的学科。然而，与经典检测一样，量子检测中也存在规避行为。在本文中，我们描述了一个对抗性量子检测场景，其中检测器是被动的，并且不知道攻击者已经扭曲了量子态。我们将被动检测器的性能与非对抗性检测器的性能进行了比较，以演示规避行为如何破坏量子检测的性能。我们用量子雷达探测目标的一个案例来验证我们的分析结果。



## **37. Semantic Adversarial Attacks on Face Recognition through Significant Attributes**

基于重要属性的人脸识别语义对抗攻击 cs.CV

13 pages, 8 figures, 3 tables

**SubmitDate**: 2023-01-28    [abs](http://arxiv.org/abs/2301.12046v1) [paper-pdf](http://arxiv.org/pdf/2301.12046v1)

**Authors**: Yasmeen M. Khedr, Yifeng Xiong, Kun He

**Abstract**: Face recognition is known to be vulnerable to adversarial face images. Existing works craft face adversarial images by indiscriminately changing a single attribute without being aware of the intrinsic attributes of the images. To this end, we propose a new Semantic Adversarial Attack called SAA-StarGAN that tampers with the significant facial attributes for each image. We predict the most significant attributes by applying the cosine similarity or probability score. The probability score method is based on training a Face Verification model for an attribute prediction task to obtain a class probability score for each attribute. The prediction process will help craft adversarial face images more easily and efficiently, as well as improve the adversarial transferability. Then, we change the most significant facial attributes, with either one or more of the facial attributes for impersonation and dodging attacks in white-box and black-box settings. Experimental results show that our method could generate diverse and realistic adversarial face images meanwhile avoid affecting human perception of the face recognition. SAA-StarGAN achieves an 80.5% attack success rate against black-box models, outperforming existing methods by 35.5% under the impersonation attack. Concerning the black-box setting, SAA-StarGAN achieves high attack success rates on various models. The experiments confirm that predicting the most important attributes significantly affects the success of adversarial attacks in both white-box and black-box settings and could enhance the transferability of the crafted adversarial examples.

摘要: 众所周知，人脸识别容易受到敌意人脸图像的攻击。现有的工艺作品通过不分青红皂白地改变单一属性来面对敌对图像，而不知道图像的内在属性。为此，我们提出了一种新的语义攻击，称为SAA-StarGAN，它篡改了每幅图像的重要面部属性。我们通过应用余弦相似度或概率分数来预测最重要的属性。概率分数方法基于为属性预测任务训练人脸验证模型以获得每个属性的类别概率分数。预测过程将有助于更容易和更有效地制作对抗性人脸图像，以及提高对抗性可转移性。然后，我们更改最重要的面部属性，在白盒和黑盒设置中使用一个或多个用于模拟和躲避攻击的面部属性。实验结果表明，该方法能够生成多样、逼真的对抗性人脸图像，同时避免了影响人类对人脸识别的感知。SAA-StarGAN对黑盒模型的攻击成功率达到80.5%，在模拟攻击下的性能比现有方法高出35.5%。在黑盒设置方面，SAA-StarGan在各种型号上都取得了较高的攻击成功率。实验证实，预测最重要的属性对白盒和黑盒环境下的对抗性攻击的成功都有显著影响，并且可以增强特制的对抗性例子的可转移性。



## **38. Analyzing Robustness of the Deep Reinforcement Learning Algorithm in Ramp Metering Applications Considering False Data Injection Attack and Defense**

考虑虚假数据注入攻防的深度强化学习算法在匝道控制中的稳健性分析 cs.LG

15 pages, 6 figures

**SubmitDate**: 2023-01-28    [abs](http://arxiv.org/abs/2301.12036v1) [paper-pdf](http://arxiv.org/pdf/2301.12036v1)

**Authors**: Diyi Liu, Lanmin Liu, Lee D Han

**Abstract**: Decades of practices of ramp metering, by controlling downstream volume and smoothing the interweaving traffic, have proved that ramp metering can decrease total travel time, mitigate shockwaves, decrease rear-end collisions, reduce pollution, etc. Besides traditional methods like ALIENA algorithms, Deep Reinforcement Learning algorithms have been established recently to build finer control on ramp metering. However, those Deep Learning models may be venerable to adversarial attacks. Thus, it is important to investigate the robustness of those models under False Data Injection adversarial attack. Furthermore, algorithms capable of detecting anomaly data from clean data are the key to safeguard Deep Learning algorithm. In this study, an online algorithm that can distinguish adversarial data from clean data are tested. Results found that in most cases anomaly data can be distinguished from clean data, although their difference is too small to be manually distinguished by humans. In practice, whenever adversarial/hazardous data is detected, the system can fall back to a fixed control program, and experts should investigate the detectors status or security protocols afterwards before real damages happen.

摘要: 几十年的匝道控制实践证明，通过控制下游流量和平滑交织的交通，匝道控制可以减少总行程时间，缓解冲击波，减少追尾碰撞，减少污染等。除了Aliena算法等传统方法外，最近还建立了深度强化学习算法，以对匝道控制进行更精细的控制。然而，这些深度学习模型可能会受到对手攻击的尊敬。因此，研究这些模型在虚假数据注入攻击下的稳健性具有重要意义。此外，能够从干净的数据中检测出异常数据的算法是保障深度学习算法的关键。在这项研究中，测试了一种可以区分敌意数据和干净数据的在线算法。结果发现，在大多数情况下，异常数据和干净数据可以区分开来，尽管它们的差异太小，无法由人工区分。在实践中，每当检测到敌意/危险数据时，系统都可以退回到固定的控制程序，专家应该在真正的损害发生之前调查检测器的状态或安全协议。



## **39. Alignment with human representations supports robust few-shot learning**

与人类表达方式保持一致，支持可靠的少数几次学习 cs.LG

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2301.11990v1) [paper-pdf](http://arxiv.org/pdf/2301.11990v1)

**Authors**: Ilia Sucholutsky, Thomas L. Griffiths

**Abstract**: Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.

摘要: 我们是否应该关心人工智能系统是否具有与人类相似的世界表示法？我们提供的信息论分析表明，与人类的表征一致性程度与在少数几次学习任务上的表现之间应该存在U型关系。我们在对491个计算机视觉模型的性能分析中发现了这种关系，并从经验上证实了这一预测。我们还表明，高度对齐的模型对敌意攻击和域转移都具有更强的鲁棒性。我们的结果表明，人的一致性通常是模型有效利用有限数据、健壮性和泛化良好的充分条件，但不是必要条件。



## **40. Certified Invertibility in Neural Networks via Mixed-Integer Programming**

基于混合整数规划的神经网络可逆性证明 cs.LG

24 pages, 7 figures

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2301.11783v1) [paper-pdf](http://arxiv.org/pdf/2301.11783v1)

**Authors**: Tianqi Cui, Thomas Bertalan, George J. Pappas, Manfred Morari, Ioannis G. Kevrekidis, Mahyar Fazlyab

**Abstract**: Neural networks are notoriously vulnerable to adversarial attacks -- small imperceptible perturbations that can change the network's output drastically. In the reverse direction, there may exist large, meaningful perturbations that leave the network's decision unchanged (excessive invariance, nonivertibility). We study the latter phenomenon in two contexts: (a) discrete-time dynamical system identification, as well as (b) calibration of the output of one neural network to the output of another (neural network matching). For ReLU networks and $L_p$ norms ($p=1,2,\infty$), we formulate these optimization problems as mixed-integer programs (MIPs) that apply to neural network approximators of dynamical systems. We also discuss the applicability of our results to invertibility certification in transformations between neural networks (e.g. at different levels of pruning).

摘要: 众所周知，神经网络容易受到对抗性攻击--微小的、无法察觉的干扰，可能会极大地改变网络的输出。在相反的方向上，可能存在使网络的决策保持不变的大的、有意义的扰动(过度的不变性、不变性)。我们在两种情况下研究后一种现象：(A)离散时间动态系统辨识，以及(B)将一个神经网络的输出校准为另一个神经网络的输出(神经网络匹配)。对于RELU网络和$L_p$范数($p=1，2，\inty)，我们将这些优化问题表示为适用于动态系统的神经网络逼近器的混合整数规划(MIP)。我们还讨论了我们的结果在神经网络之间的变换(例如，在不同级别的剪枝)中的可逆性证明的适用性。



## **41. CAPoW: Context-Aware AI-Assisted Proof of Work based DDoS Defense**

CAPOW：基于上下文感知AI辅助工作证明的DDoS防御 cs.CR

8 pages

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2301.11767v1) [paper-pdf](http://arxiv.org/pdf/2301.11767v1)

**Authors**: Trisha Chakraborty, Shaswata Mitra, Sudip Mittal

**Abstract**: Critical servers can be secured against distributed denial of service (DDoS) attacks using proof of work (PoW) systems assisted by an Artificial Intelligence (AI) that learns contextual network request patterns. In this work, we introduce CAPoW, a context-aware anti-DDoS framework that injects latency adaptively during communication by utilizing context-aware PoW puzzles. In CAPoW, a security professional can define relevant request context attributes which can be learned by the AI system. These contextual attributes can include information about the user request, such as IP address, time, flow-level information, etc., and are utilized to generate a contextual score for incoming requests that influence the hardness of a PoW puzzle. These puzzles need to be solved by a user before the server begins to process their request. Solving puzzles slow down the volume of incoming adversarial requests. Additionally, the framework compels the adversary to incur a cost per request, hence making it expensive for an adversary to prolong a DDoS attack. We include the theoretical foundations of the CAPoW framework along with a description of its implementation and evaluation.

摘要: 在学习上下文网络请求模式的人工智能(AI)的帮助下，使用工作证明(PoW)系统可以保护关键服务器免受分布式拒绝服务(DDoS)攻击。在这项工作中，我们介绍了CAPoW，一个上下文感知的反DDoS框架，它利用上下文感知的POW谜题自适应地在通信过程中注入延迟。在CAPOW中，安全专业人员可以定义AI系统可以学习的相关请求上下文属性。这些上下文属性可以包括关于用户请求的信息，诸如IP地址、时间、流级信息等，并且被用来生成影响POW难题的难度的传入请求的上下文分数。这些难题需要用户在服务器开始处理他们的请求之前解决。解决谜题会减缓收到的敌意请求的数量。此外，该框架迫使对手为每个请求招致成本，从而使对手延长DDoS攻击的成本变得高昂。我们包括CAPOW框架的理论基础以及对其实施和评估的描述。



## **42. Side Auth: Synthesizing Virtual Sensors for Authentication**

Side Auth：合成虚拟传感器进行身份验证 cs.CR

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2301.11745v1) [paper-pdf](http://arxiv.org/pdf/2301.11745v1)

**Authors**: Yan Long, Kevin Fu

**Abstract**: While the embedded security research community aims to protect systems by reducing analog sensor side channels, our work argues that sensor side channels can be beneficial to defenders. This work introduces the general problem of synthesizing virtual sensors from existing circuits to authenticate physical sensors' measurands. We investigate how to apply this approach and present a preliminary analytical framework and definitions for sensor side channels. To illustrate the general concept, we provide a proof-of-concept case study to synthesize a virtual inertial measurement unit from a camera motion side channel. Our work also provides an example of applying this technique to protect facial recognition against silicon mask spoofing attacks. Finally, we discuss downstream problems of how to ensure that side channels benefit the defender, but not the adversary, during authentication.

摘要: 虽然嵌入式安全研究社区的目标是通过减少模拟传感器侧通道来保护系统，但我们的工作认为传感器侧通道可能对防御者有利。这项工作介绍了从现有电路合成虚拟传感器来验证物理传感器被测对象的一般问题。我们研究了如何应用这一方法，并提出了传感器侧通道的初步分析框架和定义。为了说明一般概念，我们提供了一个从相机运动侧通道合成虚拟惯性测量单元的概念验证案例研究。我们的工作还提供了一个应用该技术来保护面部识别免受硅面具欺骗攻击的例子。最后，我们讨论了下游问题，即如何确保侧通道在身份验证过程中使防御者受益，而不是对手受益。



## **43. Overparameterized Linear Regression under Adversarial Attacks**

对抗性攻击下的超参数线性回归 stat.ML

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2204.06274v2) [paper-pdf](http://arxiv.org/pdf/2204.06274v2)

**Authors**: Antônio H. Ribeiro, Thomas B. Schön

**Abstract**: We study the error of linear regression in the face of adversarial attacks. In this framework, an adversary changes the input to the regression model in order to maximize the prediction error. We provide bounds on the prediction error in the presence of an adversary as a function of the parameter norm and the error in the absence of such an adversary. We show how these bounds make it possible to study the adversarial error using analysis from non-adversarial setups. The obtained results shed light on the robustness of overparameterized linear models to adversarial attacks. Adding features might be either a source of additional robustness or brittleness. On the one hand, we use asymptotic results to illustrate how double-descent curves can be obtained for the adversarial error. On the other hand, we derive conditions under which the adversarial error can grow to infinity as more features are added, while at the same time, the test error goes to zero. We show this behavior is caused by the fact that the norm of the parameter vector grows with the number of features. It is also established that $\ell_\infty$ and $\ell_2$-adversarial attacks might behave fundamentally differently due to how the $\ell_1$ and $\ell_2$-norms of random projections concentrate. We also show how our reformulation allows for solving adversarial training as a convex optimization problem. This fact is then exploited to establish similarities between adversarial training and parameter-shrinking methods and to study how the training might affect the robustness of the estimated models.

摘要: 我们研究了线性回归在面对敌方攻击时的误差。在该框架中，对手改变回归模型的输入，以最大化预测误差。我们给出了对手存在时预测误差作为参数范数的函数的界，以及没有对手时预测误差的界。我们展示了这些界限如何使得使用非对抗性设置的分析来研究对抗性错误成为可能。所得结果揭示了超参数线性模型对敌意攻击的稳健性。添加功能可能会增加健壮性或脆性。一方面，我们使用渐近结果来说明如何获得对抗性误差的双下降曲线。另一方面，我们推导了当增加更多的特征时，对抗性误差可以增长到无穷大，同时测试误差趋于零的条件。我们证明了这种行为是由于参数向量的范数随着特征数的增加而增长的。还确定了由于随机投影的$\ell_1$和$\ell_2$-范数的集中，$\ell_\inty$和$\ell_2$-对抗性攻击的行为可能从根本上不同。我们还展示了我们的重新公式如何允许将对抗性训练作为一个凸优化问题来解决。然后利用这一事实来建立对抗性训练和参数收缩方法之间的相似性，并研究训练可能如何影响估计模型的稳健性。



## **44. Learning to Unlearn: Instance-wise Unlearning for Pre-trained Classifiers**

学习遗忘：基于实例的预先训练分类器的遗忘 cs.LG

Preprint

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2301.11578v1) [paper-pdf](http://arxiv.org/pdf/2301.11578v1)

**Authors**: Sungmin Cha, Sungjun Cho, Dasol Hwang, Honglak Lee, Taesup Moon, Moontae Lee

**Abstract**: Since the recent advent of regulations for data protection (e.g., the General Data Protection Regulation), there has been increasing demand in deleting information learned from sensitive data in pre-trained models without retraining from scratch. The inherent vulnerability of neural networks towards adversarial attacks and unfairness also calls for a robust method to remove or correct information in an instance-wise fashion, while retaining the predictive performance across remaining data. To this end, we define instance-wise unlearning, of which the goal is to delete information on a set of instances from a pre-trained model, by either misclassifying each instance away from its original prediction or relabeling the instance to a different label. We also propose two methods that reduce forgetting on the remaining data: 1) utilizing adversarial examples to overcome forgetting at the representation-level and 2) leveraging weight importance metrics to pinpoint network parameters guilty of propagating unwanted information. Both methods only require the pre-trained model and data instances to forget, allowing painless application to real-life settings where the entire training set is unavailable. Through extensive experimentation on various image classification benchmarks, we show that our approach effectively preserves knowledge of remaining data while unlearning given instances in both single-task and continual unlearning scenarios.

摘要: 自从最近出现了数据保护条例(例如，《一般数据保护条例》)以来，越来越多的人要求在预先训练的模型中删除从敏感数据中学习的信息，而不需要从头开始进行再培训。神经网络对敌意攻击和不公平的固有脆弱性也需要一种健壮的方法来以实例方式移除或纠正信息，同时保持对剩余数据的预测性能。为此，我们定义了基于实例的遗忘，其目标是通过将每个实例从其原始预测中错误分类或将实例重新标记到不同的标签来从预先训练的模型中删除关于一组实例的信息。我们还提出了两种减少对剩余数据的遗忘的方法：1)利用对抗性例子在表示级克服遗忘；2)利用权重重要性度量来精确定位传播无用信息的网络参数。这两种方法只需要忘记预先训练的模型和数据实例，从而可以轻松地应用到无法获得整个训练集的现实生活环境中。通过在不同图像分类基准上的广泛实验，我们的方法有效地保留了剩余数据的知识，同时在单任务和连续遗忘场景中都忘记了给定的实例。



## **45. Robust Transformer with Locality Inductive Bias and Feature Normalization**

具有局部感应偏差和特征归一化的鲁棒变压器 cs.CV

9 pages, 3 Figures, 6 Tables

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2301.11553v1) [paper-pdf](http://arxiv.org/pdf/2301.11553v1)

**Authors**: Omid Nejati Manzari, Hossein Kashiani, Hojat Asgarian Dehkordi, Shahriar Baradaran Shokouhi

**Abstract**: Vision transformers have been demonstrated to yield state-of-the-art results on a variety of computer vision tasks using attention-based networks. However, research works in transformers mostly do not investigate robustness/accuracy trade-off, and they still struggle to handle adversarial perturbations. In this paper, we explore the robustness of vision transformers against adversarial perturbations and try to enhance their robustness/accuracy trade-off in white box attack settings. To this end, we propose Locality iN Locality (LNL) transformer model. We prove that the locality introduction to LNL contributes to the robustness performance since it aggregates local information such as lines, edges, shapes, and even objects. In addition, to further improve the robustness performance, we encourage LNL to extract training signal from the moments (a.k.a., mean and standard deviation) and the normalized features. We validate the effectiveness and generality of LNL by achieving state-of-the-art results in terms of accuracy and robustness metrics on German Traffic Sign Recognition Benchmark (GTSRB) and Canadian Institute for Advanced Research (CIFAR-10). More specifically, for traffic sign classification, the proposed LNL yields gains of 1.1% and ~35% in terms of clean and robustness accuracy compared to the state-of-the-art studies.

摘要: 视觉转换器已经被证明可以使用基于注意力的网络在各种计算机视觉任务中产生最先进的结果。然而，变压器的研究工作大多不研究稳健性和准确性之间的权衡，它们仍然难以处理对抗性扰动。在这篇文章中，我们探讨了视觉转换器对对抗扰动的稳健性，并试图在白盒攻击环境下增强它们的稳健性和准确性之间的权衡。为此，我们提出了局部性(LNL)变换模型。我们证明了LNL的局部性引入有助于提高稳健性，因为它聚集了线、边、形状甚至对象等局部信息。此外，为了进一步提高鲁棒性，我们鼓励LNL从矩(也称为均值和标准差)和归一化特征中提取训练信号。我们通过在德国交通标志识别基准(GTSRB)和加拿大高级研究所(CIFAR-10)上获得最先进的准确率和稳健性指标来验证LNL的有效性和通用性。更具体地说，对于交通标志分类，与最先进的研究相比，所提出的LNL在清洁和稳健性方面的准确率分别提高了1.1%和~35%。



## **46. Targeted Attacks on Timeseries Forecasting**

针对时间序列预测的有针对性的攻击 cs.LG

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2301.11544v1) [paper-pdf](http://arxiv.org/pdf/2301.11544v1)

**Authors**: Yuvaraj Govindarajulu, Avinash Amballa, Pavan Kulkarni, Manojkumar Parmar

**Abstract**: Real-world deep learning models developed for Time Series Forecasting are used in several critical applications ranging from medical devices to the security domain. Many previous works have shown how deep learning models are prone to adversarial attacks and studied their vulnerabilities. However, the vulnerabilities of time series models for forecasting due to adversarial inputs are not extensively explored. While the attack on a forecasting model might aim to deteriorate the performance of the model, it is more effective, if the attack is focused on a specific impact on the model's output. In this paper, we propose a novel formulation of Directional, Amplitudinal, and Temporal targeted adversarial attacks on time series forecasting models. These targeted attacks create a specific impact on the amplitude and direction of the output prediction. We use the existing adversarial attack techniques from the computer vision domain and adapt them for time series. Additionally, we propose a modified version of the Auto Projected Gradient Descent attack for targeted attacks. We examine the impact of the proposed targeted attacks versus untargeted attacks. We use KS-Tests to statistically demonstrate the impact of the attack. Our experimental results show how targeted attacks on time series models are viable and are more powerful in terms of statistical similarity. It is, hence difficult to detect through statistical methods. We believe that this work opens a new paradigm in the time series forecasting domain and represents an important consideration for developing better defenses.

摘要: 为时间序列预测开发的真实世界深度学习模型被用于从医疗设备到安全领域的几个关键应用。许多以前的工作已经证明了深度学习模型如何容易受到对抗性攻击，并研究了它们的脆弱性。然而，由于对抗性的输入，时间序列模型用于预测的脆弱性并没有得到广泛的探讨。虽然对预测模型的攻击可能旨在降低模型的性能，但如果攻击集中在对模型输出的特定影响上，则会更有效。在本文中，我们提出了一种新的针对时间序列预测模型的定向、纵向和时态定向攻击的公式。这些有针对性的攻击会对输出预测的幅度和方向产生特定影响。我们使用计算机视觉领域现有的对抗性攻击技术，并将其应用于时间序列。此外，我们还提出了一种针对定向攻击的自动投影梯度下降攻击的改进版本。我们考察了拟议的定向攻击与非定向攻击的影响。我们使用KS-测试来从统计上证明攻击的影响。我们的实验结果表明，针对时间序列模型的目标攻击是可行的，并且在统计相似性方面更加强大。因此，很难通过统计方法来检测。我们相信，这项工作开启了时间序列预测领域的新范式，并代表了开发更好的防御措施的重要考虑因素。



## **47. RAPTOR: Advanced Persistent Threat Detection in Industrial IoT via Attack Stage Correlation**

Raptor：通过攻击阶段关联实现工业物联网的高级持续威胁检测 cs.CR

Submitted to IEEE IoT Journal for review

**SubmitDate**: 2023-01-27    [abs](http://arxiv.org/abs/2301.11524v1) [paper-pdf](http://arxiv.org/pdf/2301.11524v1)

**Authors**: Ayush Kumar, Vrizlynn L. L. Thing

**Abstract**: IIoT (Industrial Internet-of-Things) systems are getting more prone to attacks by APT (Advanced Persistent Threat) adversaries. Past APT attacks on IIoT systems such as the 2016 Ukrainian power grid attack which cut off the capital Kyiv off power for an hour and the 2017 Saudi petrochemical plant attack which almost shut down the plant's safety controllers have shown that APT campaigns can disrupt industrial processes, shut down critical systems and endanger human lives. In this work, we propose RAPTOR, a system to detect APT campaigns in IIoT environments. RAPTOR detects and correlates various APT attack stages (adapted to IIoT) using multiple data sources. Subsequently, it constructs a high-level APT campaign graph which can be used by cybersecurity analysts towards attack analysis and mitigation. A performance evaluation of RAPTOR's APT stage detection stages shows high precision and low false positive/negative rates. We also show that RAPTOR is able to construct the APT campaign graph for APT attacks (modelled after real-world attacks on ICS/OT infrastructure) executed on our IIoT testbed.

摘要: IIoT(工业物联网)系统越来越容易受到APT(高级持久威胁)对手的攻击。过去对IIoT系统的APT攻击，例如2016年乌克兰电网袭击导致首都基辅停电一小时，以及2017年沙特石化厂袭击事件，几乎关闭了工厂的安全控制器，这些都表明，APT行动可以扰乱工业流程，关闭关键系统，并危及人类生命。在这项工作中，我们提出了Raptor，一个用于检测IIoT环境中的APT活动的系统。Raptor使用多个数据源检测和关联不同的APT攻击阶段(适应IIoT)。随后，构建了一个高级APT运动图，可供网络安全分析人员用于攻击分析和缓解。对Raptor的APT阶段检测阶段的性能评估表明，该阶段的检测精度高，假阳性/阴性率低。我们还展示了Raptor能够为在我们的IIoT试验台上执行的APT攻击(模仿真实世界对ICS/OT基础设施的攻击)构建APT活动图。



## **48. Attacking Important Pixels for Anchor-free Detectors**

攻击无锚点探测器的重要象素 cs.CV

Yunxu Xie and Shu Hu contributed equally

**SubmitDate**: 2023-01-26    [abs](http://arxiv.org/abs/2301.11457v1) [paper-pdf](http://arxiv.org/pdf/2301.11457v1)

**Authors**: Yunxu Xie, Shu Hu, Xin Wang, Quanyu Liao, Bin Zhu, Xi Wu, Siwei Lyu

**Abstract**: Deep neural networks have been demonstrated to be vulnerable to adversarial attacks: subtle perturbation can completely change the prediction result. Existing adversarial attacks on object detection focus on attacking anchor-based detectors, which may not work well for anchor-free detectors. In this paper, we propose the first adversarial attack dedicated to anchor-free detectors. It is a category-wise attack that attacks important pixels of all instances of a category simultaneously. Our attack manifests in two forms, sparse category-wise attack (SCA) and dense category-wise attack (DCA), that minimize the $L_0$ and $L_\infty$ norm-based perturbations, respectively. For DCA, we present three variants, DCA-G, DCA-L, and DCA-S, that select a global region, a local region, and a semantic region, respectively, to attack. Our experiments on large-scale benchmark datasets including PascalVOC, MS-COCO, and MS-COCO Keypoints indicate that our proposed methods achieve state-of-the-art attack performance and transferability on both object detection and human pose estimation tasks.

摘要: 深度神经网络已经被证明容易受到对手的攻击：微小的扰动可以完全改变预测结果。现有的对抗性目标检测攻击主要集中在攻击基于锚点的检测器，这种攻击对于无锚点检测器可能不起作用。在本文中，我们提出了第一个致力于无锚点检测器的对抗性攻击。这是一种针对类别的攻击，它同时攻击某个类别的所有实例的重要像素。我们的攻击表现为两种形式，稀疏类别攻击(SCA)和稠密类别攻击(DCA)，它们分别最小化了基于$L_0$和$L_inty$范数的扰动。对于DCA，我们提出了三种变体DCA-G、DCA-L和DCA-S，分别选择全局区域、局部区域和语义区域进行攻击。我们在PascalVOC、MS-COCO和MS-COCO关键点等大规模基准数据集上的实验表明，我们提出的方法在目标检测和人体姿态估计任务上都达到了最先进的攻击性能和可转移性。



## **49. Certified Interpretability Robustness for Class Activation Mapping**

类激活映射的证明可解释性鲁棒性 cs.LG

13 pages, 5 figures. Accepted to Machine Learning for Autonomous  Driving Workshop at NeurIPS 2020

**SubmitDate**: 2023-01-26    [abs](http://arxiv.org/abs/2301.11324v1) [paper-pdf](http://arxiv.org/pdf/2301.11324v1)

**Authors**: Alex Gu, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel

**Abstract**: Interpreting machine learning models is challenging but crucial for ensuring the safety of deep networks in autonomous driving systems. Due to the prevalence of deep learning based perception models in autonomous vehicles, accurately interpreting their predictions is crucial. While a variety of such methods have been proposed, most are shown to lack robustness. Yet, little has been done to provide certificates for interpretability robustness. Taking a step in this direction, we present CORGI, short for Certifiably prOvable Robustness Guarantees for Interpretability mapping. CORGI is an algorithm that takes in an input image and gives a certifiable lower bound for the robustness of the top k pixels of its CAM interpretability map. We show the effectiveness of CORGI via a case study on traffic sign data, certifying lower bounds on the minimum adversarial perturbation not far from (4-5x) state-of-the-art attack methods.

摘要: 解释机器学习模型是具有挑战性的，但对于确保自动驾驶系统中深层网络的安全至关重要。由于基于深度学习的感知模型在自动驾驶汽车中的普遍应用，准确解释它们的预测是至关重要的。虽然已经提出了各种这样的方法，但大多数都被证明缺乏稳健性。然而，在提供可解释性健壮性证书方面所做的工作很少。朝着这个方向迈出了一步，我们提出了CORGI，即可证明可证明的可解释映射的健壮性保证。CORGI是一种算法，它接收输入图像，并为其CAM可解释图的前k个像素的稳健性给出可证明的下界。通过对交通标志数据的案例研究，我们展示了COGI的有效性，证明了最小对抗性扰动的下界与最先进的攻击方法(4-5x)不远。



## **50. Hybrid Protection of Digital FIR Filters**

数字FIR滤波器的混合保护 cs.CR

**SubmitDate**: 2023-01-26    [abs](http://arxiv.org/abs/2301.11115v1) [paper-pdf](http://arxiv.org/pdf/2301.11115v1)

**Authors**: Levent Aksoy, Quang-Linh Nguyen, Felipe Almeida, Jaan Raik, Marie-Lise Flottes, Sophie Dupuis, Samuel Pagliarini

**Abstract**: A digital Finite Impulse Response (FIR) filter is a ubiquitous block in digital signal processing applications and its behavior is determined by its coefficients. To protect filter coefficients from an adversary, efficient obfuscation techniques have been proposed, either by hiding them behind decoys or replacing them by key bits. In this article, we initially introduce a query attack that can discover the secret key of such obfuscated FIR filters, which could not be broken by existing prominent attacks. Then, we propose a first of its kind hybrid technique, including both hardware obfuscation and logic locking using a point function for the protection of parallel direct and transposed forms of digital FIR filters. Experimental results show that the hybrid protection technique can lead to FIR filters with higher security while maintaining the hardware complexity competitive or superior to those locked by prominent logic locking methods. It is also shown that the protected multiplier blocks and FIR filters are resilient to existing attacks. The results on different forms and realizations of FIR filters show that the parallel direct form FIR filter has a promising potential for a secure design.

摘要: 数字有限脉冲响应(FIR)滤波器是数字信号处理应用中普遍存在的一个模块，其性能由其系数决定。为了保护滤波系数不被攻击者攻击，已经提出了有效的混淆技术，要么将它们隐藏在诱饵后面，要么用密钥位代替它们。在本文中，我们首先介绍了一种查询攻击，它可以发现这种模糊FIR滤波器的密钥，而这些密钥是现有的显著攻击无法破解的。然后，我们提出了一种第一种混合技术，包括硬件混淆和使用点函数的逻辑锁定，用于保护并行的直接和转置形式的数字FIR滤波器。实验结果表明，该混合保护技术可以在保持硬件复杂度与传统逻辑锁定方法相当或更好的情况下，使FIR滤波器具有更高的安全性。结果还表明，受保护的乘法器块和FIR滤波器对现有的攻击具有很强的抵抗力。对不同形式和不同实现方式的FIR滤波器的结果表明，并行直接形式FIR滤波器在安全设计方面具有很好的潜力。



