# Latest Adversarial Attack Papers
**update at 2023-07-04 16:27:28**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. When Can Linear Learners be Robust to Indiscriminate Poisoning Attacks?**

线性学习者何时才能对不分青红皂白的中毒攻击表现得很健壮？ cs.LG

**SubmitDate**: 2023-07-03    [abs](http://arxiv.org/abs/2307.01073v1) [paper-pdf](http://arxiv.org/pdf/2307.01073v1)

**Authors**: Fnu Suya, Xiao Zhang, Yuan Tian, David Evans

**Abstract**: We study indiscriminate poisoning for linear learners where an adversary injects a few crafted examples into the training data with the goal of forcing the induced model to incur higher test error. Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners. For theoretical Gaussian distributions, we rigorously characterize the behavior of an optimal poisoning attack, defined as the poisoning strategy that attains the maximum risk of the induced model at a given poisoning budget. Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small. These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks.

摘要: 我们研究了线性学习者的不分青红皂白的中毒，其中对手向训练数据中注入一些精心制作的示例，目的是迫使诱导模型招致更高的测试错误。受一些数据集上的线性学习者即使在没有任何防御的情况下也能够抵抗最著名的攻击的观察的启发，我们进一步调查了数据集对于线性学习者的不分青红皂白的中毒攻击是否具有内在的健壮性。对于理论上的高斯分布，我们严格地刻画了最优中毒攻击的行为，定义为在给定的中毒预算下，达到诱导模型的最大风险的中毒策略。我们的结果证明，如果类的数据分布是分开的且方差很小，并且包含所有允许毒点的约束集的大小也很小，那么线性学习器确实可以对不分青红皂白的中毒具有健壮性。这些发现在很大程度上解释了在基准数据集上针对线性学习者的最新中毒攻击在经验攻击性能上的巨大差异，为理解一些学习任务容易受到数据中毒攻击的根本原因迈出了重要的第一步。



## **2. Enhancing the Robustness of QMIX against State-adversarial Attacks**

增强QMIX对状态对抗攻击的健壮性 cs.LG

**SubmitDate**: 2023-07-03    [abs](http://arxiv.org/abs/2307.00907v1) [paper-pdf](http://arxiv.org/pdf/2307.00907v1)

**Authors**: Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang, Jiacun Wang

**Abstract**: Deep reinforcement learning (DRL) performance is generally impacted by state-adversarial attacks, a perturbation applied to an agent's observation. Most recent research has concentrated on robust single-agent reinforcement learning (SARL) algorithms against state-adversarial attacks. Still, there has yet to be much work on robust multi-agent reinforcement learning. Using QMIX, one of the popular cooperative multi-agent reinforcement algorithms, as an example, we discuss four techniques to improve the robustness of SARL algorithms and extend them to multi-agent scenarios. To increase the robustness of multi-agent reinforcement learning (MARL) algorithms, we train models using a variety of attacks in this research. We then test the models taught using the other attacks by subjecting them to the corresponding attacks throughout the training phase. In this way, we organize and summarize techniques for enhancing robustness when used with MARL.

摘要: 深度强化学习(DRL)的性能通常受到状态对抗攻击的影响，状态对抗攻击是一种应用于代理观察的扰动。最近的研究集中在针对状态对抗攻击的健壮单智能体强化学习(SARL)算法。然而，在稳健的多智能体强化学习方面还没有太多的工作。以目前流行的协作多智能体增强算法QMIX为例，讨论了四种提高SARL算法健壮性的技术，并将其扩展到多智能体场景。为了提高多智能体强化学习(MAIL)算法的健壮性，在本研究中，我们使用各种攻击来训练模型。然后，我们测试使用其他攻击教授的模型，方法是在整个培训阶段对它们进行相应的攻击。通过这种方式，我们组织和总结了在与Marl一起使用时增强健壮性的技术。



## **3. Data Poisoning Attack Aiming the Vulnerability of Continual Learning**

针对持续学习漏洞的数据中毒攻击 cs.LG

ICIP 2023 (NeurIPS 2022 ML Safety Workshop accepted paper)

**SubmitDate**: 2023-07-03    [abs](http://arxiv.org/abs/2211.15875v2) [paper-pdf](http://arxiv.org/pdf/2211.15875v2)

**Authors**: Gyojin Han, Jaehyun Choi, Hyeong Gwon Hong, Junmo Kim

**Abstract**: Generally, regularization-based continual learning models limit access to the previous task data to imitate the real-world constraints related to memory and privacy. However, this introduces a problem in these models by not being able to track the performance on each task. In essence, current continual learning methods are susceptible to attacks on previous tasks. We demonstrate the vulnerability of regularization-based continual learning methods by presenting a simple task-specific data poisoning attack that can be used in the learning process of a new task. Training data generated by the proposed attack causes performance degradation on a specific task targeted by the attacker. We experiment with the attack on the two representative regularization-based continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), trained with variants of MNIST dataset. The experiment results justify the vulnerability proposed in this paper and demonstrate the importance of developing continual learning models that are robust to adversarial attacks.

摘要: 一般而言，基于正则化的持续学习模型限制了对先前任务数据的访问，以模仿现实世界中与记忆和隐私相关的约束。然而，这在这些模型中引入了一个问题，因为无法跟踪每个任务的性能。从本质上讲，当前的持续学习方法很容易受到对以前任务的攻击。我们通过一个简单的任务特定的数据中毒攻击来证明基于正则化的持续学习方法的脆弱性，该攻击可以用于新任务的学习过程。建议的攻击生成的训练数据会导致攻击者针对的特定任务的性能下降。我们对两种典型的基于正则化的连续学习方法--弹性权值合并(EWC)和突触智能(SI)--进行了实验，并用MNIST数据集的变体进行了训练。实验结果验证了本文提出的脆弱性，并证明了开发对对手攻击具有健壮性的持续学习模型的重要性。



## **4. Evaluating the Adversarial Robustness of Convolution-based Human Motion Prediction**

基于卷积的人体运动预测的对抗稳健性评价 cs.CV

**SubmitDate**: 2023-07-03    [abs](http://arxiv.org/abs/2306.11990v2) [paper-pdf](http://arxiv.org/pdf/2306.11990v2)

**Authors**: Chengxu Duan, Zhicheng Zhang, Xiaoli Liu, Yonghao Dang, Jianqin Yin

**Abstract**: Human motion prediction has achieved a brilliant performance with the help of CNNs, which facilitates human-machine cooperation. However, currently, there is no work evaluating the potential risk in human motion prediction when facing adversarial attacks, which may cause danger in real applications. The adversarial attack will face two problems against human motion prediction: 1. For naturalness, pose data is highly related to the physical dynamics of human skeletons where Lp norm constraints cannot constrain the adversarial example well; 2. Unlike the pixel value in images, pose data is diverse at scale because of the different acquisition equipment and the data processing, which makes it hard to set fixed parameters to perform attacks. To solve the problems above, we propose a new adversarial attack method that perturbs the input human motion sequence by maximizing the prediction error with physical constraints. Specifically, we introduce a novel adaptable scheme that facilitates the attack to suit the scale of the target pose and two physical constraints to enhance the imperceptibility of the adversarial example. The evaluating experiments on three datasets show that the prediction errors of all target models are enlarged significantly, which means current convolution-based human motion prediction models can be easily disturbed under the proposed attack. The quantitative analysis shows that prior knowledge and semantic information modeling can be the key to the adversarial robustness of human motion predictors. The qualitative results indicate that the adversarial sample is hard to be noticed when compared frame by frame but is relatively easy to be detected when the sample is animated.

摘要: 在人工神经网络的帮助下，人体运动预测取得了很好的效果，有利于人机协作。然而，目前还没有对人体运动预测中的潜在风险进行评估的工作，这在实际应用中可能会造成危险。对抗性攻击将面临两个针对人体运动预测的问题：1.对于自然度，姿势数据与人体骨骼的物理动力学高度相关，其中Lp范数约束不能很好地约束对抗性示例；2.与图像中的像素值不同，由于采集设备和数据处理的不同，姿势数据在尺度上是多样的，这使得很难设置固定的参数来执行攻击。为了解决上述问题，我们提出了一种新的对抗性攻击方法，该方法通过在物理约束下最大化预测误差来扰动输入的人体运动序列。具体地说，我们引入了一种新的自适应方案来促进攻击以适应目标姿态的规模和两个物理约束来增强对抗性例子的不可见性。在三个数据集上的评估实验表明，所有目标模型的预测误差都显著增大，这意味着现有的基于卷积的人体运动预测模型在所提出的攻击下很容易受到干扰。定量分析表明，先验知识和语义信息建模是人体运动预测器对抗性健壮性的关键。定性结果表明，对抗性样本在逐帧比较时很难被注意到，但在样本被动画时相对容易被检测到。



## **5. Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data**

偷偷摸摸的尖峰：用神经形态数据揭示尖峰神经网络中的秘密后门攻击 cs.CR

**SubmitDate**: 2023-07-03    [abs](http://arxiv.org/abs/2302.06279v2) [paper-pdf](http://arxiv.org/pdf/2302.06279v2)

**Authors**: Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Aitor Urbieta

**Abstract**: Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.   This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse triggers. Specifically, we explore backdoor triggers within neuromorphic data that can manipulate their position and color, providing a broader scope of possibilities than conventional triggers in domains like images. We present various attack strategies, achieving an attack success rate of up to 100\% while maintaining a negligible impact on clean accuracy. Furthermore, we assess these attacks' stealthiness, revealing that our most potent attacks possess significant stealth capabilities. Lastly, we adapt several state-of-the-art defenses from the image domain, evaluating their efficacy on neuromorphic data and uncovering instances where they fall short, leading to compromised performance.

摘要: 深度神经网络(DNN)在包括图像和语音识别在内的各种任务中表现出了显著的性能。然而，要最大限度地发挥DNN的有效性，需要通过训练对大量的超参数和网络参数进行细致的优化。此外，高性能的DNN需要许多参数，这些参数在训练过程中消耗大量的能量。为了克服这些挑战，研究人员转向尖峰神经网络(SNN)，它提供了更高的能量效率和生物上可信的数据处理能力，使其非常适合于感觉数据任务，特别是在神经形态数据中。尽管SNN具有优势，但与DNN一样，SNN也容易受到各种威胁，包括敌意示例和后门攻击。然而，在理解和对抗这些攻击方面，SNN领域仍然需要探索。本文使用神经形态数据集和不同的触发器来深入研究SNN中的后门攻击。具体地说，我们探索神经形态数据中的后门触发器，这些数据可以操纵它们的位置和颜色，提供比图像等领域的传统触发器更广泛的可能性。我们提出了不同的攻击策略，实现了高达100%的攻击成功率，同时保持了对干净准确性的微小影响。此外，我们评估了这些攻击的隐蔽性，揭示了我们最强大的攻击具有显著的隐形能力。最后，我们从图像领域采用了几种最先进的防御措施，评估了它们在神经形态数据上的有效性，并发现了它们不足的地方，导致了性能下降。



## **6. Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks**

功能分区聚合：针对联合$\ELL_0$攻击的快速认证防御 cs.LG

**SubmitDate**: 2023-07-03    [abs](http://arxiv.org/abs/2302.11628v2) [paper-pdf](http://arxiv.org/pdf/2302.11628v2)

**Authors**: Zayd Hammoudeh, Daniel Lowd

**Abstract**: Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subset of the features. $\ell_0$ robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art $\ell_0$ certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to 3,000${\times}$ faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.

摘要: 稀疏或$\ELL_0$对抗性攻击任意扰乱未知特征的子集。$\ELL_0$稳健性分析特别适合于要素具有不同类型或比例的异类(表格)数据。最先进的$\ELL_0$认证防御基于随机平滑，仅适用于躲避攻击。本文提出了特征分区聚合(FPA)--一种针对$\ELL_0$逃避、后门和中毒攻击的联合认证防御。FPA通过在不相交的特征集上训练子模型的集成来产生更强的稳健性保证。与最先进的$\ELL_0$防御相比，FPA的速度快达3,000美元，并提供更大的中位数稳健性保证(例如，CIFAR10的中位数证书为13像素超过10，MNIST的中位数证书为10以上的12像素，天气的中位数证书为4，Ames的中位数证书为1以上)，这意味着FPA基本上免费提供了额外的稳健性维度。



## **7. From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy**

从ChatGPT到ThreatGPT：生成性人工智能对网络安全和隐私的影响 cs.CR

**SubmitDate**: 2023-07-03    [abs](http://arxiv.org/abs/2307.00691v1) [paper-pdf](http://arxiv.org/pdf/2307.00691v1)

**Authors**: Maanak Gupta, CharanKumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra Praharaj

**Abstract**: Undoubtedly, the evolution of Generative AI (GenAI) models has been the highlight of digital transformation in the year 2022. As the different GenAI models like ChatGPT and Google Bard continue to foster their complexity and capability, it's critical to understand its consequences from a cybersecurity perspective. Several instances recently have demonstrated the use of GenAI tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. This research paper highlights the limitations, challenges, potential risks, and opportunities of GenAI in the domain of cybersecurity and privacy. The work presents the vulnerabilities of ChatGPT, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. This paper demonstrates successful example attacks like Jailbreaks, reverse psychology, and prompt injection attacks on the ChatGPT. The paper also investigates how cyber offenders can use the GenAI tools in developing cyber attacks, and explore the scenarios where ChatGPT can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. This paper then examines defense techniques and uses GenAI tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. We will also discuss the social, legal, and ethical implications of ChatGPT. In conclusion, the paper highlights open challenges and future directions to make this GenAI secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts.

摘要: 毫无疑问，产生式AI(GenAI)模式的演变一直是2022年数字化转型的亮点。随着ChatGPT和Google Bard等不同的GenAI模型继续培养其复杂性和能力，从网络安全的角度理解其后果至关重要。最近的几个例子表明，GenAI工具在网络安全的防御和进攻方面都有使用，并侧重于这项技术所具有的社会、伦理和隐私影响。这份研究报告强调了GenAI在网络安全和隐私领域的限制、挑战、潜在风险和机遇。这项工作揭示了ChatGPT的漏洞，恶意用户可以利用该漏洞绕过模型上的道德约束渗出恶意信息。本文演示了对ChatGPT的越狱、反向心理和快速注入攻击等成功的示例攻击。本文还研究了网络攻击者如何使用GenAI工具开发网络攻击，并探索了攻击者可以使用ChatGPT来创建社会工程攻击、网络钓鱼攻击、自动黑客攻击、攻击有效负载生成、恶意软件创建和多态恶意软件的场景。然后，本文研究了防御技术，并使用GenAI工具来改进安全措施，包括网络防御自动化、报告、威胁情报、安全代码生成和检测、攻击识别、制定道德准则、事件响应计划和恶意软件检测。我们还将讨论ChatGPT的社会、法律和伦理影响。最后，白皮书强调了开放的挑战和未来的方向，以使这一GenAI安全、安全、值得信赖和道德，因为社区了解其网络安全影响。



## **8. Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint**

具有真正满足的不等式约束的软执行者-批评者算法 cs.LG

10 pages, 9 figures

**SubmitDate**: 2023-07-02    [abs](http://arxiv.org/abs/2303.04356v2) [paper-pdf](http://arxiv.org/pdf/2303.04356v2)

**Authors**: Taisuke Kobayashi

**Abstract**: Soft actor-critic (SAC) in reinforcement learning is expected to be one of the next-generation robot control schemes. Its ability to maximize policy entropy would make a robotic controller robust to noise and perturbation, which is useful for real-world robot applications. However, the priority of maximizing the policy entropy is automatically tuned in the current implementation, the rule of which can be interpreted as one for equality constraint, binding the policy entropy into its specified lower bound. The current SAC is therefore no longer maximize the policy entropy, contrary to our expectation. To resolve this issue in SAC, this paper improves its implementation with a learnable state-dependent slack variable for appropriately handling the inequality constraint to maximize the policy entropy by reformulating it as the corresponding equality constraint. The introduced slack variable is optimized by a switching-type loss function that takes into account the dual objectives of satisfying the equality constraint and checking the lower bound. In Mujoco and Pybullet simulators, the modified SAC statistically achieved the higher robustness for adversarial attacks than before while regularizing the norm of action. A real-robot variable impedance task was demonstrated for showing the applicability of the modified SAC to real-world robot control. In particular, the modified SAC maintained adaptive behaviors for physical human-robot interaction, which had no experience at all during training. https://youtu.be/EH3xVtlVaJw

摘要: 强化学习中的软行动者-批评者(SAC)控制方案有望成为下一代机器人控制方案之一。其最大化策略熵的能力将使机器人控制器对噪声和扰动具有健壮性，这对现实世界的机器人应用很有用。然而，在当前的实现中，最大化策略熵的优先级是自动调整的，其规则可以解释为等式约束的规则，将策略熵绑定到其指定的下界。因此，当前的国资委不再是政策熵的最大化，与我们的预期相反。为了解决SAC中的这一问题，本文改进了SAC的实现，引入了一个可学习的状态依赖松弛变量，以适当地处理不平等约束，通过将其重新表示为相应的等式约束来最大化政策熵。引入的松弛变量通过切换型损失函数进行优化，该函数考虑了满足等式约束和检查下界的双重目标。在Mujoco和PYBullet模拟器中，改进的SAC在正规化行为规范的同时，在统计上获得了比以前更高的对抗攻击的鲁棒性。为了说明改进的SAC在实际机器人控制中的适用性，给出了一个真实的机器人变阻抗任务。特别是，改进的SAC保持了人与机器人物理交互的适应行为，在训练过程中完全没有经验。Https://youtu.be/EH3xVtlVaJw



## **9. X-Detect: Explainable Adversarial Patch Detection for Object Detectors in Retail**

X-Detect：零售业目标检测器的可解释敌意补丁检测 cs.CV

**SubmitDate**: 2023-07-02    [abs](http://arxiv.org/abs/2306.08422v2) [paper-pdf](http://arxiv.org/pdf/2306.08422v2)

**Authors**: Omer Hofman, Amit Giloni, Yarin Hayun, Ikuya Morikawa, Toshiya Shimizu, Yuval Elovici, Asaf Shabtai

**Abstract**: Object detection models, which are widely used in various domains (such as retail), have been shown to be vulnerable to adversarial attacks. Existing methods for detecting adversarial attacks on object detectors have had difficulty detecting new real-life attacks. We present X-Detect, a novel adversarial patch detector that can: i) detect adversarial samples in real time, allowing the defender to take preventive action; ii) provide explanations for the alerts raised to support the defender's decision-making process, and iii) handle unfamiliar threats in the form of new attacks. Given a new scene, X-Detect uses an ensemble of explainable-by-design detectors that utilize object extraction, scene manipulation, and feature transformation techniques to determine whether an alert needs to be raised. X-Detect was evaluated in both the physical and digital space using five different attack scenarios (including adaptive attacks) and the COCO dataset and our new Superstore dataset. The physical evaluation was performed using a smart shopping cart setup in real-world settings and included 17 adversarial patch attacks recorded in 1,700 adversarial videos. The results showed that X-Detect outperforms the state-of-the-art methods in distinguishing between benign and adversarial scenes for all attack scenarios while maintaining a 0% FPR (no false alarms) and providing actionable explanations for the alerts raised. A demo is available.

摘要: 目标检测模型被广泛应用于各个领域(如零售)，已被证明容易受到对手攻击。现有的用于检测对象检测器上的敌意攻击的方法已经很难检测到新的现实生活中的攻击。我们提出了X-Detect，这是一种新型的对抗性补丁检测器，它可以：i)实时检测对手样本，允许防御者采取预防措施；ii)为支持防御者决策过程而发出的警报提供解释；iii)处理新攻击形式的陌生威胁。给定一个新场景，X-Detect使用一组可通过设计解释的检测器，这些检测器利用对象提取、场景操作和特征转换技术来确定是否需要发出警报。X-Detect在物理和数字空间中使用五种不同的攻击场景(包括自适应攻击)以及Coco数据集和我们新的Superstore数据集进行了评估。物理评估是使用真实世界设置中的智能购物车进行的，包括1700个对抗性视频中记录的17个对抗性补丁攻击。结果表明，X-Detect在区分所有攻击场景的良性和敌意场景方面优于最先进的方法，同时保持0%的FPR(无错误警报)，并为发出的警报提供可行的解释。现已提供演示。



## **10. Query-Efficient Decision-based Black-Box Patch Attack**

基于查询高效决策的黑盒补丁攻击 cs.CV

**SubmitDate**: 2023-07-02    [abs](http://arxiv.org/abs/2307.00477v1) [paper-pdf](http://arxiv.org/pdf/2307.00477v1)

**Authors**: Zhaoyu Chen, Bo Li, Shuang Wu, Shouhong Ding, Wenqiang Zhang

**Abstract**: Deep neural networks (DNNs) have been showed to be highly vulnerable to imperceptible adversarial perturbations. As a complementary type of adversary, patch attacks that introduce perceptible perturbations to the images have attracted the interest of researchers. Existing patch attacks rely on the architecture of the model or the probabilities of predictions and perform poorly in the decision-based setting, which can still construct a perturbation with the minimal information exposed -- the top-1 predicted label. In this work, we first explore the decision-based patch attack. To enhance the attack efficiency, we model the patches using paired key-points and use targeted images as the initialization of patches, and parameter optimizations are all performed on the integer domain. Then, we propose a differential evolutionary algorithm named DevoPatch for query-efficient decision-based patch attacks. Experiments demonstrate that DevoPatch outperforms the state-of-the-art black-box patch attacks in terms of patch area and attack success rate within a given query budget on image classification and face verification. Additionally, we conduct the vulnerability evaluation of ViT and MLP on image classification in the decision-based patch attack setting for the first time. Using DevoPatch, we can evaluate the robustness of models to black-box patch attacks. We believe this method could inspire the design and deployment of robust vision models based on various DNN architectures in the future.

摘要: 深度神经网络(DNN)已被证明对难以察觉的对抗性扰动具有很强的脆弱性。作为一种互补类型的攻击，补丁攻击给图像带来了可感知的扰动，引起了研究人员的兴趣。现有的补丁攻击依赖于模型的体系结构或预测概率，在基于决策的环境下性能较差，仍然可以利用暴露的最小信息-TOP-1预测标签来构造扰动。在本工作中，我们首先探讨了基于决策的补丁攻击。为了提高攻击效率，我们使用成对的关键点对补丁进行建模，并使用目标图像作为补丁的初始化，并且参数优化都是在整数域上进行的。在此基础上，提出了一种用于查询高效的基于决策的补丁攻击的差分进化算法DevoPatch。实验表明，在给定的查询预算下，DevoPatch在图像分类和人脸验证方面，在补丁面积和攻击成功率方面都优于最先进的黑盒补丁攻击。此外，我们还首次在基于决策的补丁攻击环境下对VIT和MLP在图像分类上的脆弱性进行了评估。使用DevoPatch，我们可以评估模型对黑盒补丁攻击的稳健性。我们相信，这种方法可以启发未来基于各种DNN体系结构的健壮视觉模型的设计和部署。



## **11. Brightness-Restricted Adversarial Attack Patch**

亮度受限的对抗性攻击补丁 cs.CV

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2307.00421v1) [paper-pdf](http://arxiv.org/pdf/2307.00421v1)

**Authors**: Mingzhen Shao

**Abstract**: Adversarial attack patches have gained increasing attention due to their practical applicability in physical-world scenarios. However, the bright colors used in attack patches represent a significant drawback, as they can be easily identified by human observers. Moreover, even though these attacks have been highly successful in deceiving target networks, which specific features of the attack patch contribute to its success are still unknown. Our paper introduces a brightness-restricted patch (BrPatch) that uses optical characteristics to effectively reduce conspicuousness while preserving image independence. We also conducted an analysis of the impact of various image features (such as color, texture, noise, and size) on the effectiveness of an attack patch in physical-world deployment. Our experiments show that attack patches exhibit strong redundancy to brightness and are resistant to color transfer and noise. Based on our findings, we propose some additional methods to further reduce the conspicuousness of BrPatch. Our findings also explain the robustness of attack patches observed in physical-world scenarios.

摘要: 对抗性攻击补丁由于其在物理世界场景中的实用适用性而受到越来越多的关注。然而，攻击补丁中使用的明亮颜色代表着一个重大缺陷，因为它们很容易被人类观察者识别出来。此外，尽管这些攻击在欺骗目标网络方面已经非常成功，但攻击补丁的哪些特定功能有助于其成功仍是未知的。本文提出了一种亮度受限的补丁(BrPatch)，它利用图像的光学特性，在保持图像独立性的同时，有效地降低了图像的显着性。我们还分析了各种图像特征(如颜色、纹理、噪声和大小)对物理世界部署中攻击补丁有效性的影响。实验表明，攻击补丁对亮度具有很强的冗余性，并且对颜色传递和噪声具有较强的抵抗能力。基于我们的发现，我们提出了一些额外的方法来进一步降低BrPatch的显着性。我们的发现也解释了在物理世界场景中观察到的攻击补丁的健壮性。



## **12. CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis**

CasTGAN：用于现实表格数据合成的级联生成性对抗网络 cs.LG

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2307.00384v1) [paper-pdf](http://arxiv.org/pdf/2307.00384v1)

**Authors**: Abdallah Alshantti, Damiano Varagnolo, Adil Rasheed, Aria Rahmati, Frank Westad

**Abstract**: Generative adversarial networks (GANs) have drawn considerable attention in recent years for their proven capability in generating synthetic data which can be utilized for multiple purposes. While GANs have demonstrated tremendous successes in producing synthetic data samples that replicate the dynamics of the original datasets, the validity of the synthetic data and the underlying privacy concerns represent major challenges which are not sufficiently addressed. In this work, we design a cascaded tabular GAN framework (CasTGAN) for generating realistic tabular data with a specific focus on the validity of the output. In this context, validity refers to the the dependency between features that can be found in the real data, but is typically misrepresented by traditional generative models. Our key idea entails that employing a cascaded architecture in which a dedicated generator samples each feature, the synthetic output becomes more representative of the real data. Our experimental results demonstrate that our model well captures the constraints and the correlations between the features of the real data, especially the high dimensional datasets. Furthermore, we evaluate the risk of white-box privacy attacks on our model and subsequently show that applying some perturbations to the auxiliary learners in CasTGAN increases the overall robustness of our model against targeted attacks.

摘要: 近年来，生成性对抗网络(GAN)因其在生成可用于多种目的的合成数据方面的能力而引起了相当大的关注。尽管Gans在生产复制原始数据集动态的合成数据样本方面取得了巨大成功，但合成数据的有效性和潜在的隐私问题是没有得到充分解决的主要挑战。在这项工作中，我们设计了一个级联表格GAN框架(CasTGAN)，用于生成真实的表格数据，并特别关注输出的有效性。在这种情况下，有效性是指在真实数据中可以找到的特征之间的依赖关系，但传统的生成模型通常会错误地表示这些特征。我们的关键思想是采用级联结构，其中由专用生成器对每个特征进行采样，合成输出变得更能代表真实数据。实验结果表明，我们的模型很好地捕捉了真实数据，特别是高维数据集的特征之间的约束和相关性。此外，我们评估了我们的模型受到白盒隐私攻击的风险，并随后表明，对CasTGAN中的辅助学习器应用一些扰动可以提高模型对目标攻击的整体稳健性。



## **13. A First Order Meta Stackelberg Method for Robust Federated Learning (Technical Report)**

一种稳健联邦学习的一阶Meta Stackelberg方法(技术报告) cs.CR

Accepted to ICML 2023 Workshop on The 2nd New Frontiers In  Adversarial Machine Learning. Workshop Proceedings version: arXiv:2306.13800

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2306.13273v2) [paper-pdf](http://arxiv.org/pdf/2306.13273v2)

**Authors**: Henger Li, Tianyi Xu, Tao Li, Yunian Pan, Quanyan Zhu, Zizhan Zheng

**Abstract**: Recent research efforts indicate that federated learning (FL) systems are vulnerable to a variety of security breaches. While numerous defense strategies have been suggested, they are mainly designed to counter specific attack patterns and lack adaptability, rendering them less effective when facing uncertain or adaptive threats. This work models adversarial FL as a Bayesian Stackelberg Markov game (BSMG) between the defender and the attacker to address the lack of adaptability to uncertain adaptive attacks. We further devise an effective meta-learning technique to solve for the Stackelberg equilibrium, leading to a resilient and adaptable defense. The experiment results suggest that our meta-Stackelberg learning approach excels in combating intense model poisoning and backdoor attacks of indeterminate types.

摘要: 最近的研究表明，联邦学习(FL)系统容易受到各种安全漏洞的攻击。虽然已经提出了许多防御策略，但它们主要是针对特定的攻击模式而设计的，缺乏适应性，使得它们在面临不确定或适应性威胁时效率较低。该工作将对抗性FL建模为防御者和攻击者之间的贝叶斯Stackelberg马尔可夫博弈(BSMG)，以解决对不确定自适应攻击缺乏适应性的问题。我们进一步设计了一种有效的元学习技术来求解Stackelberg均衡，从而导致具有弹性和适应性的防御。实验结果表明，我们的Meta-Stackelberg学习方法在对抗强烈的模型中毒和不确定类型的后门攻击方面表现出色。



## **14. A First Order Meta Stackelberg Method for Robust Federated Learning**

一种用于鲁棒联邦学习的一阶Meta Stackelberg方法 cs.LG

Accepted to ICML 2023 Workshop on The 2nd New Frontiers In  Adversarial Machine Learning. Associated technical report arXiv:2306.13273

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2306.13800v2) [paper-pdf](http://arxiv.org/pdf/2306.13800v2)

**Authors**: Yunian Pan, Tao Li, Henger Li, Tianyi Xu, Zizhan Zheng, Quanyan Zhu

**Abstract**: Previous research has shown that federated learning (FL) systems are exposed to an array of security risks. Despite the proposal of several defensive strategies, they tend to be non-adaptive and specific to certain types of attacks, rendering them ineffective against unpredictable or adaptive threats. This work models adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) to capture the defender's incomplete information of various attack types. We propose meta-Stackelberg learning (meta-SL), a provably efficient meta-learning algorithm, to solve the equilibrium strategy in BSMG, leading to an adaptable FL defense. We demonstrate that meta-SL converges to the first-order $\varepsilon$-equilibrium point in $O(\varepsilon^{-2})$ gradient iterations, with $O(\varepsilon^{-4})$ samples needed per iteration, matching the state of the art. Empirical evidence indicates that our meta-Stackelberg framework performs exceptionally well against potent model poisoning and backdoor attacks of an uncertain nature.

摘要: 先前的研究表明，联合学习(FL)系统面临一系列安全风险。尽管提出了几种防御战略，但它们往往是非适应性的，并且特定于某些类型的攻击，使得它们对不可预测或适应性威胁无效。该工作将对抗性联邦学习建模为贝叶斯Stackelberg马尔可夫博弈(BSMG)，以捕获防御者各种攻击类型的不完全信息。我们提出了元Stackelberg学习算法(META-SL)来解决BSMG中的均衡策略，从而得到一种自适应的FL防御。我们证明了META-SL在$O(varepsilon^{-2})$梯度迭代中收敛到一阶$varepsilon$-均衡点，每次迭代需要$O(varepsilon^{-4})$样本，与现有技术相匹配。经验证据表明，我们的Meta-Stackelberg框架在对抗强大的模型中毒和不确定性质的后门攻击时表现得非常好。



## **15. Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data**

Fedward：支持非IID数据的灵活联邦后门防御框架 cs.LG

Accepted by IEEE ICME 2023

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2307.00356v1) [paper-pdf](http://arxiv.org/pdf/2307.00356v1)

**Authors**: Zekai Chen, Fuyi Wang, Zhiwei Zheng, Ximeng Liu, Yujie Lin

**Abstract**: Federated learning (FL) enables multiple clients to collaboratively train deep learning models while considering sensitive local datasets' privacy. However, adversaries can manipulate datasets and upload models by injecting triggers for federated backdoor attacks (FBA). Existing defense strategies against FBA consider specific and limited attacker models, and a sufficient amount of noise to be injected only mitigates rather than eliminates FBA. To address these deficiencies, we introduce a Flexible Federated Backdoor Defense Framework (Fedward) to ensure the elimination of adversarial backdoors. We decompose FBA into various attacks, and design amplified magnitude sparsification (AmGrad) and adaptive OPTICS clustering (AutoOPTICS) to address each attack. Meanwhile, Fedward uses the adaptive clipping method by regarding the number of samples in the benign group as constraints on the boundary. This ensures that Fedward can maintain the performance for the Non-IID scenario. We conduct experimental evaluations over three benchmark datasets and thoroughly compare them to state-of-the-art studies. The results demonstrate the promising defense performance from Fedward, moderately improved by 33% $\sim$ 75 in clustering defense methods, and 96.98%, 90.74%, and 89.8% for Non-IID to the utmost extent for the average FBA success rate over MNIST, FMNIST, and CIFAR10, respectively.

摘要: [TencentCloudSDKException] code:ClientNetworkError message:('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')) requestId:None



## **16. Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey**

三维点云分类的对抗性攻防综述 cs.CV

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2307.00309v1) [paper-pdf](http://arxiv.org/pdf/2307.00309v1)

**Authors**: Hanieh Naderi, Ivan V. Bajić

**Abstract**: Deep learning has successfully solved a wide range of tasks in 2D vision as a dominant AI technique. Recently, deep learning on 3D point clouds is becoming increasingly popular for addressing various tasks in this field. Despite remarkable achievements, deep learning algorithms are vulnerable to adversarial attacks. These attacks are imperceptible to the human eye but can easily fool deep neural networks in the testing and deployment stage. To encourage future research, this survey summarizes the current progress on adversarial attack and defense techniques on point cloud classification. This paper first introduces the principles and characteristics of adversarial attacks and summarizes and analyzes the adversarial example generation methods in recent years. Besides, it classifies defense strategies as input transformation, data optimization, and deep model modification. Finally, it presents several challenging issues and future research directions in this domain.

摘要: 深度学习作为一种占主导地位的人工智能技术，已经成功地解决了2D视觉中的一系列任务。近年来，针对三维点云的深度学习成为解决该领域各种问题的热门方法。尽管深度学习算法取得了令人瞩目的成就，但它仍然容易受到对手的攻击。这些攻击是人眼看不见的，但在测试和部署阶段很容易就能愚弄深度神经网络。为了鼓励未来的研究，本综述总结了当前点云分类对抗性攻防技术的研究进展。本文首先介绍了对抗性攻击的原理和特点，并对近年来对抗性实例生成方法进行了总结和分析。此外，还将防御策略分为输入转换、数据优化和深度模型修改。最后提出了该领域的几个具有挑战性的问题和未来的研究方向。



## **17. Common Knowledge Learning for Generating Transferable Adversarial Examples**

用于生成可转移对抗性实例的常识学习 cs.LG

11 pages, 5 figures

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2307.00274v1) [paper-pdf](http://arxiv.org/pdf/2307.00274v1)

**Authors**: Ruijie Yang, Yuanfang Guo, Junfu Wang, Jiantao Zhou, Yunhong Wang

**Abstract**: This paper focuses on an important type of black-box attacks, i.e., transfer-based adversarial attacks, where the adversary generates adversarial examples by a substitute (source) model and utilize them to attack an unseen target model, without knowing its information. Existing methods tend to give unsatisfactory adversarial transferability when the source and target models are from different types of DNN architectures (e.g. ResNet-18 and Swin Transformer). In this paper, we observe that the above phenomenon is induced by the output inconsistency problem. To alleviate this problem while effectively utilizing the existing DNN models, we propose a common knowledge learning (CKL) framework to learn better network weights to generate adversarial examples with better transferability, under fixed network architectures. Specifically, to reduce the model-specific features and obtain better output distributions, we construct a multi-teacher framework, where the knowledge is distilled from different teacher architectures into one student network. By considering that the gradient of input is usually utilized to generated adversarial examples, we impose constraints on the gradients between the student and teacher models, to further alleviate the output inconsistency problem and enhance the adversarial transferability. Extensive experiments demonstrate that our proposed work can significantly improve the adversarial transferability.

摘要: 本文研究了一种重要的黑盒攻击类型，即基于转移的对抗性攻击，对手通过替代(源)模型生成对抗性实例，并利用它们攻击一个看不见的目标模型，而不需要知道其信息。当源模型和目标模型来自不同类型的DNN结构(例如ResNet-18和Swin Transformer)时，现有方法往往给出不令人满意的对抗性可转移性。在本文中，我们观察到上述现象是由输出不一致问题引起的。为了在有效利用现有DNN模型的同时缓解这一问题，我们提出了一种公共知识学习(CKL)框架，在固定的网络结构下学习更好的网络权重来生成具有更好可移植性的对抗性示例。具体地说，为了减少模型的特定特征并获得更好的输出分布，我们构建了一个多教师框架，其中来自不同教师体系的知识提取到一个学生网络中。考虑到输入的梯度通常被用来生成对抗性示例，我们对学生模型和教师模型之间的梯度施加约束，以进一步缓解输出不一致问题，增强对抗性可转移性。大量实验表明，我们提出的工作可以显著提高对抗性可转移性。



## **18. Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning**

隐藏在明显的视线中：在多智能体强化学习中利用差分隐私噪声进行逃避弹性局部中毒攻击 cs.LG

Accepted for publication in the proceeding of ICMLC 2023, 9-11 July  2023, The University of Adelaide, Adelaide, Australia

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2307.00268v1) [paper-pdf](http://arxiv.org/pdf/2307.00268v1)

**Authors**: Md Tamjid Hossain, Hung La

**Abstract**: Lately, differential privacy (DP) has been introduced in cooperative multiagent reinforcement learning (CMARL) to safeguard the agents' privacy against adversarial inference during knowledge sharing. Nevertheless, we argue that the noise introduced by DP mechanisms may inadvertently give rise to a novel poisoning threat, specifically in the context of private knowledge sharing during CMARL, which remains unexplored in the literature. To address this shortcoming, we present an adaptive, privacy-exploiting, and evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the inherent DP-noise to circumvent anomaly detection systems and hinder the optimal convergence of the CMARL model. We rigorously evaluate our proposed PeLPA attack in diverse environments, encompassing both non-adversarial and multiple-adversarial contexts. Our findings reveal that, in a medium-scale environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to an increase in average steps to goal by 50.69% and 64.41%, respectively. Furthermore, under similar conditions, PeLPA can result in a 1.4x and 1.6x computational time increase in optimal reward attainment and a 1.18x and 1.38x slower convergence for attacker ratios of 20% and 40%, respectively.

摘要: 最近，在协作多智能体强化学习(CMARL)中引入了差异隐私(DP)，以保护智能体在知识共享过程中的隐私不受对手推理的影响。然而，我们认为DP机制引入的噪声可能无意中引起一种新的中毒威胁，特别是在CMARL期间的私人知识共享的背景下，这在文献中仍未被探索。针对这一缺陷，我们提出了一种自适应的、利用隐私攻击和逃避弹性的局部中毒攻击(PeLPA)，该攻击利用固有的DP噪声来绕过异常检测系统并阻碍CMARL模型的最优收敛。我们在不同的环境中严格评估我们提出的PeLPA攻击，包括非对抗性和多对抗性环境。我们的研究结果表明，在中等规模的环境中，攻击者比率分别为20%和40%的PeLPA攻击可以使到达目标的平均步数分别增加50.69%和64.41%。此外，在类似的条件下，PeLPA可以使最优奖励获得的计算时间分别增加1.4倍和1.6倍，对于攻击比率分别为20%和40%的攻击者，收敛速度分别慢1.18倍和1.38倍。



## **19. A Black-box NLP Classifier Attacker**

黑盒NLP分类器攻击者 cs.LG

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2112.11660v3) [paper-pdf](http://arxiv.org/pdf/2112.11660v3)

**Authors**: Yueyang Liu, Hunmin Lee, Zhipeng Cai

**Abstract**: Deep neural networks have a wide range of applications in solving various real-world tasks and have achieved satisfactory results, in domains such as computer vision, image classification, and natural language processing. Meanwhile, the security and robustness of neural networks have become imperative, as diverse researches have shown the vulnerable aspects of neural networks. Case in point, in Natural language processing tasks, the neural network may be fooled by an attentively modified text, which has a high similarity to the original one. As per previous research, most of the studies are focused on the image domain; Different from image adversarial attacks, the text is represented in a discrete sequence, traditional image attack methods are not applicable in the NLP field. In this paper, we propose a word-level NLP sentiment classifier attack model, which includes a self-attention mechanism-based word selection method and a greedy search algorithm for word substitution. We experiment with our attack model by attacking GRU and 1D-CNN victim models on IMDB datasets. Experimental results demonstrate that our model achieves a higher attack success rate and more efficient than previous methods due to the efficient word selection algorithms are employed and minimized the word substitute number. Also, our model is transferable, which can be used in the image domain with several modifications.

摘要: 深度神经网络在计算机视觉、图像分类、自然语言处理等领域有着广泛的应用，并取得了令人满意的结果。与此同时，随着各种研究表明神经网络的脆弱方面，神经网络的安全性和健壮性变得势在必行。例如，在自然语言处理任务中，神经网络可能会被精心修改的文本所愚弄，因为它与原始文本具有很高的相似性。根据以往的研究，大多数研究都集中在图像领域；与图像对抗性攻击不同，文本是离散序列表示的，传统的图像攻击方法不适用于自然语言处理领域。本文提出了一个词级NLP情感分类器攻击模型，该模型包括一种基于自我注意机制的词选择方法和一种贪婪的词替换搜索算法。我们在IMDB数据集上通过攻击GRU和1D-CNN受害者模型来测试我们的攻击模型。实验结果表明，由于采用了高效的选词算法和最小化了替换词的数量，该模型获得了更高的攻击成功率和更高的效率。此外，我们的模型是可移植的，只需进行几次修改就可以在图像域使用。



## **20. SecBeam: Securing mmWave Beam Alignment against Beam-Stealing Attacks**

SecBeam：保护毫米波波束对准免受波束窃取攻击 cs.CR

**SubmitDate**: 2023-07-01    [abs](http://arxiv.org/abs/2307.00178v1) [paper-pdf](http://arxiv.org/pdf/2307.00178v1)

**Authors**: Jingcheng Li, Loukas Lazos, Ming Li

**Abstract**: Millimeter wave (mmWave) communications employ narrow-beam directional communications to compensate for the high path loss at mmWave frequencies. Compared to their omnidirectional counterparts, an additional step of aligning the transmitter's and receiver's antennas is required. In current standards such as 802.11ad, this beam alignment process is implemented via an exhaustive search through the horizontal plane known as beam sweeping. However, the beam sweeping process is unauthenticated. As a result, an adversary, Mallory, can launch an active beam-stealing attack by injecting forged beacons of high power, forcing the legitimate devices to beamform towards her direction. Mallory is now in control of the communication link between the two devices, thus breaking the false sense of security given by the directionality of mmWave transmissions.   Prior works have added integrity protection to beam alignment messages to prevent forgeries. In this paper, we demonstrate a new beam-stealing attack that does not require message forging. We show that Mallory can amplify and relay a beam sweeping frame from her direction without altering its contents. Intuitively, cryptographic primitives cannot verify physical properties such as the SNR used in beam selection. We propose a new beam sweeping protocol called SecBeam that utilizes power/sector randomization and coarse angle-of-arrival information to detect amplify-and-relay attacks. We demonstrate the security and performance of SecBeam using an experimental mmWave platform and via ray-tracing simulations.

摘要: 毫米波(毫米波)通信使用窄波束定向通信来补偿毫米波频率下的高路径损耗。与它们的全方位对应物相比，需要一个额外的步骤来对准发射器和接收器的天线。在当前的标准中，例如802.11ad，这种波束对准过程是通过称为波束扫描的水平面的穷举搜索来实现的。然而，波束扫描过程未经验证。因此，对手Mallory可以通过注入高功率的伪造信标来发动主动窃取波束攻击，迫使合法设备朝着她的方向波束成形。马洛里现在控制着这两个设备之间的通信链路，从而打破了毫米波传输的方向性带来的错误安全感。以前的工作已经为波束对准消息增加了完整性保护，以防止伪造。在本文中，我们证明了一种新的不需要消息伪造的波束窃取攻击。我们证明了Mallory可以在不改变其内容的情况下放大和中继从她的方向扫描的光束帧。直观地说，密码原语不能验证物理属性，例如在波束选择中使用的SNR。我们提出了一种新的波束扫描协议SecBeam，该协议利用功率/扇区随机化和粗到达角信息来检测放大和中继攻击。我们使用一个实验性的毫米波平台和光线跟踪模拟来演示SecBeam的安全性和性能。



## **21. Beyond Neural-on-Neural Approaches to Speaker Gender Protection**

超越神经对神经的说话人性别保护方法 eess.AS

**SubmitDate**: 2023-06-30    [abs](http://arxiv.org/abs/2306.17700v1) [paper-pdf](http://arxiv.org/pdf/2306.17700v1)

**Authors**: Loes van Bemmel, Zhuoran Liu, Nik Vaessen, Martha Larson

**Abstract**: Recent research has proposed approaches that modify speech to defend against gender inference attacks. The goal of these protection algorithms is to control the availability of information about a speaker's gender, a privacy-sensitive attribute. Currently, the common practice for developing and testing gender protection algorithms is "neural-on-neural", i.e., perturbations are generated and tested with a neural network. In this paper, we propose to go beyond this practice to strengthen the study of gender protection. First, we demonstrate the importance of testing gender inference attacks that are based on speech features historically developed by speech scientists, alongside the conventionally used neural classifiers. Next, we argue that researchers should use speech features to gain insight into how protective modifications change the speech signal. Finally, we point out that gender-protection algorithms should be compared with novel "vocal adversaries", human-executed voice adaptations, in order to improve interpretability and enable before-the-mic protection.

摘要: 最近的研究提出了修改语音以防御性别推断攻击的方法。这些保护算法的目标是控制有关说话人性别的信息的可用性，这是一种隐私敏感属性。目前，开发和测试性别保护算法的常见做法是“神经对神经”，即产生扰动并使用神经网络进行测试。在本文中，我们建议超越这一做法，加强对性别保护的研究。首先，我们证明了测试性别推断攻击的重要性，这些攻击基于语音科学家历史上开发的语音特征，以及常规使用的神经分类器。接下来，我们认为研究人员应该使用语音特征来洞察保护性修改如何改变语音信号。最后，我们指出，应该将性别保护算法与人类执行的语音改编的新颖的“声音对手”进行比较，以提高可解释性，并实现麦克风前的保护。



## **22. MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection**

MalProtect：基于ML的恶意软件检测中对抗恶意查询攻击的状态防御 cs.LG

**SubmitDate**: 2023-06-30    [abs](http://arxiv.org/abs/2302.10739v3) [paper-pdf](http://arxiv.org/pdf/2302.10739v3)

**Authors**: Aqib Rashid, Jose Such

**Abstract**: ML models are known to be vulnerable to adversarial query attacks. In these attacks, queries are iteratively perturbed towards a particular class without any knowledge of the target model besides its output. The prevalence of remotely-hosted ML classification models and Machine-Learning-as-a-Service platforms means that query attacks pose a real threat to the security of these systems. To deal with this, stateful defenses have been proposed to detect query attacks and prevent the generation of adversarial examples by monitoring and analyzing the sequence of queries received by the system. Several stateful defenses have been proposed in recent years. However, these defenses rely solely on similarity or out-of-distribution detection methods that may be effective in other domains. In the malware detection domain, the methods to generate adversarial examples are inherently different, and therefore we find that such detection mechanisms are significantly less effective. Hence, in this paper, we present MalProtect, which is a stateful defense against query attacks in the malware detection domain. MalProtect uses several threat indicators to detect attacks. Our results show that it reduces the evasion rate of adversarial query attacks by 80+\% in Android and Windows malware, across a range of attacker scenarios. In the first evaluation of its kind, we show that MalProtect outperforms prior stateful defenses, especially under the peak adversarial threat.

摘要: 众所周知，ML模型容易受到敌意查询攻击。在这些攻击中，查询被迭代地扰动到特定的类，除了其输出之外，不知道目标模型。远程托管的ML分类模型和机器学习即服务平台的流行意味着查询攻击对这些系统的安全构成了真正的威胁。为了解决这个问题，已经提出了状态防御来检测查询攻击，并通过监控和分析系统接收到的查询序列来防止敌对实例的生成。近年来，有人提出了几项有状态的辩护。然而，这些防御完全依赖于可能在其他领域有效的相似性或分布外检测方法。在恶意软件检测领域，生成恶意示例的方法本质上是不同的，因此我们发现这种检测机制的有效性显著降低。因此，在本文中，我们提出了MalProtect，它是恶意软件检测领域中针对查询攻击的一种状态防御。MalProtect使用多个威胁指示器来检测攻击。我们的结果表明，在各种攻击场景下，该算法将Android和Windows恶意软件中恶意查询攻击的逃避率降低了80%+\%。在该类型的第一次评估中，我们表明MalProtect的性能优于先前的状态防御，特别是在峰值敌意威胁下。



## **23. Efficient Backdoor Removal Through Natural Gradient Fine-tuning**

通过自然梯度微调高效删除后门 cs.CV

**SubmitDate**: 2023-06-30    [abs](http://arxiv.org/abs/2306.17441v1) [paper-pdf](http://arxiv.org/pdf/2306.17441v1)

**Authors**: Nazmul Karim, Abdullah Al Arafat, Umar Khalid, Zhishan Guo, Naznin Rahnavard

**Abstract**: The success of a deep neural network (DNN) heavily relies on the details of the training scheme; e.g., training data, architectures, hyper-parameters, etc. Recent backdoor attacks suggest that an adversary can take advantage of such training details and compromise the integrity of a DNN. Our studies show that a backdoor model is usually optimized to a bad local minima, i.e. sharper minima as compared to a benign model. Intuitively, a backdoor model can be purified by reoptimizing the model to a smoother minima through fine-tuning with a few clean validation data. However, fine-tuning all DNN parameters often requires huge computational costs and often results in sub-par clean test performance. To address this concern, we propose a novel backdoor purification technique, Natural Gradient Fine-tuning (NGF), which focuses on removing the backdoor by fine-tuning only one layer. Specifically, NGF utilizes a loss surface geometry-aware optimizer that can successfully overcome the challenge of reaching a smooth minima under a one-layer optimization scenario. To enhance the generalization performance of our proposed method, we introduce a clean data distribution-aware regularizer based on the knowledge of loss surface curvature matrix, i.e., Fisher Information Matrix. Extensive experiments show that the proposed method achieves state-of-the-art performance on a wide range of backdoor defense benchmarks: four different datasets- CIFAR10, GTSRB, Tiny-ImageNet, and ImageNet; 13 recent backdoor attacks, e.g. Blend, Dynamic, WaNet, ISSBA, etc.

摘要: 深度神经网络(DNN)的成功在很大程度上依赖于训练方案的细节；例如，训练数据、体系结构、超参数等。最近的后门攻击表明，对手可以利用这些训练细节并损害DNN的完整性。我们的研究表明，与良性模型相比，后门模型通常会优化到较差的局部最小值，即更尖锐的最小值。直观地说，后门模型可以通过使用几个干净的验证数据进行微调，将模型重新优化到更平滑的最小值来进行净化。然而，微调所有DNN参数通常需要巨大的计算成本，并且通常会导致测试性能低于平均水平。为了解决这个问题，我们提出了一种新的后门净化技术，自然梯度微调(NGF)，它专注于通过微调一层来消除后门。具体地说，NGF利用了一种损失面几何感知优化器，它可以成功地克服在单层优化场景下达到平滑最小值的挑战。为了提高该方法的泛化性能，我们引入了一种干净的基于损失曲面曲率矩阵知识的数据分布感知正则化方法，即Fisher信息矩阵。大量实验表明，该方法在CIFAR10、GTSRB、Tiny-ImageNet和ImageNet四个不同的数据集上，以及最近的13个后门攻击，如Blend、Dynamic、WaNet、Issba等上，都达到了最先进的性能。



## **24. Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection**

遥感显著目标检测中对敌云攻击的防御 cs.CV

**SubmitDate**: 2023-06-30    [abs](http://arxiv.org/abs/2306.17431v1) [paper-pdf](http://arxiv.org/pdf/2306.17431v1)

**Authors**: Huiming Sun, Lan Fu, Jinlong Li, Qing Guo, Zibo Meng, Tianyun Zhang, Yuewei Lin, Hongkai Yu

**Abstract**: etecting the salient objects in a remote sensing image has wide applications for the interdisciplinary research. Many existing deep learning methods have been proposed for Salient Object Detection (SOD) in remote sensing images and get remarkable results. However, the recent adversarial attack examples, generated by changing a few pixel values on the original remote sensing image, could result in a collapse for the well-trained deep learning based SOD model. Different with existing methods adding perturbation to original images, we propose to jointly tune adversarial exposure and additive perturbation for attack and constrain image close to cloudy image as Adversarial Cloud. Cloud is natural and common in remote sensing images, however, camouflaging cloud based adversarial attack and defense for remote sensing images are not well studied before. Furthermore, we design DefenseNet as a learn-able pre-processing to the adversarial cloudy images so as to preserve the performance of the deep learning based remote sensing SOD model, without tuning the already deployed deep SOD model. By considering both regular and generalized adversarial examples, the proposed DefenseNet can defend the proposed Adversarial Cloud in white-box setting and other attack methods in black-box setting. Experimental results on a synthesized benchmark from the public remote sensing SOD dataset (EORSSD) show the promising defense against adversarial cloud attacks.

摘要: 遥感图像中显著目标的提取在多学科交叉研究中有着广泛的应用。已有的许多深度学习方法被提出用于遥感图像中的显著目标检测，并取得了显著的效果。然而，最近通过改变原始遥感图像上的几个像素值而生成的对抗性攻击实例，可能会导致基于深度学习的训练有素的SOD模型崩溃。与已有的在原始图像上添加扰动的方法不同，我们提出了联合调整攻击的对抗性曝光和加性扰动，并将接近云图的图像约束为对抗性云。云层是遥感图像中常见的自然现象，但基于云层伪装的遥感图像对抗攻防研究较少。此外，我们将DefenseNet设计为对敌意云图进行可学习的预处理，以保持基于深度学习的遥感SOD模型的性能，而不需要调整已经部署的深度SOD模型。通过考虑常规和广义的对抗性实例，所提出的防御网络可以在白盒环境下防御所提出的对抗性云，并在黑盒环境下防御其他攻击方法。在一个基于公共遥感数据集(EORSSD)的合成基准上的实验结果表明，该方法能够有效地防御敌意云攻击。



## **25. LTD: Low Temperature Distillation for Robust Adversarial Training**

LTD：低温蒸馏用于强大的对抗性训练 cs.CV

**SubmitDate**: 2023-06-30    [abs](http://arxiv.org/abs/2111.02331v3) [paper-pdf](http://arxiv.org/pdf/2111.02331v3)

**Authors**: Erh-Chung Chen, Che-Rung Lee

**Abstract**: Adversarial training has been widely used to enhance the robustness of neural network models against adversarial attacks. Despite the popularity of neural network models, a significant gap exists between the natural and robust accuracy of these models. In this paper, we identify one of the primary reasons for this gap is the common use of one-hot vectors as labels, which hinders the learning process for image recognition. Representing ambiguous images with one-hot vectors is imprecise and may lead the model to suboptimal solutions. To overcome this issue, we propose a novel method called Low Temperature Distillation (LTD) that generates soft labels using the modified knowledge distillation framework. Unlike previous approaches, LTD uses a relatively low temperature in the teacher model and fixed, but different temperatures for the teacher and student models. This modification boosts the model's robustness without encountering the gradient masking problem that has been addressed in defensive distillation. The experimental results demonstrate the effectiveness of the proposed LTD method combined with previous techniques, achieving robust accuracy rates of 58.19%, 31.13%, and 42.08% on CIFAR-10, CIFAR-100, and ImageNet data sets, respectively, without additional unlabeled data.

摘要: 对抗性训练已被广泛应用于增强神经网络模型对对抗性攻击的鲁棒性。尽管神经网络模型很受欢迎，但这些模型的自然精度和稳健精度之间存在着巨大的差距。在本文中，我们发现造成这一差距的主要原因之一是普遍使用单一热点向量作为标签，这阻碍了图像识别的学习过程。用一个热点向量表示模糊图像是不精确的，并且可能导致模型得到次优解。为了解决这个问题，我们提出了一种新的方法，称为低温蒸馏(LTD)，它使用改进的知识蒸馏框架来生成软标签。与以前的方法不同，LTD在教师模型中使用相对较低的温度，并为教师和学生模型使用固定但不同的温度。这种修改增强了模型的稳健性，而不会遇到在防御蒸馏中已经解决的梯度掩蔽问题。实验结果证明了LTD方法的有效性，在CIFAR-10、CIFAR-100和ImageNet数据集上，在不增加额外的未标记数据的情况下，分别获得了58.19%、31.13%和42.08%的稳健准确率。



## **26. Secret-Free Device Pairing in the mmWave Band**

毫米波频段的免密设备配对 cs.CR

14 pages, 16 figures

**SubmitDate**: 2023-06-29    [abs](http://arxiv.org/abs/2306.17330v1) [paper-pdf](http://arxiv.org/pdf/2306.17330v1)

**Authors**: Ziqi Xu, Jingcheng Li, Yanjun Pan, Ming Li, Loukas Lazos

**Abstract**: Many Next Generation (NextG) applications feature devices that are capable of communicating and sensing in the Millimeter-Wave (mmWave) bands. Trust establishment is an important first step to bootstrap secure mmWave communication links, which is challenging due to the lack of prior secrets and the fact that traditional cryptographic authentication methods cannot bind digital trust with physical properties. Previously, context-based device pairing approaches were proposed to extract shared secrets from common context, using various sensing modalities. However, they suffer from various limitations in practicality and security.   In this work, we propose the first secret-free device pairing scheme in the mmWave band that explores the unique physical-layer properties of mmWave communications. Our basic idea is to let Alice and Bob derive common randomness by sampling physical activity in the surrounding environment that disturbs their wireless channel. They construct reliable fingerprints of the activity by extracting event timing information from the channel state. We further propose an uncoordinated path hopping mechanism to resolve the challenges of beam alignment for activity sensing without prior trust. A key novelty of our protocol is that it remains secure against both co-located passive adversaries and active Man-in-the-Middle attacks, which is not possible with existing context-based pairing approaches. We implement our protocol in a 28GHz mmWave testbed, and experimentally evaluate its security in realistic indoor environments. Results show that our protocol can effectively thwart several different types of adversaries.

摘要: 许多下一代(NextG)应用具有能够在毫米波(毫米波)频段进行通信和传感的设备。信任建立是引导安全毫米波通信链路的重要第一步，由于缺乏先验秘密以及传统密码认证方法不能将数字信任与物理属性绑定，这是具有挑战性的。以前，人们提出了基于上下文的设备配对方法，使用各种感知模式从公共上下文中提取共享秘密。然而，它们在实用性和安全性方面受到各种限制。在这项工作中，我们提出了第一个在毫米波频段的无秘密设备配对方案，该方案探索了毫米波通信的独特的物理层属性。我们的基本想法是让Alice和Bob通过采样周围环境中干扰他们无线信道的身体活动来获得共同的随机性。它们通过从信道状态中提取事件定时信息来构建活动的可靠指纹。我们进一步提出了一种非协调路径跳跃机制来解决无先验信任的活动感知中波束对准的挑战。我们协议的一个关键创新之处在于，它仍然能够安全地抵御共同定位的被动对手和主动的中间人攻击，而这是现有的基于上下文的配对方法所不可能实现的。我们在28 GHz毫米波实验床上实现了我们的协议，并在真实的室内环境中对其安全性进行了实验评估。结果表明，该协议能够有效地抵御多种不同类型的攻击。



## **27. Defending Black-box Classifiers by Bayesian Boundary Correction**

基于贝叶斯边界校正的黑盒分类器防御 cs.CV

arXiv admin note: text overlap with arXiv:2203.04713

**SubmitDate**: 2023-06-29    [abs](http://arxiv.org/abs/2306.16979v1) [paper-pdf](http://arxiv.org/pdf/2306.16979v1)

**Authors**: He Wang, Yunfeng Diao

**Abstract**: Classifiers based on deep neural networks have been recently challenged by Adversarial Attack, where the widely existing vulnerability has invoked the research in defending them from potential threats. Given a vulnerable classifier, existing defense methods are mostly white-box and often require re-training the victim under modified loss functions/training regimes. While the model/data/training specifics of the victim are usually unavailable to the user, re-training is unappealing, if not impossible for reasons such as limited computational resources. To this end, we propose a new black-box defense framework. It can turn any pre-trained classifier into a resilient one with little knowledge of the model specifics. This is achieved by new joint Bayesian treatments on the clean data, the adversarial examples and the classifier, for maximizing their joint probability. It is further equipped with a new post-train strategy which keeps the victim intact. We name our framework Bayesian Boundary Correction (BBC). BBC is a general and flexible framework that can easily adapt to different data types. We instantiate BBC for image classification and skeleton-based human activity recognition, for both static and dynamic data. Exhaustive evaluation shows that BBC has superior robustness and can enhance robustness without severely hurting the clean accuracy, compared with existing defense methods.

摘要: 基于深度神经网络的分类器最近受到了敌意攻击的挑战，其中广泛存在的漏洞引发了保护它们免受潜在威胁的研究。在给定一个易受攻击的分类器的情况下，现有的防御方法大多是白盒的，并且经常需要根据修改的损失函数/训练制度重新训练受害者。虽然受害者的模型/数据/培训细节通常对用户不可用，但重新培训是没有吸引力的，如果不是因为有限的计算资源等原因不可能的话。为此，我们提出了一种新的黑盒防御框架。它可以将任何预先训练的分类器变成一个有弹性的分类器，而对模型细节知之甚少。这是通过对干净数据、对抗性样本和分类器进行新的联合贝叶斯处理来实现的，以最大化它们的联合概率。它进一步配备了新的列车后战略，使受害者完好无损。我们将我们的框架命名为贝叶斯边界校正(BBC)。BBC是一个通用和灵活的框架，可以很容易地适应不同的数据类型。对于静态和动态数据，我们实例化了用于图像分类和基于骨骼的人体活动识别的BBC。详尽的评估表明，与现有的防御方法相比，BBC具有更好的稳健性，可以在不严重损害干净精度的情况下增强稳健性。



## **28. "That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks**

“这是一个可疑的反应！”：解读Logits变量以检测NLP对手攻击 cs.AI

ACL 2022

**SubmitDate**: 2023-06-29    [abs](http://arxiv.org/abs/2204.04636v2) [paper-pdf](http://arxiv.org/pdf/2204.04636v2)

**Authors**: Edoardo Mosca, Shreyash Agarwal, Javier Rando, Georg Groh

**Abstract**: Adversarial attacks are a major challenge faced by current machine learning research. These purposely crafted inputs fool even the most advanced models, precluding their deployment in safety-critical applications. Extensive research in computer vision has been carried to develop reliable defense strategies. However, the same issue remains less explored in natural language processing. Our work presents a model-agnostic detector of adversarial text examples. The approach identifies patterns in the logits of the target classifier when perturbing the input text. The proposed detector improves the current state-of-the-art performance in recognizing adversarial inputs and exhibits strong generalization capabilities across different NLP models, datasets, and word-level attacks.

摘要: 对抗性攻击是当前机器学习研究面临的一大挑战。这些刻意制作的输入甚至欺骗了最先进的型号，使它们无法部署在安全关键应用程序中。为了制定可靠的防御策略，人们在计算机视觉方面进行了广泛的研究。然而，在自然语言处理中，同样的问题仍然被较少地探讨。我们的工作提出了一个模型不可知的对抗性文本例子的检测器。该方法在干扰输入文本时识别目标分类器的逻辑中的模式。提出的检测器提高了当前在识别敌意输入方面的最新性能，并在不同的NLP模型、数据集和词级攻击中显示出强大的泛化能力。



## **29. Towards Optimal Randomized Strategies in Adversarial Example Game**

对抗性范例博弈中的最优随机策略 cs.LG

Extended version of paper https://doi.org/10.1609/aaai.v37i9.26247  which appeared in AAAI 2023

**SubmitDate**: 2023-06-29    [abs](http://arxiv.org/abs/2306.16738v1) [paper-pdf](http://arxiv.org/pdf/2306.16738v1)

**Authors**: Jiahao Xie, Chao Zhang, Weijie Liu, Wensong Bai, Hui Qian

**Abstract**: The vulnerability of deep neural network models to adversarial example attacks is a practical challenge in many artificial intelligence applications. A recent line of work shows that the use of randomization in adversarial training is the key to find optimal strategies against adversarial example attacks. However, in a fully randomized setting where both the defender and the attacker can use randomized strategies, there are no efficient algorithm for finding such an optimal strategy. To fill the gap, we propose the first algorithm of its kind, called FRAT, which models the problem with a new infinite-dimensional continuous-time flow on probability distribution spaces. FRAT maintains a lightweight mixture of models for the defender, with flexibility to efficiently update mixing weights and model parameters at each iteration. Furthermore, FRAT utilizes lightweight sampling subroutines to construct a random strategy for the attacker. We prove that the continuous-time limit of FRAT converges to a mixed Nash equilibria in a zero-sum game formed by a defender and an attacker. Experimental results also demonstrate the efficiency of FRAT on CIFAR-10 and CIFAR-100 datasets.

摘要: 在许多人工智能应用中，深度神经网络模型对敌意示例攻击的脆弱性是一个实际挑战。最近的一系列工作表明，在对抗性训练中使用随机化是找到对抗对抗性范例攻击的最佳策略的关键。然而，在防御者和攻击者都可以使用随机化策略的完全随机化设置中，没有有效的算法来寻找这样的最优策略。为了填补这一空白，我们提出了第一个算法，称为FRAT，它用概率分布空间上的一个新的无限维连续时间流来模拟该问题。FRAT为防守者维护了一个轻量级的模型混合，具有在每次迭代中高效地更新混合权重和模型参数的灵活性。此外，FRAT利用轻量级抽样子例程为攻击者构建随机策略。我们证明了在一个由防御者和攻击者组成的零和博弈中，兄弟会的连续时间限制收敛到一个混合纳什均衡。实验结果也证明了FRAT在CIFAR-10和CIFAR-100数据集上的有效性。



## **30. Randomized Reversible Gate-Based Obfuscation for Secured Compilation of Quantum Circuit**

基于随机可逆门的量子电路安全编译混淆算法 quant-ph

11 pages, 12 figures, conference

**SubmitDate**: 2023-06-29    [abs](http://arxiv.org/abs/2305.01133v2) [paper-pdf](http://arxiv.org/pdf/2305.01133v2)

**Authors**: Subrata Das, Swaroop Ghosh

**Abstract**: The success of quantum circuits in providing reliable outcomes for a given problem depends on the gate count and depth in near-term noisy quantum computers. Quantum circuit compilers that decompose high-level gates to native gates of the hardware and optimize the circuit play a key role in quantum computing. However, the quality and time complexity of the optimization process can vary significantly especially for practically relevant large-scale quantum circuits. As a result, third-party (often less-trusted/untrusted) compilers have emerged, claiming to provide better and faster optimization of complex quantum circuits than so-called trusted compilers. However, untrusted compilers can pose severe security risks, such as the theft of sensitive intellectual property (IP) embedded within the quantum circuit. We propose an obfuscation technique for quantum circuits using randomized reversible gates to protect them from such attacks during compilation. The idea is to insert a small random circuit into the original circuit and send it to the untrusted compiler. Since the circuit function is corrupted, the adversary may get incorrect IP. However, the user may also get incorrect output post-compilation. To circumvent this issue, we concatenate the inverse of the random circuit in the compiled circuit to recover the original functionality. We demonstrate the practicality of our method by conducting exhaustive experiments on a set of benchmark circuits and measuring the quality of obfuscation by calculating the Total Variation Distance (TVD) metric. Our method achieves TVD of up to 1.92 and performs at least 2X better than a previously reported obfuscation method. We also propose a novel adversarial reverse engineering (RE) approach and show that the proposed obfuscation is resilient against RE attacks. The proposed technique introduces minimal degradation in fidelity (~1% to ~3% on average).

摘要: 量子电路在为给定问题提供可靠结果方面的成功取决于近期嘈杂的量子计算机中的门数量和深度。量子电路编译器将高层门分解为硬件的本机门，并对电路进行优化，在量子计算中发挥着关键作用。然而，优化过程的质量和时间复杂性可能会有很大的变化，特别是对于实际相关的大规模量子电路。因此，第三方(通常不太可信/不可信)编译器应运而生，声称比所谓的可信编译器提供更好、更快的复杂量子电路优化。然而，不可信的编译器可能会带来严重的安全风险，例如嵌入量子电路中的敏感知识产权(IP)被窃取。我们提出了一种量子电路的混淆技术，该技术使用随机化的可逆门来保护它们在编译过程中免受此类攻击。其想法是在原始电路中插入一个小的随机电路，并将其发送给不可信的编译器。由于电路功能被破坏，对手可能获得错误的IP。但是，用户也可能在编译后得到不正确的输出。为了避免这个问题，我们将编译电路中的随机电路的逆连接起来，以恢复原来的功能。我们在一组基准电路上进行了详尽的实验，并通过计算总变化距离(TVD)度量来衡量混淆质量，从而证明了该方法的实用性。我们的方法获得了高达1.92的TVD，并且比先前报道的混淆方法的性能至少提高了2倍。我们还提出了一种新的对抗性逆向工程(RE)方法，并证明了该方法对逆向工程攻击具有较强的抵抗力。提出的技术在保真度方面引入了最小的降级(平均~1%到~3%)。



## **31. PASS: A Parameter Audit-based Secure and Fair Federated Learning Scheme against Free-Rider Attack**

PASS：一种基于参数审计的抗搭便车攻击安全公平联邦学习方案 cs.CR

11 pages, 13 figures, 5 tables. Accepted by IoTJ. For associated  file, see early access https://ieeexplore.ieee.org/document/10160204

**SubmitDate**: 2023-06-29    [abs](http://arxiv.org/abs/2207.07292v2) [paper-pdf](http://arxiv.org/pdf/2207.07292v2)

**Authors**: Jianhua Wang, Xiaolin Chang, Jelena Mišić, Vojislav B. Mišić, Yixiang Wang

**Abstract**: Federated Learning (FL) as a secure distributed learning framework gains interests in Internet of Things (IoT) due to its capability of protecting the privacy of participant data. However, traditional FL systems are vulnerable to Free-Rider (FR) attacks, which causes unfairness, privacy leakage and inferior performance to FL systems. The prior defense mechanisms against FR attacks assumed that malicious clients (namely, adversaries) declare less than 50% of the total amount of clients. Moreover, they aimed for Anonymous FR (AFR) attacks and lost effectiveness in resisting Selfish FR (SFR) attacks. In this paper, we propose a Parameter Audit-based Secure and fair federated learning Scheme (PASS) against FR attack. PASS has the following key features: (a) prevent from privacy leakage with less accuracy loss; (b) be effective in countering both AFR and SFR attacks; (c) work well no matter whether AFR and SFR adversaries occupy the majority of clients or not. Extensive experimental results validate that PASS: (a) has the same level as the State-Of-The-Art method in mean square error against privacy leakage; (b) defends against AFR and SFR attacks in terms of a higher defense success rate, lower false positive rate, and higher F1-score; (c) is still effective where adversaries exceed 50%, with F1-score 89% against AFR attack and F1-score 87% against SFR attack. Note that PASS produces no negative effect on FL accuracy when there is no FR adversary.

摘要: 联邦学习(FL)作为一种安全的分布式学习框架，因其能够保护参与者数据隐私而受到物联网(IoT)的关注。然而，传统的FL系统容易受到搭便车(FR)攻击，导致不公平、隐私泄露和性能下降。以往针对FR攻击的防御机制假设恶意客户端(即对手)申报的客户端数量不到客户端总数的50%。此外，它们的目标是匿名FR(AFR)攻击，在抵抗自私FR(SFR)攻击方面失去了效力。针对FR攻击，提出了一种基于参数审计的安全公平的联邦学习方案(PASS)。PASS具有以下主要特点：(A)以较少的准确性损失防止隐私泄露；(B)同时有效地对抗AFR和SFR攻击；(C)无论AFR和SFR对手是否占据大多数客户端，都能很好地工作。广泛的实验结果验证了PASS：(A)在防止隐私泄露的均方误差方面与最先进的方法具有相同的水平；(B)在防御成功率、较低的假阳性率和较高的F1得分方面防御AFR和SFR攻击；(C)在对手超过50%的情况下仍然有效，F1-针对AFR攻击的得分为89%，而F1-得分对于SFR攻击为87%。请注意，当没有FR对手时，传球不会对FL的准确性产生负面影响。



## **32. Towards Blockchain-Assisted Privacy-Aware Data Sharing For Edge Intelligence: A Smart Healthcare Perspective**

面向边缘智能的区块链辅助隐私感知数据共享：智能医疗视角 cs.CR

**SubmitDate**: 2023-06-29    [abs](http://arxiv.org/abs/2306.16630v1) [paper-pdf](http://arxiv.org/pdf/2306.16630v1)

**Authors**: Youyang Qu, Lichuan Ma, Wenjie Ye, Xuemeng Zhai, Shui Yu, Yunfeng Li, David Smith

**Abstract**: The popularization of intelligent healthcare devices and big data analytics significantly boosts the development of smart healthcare networks (SHNs). To enhance the precision of diagnosis, different participants in SHNs share health data that contains sensitive information. Therefore, the data exchange process raises privacy concerns, especially when the integration of health data from multiple sources (linkage attack) results in further leakage. Linkage attack is a type of dominant attack in the privacy domain, which can leverage various data sources for private data mining. Furthermore, adversaries launch poisoning attacks to falsify the health data, which leads to misdiagnosing or even physical damage. To protect private health data, we propose a personalized differential privacy model based on the trust levels among users. The trust is evaluated by a defined community density, while the corresponding privacy protection level is mapped to controllable randomized noise constrained by differential privacy. To avoid linkage attacks in personalized differential privacy, we designed a noise correlation decoupling mechanism using a Markov stochastic process. In addition, we build the community model on a blockchain, which can mitigate the risk of poisoning attacks during differentially private data transmission over SHNs. To testify the effectiveness and superiority of the proposed approach, we conduct extensive experiments on benchmark datasets.

摘要: 智能医疗设备和大数据分析的普及极大地推动了智能医疗网络(SHN)的发展。为了提高诊断的准确性，SHNS的不同参与者共享包含敏感信息的健康数据。因此，数据交换过程引起了隐私问题，特别是在整合来自多个来源的健康数据(联动攻击)导致进一步泄漏的情况下。链接攻击是隐私领域的一种主要攻击，它可以利用各种数据源进行隐私数据挖掘。此外，对手还会发起中毒攻击，伪造健康数据，从而导致误诊甚至身体损害。为了保护隐私健康数据，我们提出了一种基于用户之间信任级别的个性化差异隐私模型。信任度通过定义的社区密度进行评估，而相应的隐私保护级别被映射到受差异隐私约束的可控随机噪声。为了避免个性化差分隐私中的链接攻击，设计了一种基于马尔可夫随机过程的噪声相关解耦机制。此外，我们将社区模型构建在区块链上，这可以降低在SHN上传输不同私有数据期间的中毒攻击风险。为了验证该方法的有效性和优越性，我们在基准数据集上进行了大量的实验。



## **33. Does Saliency-Based Training bring Robustness for Deep Neural Networks in Image Classification?**

基于显著度的训练是否为深度神经网络在图像分类中带来稳健性？ cs.CV

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.16581v1) [paper-pdf](http://arxiv.org/pdf/2306.16581v1)

**Authors**: Ali Karkehabadi

**Abstract**: Deep Neural Networks are powerful tools to understand complex patterns and making decisions. However, their black-box nature impedes a complete understanding of their inner workings. While online saliency-guided training methods try to highlight the prominent features in the model's output to alleviate this problem, it is still ambiguous if the visually explainable features align with robustness of the model against adversarial examples. In this paper, we investigate the saliency trained model's vulnerability to adversarial examples methods. Models are trained using an online saliency-guided training method and evaluated against popular algorithms of adversarial examples. We quantify the robustness and conclude that despite the well-explained visualizations in the model's output, the salient models suffer from the lower performance against adversarial examples attacks.

摘要: 深度神经网络是理解复杂模式和做出决策的强大工具。然而，它们的黑匣子性质阻碍了对其内部工作原理的完全理解。虽然在线显著引导训练方法试图突出模型输出中的显著特征来缓解这一问题，但如果视觉上可解释的特征与模型对敌方例子的稳健性一致，则仍然是模糊的。在这篇文章中，我们研究了显著训练模型对对抗性例子方法的脆弱性。模型使用在线显著引导训练方法进行训练，并对照对抗性例子的流行算法进行评估。我们量化了健壮性，并得出结论，尽管模型输出中的可视化得到了很好的解释，但显著的模型对敌意示例攻击的性能较低。



## **34. Data-free Defense of Black Box Models Against Adversarial Attacks**

黑箱模型抵抗对手攻击的无数据防御 cs.LG

Neural Networks Journal (Under Review)

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2211.01579v2) [paper-pdf](http://arxiv.org/pdf/2211.01579v2)

**Authors**: Gaurav Kumar Nayak, Inder Khatri, Ruchit Rawal, Anirban Chakraborty

**Abstract**: Several companies often safeguard their trained deep models (i.e., details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as black boxes through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose 'wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our 'wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a 'regenerator' network with the objective of retrieving the coefficients such that the reconstructed image yields similar to original predictions on the surrogate model. At test time, WNR combined with trained regenerator network is prepended to the black box network, resulting in a high boost in adversarial accuracy. Our method improves the adversarial accuracy on CIFAR-10 by 38.98% and 32.01% on state-of-the-art Auto Attack compared to baseline, even when the attacker uses surrogate architecture (Alexnet-half and Alexnet) similar to the black box architecture (Alexnet) with same model stealing strategy as defender. The code is available at https://github.com/vcl-iisc/data-free-black-box-defense

摘要: 几家公司经常保护他们训练有素的深度模型(即架构细节、学习的重量、训练细节等)。通过API仅将第三方用户暴露为黑盒。此外，由于专有原因或敏感性问题，它们甚至可能无法提供对培训数据的访问。在这项工作中，我们提出了一种新的黑盒模型在无数据环境下抵抗敌意攻击的防御机制。我们通过产生式模型构造合成数据，并使用模型窃取技术训练代理网络。为了最大限度地减少扰动样本带来的有害污染，我们提出了小波去噪器(WNR)，它对输入图像进行离散小波分解，并仔细地选择由我们的小波系数选择模块(WCSM)确定的几个重要系数。为了恢复经过WNR去噪后的图像的高频内容，我们进一步训练了一个‘再生器’网络，目的是恢复系数，使重建的图像产生与原始预测相似的代理模型。在测试时，将WNR与训练好的再生器网络相结合，加入到黑盒网络中，大大提高了对抗的准确率。与基准相比，我们的方法在CIFAR-10上的攻击准确率分别提高了38.98%和32.01%，即使攻击者使用类似于黑盒体系结构(Alexnet)的代理体系结构(Alexnet-Half和Alexnet)，并且与防御者使用相同的模型窃取策略。代码可在https://github.com/vcl-iisc/data-free-black-box-defense上获得



## **35. Can AI-Generated Text be Reliably Detected?**

能否可靠地检测到人工智能生成的文本？ cs.CL

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2303.11156v2) [paper-pdf](http://arxiv.org/pdf/2303.11156v2)

**Authors**: Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi

**Abstract**: In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as particular writing styles, clever prompt design, or text paraphrasing. We also extend the impossibility result to include the case where pseudorandom number generators are used for AI-text generation instead of true randomness. We show that the same result holds with a negligible correction term for all polynomial-time computable detectors. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden LLM text signatures and add them to human-generated text to be detected as text generated by the LLMs, potentially causing reputational damage to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text.

摘要: 在本文中，我们从经验和理论上证明了几种人工智能文本检测器在实际场景中是不可靠的。我们的实验表明，在大型语言模型(LLM)上应用轻微的释义攻击可以破坏一系列检测器，包括使用水印方案的检测器以及基于神经网络的检测器和零镜头分类器。我们的实验表明，设计用于逃避意译攻击的基于检索的检测器仍然容易受到递归意译攻击。然后，我们提供了一个理论上不可能的结果，表明随着语言模型变得更加复杂和更好地模拟人类文本，即使是最好的检测器的性能也会下降。对于试图模仿人类文本的足够高级的语言模型来说，即使是最好的检测器也可能只比随机分类器的性能略好一些。我们的结果足够通用，可以捕捉特定的场景，例如特定的写作风格、巧妙的提示设计或文本释义。我们还将不可能的结果扩展到包括伪随机数生成器用于人工智能文本生成而不是真随机性的情况。我们证明，对于所有多项式时间可计算的检测器，相同的结果是成立的，但校正项可以忽略不计。最后，我们证明了即使是受水印方案保护的LLM也容易受到欺骗攻击，在这种攻击中，敌意的人类可以推断隐藏的LLM文本签名，并将它们添加到人类生成的文本中，以检测为LLMS生成的文本，这可能会对其开发者造成声誉损害。我们相信，这些结果可以在社区中就人工智能生成的文本的道德和可靠使用展开诚实的对话。



## **36. Enhancing Adversarial Training via Reweighting Optimization Trajectory**

通过重新加权优化轨迹加强对抗性训练 cs.LG

Accepted by ECML 2023

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.14275v2) [paper-pdf](http://arxiv.org/pdf/2306.14275v2)

**Authors**: Tianjin Huang, Shiwei Liu, Tianlong Chen, Meng Fang, Li Shen, Vlaod Menkovski, Lu Yin, Yulong Pei, Mykola Pechenizkiy

**Abstract**: Despite the fact that adversarial training has become the de facto method for improving the robustness of deep neural networks, it is well-known that vanilla adversarial training suffers from daunting robust overfitting, resulting in unsatisfactory robust generalization. A number of approaches have been proposed to address these drawbacks such as extra regularization, adversarial weights perturbation, and training with more data over the last few years. However, the robust generalization improvement is yet far from satisfactory. In this paper, we approach this challenge with a brand new perspective -- refining historical optimization trajectories. We propose a new method named \textbf{Weighted Optimization Trajectories (WOT)} that leverages the optimization trajectories of adversarial training in time. We have conducted extensive experiments to demonstrate the effectiveness of WOT under various state-of-the-art adversarial attacks. Our results show that WOT integrates seamlessly with the existing adversarial training methods and consistently overcomes the robust overfitting issue, resulting in better adversarial robustness. For example, WOT boosts the robust accuracy of AT-PGD under AA-$L_{\infty}$ attack by 1.53\% $\sim$ 6.11\% and meanwhile increases the clean accuracy by 0.55\%$\sim$5.47\% across SVHN, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.

摘要: 尽管对抗性训练已经成为提高深度神经网络鲁棒性的事实上的方法，但众所周知，对抗性训练存在令人望而生畏的健壮性过拟合问题，导致不能令人满意的健壮泛化。在过去的几年里，已经提出了一些方法来解决这些缺点，例如额外的正则化、对抗性权重扰动和使用更多数据进行训练。然而，健壮的泛化改进还远远不能令人满意。在本文中，我们以一种全新的视角来应对这一挑战--提炼历史优化轨迹。我们提出了一种新的方法我们已经进行了大量的实验，以证明WOT在各种最先进的对抗性攻击下的有效性。实验结果表明，WOT算法与现有的对抗性训练方法无缝结合，始终克服了健壮性超调的问题，具有更好的对抗性。例如，WOT将AT-PGD在AA-L攻击下的稳健准确率提高了1.53$\sim$6.11\%，同时在SVHN、CIFAR-10、CIFAR-100和微型ImageNet数据集中将CLEAN准确率提高了0.55$\sim$5.47\%。



## **37. Temporal Robustness against Data Poisoning**

抗数据中毒的时间稳健性 cs.LG

13 pages, 7 figures

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2302.03684v2) [paper-pdf](http://arxiv.org/pdf/2302.03684v2)

**Authors**: Wenxiao Wang, Soheil Feizi

**Abstract**: Data poisoning considers cases when an adversary manipulates the behavior of machine learning algorithms through malicious training data. Existing threat models of data poisoning center around a single metric, the number of poisoned samples. In consequence, if attackers can poison more samples than expected with affordable overhead, as in many practical scenarios, they may be able to render existing defenses ineffective in a short time. To address this issue, we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past. Benefiting from these timestamps, we propose a temporal threat model of data poisoning with two novel metrics, earliness and duration, which respectively measure how long an attack started in advance and how long an attack lasted. Using these metrics, we define the notions of temporal robustness against data poisoning, providing a meaningful sense of protection even with unbounded amounts of poisoned samples. We present a benchmark with an evaluation protocol simulating continuous data collection and periodic deployments of updated models, thus enabling empirical evaluation of temporal robustness. Lastly, we develop and also empirically verify a baseline defense, namely temporal aggregation, offering provable temporal robustness and highlighting the potential of our temporal threat model for data poisoning.

摘要: 数据中毒考虑对手通过恶意训练数据操纵机器学习算法的行为的情况。现有的数据中毒威胁模型主要以中毒样本数量这一单一指标为中心。因此，如果攻击者可以用负担得起的开销毒化比预期更多的样本，就像在许多实际场景中一样，他们可能能够在短时间内使现有防御系统失效。为了解决这个问题，我们利用表示数据出生日期的时间戳，这些数据通常是可用的，但在过去被忽视了。得益于这些时间戳，我们提出了一种数据中毒的时态威胁模型，该模型采用了两个新的度量标准：提前期和持续时间，分别衡量攻击提前开始的时间和攻击持续的时间。使用这些度量，我们定义了针对数据中毒的时间稳健性概念，即使在无限数量的有毒样本的情况下也提供了有意义的保护感。我们提出了一个基准测试和评估协议，模拟了连续的数据收集和定期部署更新的模型，从而能够对时间稳健性进行经验评估。最后，我们开发了一个基线防御，也就是时间聚合，提供了可证明的时间稳健性，并突出了我们的时间威胁模型对数据中毒的潜力。



## **38. On the Exploitability of Instruction Tuning**

论指令调优的可开发性 cs.CR

19 pages, 9 figures

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.17194v1) [paper-pdf](http://arxiv.org/pdf/2306.17194v1)

**Authors**: Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom Goldstein

**Abstract**: Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs. Code is available at \url{https://github.com/azshue/AutoPoison}.

摘要: 指令调优是使大型语言模型与人的意图保持一致的一种有效技术。在这项工作中，我们调查了对手如何通过向训练数据中注入特定的指令遵循示例来利用指令调整，从而故意改变模型的行为。例如，对手可以通过注入提到目标内容的训练示例并从下游模型中引出此类行为来实现内容注入。为了实现这一目标，我们提出了一种自动化的数据中毒管道--Texttit{AutoPoison}。在甲骨文LLM的帮助下，它自然而连贯地将各种攻击目标整合到有毒数据中。我们展示了两个示例攻击：内容注入攻击和过度拒绝攻击，每个攻击都旨在诱导特定的可利用行为。我们对我们的数据中毒方案的强度和隐蔽性进行量化和基准测试。我们的结果表明，AutoPoison允许对手通过只毒化一小部分数据来改变模型的行为，同时在中毒的示例中保持高级别的隐蔽性。我们希望我们的工作有助于阐明数据质量如何影响指令调优模型的行为，并提高人们对数据质量对于负责任的LLM部署的重要性的认识。代码位于\url{https://github.com/azshue/AutoPoison}.



## **39. Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation**

通过多教师对抗性蒸馏缓解精度与稳健性的权衡 cs.LG

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.16170v1) [paper-pdf](http://arxiv.org/pdf/2306.16170v1)

**Authors**: Shiji Zhao, Xizhe Wang, Xingxing Wei

**Abstract**: Adversarial training is a practical approach for improving the robustness of deep neural networks against adversarial attacks. Although bringing reliable robustness, the performance toward clean examples is negatively affected after adversarial training, which means a trade-off exists between accuracy and robustness. Recently, some studies have tried to use knowledge distillation methods in adversarial training, achieving competitive performance in improving the robustness but the accuracy for clean samples is still limited. In this paper, to mitigate the accuracy-robustness trade-off, we introduce the Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's adversarial training process by applying a strong clean teacher and a strong robust teacher to handle the clean examples and adversarial examples, respectively. During the optimization process, to ensure that different teachers show similar knowledge scales, we design the Entropy-Based Balance algorithm to adjust the teacher's temperature and keep the teachers' information entropy consistent. Besides, to ensure that the student has a relatively consistent learning speed from multiple teachers, we propose the Normalization Loss Balance algorithm to adjust the learning weights of different types of knowledge. A series of experiments conducted on public datasets demonstrate that MTARD outperforms the state-of-the-art adversarial training and distillation methods against various adversarial attacks.

摘要: 对抗性训练是提高深层神经网络抗敌意攻击能力的一种实用方法。虽然带来了可靠的稳健性，但经过对抗性训练后，对干净样本的性能会受到负面影响，这意味着在准确性和稳健性之间存在权衡。近年来，一些研究尝试将知识提取方法应用于对抗性训练，在提高鲁棒性方面取得了较好的性能，但对清洁样本的准确率仍然有限。为了缓解准确性和稳健性之间的权衡，我们引入了多教师对抗稳健性蒸馏(MTARD)来指导模型的对抗训练过程，分别采用强清洁教师和强稳健教师来处理干净实例和对抗性实例。在优化过程中，为了保证不同教师表现出相似的知识尺度，设计了基于熵的均衡算法来调整教师的温度，保持教师信息熵的一致性。此外，为了确保学生从多个老师那里获得相对一致的学习速度，我们提出了归一化损失平衡算法来调整不同类型知识的学习权重。在公开数据集上进行的一系列实验表明，MTARD在对抗各种对抗性攻击方面优于最先进的对抗性训练和蒸馏方法。



## **40. Distributional Modeling for Location-Aware Adversarial Patches**

位置感知敌方补丁的分布式建模 cs.CV

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.16131v1) [paper-pdf](http://arxiv.org/pdf/2306.16131v1)

**Authors**: Xingxing Wei, Shouwei Ruan, Yinpeng Dong, Hang Su

**Abstract**: Adversarial patch is one of the important forms of performing adversarial attacks in the physical world. To improve the naturalness and aggressiveness of existing adversarial patches, location-aware patches are proposed, where the patch's location on the target object is integrated into the optimization process to perform attacks. Although it is effective, efficiently finding the optimal location for placing the patches is challenging, especially under the black-box attack settings. In this paper, we propose the Distribution-Optimized Adversarial Patch (DOPatch), a novel method that optimizes a multimodal distribution of adversarial locations instead of individual ones. DOPatch has several benefits: Firstly, we find that the locations' distributions across different models are pretty similar, and thus we can achieve efficient query-based attacks to unseen models using a distributional prior optimized on a surrogate model. Secondly, DOPatch can generate diverse adversarial samples by characterizing the distribution of adversarial locations. Thus we can improve the model's robustness to location-aware patches via carefully designed Distributional-Modeling Adversarial Training (DOP-DMAT). We evaluate DOPatch on various face recognition and image recognition tasks and demonstrate its superiority and efficiency over existing methods. We also conduct extensive ablation studies and analyses to validate the effectiveness of our method and provide insights into the distribution of adversarial locations.

摘要: 对抗性补丁是物理世界中实施对抗性攻击的重要形式之一。为了提高现有敌方补丁的自然度和攻击性，提出了位置感知补丁，将补丁在目标对象上的位置整合到优化过程中进行攻击。虽然它是有效的，但高效地找到放置补丁的最佳位置是具有挑战性的，特别是在黑盒攻击环境下。在本文中，我们提出了分布优化的敌方补丁(DOPatch)，这是一种优化敌方位置的多模式分布而不是单个位置的新方法。DOPatch有几个优点：首先，我们发现位置在不同模型上的分布非常相似，因此我们可以利用在代理模型上优化的分布先验来实现对不可见模型的高效基于查询的攻击。其次，DOPatch可以通过刻画敌方位置的分布来生成不同的敌方样本。因此，我们可以通过精心设计的分布式建模对抗训练(DOP-DMAT)来提高模型对位置感知补丁的健壮性。我们对DOPatch在各种人脸识别和图像识别任务上的性能进行了评估，并证明了它比现有方法的优越性和有效性。我们还进行了广泛的消融研究和分析，以验证我们方法的有效性，并为敌方位置的分布提供见解。



## **41. Evaluating Similitude and Robustness of Deep Image Denoising Models via Adversarial Attack**

对抗性攻击下深度图像去噪模型的相似性和稳健性评价 cs.CV

12 pages, 15 figures

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.16050v1) [paper-pdf](http://arxiv.org/pdf/2306.16050v1)

**Authors**: Jie Ning, Yao Li, Zhichang Guo

**Abstract**: Deep neural networks (DNNs) have a wide range of applications in the field of image denoising, and they are superior to traditional image denoising. However, DNNs inevitably show vulnerability, which is the weak robustness in the face of adversarial attacks. In this paper, we find some similitudes between existing deep image denoising methods, as they are consistently fooled by adversarial attacks. First, denoising-PGD is proposed which is a denoising model full adversarial method. The current mainstream non-blind denoising models (DnCNN, FFDNet, ECNDNet, BRDNet), blind denoising models (DnCNN-B, Noise2Noise, RDDCNN-B, FAN), and plug-and-play (DPIR, CurvPnP) and unfolding denoising models (DeamNet) applied to grayscale and color images can be attacked by the same set of methods. Second, since the transferability of denoising-PGD is prominent in the image denoising task, we design experiments to explore the characteristic of the latent under the transferability. We correlate transferability with similitude and conclude that the deep image denoising models have high similitude. Third, we investigate the characteristic of the adversarial space and use adversarial training to complement the vulnerability of deep image denoising to adversarial attacks on image denoising. Finally, we constrain this adversarial attack method and propose the L2-denoising-PGD image denoising adversarial attack method that maintains the Gaussian distribution. Moreover, the model-driven image denoising BM3D shows some resistance in the face of adversarial attacks.

摘要: 深度神经网络(DNN)在图像去噪领域有着广泛的应用，并且优于传统的图像去噪。然而，DNN不可避免地表现出脆弱性，这就是面对对手攻击时的弱健壮性。在本文中，我们发现了现有的深度图像去噪方法之间的一些相似之处，因为它们总是被对手攻击所愚弄。首先，提出了一种完全对抗去噪模型的去噪方法--PGD。目前主流的非盲去噪模型(DnCNN、FFDNet、ECNDNet、BRDNet)、盲去噪模型(DnCNN-B、Noise2Noise、RDDCNN-B、FAN)、即插即用模型(DPIR、CurvPnP)和展开去噪模型(DeamNet)适用于灰度和彩色图像，可以用同一套方法攻击。其次，针对去噪-PGD在图像去噪任务中的可转移性突出的特点，设计了实验来探索可转移性下的潜伏期特征。将可转移性与相似性联系起来，得出深层图像去噪模型具有较高的相似性的结论。第三，研究了对抗性空间的特点，并利用对抗性训练来弥补深层图像去噪对图像去噪的对抗性攻击的脆弱性。最后，对这种对抗攻击方法进行了约束，提出了保持高斯分布的L2-去噪-PGD图像去噪对抗攻击方法。此外，模型驱动的图像去噪算法BM3D在面对敌方攻击时表现出一定的抵抗力。



## **42. Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound**

利用对抗性超声对说话人识别系统进行注册阶段的后门攻击 cs.SD

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.16022v1) [paper-pdf](http://arxiv.org/pdf/2306.16022v1)

**Authors**: Xinfeng Li, Junning Ze, Chen Yan, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu

**Abstract**: Automatic Speaker Recognition Systems (SRSs) have been widely used in voice applications for personal identification and access control. A typical SRS consists of three stages, i.e., training, enrollment, and recognition. Previous work has revealed that SRSs can be bypassed by backdoor attacks at the training stage or by adversarial example attacks at the recognition stage. In this paper, we propose TUNER, a new type of backdoor attack against the enrollment stage of SRS via adversarial ultrasound modulation, which is inaudible, synchronization-free, content-independent, and black-box. Our key idea is to first inject the backdoor into the SRS with modulated ultrasound when a legitimate user initiates the enrollment, and afterward, the polluted SRS will grant access to both the legitimate user and the adversary with high confidence. Our attack faces a major challenge of unpredictable user articulation at the enrollment stage. To overcome this challenge, we generate the ultrasonic backdoor by augmenting the optimization process with random speech content, vocalizing time, and volume of the user. Furthermore, to achieve real-world robustness, we improve the ultrasonic signal over traditional methods using sparse frequency points, pre-compensation, and single-sideband (SSB) modulation. We extensively evaluate TUNER on two common datasets and seven representative SRS models. Results show that our attack can successfully bypass speaker recognition systems while remaining robust to various speakers, speech content, et

摘要: 自动说话人识别系统(SRSS)已被广泛应用于个人身份识别和访问控制的语音应用中。一个典型的SRS包括三个阶段，即培训、注册和认可。先前的工作表明，在训练阶段通过后门攻击或在识别阶段通过对抗性示例攻击可以绕过SRSS。本文提出了一种基于对抗性超声调制的针对SRS注册阶段的新型后门攻击--Tuner，它具有不可听、无同步、内容无关、黑盒等特点。我们的核心思想是，当合法用户发起注册时，首先使用调制超声波将后门注入SRS，然后，受污染的SRS将以高置信度向合法用户和对手授予访问权限。我们的攻击在注册阶段面临着不可预测的用户表达的重大挑战。为了克服这一挑战，我们通过增加随机语音内容、发声时间和用户音量的优化过程来生成超声波后门。此外，为了实现真实世界的稳健性，我们使用稀疏频点、预补偿和单边带(SSB)调制对超声信号进行了改进。我们在两个常见的数据集和七个有代表性的SRS模型上对Tuner进行了广泛的评估。结果表明，我们的攻击可以成功地绕过说话人识别系统，同时对不同的说话人、语音内容等保持鲁棒性



## **43. What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?**

状态对抗性多智能体强化学习的解决方案是什么？ cs.AI

Workshop on New Frontiers in Learning, Control, and Dynamical Systems  at the International Conference on Machine Learning (ICML), Honolulu, Hawaii,  USA, 2023

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2212.02705v4) [paper-pdf](http://arxiv.org/pdf/2212.02705v4)

**Authors**: Songyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, Fei Miao

**Abstract**: Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate the fundamental properties of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertainties. Our experiments demonstrate that our algorithm outperforms existing methods when faced with state perturbations and greatly improves the robustness of MARL policies. Our code is public on https://songyanghan.github.io/what_is_solution/.

摘要: 多智能体强化学习(MAIL)的各种方法都是在假设智能体的策略基于准确的状态信息的基础上提出的。然而，通过深度强化学习(DRL)学习的策略容易受到对抗性状态扰动攻击。在这项工作中，我们提出了一种状态-对手马尔可夫博弈(SAMG)，并首次尝试研究了状态不确定条件下Marl的基本性质。我们的分析表明，最优代理策略和稳健纳什均衡等解的概念在SAMG中并不总是存在的。为了规避这一困难，我们考虑了一个新的解决方案概念，称为稳健代理策略，其中代理的目标是最大化最坏情况下的预期状态值。我们证明了有限状态和有限动作SAMG的鲁棒代理策略的存在性。此外，我们还提出了一种健壮的多智能体对抗行为者-批评者(RMA3C)算法来学习状态不确定条件下MAIL智能体的健壮策略。实验表明，该算法在面对状态扰动时的性能优于已有方法，并大大提高了MAIL策略的稳健性。我们的代码在https://songyanghan.github.io/what_is_solution/.上是公开的



## **44. Boosting Adversarial Transferability with Learnable Patch-wise Masks**

用可学习的补丁口罩提高对手的可转移性 cs.CV

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.15931v1) [paper-pdf](http://arxiv.org/pdf/2306.15931v1)

**Authors**: Xingxing Wei, Shiji Zhao

**Abstract**: Adversarial examples have raised widespread attention in security-critical applications because of their transferability across different models. Although many methods have been proposed to boost adversarial transferability, a gap still exists in the practical demand. In this paper, we argue that the model-specific discriminative regions are a key factor to cause the over-fitting to the source model, and thus reduce the transferability to the target model. For that, a patch-wise mask is utilized to prune the model-specific regions when calculating adversarial perturbations. To accurately localize these regions, we present a learnable approach to optimize the mask automatically. Specifically, we simulate the target models in our framework, and adjust the patch-wise mask according to the feedback of simulated models. To improve the efficiency, Differential Evolutionary (DE) algorithm is utilized to search for patch-wise masks for a specific image. During iterative attacks, the learned masks are applied to the image to drop out the patches related to model-specific regions, thus making the gradients more generic and improving the adversarial transferability. The proposed approach is a pre-processing method and can be integrated with existing gradient-based methods to further boost the transfer attack success rate. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our method. We incorporate the proposed approach with existing methods in the ensemble attacks and achieve an average success rate of 93.01% against seven advanced defense methods, which can effectively enhance the state-of-the-art transfer-based attack performance.

摘要: 对抗性的例子在安全关键应用中引起了广泛的关注，因为它们可以在不同的模型之间转移。虽然已经提出了许多方法来提高对抗性可转移性，但在实际需求中仍存在差距。在本文中，我们认为，特定于模型的区分区域是导致对源模型过度拟合从而降低到目标模型的可转换性的关键因素。为此，在计算对抗性扰动时，使用补丁掩码来修剪特定于模型的区域。为了准确地定位这些区域，我们提出了一种可学习的方法来自动优化掩码。具体地说，我们在我们的框架中模拟目标模型，并根据模拟模型的反馈调整面片掩码。为了提高算法的效率，采用了差分进化算法来搜索特定图像的面片模板。在迭代攻击过程中，将学习到的模板应用于图像，去除与模型特定区域相关的补丁，从而使梯度更具通用性，提高了对抗可转移性。该方法是一种预处理方法，可以与现有的基于梯度的方法相结合，进一步提高传输攻击的成功率。在ImageNet数据集上的大量实验证明了该方法的有效性。将提出的方法与现有的集成攻击方法相结合，对7种先进的防御方法取得了93.01%的平均成功率，有效地提高了基于传输的攻击性能。



## **45. A Diamond Model Analysis on Twitter's Biggest Hack**

Twitter最大黑客攻击的钻石模型分析 cs.CR

8 pages, 3 figures, 2 tables

**SubmitDate**: 2023-06-28    [abs](http://arxiv.org/abs/2306.15878v1) [paper-pdf](http://arxiv.org/pdf/2306.15878v1)

**Authors**: Chaitanya Rahalkar

**Abstract**: Cyberattacks have prominently increased over the past few years now, and have targeted actors from a wide variety of domains. Understanding the motivation, infrastructure, attack vectors, etc. behind such attacks is vital to proactively work against preventing such attacks in the future and also to analyze the economic and social impact of such attacks. In this paper, we leverage the diamond model to perform an intrusion analysis case study of the 2020 Twitter account hijacking Cyberattack. We follow this standardized incident response model to map the adversary, capability, infrastructure, and victim and perform a comprehensive analysis of the attack, and the impact posed by the attack from a Cybersecurity policy standpoint.

摘要: 网络攻击在过去几年里显著增加，目标是来自不同领域的参与者。了解此类攻击背后的动机、基础设施、攻击媒介等对于主动预防未来此类攻击以及分析此类攻击的经济和社会影响至关重要。在本文中，我们利用钻石模型对2020年Twitter账户劫持网络攻击进行了入侵分析案例研究。我们遵循这个标准化的事件响应模型来映射对手、能力、基础设施和受害者，并从网络安全策略的角度对攻击和攻击造成的影响进行全面分析。



## **46. Condorcet Attack Against Fair Transaction Ordering**

针对公平交易排序的Condorcet攻击 cs.CR

**SubmitDate**: 2023-06-27    [abs](http://arxiv.org/abs/2306.15743v1) [paper-pdf](http://arxiv.org/pdf/2306.15743v1)

**Authors**: Mohammad Amin Vafadar, Majid Khabbazian

**Abstract**: We introduce the Condorcet attack, a new threat to fair transaction ordering. Specifically, the attack undermines batch-order-fairness, the strongest notion of transaction fair ordering proposed to date. The batch-order-fairness guarantees that a transaction tx is ordered before tx' if a majority of nodes in the system receive tx before tx'; the only exception (due to an impossibility result) is when tx and tx' fall into a so-called "Condorcet cycle". When this happens, tx and tx' along with other transactions within the cycle are placed in a batch, and any unfairness inside a batch is ignored. In the Condorcet attack, an adversary attempts to undermine the system's fairness by imposing Condorcet cycles to the system. In this work, we show that the adversary can indeed impose a Condorcet cycle by submitting as few as two otherwise legitimate transactions to the system. Remarkably, the adversary (e.g., a malicious client) can achieve this even when all the nodes in the system behave honestly. A notable feature of the attack is that it is capable of "trapping" transactions that do not naturally fall inside a cycle, i.e. those that are transmitted at significantly different times (with respect to the network latency). To mitigate the attack, we propose three methods based on three different complementary approaches. We show the effectiveness of the proposed mitigation methods through simulations, and explain their limitations.

摘要: 我们引入了Condorcet攻击，这是对公平交易秩序的一种新威胁。具体地说，这次攻击破坏了批次排序公平，这是迄今为止提出的最强的交易公平排序概念。如果系统中的大多数节点在Tx‘之前接收到Tx，则批排序公平性保证了事务Tx在Tx’之前被排序；唯一的例外(由于不可能的结果)是当Tx和Tx‘落入所谓的“Condorcet循环”时。当这种情况发生时，Tx和Tx‘连同周期内的其他交易被放置在批中，并且批内的任何不公平都被忽略。在Condorcet攻击中，攻击者试图通过将Condorcet循环强加给系统来破坏系统的公平性。在这项工作中，我们证明了对手确实可以通过向系统提交少至两个原本合法的交易来施加Condorcet循环。值得注意的是，即使当系统中的所有节点都诚实地运行时，敌手(例如，恶意客户端)也可以实现这一点。该攻击的一个显著特征是，它能够“捕获”并非自然落入周期内的事务，即在显著不同的时间传输的事务(就网络延迟而言)。为了缓解攻击，我们基于三种不同的互补方法提出了三种方法。我们通过仿真验证了所提出的抑制方法的有效性，并解释了它们的局限性。



## **47. Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets**

合作还是竞争：通过自适应预算避免玩家主导多目标稳健性 cs.AI

**SubmitDate**: 2023-06-27    [abs](http://arxiv.org/abs/2306.15482v1) [paper-pdf](http://arxiv.org/pdf/2306.15482v1)

**Authors**: Yimu Wang, Dinghuai Zhang, Yihan Wu, Heng Huang, Hongyang Zhang

**Abstract**: Despite incredible advances, deep learning has been shown to be susceptible to adversarial attacks. Numerous approaches have been proposed to train robust networks both empirically and certifiably. However, most of them defend against only a single type of attack, while recent work takes steps forward in defending against multiple attacks. In this paper, to understand multi-target robustness, we view this problem as a bargaining game in which different players (adversaries) negotiate to reach an agreement on a joint direction of parameter updating. We identify a phenomenon named player domination in the bargaining game, namely that the existing max-based approaches, such as MAX and MSD, do not converge. Based on our theoretical analysis, we design a novel framework that adjusts the budgets of different adversaries to avoid any player dominance. Experiments on standard benchmarks show that employing the proposed framework to the existing approaches significantly advances multi-target robustness.

摘要: 尽管取得了令人难以置信的进步，但深度学习已被证明容易受到对手的攻击。已经提出了许多方法来训练稳健的网络，既有经验的，也有可证明的。然而，它们中的大多数只防御一种类型的攻击，而最近的工作在防御多种攻击方面取得了进展。在本文中，为了理解多目标的稳健性，我们将这一问题视为一个讨价还价博弈，不同的参与者(对手)通过协商就参数更新的共同方向达成协议。我们在讨价还价博弈中发现了一种称为玩家支配的现象，即现有的基于最大值的方法，如MAX和MSD，不收敛。在理论分析的基础上，我们设计了一个新的框架来调整不同对手的预算，以避免任何玩家的支配地位。在标准基准上的实验表明，在现有方法的基础上使用该框架可以显著提高多目标的稳健性。



## **48. Robust Proxy: Improving Adversarial Robustness by Robust Proxy Learning**

稳健代理：通过稳健代理学习提高对手的稳健性 cs.CV

Accepted at IEEE Transactions on Information Forensics and Security  (TIFS)

**SubmitDate**: 2023-06-27    [abs](http://arxiv.org/abs/2306.15457v1) [paper-pdf](http://arxiv.org/pdf/2306.15457v1)

**Authors**: Hong Joo Lee, Yong Man Ro

**Abstract**: Recently, it has been widely known that deep neural networks are highly vulnerable and easily broken by adversarial attacks. To mitigate the adversarial vulnerability, many defense algorithms have been proposed. Recently, to improve adversarial robustness, many works try to enhance feature representation by imposing more direct supervision on the discriminative feature. However, existing approaches lack an understanding of learning adversarially robust feature representation. In this paper, we propose a novel training framework called Robust Proxy Learning. In the proposed method, the model explicitly learns robust feature representations with robust proxies. To this end, firstly, we demonstrate that we can generate class-representative robust features by adding class-wise robust perturbations. Then, we use the class representative features as robust proxies. With the class-wise robust features, the model explicitly learns adversarially robust features through the proposed robust proxy learning framework. Through extensive experiments, we verify that we can manually generate robust features, and our proposed learning framework could increase the robustness of the DNNs.

摘要: 近年来，众所周知，深度神经网络具有很高的脆弱性，很容易被敌方攻击攻破。为了缓解恶意攻击的脆弱性，人们提出了许多防御算法。近年来，为了提高对抗的稳健性，许多工作试图通过对区分特征施加更直接的监督来增强特征表示。然而，现有的方法缺乏对学习对抗性稳健特征表示的理解。在本文中，我们提出了一种新的训练框架，称为稳健代理学习。在所提出的方法中，模型通过稳健的代理显式地学习稳健的特征表示。为此，首先，我们证明了我们可以通过添加类稳健扰动来生成具有类代表性的鲁棒特征。然后，我们使用类代表特征作为健壮性代理。该模型利用分类稳健特征，通过所提出的稳健代理学习框架显式地学习对抗性稳健特征。通过大量的实验，我们验证了我们可以手动生成健壮的特征，并且我们提出的学习框架可以提高DNN的健壮性。



## **49. Advancing Adversarial Training by Injecting Booster Signal**

注入助推器信号推进对抗性训练 cs.CV

Accepted at IEEE Transactions on Neural Networks and Learning Systems

**SubmitDate**: 2023-06-27    [abs](http://arxiv.org/abs/2306.15451v1) [paper-pdf](http://arxiv.org/pdf/2306.15451v1)

**Authors**: Hong Joo Lee, Youngjoon Yu, Yong Man Ro

**Abstract**: Recent works have demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarial attacks. To defend against adversarial attacks, many defense strategies have been proposed, among which adversarial training has been demonstrated to be the most effective strategy. However, it has been known that adversarial training sometimes hurts natural accuracy. Then, many works focus on optimizing model parameters to handle the problem. Different from the previous approaches, in this paper, we propose a new approach to improve the adversarial robustness by using an external signal rather than model parameters. In the proposed method, a well-optimized universal external signal called a booster signal is injected into the outside of the image which does not overlap with the original content. Then, it boosts both adversarial robustness and natural accuracy. The booster signal is optimized in parallel to model parameters step by step collaboratively. Experimental results show that the booster signal can improve both the natural and robust accuracies over the recent state-of-the-art adversarial training methods. Also, optimizing the booster signal is general and flexible enough to be adopted on any existing adversarial training methods.

摘要: 最近的研究表明，深度神经网络(DNN)非常容易受到敌意攻击。为了防御对抗性攻击，人们提出了许多防御策略，其中对抗性训练被证明是最有效的策略。然而，众所周知，对抗性训练有时会损害自然的准确性。于是，很多工作都集中在优化模型参数来处理这一问题上。与以往的方法不同，本文提出了一种利用外部信号而不是模型参数来提高对抗稳健性的新方法。在该方法中，一种经过优化的通用外部信号被注入到图像的外部，该信号与原始内容不重叠。然后，它既增强了对手的健壮性，又提高了自然的准确性。协同地对升压信号进行并行优化以逐步建立模型参数。实验结果表明，与目前最先进的对抗性训练方法相比，增强信号可以提高自然准确率和稳健准确率。此外，优化助推信号是通用的和灵活的，可以在任何现有的对抗性训练方法中采用。



## **50. Adversarial Training for Graph Neural Networks**

图神经网络的对抗性训练 cs.LG

**SubmitDate**: 2023-06-27    [abs](http://arxiv.org/abs/2306.15427v1) [paper-pdf](http://arxiv.org/pdf/2306.15427v1)

**Authors**: Lukas Gosch, Simon Geisler, Daniel Sturm, Bertrand Charpentier, Daniel Zügner, Stephan Günnemann

**Abstract**: Despite its success in the image domain, adversarial training does not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.

摘要: 尽管它在图像领域取得了成功，但对抗性训练并不是图神经网络(GNN)对抗图结构扰动的有效防御。在固定对抗性训练的过程中，(1)我们展示并克服了以前工作中采用的图学习设置的基本理论和实践限制；(2)我们揭示了基于可学习图扩散的更灵活的GNN能够适应对抗性扰动，而学习的消息传递方案自然是可解释的；(3)我们引入了针对结构扰动的第一次攻击，虽然一次针对多个节点，但能够处理全局(图级)和局部(节点级)约束。包括这些贡献，我们证明了对抗性训练是对对抗性结构扰动的一种最先进的防御。



