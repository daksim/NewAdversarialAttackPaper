# Latest Adversarial Attack Papers
**update at 2024-04-28 10:30:03**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. A Self-Organizing Clustering System for Unsupervised Distribution Shift Detection**

用于无监督分布漂移检测的自组织集群系统 cs.LG

Accepted manuscript in the IEEE International Joint Conference of  Neural Networks (IJCNN), 2024

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2404.16656v1) [paper-pdf](http://arxiv.org/pdf/2404.16656v1)

**Authors**: Sebastián Basterrech, Line Clemmensen, Gerardo Rubino

**Abstract**: Modeling non-stationary data is a challenging problem in the field of continual learning, and data distribution shifts may result in negative consequences on the performance of a machine learning model. Classic learning tools are often vulnerable to perturbations of the input covariates, and are sensitive to outliers and noise, and some tools are based on rigid algebraic assumptions. Distribution shifts are frequently occurring due to changes in raw materials for production, seasonality, a different user base, or even adversarial attacks. Therefore, there is a need for more effective distribution shift detection techniques.   In this work, we propose a continual learning framework for monitoring and detecting distribution changes. We explore the problem in a latent space generated by a bio-inspired self-organizing clustering and statistical aspects of the latent space. In particular, we investigate the projections made by two topology-preserving maps: the Self-Organizing Map and the Scale Invariant Map. Our method can be applied in both a supervised and an unsupervised context. We construct the assessment of changes in the data distribution as a comparison of Gaussian signals, making the proposed method fast and robust. We compare it to other unsupervised techniques, specifically Principal Component Analysis (PCA) and Kernel-PCA. Our comparison involves conducting experiments using sequences of images (based on MNIST and injected shifts with adversarial samples), chemical sensor measurements, and the environmental variable related to ozone levels. The empirical study reveals the potential of the proposed approach.

摘要: 非平稳数据建模是持续学习领域中的一个具有挑战性的问题，而数据分布的变化可能会对机器学习模型的性能造成负面影响。传统的学习工具往往容易受到输入协变量的扰动，对异常值和噪声敏感，而且一些工具是基于严格的代数假设的。由于生产原材料的变化、季节性、不同的用户群，甚至是对抗性的攻击，分销转变经常发生。因此，需要更有效的分布移位检测技术。在这项工作中，我们提出了一个持续学习框架，用于监测和检测分布变化。我们在由生物启发的自组织聚类和统计方面产生的潜在空间中探索这一问题。特别地，我们研究了两种保持拓扑的映射：自组织映射和比例不变映射。我们的方法可以应用于有监督和无监督的环境中。我们构造了对数据分布变化的评估作为对高斯信号的比较，使得所提出的方法快速且稳健。我们将其与其他非监督技术，特别是主成分分析(PCA)和核主成分分析(Kernel-PCA)进行了比较。我们的比较包括使用图像序列(基于MNIST和带有对抗性样本的注入位移)、化学传感器测量以及与臭氧水平相关的环境变量进行实验。实证研究揭示了该方法的潜力。



## **2. Frosty: Bringing strong liveness guarantees to the Snow family of consensus protocols**

Frosty：为Snow家族的共识协议带来强大的活力保证 cs.DC

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2404.14250v2) [paper-pdf](http://arxiv.org/pdf/2404.14250v2)

**Authors**: Aaron Buchwald, Stephen Buttolph, Andrew Lewis-Pye, Patrick O'Grady, Kevin Sekniqi

**Abstract**: Snowman is the consensus protocol implemented by the Avalanche blockchain and is part of the Snow family of protocols, first introduced through the original Avalanche leaderless consensus protocol. A major advantage of Snowman is that each consensus decision only requires an expected constant communication overhead per processor in the `common' case that the protocol is not under substantial Byzantine attack, i.e. it provides a solution to the scalability problem which ensures that the expected communication overhead per processor is independent of the total number of processors $n$ during normal operation. This is the key property that would enable a consensus protocol to scale to 10,000 or more independent validators (i.e. processors). On the other hand, the two following concerns have remained:   (1) Providing formal proofs of consistency for Snowman has presented a formidable challenge.   (2) Liveness attacks exist in the case that a Byzantine adversary controls more than $O(\sqrt{n})$ processors, slowing termination to more than a logarithmic number of steps.   In this paper, we address the two issues above. We consider a Byzantine adversary that controls at most $f<n/5$ processors. First, we provide a simple proof of consistency for Snowman. Then we supplement Snowman with a `liveness module' that can be triggered in the case that a substantial adversary launches a liveness attack, and which guarantees liveness in this event by temporarily forgoing the communication complexity advantages of Snowman, but without sacrificing these low communication complexity advantages during normal operation.

摘要: 雪人是雪崩区块链实施的共识协议，是雪诺协议家族的一部分，最初是通过最初的雪崩无领导共识协议引入的。Snowman的一个主要优势是，在协议没有受到实质性拜占庭攻击的情况下，每个协商一致的决定只需要每个处理器预期的恒定通信开销，即它提供了对可伸缩性问题的解决方案，该解决方案确保在正常操作期间每个处理器的预期通信开销与处理器总数$n$无关。这是使共识协议能够扩展到10,000个或更多独立验证器(即处理器)的关键属性。另一方面，以下两个问题仍然存在：(1)为雪人提供一致性的正式证据是一个巨大的挑战。(2)当拜占庭敌手控制超过$O(\Sqrt{n})$个处理器时，存在活性攻击，从而将终止速度减慢到超过对数步数。在本文中，我们解决了上述两个问题。我们考虑一个拜占庭对手，它至多控制$f<n/5$处理器。首先，我们为雪人提供了一个简单的一致性证明。然后，我们给Snowman增加了一个活跃度模块，该模块可以在强大的对手发起活跃度攻击的情况下触发，并通过暂时放弃Snowman的通信复杂性优势来保证在这种情况下的活跃性，但在正常运行时不会牺牲这些低通信复杂性的优势。



## **3. PAD: Patch-Agnostic Defense against Adversarial Patch Attacks**

PAD：针对对抗性补丁攻击的补丁不可知防御 cs.CV

Accepted by CVPR 2024

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2404.16452v1) [paper-pdf](http://arxiv.org/pdf/2404.16452v1)

**Authors**: Lihua Jing, Rui Wang, Wenqi Ren, Xin Dong, Cong Zou

**Abstract**: Adversarial patch attacks present a significant threat to real-world object detectors due to their practical feasibility. Existing defense methods, which rely on attack data or prior knowledge, struggle to effectively address a wide range of adversarial patches. In this paper, we show two inherent characteristics of adversarial patches, semantic independence and spatial heterogeneity, independent of their appearance, shape, size, quantity, and location. Semantic independence indicates that adversarial patches operate autonomously within their semantic context, while spatial heterogeneity manifests as distinct image quality of the patch area that differs from original clean image due to the independent generation process. Based on these observations, we propose PAD, a novel adversarial patch localization and removal method that does not require prior knowledge or additional training. PAD offers patch-agnostic defense against various adversarial patches, compatible with any pre-trained object detectors. Our comprehensive digital and physical experiments involving diverse patch types, such as localized noise, printable, and naturalistic patches, exhibit notable improvements over state-of-the-art works. Our code is available at https://github.com/Lihua-Jing/PAD.

摘要: 对抗性补丁攻击由于其实用的可行性，对现实世界中的目标检测器构成了巨大的威胁。现有的防御方法依赖于攻击数据或先验知识，难以有效地应对广泛的对抗性补丁。在这篇文章中，我们展示了对抗性斑块的两个固有特征，语义独立性和空间异质性，与它们的外观、形状、大小、数量和位置无关。语义独立性表明对抗性斑块在其语义语境中自主运行，而空间异质性则表现为斑块区域的不同图像质量，由于独立的生成过程而不同于原始的干净图像。基于这些观察，我们提出了一种新的对抗性补丁定位和移除方法PAD，该方法不需要先验知识或额外的训练。PAD提供与补丁无关的防御，可抵御各种敌意补丁，兼容任何预先训练的物体探测器。我们全面的数字和物理实验涉及不同的补丁类型，如局部噪声、可打印和自然补丁，显示出比最先进的作品显著的改进。我们的代码可以在https://github.com/Lihua-Jing/PAD.上找到



## **4. Constructing Optimal Noise Channels for Enhanced Robustness in Quantum Machine Learning**

构建最佳噪音通道以增强量子机器学习的鲁棒性 quant-ph

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2404.16417v1) [paper-pdf](http://arxiv.org/pdf/2404.16417v1)

**Authors**: David Winderl, Nicola Franco, Jeanette Miriam Lorenz

**Abstract**: With the rapid advancement of Quantum Machine Learning (QML), the critical need to enhance security measures against adversarial attacks and protect QML models becomes increasingly evident. In this work, we outline the connection between quantum noise channels and differential privacy (DP), by constructing a family of noise channels which are inherently $\epsilon$-DP: $(\alpha, \gamma)$-channels. Through this approach, we successfully replicate the $\epsilon$-DP bounds observed for depolarizing and random rotation channels, thereby affirming the broad generality of our framework. Additionally, we use a semi-definite program to construct an optimally robust channel. In a small-scale experimental evaluation, we demonstrate the benefits of using our optimal noise channel over depolarizing noise, particularly in enhancing adversarial accuracy. Moreover, we assess how the variables $\alpha$ and $\gamma$ affect the certifiable robustness and investigate how different encoding methods impact the classifier's robustness.

摘要: 随着量子机器学习(QML)的快速发展，加强针对敌意攻击的安全措施和保护QML模型的迫切需求日益明显。在这项工作中，我们通过构造一族噪声通道来概述量子噪声通道和差分隐私(DP)之间的联系，这些噪声通道本质上是$-DP：$(\α，\伽马)$-通道。通过这种方法，我们成功地复制了观察到的去极化和随机旋转通道的$-dp界限，从而肯定了我们框架的广泛普适性。此外，我们使用一个半定规划来构造一个最优的稳健信道。在一个小规模的实验评估中，我们展示了使用我们的最优噪声通道相对于去极化噪声的好处，特别是在提高对手的准确性方面。此外，我们评估了变量$\α$和$\Gamma$如何影响可证明的稳健性，并研究了不同的编码方法如何影响分类器的稳健性。



## **5. Rethinking Impersonation and Dodging Attacks on Face Recognition Systems**

重新思考模仿并躲避对人脸识别系统的攻击 cs.CV

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2401.08903v3) [paper-pdf](http://arxiv.org/pdf/2401.08903v3)

**Authors**: Fengfan Zhou, Qianyu Zhou, Bangjie Yin, Hui Zheng, Xuequan Lu, Lizhuang Ma, Hefei Ling

**Abstract**: Face Recognition (FR) systems can be easily deceived by adversarial examples that manipulate benign face images through imperceptible perturbations. Adversarial attacks on FR encompass two types: impersonation (targeted) attacks and dodging (untargeted) attacks. Previous methods often achieve a successful impersonation attack on FR; However, it does not necessarily guarantee a successful dodging attack on FR in the black-box setting. In this paper, our key insight is that the generation of adversarial examples should perform both impersonation and dodging attacks simultaneously. To this end, we propose a novel attack method termed as Adversarial Pruning (Adv-Pruning), to fine-tune existing adversarial examples to enhance their dodging capabilities while preserving their impersonation capabilities. Adv-Pruning consists of Priming, Pruning, and Restoration stages. Concretely, we propose Adversarial Priority Quantification to measure the region-wise priority of original adversarial perturbations, identifying and releasing those with minimal impact on absolute model output variances. Then, Biased Gradient Adaptation is presented to adapt the adversarial examples to traverse the decision boundaries of both the attacker and victim by adding perturbations favoring dodging attacks on the vacated regions, preserving the prioritized features of the original perturbations while boosting dodging performance. As a result, we can maintain the impersonation capabilities of original adversarial examples while effectively enhancing dodging capabilities. Comprehensive experiments demonstrate the superiority of our method compared with state-of-the-art adversarial attacks.

摘要: 人脸识别(FR)系统很容易被敌意的例子欺骗，这些例子通过潜移默化的扰动来操纵良性的人脸图像。对FR的敌意攻击包括两种类型：模仿(目标)攻击和躲避(非目标)攻击。以前的方法往往能成功地实现对FR的模仿攻击，但在黑盒环境下，这并不一定能保证对FR的成功躲避攻击。在本文中，我们的主要观点是，生成敌意示例应该同时执行模仿攻击和躲避攻击。为此，我们提出了一种新的攻击方法，称为对抗性剪枝(ADV-Puning)，对现有的对抗性实例进行微调，以增强它们的躲避能力，同时保持它们的模拟能力。高级修剪包括启动、修剪和恢复三个阶段。具体地说，我们提出了对抗性优先级量化来度量原始对抗性扰动的区域优先级，识别并释放那些对绝对模型输出方差影响最小的扰动。然后，通过在空闲区域上添加有利于躲避攻击的扰动，保留了原始扰动的优先特征，同时提高了躲避性能，提出了有偏梯度自适应算法，使敌意例子能够穿越攻击者和受害者的决策边界。因此，我们可以在保持原始对抗性例子的模拟能力的同时，有效地增强躲避能力。综合实验表明，该方法与目前最先进的对抗性攻击方法相比具有一定的优越性。



## **6. IDEA: Invariant Defense for Graph Adversarial Robustness**

IDEA：图形对抗鲁棒性的不变防御 cs.LG

Submitted to Information Sciences

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2305.15792v2) [paper-pdf](http://arxiv.org/pdf/2305.15792v2)

**Authors**: Shuchang Tao, Qi Cao, Huawei Shen, Yunfan Wu, Bingbing Xu, Xueqi Cheng

**Abstract**: Despite the success of graph neural networks (GNNs), their vulnerability to adversarial attacks poses tremendous challenges for practical applications. Existing defense methods suffer from severe performance decline under unseen attacks, due to either limited observed adversarial examples or pre-defined heuristics. To address these limitations, we analyze the causalities in graph adversarial attacks and conclude that causal features are key to achieve graph adversarial robustness, owing to their determinedness for labels and invariance across attacks. To learn these causal features, we innovatively propose an Invariant causal DEfense method against adversarial Attacks (IDEA). We derive node-based and structure-based invariance objectives from an information-theoretic perspective. IDEA ensures strong predictability for labels and invariant predictability across attacks, which is provably a causally invariant defense across various attacks. Extensive experiments demonstrate that IDEA attains state-of-the-art defense performance under all five attacks on all five datasets. The implementation of IDEA is available at https://anonymous.4open.science/r/IDEA.

摘要: 尽管图神经网络(GNN)取得了成功，但它们对敌意攻击的脆弱性给实际应用带来了巨大的挑战。现有的防御方法在不可见攻击下性能严重下降，这要么是由于观察到的对抗性例子有限，要么是由于预先定义的启发式方法。为了解决这些局限性，我们分析了图对抗攻击中的因果关系，并得出结论，因果特征是实现图对抗稳健性的关键，因为它们对于标签的确定性和攻击之间的不变性。为了学习这些因果特征，我们创新性地提出了一种对抗攻击的不变因果防御方法(IDEA)。我们从信息论的角度推导出基于节点和基于结构的不变性目标。IDEA确保标签的强可预测性和跨攻击的不变预测性，这可以证明是跨各种攻击的因果不变防御。广泛的实验表明，在所有五个数据集的所有五个攻击下，该想法获得了最先进的防御性能。IDEA的实施可在https://anonymous.4open.science/r/IDEA.上获得



## **7. Don't Say No: Jailbreaking LLM by Suppressing Refusal**

不要说不：通过压制拒绝来越狱法学硕士 cs.CL

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2404.16369v1) [paper-pdf](http://arxiv.org/pdf/2404.16369v1)

**Authors**: Yukai Zhou, Wenjie Wang

**Abstract**: Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to "jailbreaking" attacks, where carefully crafted prompts elicit them to produce toxic content. One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response. However, the typical attack in this category GCG has very limited attack success rate. In this study, to better study the jailbreak attack, we introduce the DSN (Don't Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals. In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack. The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances. To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators. Extensive experiments demonstrate the potency of the DSN and the effectiveness of ensemble evaluation compared to baseline methods.

摘要: 确保大型语言模型(LLM)的安全一致性对于生成与人类价值观一致的响应至关重要。尽管LLM能够识别和避免有害的查询，但它们很容易受到“越狱”攻击，在这种攻击中，精心制作的提示会诱使它们产生有毒内容。越狱攻击的一类是通过诱使LLM产生肯定的回应，将任务重新制定为对抗性攻击。然而，在这类典型攻击中，GCG的攻击成功率非常有限。在本研究中，为了更好地研究越狱攻击，我们引入了DSN(Don‘t Say No)攻击，它不仅促使LLMS产生肯定的反应，而且新颖地增强了抑制拒绝的目标。此外，越狱攻击的另一个挑战是评估，因为很难直接和准确地评估攻击的危害性。现有的拒绝关键词匹配等评价方法暴露出大量的误报和漏报实例，具有一定的局限性。为了克服这一挑战，我们提出了一种集成评估流水线，其中包括自然语言推理(NLI)矛盾评估和两个外部LLM评估器。大量实验证明了DSN的有效性和集成评估与基线方法相比的有效性。



## **8. PA-Boot: A Formally Verified Authentication Protocol for Multiprocessor Secure Boot**

PA-Boot：用于多处理器安全引导的正式验证认证协议 cs.CR

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2209.07936v2) [paper-pdf](http://arxiv.org/pdf/2209.07936v2)

**Authors**: Zhuoruo Zhang, Chenyang Yu, Rui Chang, Mingshuai Chen, Bo Feng, He Huang, Qinming Dai, Wenbo Shen, Yongwang Zhao

**Abstract**: Hardware supply-chain attacks are raising significant security threats to the boot process of multiprocessor systems. This paper identifies a new, prevalent hardware supply-chain attack surface that can bypass multiprocessor secure boot due to the absence of processor-authentication mechanisms. To defend against such attacks, we present PA-Boot, the first formally verified processor-authentication protocol for secure boot in multiprocessor systems. PA-Boot is proved functionally correct and is guaranteed to detect multiple adversarial behaviors, e.g., processor replacements, man-in-the-middle attacks, and tampering with certificates. The fine-grained formalization of PA-Boot and its fully mechanized security proofs are carried out in the Isabelle/HOL theorem prover with 306 lemmas/theorems and ~7,100 LoC. Experiments on a proof-of-concept implementation indicate that PA-Boot can effectively identify boot-process attacks with a considerably minor overhead and thereby improve the security of multiprocessor systems.

摘要: 硬件供应链攻击正在给多处理器系统的引导过程带来严重的安全威胁。本文提出了一种新的、流行的硬件供应链攻击面，由于缺乏处理器认证机制，该攻击面可以绕过多处理器安全引导。为了防御此类攻击，我们提出了PA-Boot，这是第一个经过正式验证的用于多处理器系统安全引导的处理器认证协议。PA-Boot被证明在功能上是正确的，并保证可以检测到多种敌对行为，例如处理器更换、中间人攻击和篡改证书。PA-Boot的细粒度形式化及其全机械化安全证明是在Isabelle/HOL定理证明器上进行的，具有306个引理/定理和~7100个LoC。在概念验证实现上的实验表明，PA-Boot能够以相当小的开销有效地识别引导过程攻击，从而提高多处理器系统的安全性。



## **9. A Generative Framework for Low-Cost Result Validation of Machine Learning-as-a-Service Inference**

机器学习即服务推理的低成本结果验证生成框架 cs.CR

15 pages, 12 figures

**SubmitDate**: 2024-04-25    [abs](http://arxiv.org/abs/2304.00083v4) [paper-pdf](http://arxiv.org/pdf/2304.00083v4)

**Authors**: Abhinav Kumar, Miguel A. Guirao Aguilera, Reza Tourani, Satyajayant Misra

**Abstract**: The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as Augmented/Virtual Reality, integrity verification of the outsourced ML tasks is more critical--a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time integrity validation of ML-as-a-Service (MLaaS) inference. Fides features a novel and efficient distillation technique--Greedy Distillation Transfer Learning--that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergence measurements to identify, with a high likelihood, if the service model is under attack. Fides also offers a re-classification functionality that predicts the original class whenever an attack is identified. We devised a generative adversarial network framework for training the attack detection and re-classification models. The evaluation shows that Fides achieves an accuracy of up to 98% for attack detection and 94% for re-classification.

摘要: 机器学习(ML)的日益流行导致了它在各种敏感领域的部署，这导致了对ML安全和隐私的大量研究。然而，在一些应用程序中，例如增强/虚拟现实，外包的ML任务的完整性验证更为关键--这是一个没有得到太多关注的方面。现有的解决方案，如多方计算和基于证明的系统，带来了巨大的计算开销，这使得它们不适合实时应用。提出了一种新的ML-as-a-Service(ML-as-a-Service)推理的实时完整性验证框架FIDS。FIDS的特点是一种新颖而高效的蒸馏技术--贪婪蒸馏转移学习--它动态地提取和微调空间和计算效率高的验证模型，以便在可信执行环境中运行时验证相应的服务模型。FIDS具有客户端攻击检测模型，该模型使用统计分析和分歧测量来识别服务模型是否受到攻击的可能性很高。FIDS还提供了重新分类功能，该功能可以在识别攻击时预测原始类别。我们设计了一个生成式对抗性网络框架来训练攻击检测和重分类模型。评估表明，FIDS对攻击检测的准确率高达98%，对重分类的准确率高达94%。



## **10. An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape**

不断变化的威胁环境中Deepfake图像检测的最新进展分析 cs.CR

Accepted to IEEE S&P 2024; 19 pages, 10 figures

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.16212v1) [paper-pdf](http://arxiv.org/pdf/2404.16212v1)

**Authors**: Sifat Muhammad Abdullah, Aravind Cheruvu, Shravya Kanchi, Taejoong Chung, Peng Gao, Murtuza Jadliwala, Bimal Viswanath

**Abstract**: Deepfake or synthetic images produced using deep generative models pose serious risks to online platforms. This has triggered several research efforts to accurately detect deepfake images, achieving excellent performance on publicly available deepfake datasets. In this work, we study 8 state-of-the-art detectors and argue that they are far from being ready for deployment due to two recent developments. First, the emergence of lightweight methods to customize large generative models, can enable an attacker to create many customized generators (to create deepfakes), thereby substantially increasing the threat surface. We show that existing defenses fail to generalize well to such \emph{user-customized generative models} that are publicly available today. We discuss new machine learning approaches based on content-agnostic features, and ensemble modeling to improve generalization performance against user-customized models. Second, the emergence of \textit{vision foundation models} -- machine learning models trained on broad data that can be easily adapted to several downstream tasks -- can be misused by attackers to craft adversarial deepfakes that can evade existing defenses. We propose a simple adversarial attack that leverages existing foundation models to craft adversarial samples \textit{without adding any adversarial noise}, through careful semantic manipulation of the image content. We highlight the vulnerabilities of several defenses against our attack, and explore directions leveraging advanced foundation models and adversarial training to defend against this new threat.

摘要: 使用深度生成模型生成的深伪或合成图像对在线平台构成严重风险。这引发了几项准确检测深度伪图像的研究工作，在公开可用的深度伪数据集上取得了出色的性能。在这项工作中，我们研究了8个最先进的探测器，并认为由于最近的两个发展，它们还远未准备好部署。首先，自定义大型生成性模型的轻量级方法的出现，使攻击者能够创建许多自定义的生成器(以创建深度假冒)，从而大大增加了威胁表面。我们表明，现有的防御措施不能很好地推广到今天公开可用的这种\emph(用户定制的生成模型)。我们讨论了基于内容无关特征的新的机器学习方法，以及集成建模以提高针对用户定制模型的泛化性能。其次，TEXTIT(视觉基础模型)的出现--机器学习模型是基于广泛的数据训练的，可以很容易地适应几个下游任务--可能被攻击者滥用来制作可以逃避现有防御的对抗性深度假冒。我们提出了一种简单的对抗性攻击，它利用现有的基础模型，通过仔细地对图像内容进行语义处理，在不添加任何对抗性噪声的情况下制作对抗性样本。我们强调了几种防御我们攻击的漏洞，并探索了利用先进的基础模型和对抗性训练来防御这一新威胁的方向。



## **11. A Comparative Analysis of Adversarial Robustness for Quantum and Classical Machine Learning Models**

量子和经典机器学习模型对抗鲁棒性的比较分析 cs.LG

submitted to IEEE QCE24

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.16154v1) [paper-pdf](http://arxiv.org/pdf/2404.16154v1)

**Authors**: Maximilian Wendlinger, Kilian Tscharke, Pascal Debus

**Abstract**: Quantum machine learning (QML) continues to be an area of tremendous interest from research and industry. While QML models have been shown to be vulnerable to adversarial attacks much in the same manner as classical machine learning models, it is still largely unknown how to compare adversarial attacks on quantum versus classical models. In this paper, we show how to systematically investigate the similarities and differences in adversarial robustness of classical and quantum models using transfer attacks, perturbation patterns and Lipschitz bounds. More specifically, we focus on classification tasks on a handcrafted dataset that allows quantitative analysis for feature attribution. This enables us to get insight, both theoretically and experimentally, on the robustness of classification networks. We start by comparing typical QML model architectures such as amplitude and re-upload encoding circuits with variational parameters to a classical ConvNet architecture. Next, we introduce a classical approximation of QML circuits (originally obtained with Random Fourier Features sampling but adapted in this work to fit a trainable encoding) and evaluate this model, denoted Fourier network, in comparison to other architectures. Our findings show that this Fourier network can be seen as a "middle ground" on the quantum-classical boundary. While adversarial attacks successfully transfer across this boundary in both directions, we also show that regularization helps quantum networks to be more robust, which has direct impact on Lipschitz bounds and transfer attacks.

摘要: 量子机器学习(QML)仍然是研究和工业界非常感兴趣的一个领域。虽然QML模型已经被证明在很大程度上像经典机器学习模型一样容易受到敌意攻击，但如何比较量子模型和经典模型的对抗性攻击在很大程度上仍然是未知的。在这篇文章中，我们展示了如何利用转移攻击、扰动模式和Lipschitz界来系统地研究经典模型和量子模型在对抗健壮性方面的异同。更具体地说，我们专注于手工制作的数据集上的分类任务，该数据集允许对特征属性进行定量分析。这使我们能够从理论和实验上深入了解分类网络的健壮性。我们首先比较典型的QML模型体系结构，例如具有可变参数的幅度和重传编码电路与经典的ConvNet体系结构。接下来，我们介绍了QML电路的一个经典近似(最初是通过随机傅立叶特征采样获得的，但在本工作中采用了适合可训练编码的模型)，并将该模型与其他体系结构进行了比较。我们的发现表明，这个傅里叶网络可以被视为量子-经典边界上的“中间地带”。虽然对抗性攻击成功地在两个方向上跨越了这一边界，但我们也证明了正则化有助于量子网络更健壮，这对Lipschitz界和转移攻击有直接影响。



## **12. A Survey on Intermediate Fusion Methods for Collaborative Perception Categorized by Real World Challenges**

按现实世界挑战分类的协作感知中间融合方法调查 cs.CV

8 pages, 6 tables

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.16139v1) [paper-pdf](http://arxiv.org/pdf/2404.16139v1)

**Authors**: Melih Yazgan, Thomas Graf, Min Liu, J. Marius Zoellner

**Abstract**: This survey analyzes intermediate fusion methods in collaborative perception for autonomous driving, categorized by real-world challenges. We examine various methods, detailing their features and the evaluation metrics they employ. The focus is on addressing challenges like transmission efficiency, localization errors, communication disruptions, and heterogeneity. Moreover, we explore strategies to counter adversarial attacks and defenses, as well as approaches to adapt to domain shifts. The objective is to present an overview of how intermediate fusion methods effectively meet these diverse challenges, highlighting their role in advancing the field of collaborative perception in autonomous driving.

摘要: 这项调查分析了自动驾驶协作感知中的中间融合方法，并按现实世界的挑战进行分类。我们检查了各种方法，详细介绍了它们的功能和它们采用的评估指标。重点是解决传输效率、本地化错误、通信中断和多样性等挑战。此外，我们还探索对抗对抗攻击和防御的策略，以及适应领域转变的方法。目标是概述中间融合方法如何有效应对这些多样化的挑战，强调它们在推进自动驾驶协作感知领域中的作用。



## **13. Exact Recovery for System Identification with More Corrupt Data than Clean Data**

准确恢复损坏数据多于干净数据的系统识别 cs.LG

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2305.10506v3) [paper-pdf](http://arxiv.org/pdf/2305.10506v3)

**Authors**: Baturalp Yalcin, Haixiang Zhang, Javad Lavaei, Murat Arcak

**Abstract**: This paper investigates the system identification problem for linear discrete-time systems under adversaries and analyzes two lasso-type estimators. We examine both asymptotic and non-asymptotic properties of these estimators in two separate scenarios, corresponding to deterministic and stochastic models for the attack times. Since the samples collected from the system are correlated, the existing results on lasso are not applicable. We prove that when the system is stable and attacks are injected periodically, the sample complexity for exact recovery of the system dynamics is linear in terms of the dimension of the states. When adversarial attacks occur at each time instance with probability p, the required sample complexity for exact recovery scales polynomially in the dimension of the states and the probability p. This result implies almost sure convergence to the true system dynamics under the asymptotic regime. As a by-product, our estimators still learn the system correctly even when more than half of the data is compromised. We highlight that the attack vectors are allowed to be correlated with each other in this work, whereas we make some assumptions about the times at which the attacks happen. This paper provides the first mathematical guarantee in the literature on learning from correlated data for dynamical systems in the case when there is less clean data than corrupt data.

摘要: 研究了对手作用下线性离散时间系统的系统辨识问题，分析了两种套索型估值器。我们在两个不同的场景中，分别对应于攻击时间的确定性和随机性模型，研究了这些估计量的渐近和非渐近性质。由于从系统采集的样本是相关的，现有的套索结果不适用。我们证明了当系统稳定且攻击被周期性注入时，精确恢复系统动态的样本复杂度是关于状态维的线性的。当对抗性攻击发生在概率为p的每个时刻时，精确恢复所需的样本复杂度在状态和概率p的维度上呈多项式增长，这一结果意味着在渐近体制下几乎必然收敛到真正的系统动力学。作为副产品，即使超过一半的数据被泄露，我们的估计者仍然正确地学习系统。在本工作中，我们强调允许攻击向量彼此关联，而我们对攻击发生的时间做出了一些假设。本文首次为动态系统在干净数据少于损坏数据的情况下从相关数据中学习提供了数学保证。



## **14. Leverage Variational Graph Representation For Model Poisoning on Federated Learning**

利用变分图表示解决联邦学习中的模型中毒问题 cs.CR

12 pages, 8 figures, 2 tables

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.15042v2) [paper-pdf](http://arxiv.org/pdf/2404.15042v2)

**Authors**: Kai Li, Xin Yuan, Jingjing Zheng, Wei Ni, Falko Dressler, Abbas Jamalipour

**Abstract**: This paper puts forth a new training data-untethered model poisoning (MP) attack on federated learning (FL). The new MP attack extends an adversarial variational graph autoencoder (VGAE) to create malicious local models based solely on the benign local models overheard without any access to the training data of FL. Such an advancement leads to the VGAE-MP attack that is not only efficacious but also remains elusive to detection. VGAE-MP attack extracts graph structural correlations among the benign local models and the training data features, adversarially regenerates the graph structure, and generates malicious local models using the adversarial graph structure and benign models' features. Moreover, a new attacking algorithm is presented to train the malicious local models using VGAE and sub-gradient descent, while enabling an optimal selection of the benign local models for training the VGAE. Experiments demonstrate a gradual drop in FL accuracy under the proposed VGAE-MP attack and the ineffectiveness of existing defense mechanisms in detecting the attack, posing a severe threat to FL.

摘要: 提出了一种新的训练数据--非拴系模型中毒(MP)联合学习攻击。新的MP攻击扩展了对抗性变分图自动编码器(VGAE)，仅基于在没有访问FL训练数据的情况下偷听到的良性局部模型来创建恶意局部模型。这样的进步导致了VGAE-MP攻击，这种攻击不仅有效，而且仍然难以检测。VGAE-MP攻击提取良性局部模型和训练数据特征之间的图结构相关性，恶意重建图结构，并利用对抗性图结构和良性模型特征生成恶意局部模型。此外，提出了一种新的攻击算法，利用VGAE和次梯度下降来训练恶意局部模型，同时使良性局部模型能够最优地选择用于训练VGAE的局部模型。实验表明，在提出的VGAE-MP攻击下，FL的准确率逐渐下降，并且现有的防御机制在检测攻击时效率低下，对FL构成了严重的威胁。



## **15. Steal Now and Attack Later: Evaluating Robustness of Object Detection against Black-box Adversarial Attacks**

先窃取，后攻击：评估对象检测针对黑匣子对抗攻击的鲁棒性 cs.CV

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.15881v1) [paper-pdf](http://arxiv.org/pdf/2404.15881v1)

**Authors**: Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-Rung Lee

**Abstract**: Latency attacks against object detection represent a variant of adversarial attacks that aim to inflate the inference time by generating additional ghost objects in a target image. However, generating ghost objects in the black-box scenario remains a challenge since information about these unqualified objects remains opaque. In this study, we demonstrate the feasibility of generating ghost objects in adversarial examples by extending the concept of "steal now, decrypt later" attacks. These adversarial examples, once produced, can be employed to exploit potential vulnerabilities in the AI service, giving rise to significant security concerns. The experimental results demonstrate that the proposed attack achieves successful attacks across various commonly used models and Google Vision API without any prior knowledge about the target model. Additionally, the average cost of each attack is less than \$ 1 dollars, posing a significant threat to AI security.

摘要: 针对对象检测的延迟攻击代表了对抗攻击的一种变体，旨在通过在目标图像中生成额外的幽灵对象来增加推理时间。然而，在黑匣子场景中生成幽灵对象仍然是一个挑战，因为有关这些不合格对象的信息仍然不透明。在这项研究中，我们通过扩展“立即窃取，稍后解密”攻击的概念，证明了在对抗性示例中生成幽灵对象的可行性。这些对抗性示例一旦产生，就可以用来利用人工智能服务中的潜在漏洞，从而引发重大的安全问题。实验结果表明，在不了解目标模型的情况下，提出的攻击可以在各种常用模型和Google Vision API上成功实现攻击。此外，每次攻击的平均成本不到1美元，对人工智能安全构成重大威胁。



## **16. Overload: Latency Attacks on Object Detection for Edge Devices**

过载：边缘设备对象检测的延迟攻击 cs.CV

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2304.05370v3) [paper-pdf](http://arxiv.org/pdf/2304.05370v3)

**Authors**: Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-rung Lee

**Abstract**: Nowadays, the deployment of deep learning-based applications is an essential task owing to the increasing demands on intelligent services. In this paper, we investigate latency attacks on deep learning applications. Unlike common adversarial attacks for misclassification, the goal of latency attacks is to increase the inference time, which may stop applications from responding to the requests within a reasonable time. This kind of attack is ubiquitous for various applications, and we use object detection to demonstrate how such kind of attacks work. We also design a framework named Overload to generate latency attacks at scale. Our method is based on a newly formulated optimization problem and a novel technique, called spatial attention. This attack serves to escalate the required computing costs during the inference time, consequently leading to an extended inference time for object detection. It presents a significant threat, especially to systems with limited computing resources. We conducted experiments using YOLOv5 models on Nvidia NX. Compared to existing methods, our method is simpler and more effective. The experimental results show that with latency attacks, the inference time of a single image can be increased ten times longer in reference to the normal setting. Moreover, our findings pose a potential new threat to all object detection tasks requiring non-maximum suppression (NMS), as our attack is NMS-agnostic.

摘要: 如今，由于对智能服务的需求不断增加，部署基于深度学习的应用程序是一项重要的任务。本文研究了深度学习应用程序中的延迟攻击。与常见的误分类对抗性攻击不同，延迟攻击的目标是增加推理时间，这可能会使应用程序在合理的时间内停止对请求的响应。这种攻击在各种应用中普遍存在，我们使用对象检测来演示这种攻击是如何工作的。我们还设计了一个名为OverLoad的框架来生成大规模的延迟攻击。我们的方法是基于一个新提出的优化问题和一种新的技术，称为空间注意力。该攻击用于在推理时间内增加所需的计算成本，从而导致对象检测的推理时间延长。它构成了一个重大威胁，特别是对计算资源有限的系统。我们在NVIDIA NX上使用YOLOv5模型进行了实验。与已有的方法相比，我们的方法更简单有效。实验结果表明，在延迟攻击的情况下，单幅图像的推理时间可以比正常设置增加十倍。此外，我们的发现对所有需要非最大抑制(NMS)的目标检测任务构成了潜在的新威胁，因为我们的攻击是与NMS无关的。



## **17. CONNECTION: COvert chaNnel NEtwork attaCk Through bIt-rate mOdulatioN**

连接：覆盖通道网络连接通过比特率modulatiioN cs.CR

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.15858v1) [paper-pdf](http://arxiv.org/pdf/2404.15858v1)

**Authors**: Simone Soderi, Rocco De Nicola

**Abstract**: Covert channel networks are a well-known method for circumventing the security measures organizations put in place to protect their networks from adversarial attacks. This paper introduces a novel method based on bit-rate modulation for implementing covert channels between devices connected over a wide area network. This attack can be exploited to exfiltrate sensitive information from a machine (i.e., covert sender) and stealthily transfer it to a covert receiver while evading network security measures and detection systems. We explain how to implement this threat, focusing specifically on covert channel networks and their potential security risks to network information transmission. The proposed method leverages bit-rate modulation, where a high bit rate represents a '1' and a low bit rate represents a '0', enabling covert communication. We analyze the key metrics associated with covert channels, including robustness in the presence of legitimate traffic and other interference, bit-rate capacity, and bit error rate. Experiments demonstrate the good performance of this attack, which achieved 5 bps with excellent robustness and a channel capacity of up to 0.9239 bps/Hz under different noise sources. Therefore, we show that bit-rate modulation effectively violates network security and compromises sensitive data.

摘要: 隐蔽通道网络是一种众所周知的方法，用于绕过组织为保护其网络免受对手攻击而实施的安全措施。介绍了一种基于比特率调制的广域网设备间隐蔽通道的实现方法。利用这种攻击可以从机器(即秘密发送者)中渗出敏感信息，并将其秘密传输到秘密接收者，同时规避网络安全措施和检测系统。我们解释了如何实施这种威胁，特别是针对隐蔽通道网络及其对网络信息传输的潜在安全风险。所提出的方法利用比特率调制，其中高比特率代表1，低比特率代表0，从而实现隐蔽通信。我们分析了与隐蔽信道相关的关键指标，包括在存在合法业务和其他干扰的情况下的稳健性、比特率容量和误码率。实验表明，该攻击具有良好的性能，在不同的噪声源下，达到了5bps的传输速率，并且具有很好的鲁棒性，信道容量高达0.9239 bps/Hz。因此，我们证明了比特率调制有效地违反了网络安全并且危害了敏感数据。



## **18. Beyond Score Changes: Adversarial Attack on No-Reference Image Quality Assessment from Two Perspectives**

超越分数变化：从两个角度对无参考图像质量评估的对抗攻击 eess.IV

Submitted to a conference

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.13277v2) [paper-pdf](http://arxiv.org/pdf/2404.13277v2)

**Authors**: Chenxi Yang, Yujia Liu, Dingquan Li, Yan Zhong, Tingting Jiang

**Abstract**: Deep neural networks have demonstrated impressive success in No-Reference Image Quality Assessment (NR-IQA). However, recent researches highlight the vulnerability of NR-IQA models to subtle adversarial perturbations, leading to inconsistencies between model predictions and subjective ratings. Current adversarial attacks, however, focus on perturbing predicted scores of individual images, neglecting the crucial aspect of inter-score correlation relationships within an entire image set. Meanwhile, it is important to note that the correlation, like ranking correlation, plays a significant role in NR-IQA tasks. To comprehensively explore the robustness of NR-IQA models, we introduce a new framework of correlation-error-based attacks that perturb both the correlation within an image set and score changes on individual images. Our research primarily focuses on ranking-related correlation metrics like Spearman's Rank-Order Correlation Coefficient (SROCC) and prediction error-related metrics like Mean Squared Error (MSE). As an instantiation, we propose a practical two-stage SROCC-MSE-Attack (SMA) that initially optimizes target attack scores for the entire image set and then generates adversarial examples guided by these scores. Experimental results demonstrate that our SMA method not only significantly disrupts the SROCC to negative values but also maintains a considerable change in the scores of individual images. Meanwhile, it exhibits state-of-the-art performance across metrics with different categories. Our method provides a new perspective on the robustness of NR-IQA models.

摘要: 深度神经网络在无参考图像质量评估(NR-IQA)中取得了令人印象深刻的成功。然而，最近的研究突显了NR-IQA模型在微妙的对抗性扰动下的脆弱性，导致模型预测与主观评分之间的不一致。然而，当前的对抗性攻击集中于扰乱单个图像的预测分数，而忽略了整个图像集内分数间相关性的关键方面。同时，值得注意的是，这种相关性和排名相关性一样，在NR-IQA任务中扮演着重要的角色。为了全面探索NR-IQA模型的稳健性，我们引入了一种新的基于相关性误差的攻击框架，该框架既干扰了图像集合内的相关性，也干扰了单个图像上的分数变化。我们的研究主要集中在与排名相关的指标，如Spearman的秩次相关系数(SROCC)和与预测误差相关的指标，如均方误差(MSE)。作为一个实例，我们提出了一种实用的两阶段SROCC-MSE攻击(SMA)，它首先优化整个图像集的目标攻击分数，然后根据这些分数生成对抗性实例。实验结果表明，我们的SMA方法不仅显著地将SROCC打乱到负值，而且还保持了单个图像得分的较大变化。同时，它在不同类别的指标上展示了最先进的性能。我们的方法为NR-IQA模型的稳健性提供了一个新的视角。



## **19. An Empirical Study of Aegis**

宙斯盾的实证研究 cs.LG

9 pages, 6 figures, 3 tables

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.15784v1) [paper-pdf](http://arxiv.org/pdf/2404.15784v1)

**Authors**: Daniel Saragih, Paridhi Goel, Tejas Balaji, Alyssa Li

**Abstract**: Bit flipping attacks are one class of attacks on neural networks with numerous defense mechanisms invented to mitigate its potency. Due to the importance of ensuring the robustness of these defense mechanisms, we perform an empirical study on the Aegis framework. We evaluate the baseline mechanisms of Aegis on low-entropy data (MNIST), and we evaluate a pre-trained model with the mechanisms fine-tuned on MNIST. We also compare the use of data augmentation to the robustness training of Aegis, and how Aegis performs under other adversarial attacks, such as the generation of adversarial examples. We find that both the dynamic-exit strategy and robustness training of Aegis has some drawbacks. In particular, we see drops in accuracy when testing on perturbed data, and on adversarial examples, as compared to baselines. Moreover, we found that the dynamic exit-strategy loses its uniformity when tested on simpler datasets. The code for this project is available on GitHub.

摘要: 比特翻转攻击是对神经网络的一类攻击，它发明了多种防御机制来减轻其效力。由于确保这些防御机制的稳健性的重要性，我们对宙斯盾框架进行了实证研究。我们评估了Aegis在低熵数据（MNIST）上的基线机制，并评估了预训练的模型，该模型具有在MNIST上微调的机制。我们还比较了数据增强的使用与Aegis的稳健性训练，以及Aegis在其他对抗性攻击（例如对抗性示例的生成）下的表现。我们发现宙斯盾的动态退出策略和鲁棒性训练都存在一些缺陷。特别是，与基线相比，当对受干扰的数据和对抗性示例进行测试时，我们看到准确性有所下降。此外，我们发现，当在更简单的数据集上进行测试时，动态退出策略会失去一致性。该项目的代码可在GitHub上找到。



## **20. A General Black-box Adversarial Attack on Graph-based Fake News Detectors**

对基于图的假新闻检测器的一般黑匣子对抗攻击 cs.LG

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.15744v1) [paper-pdf](http://arxiv.org/pdf/2404.15744v1)

**Authors**: Peican Zhu, Zechen Pan, Yang Liu, Jiwei Tian, Keke Tang, Zhen Wang

**Abstract**: Graph Neural Network (GNN)-based fake news detectors apply various methods to construct graphs, aiming to learn distinctive news embeddings for classification. Since the construction details are unknown for attackers in a black-box scenario, it is unrealistic to conduct the classical adversarial attacks that require a specific adjacency matrix. In this paper, we propose the first general black-box adversarial attack framework, i.e., General Attack via Fake Social Interaction (GAFSI), against detectors based on different graph structures. Specifically, as sharing is an important social interaction for GNN-based fake news detectors to construct the graph, we simulate sharing behaviors to fool the detectors. Firstly, we propose a fraudster selection module to select engaged users leveraging local and global information. In addition, a post injection module guides the selected users to create shared relations by sending posts. The sharing records will be added to the social context, leading to a general attack against different detectors. Experimental results on empirical datasets demonstrate the effectiveness of GAFSI.

摘要: 基于图神经网络(GNN)的假新闻检测器应用多种方法构造图，目的是学习不同的新闻嵌入进行分类。由于在黑盒场景中攻击者的构造细节是未知的，因此进行需要特定邻接矩阵的经典对抗性攻击是不现实的。本文基于不同的图结构，提出了第一个针对检测器的通用黑盒对抗攻击框架，即通过伪社会交互的通用攻击(GAFSI)。具体地说，由于分享是基于GNN的假新闻检测器构建图的重要社交交互，因此我们模拟分享行为来愚弄检测器。首先，我们提出了一个欺诈者选择模块，利用局部和全局信息来选择参与用户。此外，帖子注入模块通过发送帖子来引导选定的用户创建共享关系。共享记录将被添加到社会上下文中，从而导致对不同检测器的通用攻击。在经验数据集上的实验结果证明了GAFSI的有效性。



## **21. MISLEAD: Manipulating Importance of Selected features for Learning Epsilon in Evasion Attack Deception**

MISLEAD：操纵选择功能的重要性以学习躲避攻击欺骗中的情节 cs.LG

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.15656v1) [paper-pdf](http://arxiv.org/pdf/2404.15656v1)

**Authors**: Vidit Khazanchi, Pavan Kulkarni, Yuvaraj Govindarajulu, Manojkumar Parmar

**Abstract**: Emerging vulnerabilities in machine learning (ML) models due to adversarial attacks raise concerns about their reliability. Specifically, evasion attacks manipulate models by introducing precise perturbations to input data, causing erroneous predictions. To address this, we propose a methodology combining SHapley Additive exPlanations (SHAP) for feature importance analysis with an innovative Optimal Epsilon technique for conducting evasion attacks. Our approach begins with SHAP-based analysis to understand model vulnerabilities, crucial for devising targeted evasion strategies. The Optimal Epsilon technique, employing a Binary Search algorithm, efficiently determines the minimum epsilon needed for successful evasion. Evaluation across diverse machine learning architectures demonstrates the technique's precision in generating adversarial samples, underscoring its efficacy in manipulating model outcomes. This study emphasizes the critical importance of continuous assessment and monitoring to identify and mitigate potential security risks in machine learning systems.

摘要: 由于对抗性攻击，机器学习(ML)模型中新出现的漏洞引发了人们对其可靠性的担忧。具体地说，规避攻击通过向输入数据引入精确的扰动来操纵模型，从而导致错误的预测。为了解决这一问题，我们提出了一种方法，结合Shapley Additive In释义(Shap)用于特征重要性分析和创新的最优Epsilon技术来进行规避攻击。我们的方法从基于Shap的分析开始，以了解模型漏洞，这对于设计有针对性的规避策略至关重要。采用二进制搜索算法的最优Epsilon技术有效地确定了成功躲避所需的最小epsilon。对不同机器学习体系结构的评估表明，该技术在生成对抗性样本方面具有准确性，强调了其在操纵模型结果方面的有效性。这项研究强调了持续评估和监测的重要性，以识别和缓解机器学习系统中的潜在安全风险。



## **22. Security Analysis of WiFi-based Sensing Systems: Threats from Perturbation Attacks**

基于WiFi的传感系统的安全分析：来自扰动攻击的威胁 cs.CR

**SubmitDate**: 2024-04-24    [abs](http://arxiv.org/abs/2404.15587v1) [paper-pdf](http://arxiv.org/pdf/2404.15587v1)

**Authors**: Hangcheng Cao, Wenbin Huang, Guowen Xu, Xianhao Chen, Ziyang He, Jingyang Hu, Hongbo Jiang, Yuguang Fang

**Abstract**: Deep learning technologies are pivotal in enhancing the performance of WiFi-based wireless sensing systems. However, they are inherently vulnerable to adversarial perturbation attacks, and regrettably, there is lacking serious attention to this security issue within the WiFi sensing community. In this paper, we elaborate such an attack, called WiIntruder, distinguishing itself with universality, robustness, and stealthiness, which serves as a catalyst to assess the security of existing WiFi-based sensing systems. This attack encompasses the following salient features: (1) Maximizing transferability by differentiating user-state-specific feature spaces across sensing models, leading to a universally effective perturbation attack applicable to common applications; (2) Addressing perturbation signal distortion caused by device synchronization and wireless propagation when critical parameters are optimized through a heuristic particle swarm-driven perturbation generation algorithm; and (3) Enhancing attack pattern diversity and stealthiness through random switching of perturbation surrogates generated by a generative adversarial network. Extensive experimental results confirm the practical threats of perturbation attacks to common WiFi-based services, including user authentication and respiratory monitoring.

摘要: 深度学习技术是提高基于WiFi的无线传感系统性能的关键。然而，它们天生就容易受到对抗性干扰攻击，遗憾的是，WiFi感应界对这一安全问题缺乏认真的关注。在本文中，我们详细描述了这样一种攻击，称为WiIntruder，它具有通用性、健壮性和隐蔽性，可以作为评估现有基于WiFi的感知系统的安全性的催化剂。该攻击包含以下显著特征：(1)通过区分感知模型中用户状态特定的特征空间来最大化可转移性，从而产生适用于常见应用的普遍有效的扰动攻击；(2)通过启发式粒子群驱动的扰动生成算法来优化关键参数时，解决设备同步和无线传播引起的扰动信号失真；以及(3)通过随机切换由生成性对抗网络生成的扰动代理来增强攻击模式的多样性和隐蔽性。广泛的实验结果证实了扰动攻击对常见的基于WiFi的服务的实际威胁，包括用户身份验证和呼吸监测。



## **23. OffRAMPS: An FPGA-based Intermediary for Analysis and Modification of Additive Manufacturing Control Systems**

OffRAMPS：一家基于PGA的中介机构，用于分析和修改增材制造控制系统 cs.CR

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2404.15446v1) [paper-pdf](http://arxiv.org/pdf/2404.15446v1)

**Authors**: Jason Blocklove, Md Raz, Prithwish Basu Roy, Hammond Pearce, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri

**Abstract**: Cybersecurity threats in Additive Manufacturing (AM) are an increasing concern as AM adoption continues to grow. AM is now being used for parts in the aerospace, transportation, and medical domains. Threat vectors which allow for part compromise are particularly concerning, as any failure in these domains would have life-threatening consequences. A major challenge to investigation of AM part-compromises comes from the difficulty in evaluating and benchmarking both identified threat vectors as well as methods for detecting adversarial actions. In this work, we introduce a generalized platform for systematic analysis of attacks against and defenses for 3D printers. Our "OFFRAMPS" platform is based on the open-source 3D printer control board "RAMPS." OFFRAMPS allows analysis, recording, and modification of all control signals and I/O for a 3D printer. We show the efficacy of OFFRAMPS by presenting a series of case studies based on several Trojans, including ones identified in the literature, and show that OFFRAMPS can both emulate and detect these attacks, i.e., it can both change and detect arbitrary changes to the g-code print commands.

摘要: 随着添加剂制造(AM)的采用持续增长，AM中的网络安全威胁日益受到关注。AM现在被用于航空航天、交通运输和医疗领域的部件。允许部分妥协的威胁载体尤其令人担忧，因为这些领域的任何失败都将产生危及生命的后果。调查AM部分妥协的一个主要挑战来自于评估和基准识别的威胁向量以及检测敌对行为的方法的困难。在这项工作中，我们介绍了一个通用的平台，用于系统地分析针对3D打印机的攻击和防御。我们的“出坡道”平台是基于开源3D打印机控制板“坡道”。Outramps允许对3D打印机的所有控制信号和I/O进行分析、记录和修改。我们通过基于几种特洛伊木马程序的一系列案例研究展示了出站攻击的有效性，其中包括文献中识别的木马程序，并表明出站出站可以模拟和检测这些攻击，即它既可以更改g代码打印命令，也可以检测对g代码打印命令的任意更改。



## **24. Beyond Text: Utilizing Vocal Cues to Improve Decision Making in LLMs for Robot Navigation Tasks**

超越文本：利用人声线索改善LLM机器人导航任务的决策 cs.AI

28 pages, 7 figures

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2402.03494v2) [paper-pdf](http://arxiv.org/pdf/2402.03494v2)

**Authors**: Xingpeng Sun, Haoming Meng, Souradip Chakraborty, Amrit Singh Bedi, Aniket Bera

**Abstract**: While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present \emph{Beyond Text}; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations.This approach not only achieves a 70.26\% winning rate, outperforming existing LLMs by 22.16\% to 48.30\% (gemini-1.5-pro and gpt-3.5 respectively), but also enhances robustness against token manipulation adversarial attacks, highlighted by a 22.44\% less decrease ratio than the text-only language model in winning rate. ``\textit{Beyond Text}'' marks an advancement in social robot navigation and broader Human-Robot interactions, seamlessly integrating text-based guidance with human-audio-informed language models.

摘要: 虽然LLM在处理这些人类对话中的文本方面表现出色，但它们在社交导航等场景中难以处理语言指令的细微差别，在这些场景中，模棱两可和不确定性可能会侵蚀人们对机器人和其他人工智能系统的信任。我们可以通过超越文本并另外关注这些音频反应的副语言特征来解决这一缺点。这些特征是口语交际的方面，不涉及字面上的措辞(词汇内容)，但通过说话方式传达意义和细微差别。我们提出了一种改进LLM决策的方法，该方法通过集成音频转录和这些特征的一部分来改进LLM决策，这些特征集中在人-机器人对话中的影响和更相关的方面。该方法不仅获得了70.26\%的胜率，比现有的LLM(分别为Gemini-1.5-Pro和GPT-3.5)提高了22.16\%到48.30\%，而且还增强了对令牌操纵恶意攻击的健壮性，其突出表现是胜率比纯文本语言模型降低22.44\%。这标志着社交机器人导航和更广泛的人-机器人交互方面的进步，将基于文本的指导与人-音频信息语言模型无缝地结合在一起。



## **25. JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models**

越狱长凳：越狱大型语言模型的开放鲁棒性基准 cs.CR

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2404.01318v2) [paper-pdf](http://arxiv.org/pdf/2404.01318v2)

**Authors**: Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, Eric Wong

**Abstract**: Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work -- which align with OpenAI's usage policies; (3) a standardized evaluation framework that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community. Over time, we will expand and adapt the benchmark to reflect technical and methodological advances in the research community.

摘要: 越狱攻击会导致大型语言模型(LLM)生成有害、不道德或令人反感的内容。评估这些攻击带来了许多挑战，目前收集的基准和评估技术没有充分解决这些挑战。首先，关于越狱评估没有明确的实践标准。其次，现有的工作以无与伦比的方式计算成本和成功率。第三，许多作品是不可复制的，因为它们保留了对抗性提示，涉及封闭源代码，或者依赖于不断发展的专有API。为了应对这些挑战，我们引入了JailBreak Bch，这是一款开源基准测试，具有以下组件：(1)不断发展的最新对抗性提示存储库，我们称之为越狱人工产物；(2)包含100种行为的越狱数据集，包括原始行为和源自先前工作的行为，这些行为与OpenAI的使用策略保持一致；(3)标准化评估框架，其中包括明确定义的威胁模型、系统提示、聊天模板和评分功能；以及(4)跟踪各种LLM攻击和防御性能的排行榜。我们已仔细考虑发布这一基准的潜在道德影响，并相信它将为社会带来净积极的影响。随着时间的推移，我们将扩大和调整基准，以反映研究界的技术和方法进步。



## **26. Differentially-Private Data Synthetisation for Efficient Re-Identification Risk Control**

差异私密数据合成，实现高效的重新识别风险控制 cs.LG

21 pages, 6 figures and 2 tables

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2212.00484v3) [paper-pdf](http://arxiv.org/pdf/2212.00484v3)

**Authors**: Tânia Carvalho, Nuno Moniz, Luís Antunes, Nitesh Chawla

**Abstract**: Protecting user data privacy can be achieved via many methods, from statistical transformations to generative models. However, all of them have critical drawbacks. For example, creating a transformed data set using traditional techniques is highly time-consuming. Also, recent deep learning-based solutions require significant computational resources in addition to long training phases, and differentially private-based solutions may undermine data utility. In this paper, we propose $\epsilon$-PrivateSMOTE, a technique designed for safeguarding against re-identification and linkage attacks, particularly addressing cases with a high \sloppy re-identification risk. Our proposal combines synthetic data generation via noise-induced interpolation with differential privacy principles to obfuscate high-risk cases. We demonstrate how $\epsilon$-PrivateSMOTE is capable of achieving competitive results in privacy risk and better predictive performance when compared to multiple traditional and state-of-the-art privacy-preservation methods, including generative adversarial networks, variational autoencoders, and differential privacy baselines. We also show how our method improves time requirements by at least a factor of 9 and is a resource-efficient solution that ensures high performance without specialised hardware.

摘要: 保护用户数据隐私可以通过许多方法实现，从统计转换到生成模型。然而，它们都有严重的缺陷。例如，使用传统技术创建转换后的数据集非常耗时。此外，最近基于深度学习的解决方案除了需要较长的培训阶段外，还需要大量的计算资源，而不同的基于私人的解决方案可能会破坏数据效用。在本文中，我们提出了$-PrivateSMOTE，这是一种旨在防止重新识别和链接攻击的技术，特别是针对重新识别风险较高的情况。我们的方案将通过噪声诱导内插生成的合成数据与差分隐私原则相结合来混淆高危案例。我们展示了$\epsilon$-PrivateSMOTE与多种传统和最先进的隐私保护方法相比，如何能够在隐私风险和更好的预测性能方面实现竞争结果，这些方法包括生成性对抗网络、可变自动编码器和差异隐私基线。我们还展示了我们的方法如何将时间需求提高至少9倍，并且是一种资源高效的解决方案，无需专门的硬件即可确保高性能。



## **27. Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks**

基于图神经网络的网络入侵检测系统的问题空间结构对抗攻击 cs.CR

preprint submitted to IEEE TIFS, under review

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2403.11830v2) [paper-pdf](http://arxiv.org/pdf/2403.11830v2)

**Authors**: Andrea Venturi, Dario Stabili, Mirco Marchetti

**Abstract**: Machine Learning (ML) algorithms have become increasingly popular for supporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive research has shown their vulnerability to adversarial attacks, which involve subtle perturbations to the inputs of the models aimed at compromising their performance. Recent proposals have effectively leveraged Graph Neural Networks (GNN) to produce predictions based also on the structural patterns exhibited by intrusions to enhance the detection robustness. However, the adoption of GNN-based NIDS introduces new types of risks. In this paper, we propose the first formalization of adversarial attacks specifically tailored for GNN in network intrusion detection. Moreover, we outline and model the problem space constraints that attackers need to consider to carry out feasible structural attacks in real-world scenarios. As a final contribution, we conduct an extensive experimental campaign in which we launch the proposed attacks against state-of-the-art GNN-based NIDS. Our findings demonstrate the increased robustness of the models against classical feature-based adversarial attacks, while highlighting their susceptibility to structure-based attacks.

摘要: 机器学习(ML)算法因支持网络入侵检测系统(NIDS)而变得越来越流行。然而，广泛的研究表明，它们在对抗性攻击中的脆弱性，这涉及到对模型的输入进行微妙的扰动，目的是降低它们的性能。最近的建议已经有效地利用图神经网络(GNN)来产生基于入侵表现出的结构模式的预测，以增强检测的稳健性。然而，采用基于GNN的网络入侵检测系统带来了新的风险类型。在本文中，我们首次提出了网络入侵检测中专门针对GNN的对抗性攻击的形式化描述。此外，我们概述并模拟了攻击者在现实世界场景中执行可行的结构性攻击所需考虑的问题空间约束。作为最后的贡献，我们进行了一项广泛的实验活动，在该活动中，我们对最先进的基于GNN的网络入侵检测系统发起了拟议的攻击。我们的研究结果表明，该模型在抵抗经典的基于特征的对抗性攻击时具有更强的稳健性，同时突出了它们对基于结构的攻击的敏感性。



## **28. ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations**

ALI-DPFL：具有自适应本地迭代的差异化私人联邦学习 cs.LG

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2308.10457v6) [paper-pdf](http://arxiv.org/pdf/2308.10457v6)

**Authors**: Xinpeng Ling, Jie Fu, Kuncan Wang, Haitao Liu, Zhili Chen

**Abstract**: Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks.   We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication rounds are constrained. By theoretically analyzing the convergence, we can find the optimal number of local DPSGD iterations for clients between any two sequential global updates. Based on this, we design an algorithm of Differentially Private Federated Learning with Adaptive Local Iterations (ALI-DPFL). We experiment our algorithm on the MNIST, FashionMNIST and Cifar10 datasets, and demonstrate significantly better performances than previous work in the resource-constraint scenario. Code is available at https://github.com/KnightWan/ALI-DPFL.

摘要: 联合学习(FL)是一种分布式机器学习技术，通过共享训练参数而不是原始数据，允许在多个设备或组织之间进行模型训练。然而，攻击者仍然可以通过对这些训练参数的推理攻击(例如差异攻击)来推断个人信息。因此，差分隐私(DP)被广泛应用于FL中以防止此类攻击。我们考虑在资源受限的情况下进行不同的私有联合学习，其中隐私预算和通信回合都受到限制。通过对收敛的理论分析，我们可以找到任意两个连续全局更新之间客户端的最优局部DPSGD迭代次数。在此基础上，设计了一种基于自适应局部迭代的差分私有联邦学习算法(ALI-DPFL)。我们在MNIST、FashionMNIST和Cifar10数据集上测试了我们的算法，并在资源受限的情况下展示了比以前的工作更好的性能。代码可在https://github.com/KnightWan/ALI-DPFL.上找到



## **29. Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models**

扰动注意力为您带来更多好处：有效愚弄定制扩散模型的微妙成像扰动 cs.CV

Published at CVPR 2024

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2404.15081v1) [paper-pdf](http://arxiv.org/pdf/2404.15081v1)

**Authors**: Jingyao Xu, Yuetong Lu, Yandong Li, Siyang Lu, Dongdong Wang, Xiang Wei

**Abstract**: Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples. However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability. We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs). The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images. We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models. Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner.

摘要: 扩散模型开启了产生式建模的新时代，为高效地生成高质量和真实的数据样本提供了更多的机会。然而，它们的广泛使用也给模型安全带来了新的挑战，这促使在DM上创建更有效的对抗性攻击者来了解其脆弱性。我们提出了CAAT，这是一种简单但通用和高效的方法，不需要昂贵的培训来有效地愚弄潜在扩散模型(LDM)。该方法的基础是观察到交叉注意层对梯度变化表现出更高的敏感性，允许利用发布图像上的细微扰动来显著破坏生成的图像。我们发现，在定制扩散模型的微调过程中，图像上的细微扰动会显著影响交叉注意层，从而改变文本和图像之间的映射。大量的实验表明，CAAT与多种扩散模型兼容，并且在更有效(更多噪声)和更高效(速度是Anti-DreamBooth和Mist的两倍)方面优于基线攻击方法。



## **30. Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure**

具有不确定节点特征和不确定图结构的图卷积网络的形式化验证 cs.LG

under review

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2404.15065v1) [paper-pdf](http://arxiv.org/pdf/2404.15065v1)

**Authors**: Tobias Ladner, Michael Eichelbeck, Matthias Althoff

**Abstract**: Graph neural networks are becoming increasingly popular in the field of machine learning due to their unique ability to process data structured in graphs. They have also been applied in safety-critical environments where perturbations inherently occur. However, these perturbations require us to formally verify neural networks before their deployment in safety-critical environments as neural networks are prone to adversarial attacks. While there exists research on the formal verification of neural networks, there is no work verifying the robustness of generic graph convolutional network architectures with uncertainty in the node features and in the graph structure over multiple message-passing steps. This work addresses this research gap by explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes. We demonstrate our approach on three popular benchmark datasets.

摘要: 图神经网络因其处理以图结构化的数据的独特能力而在机器学习领域变得越来越受欢迎。它们还应用于固有地发生扰动的安全关键环境中。然而，这些扰动需要我们在将神经网络部署到安全关键环境中之前对其进行正式验证，因为神经网络容易受到对抗性攻击。虽然存在关于神经网络形式验证的研究，但还没有任何工作验证通用图卷积网络架构的稳健性，因为节点特征和多个消息传递步骤中的图结构存在不确定性。这项工作通过使用（矩阵）多项分区的可达性分析明确保留基础计算中所有元素的非凸依赖性来解决这一研究空白。我们在三个流行的基准数据集上展示了我们的方法。



## **31. Manipulating Recommender Systems: A Survey of Poisoning Attacks and Countermeasures**

操纵推荐系统：中毒攻击及对策调查 cs.CR

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2404.14942v1) [paper-pdf](http://arxiv.org/pdf/2404.14942v1)

**Authors**: Thanh Toan Nguyen, Quoc Viet Hung Nguyen, Thanh Tam Nguyen, Thanh Trung Huynh, Thanh Thi Nguyen, Matthias Weidlich, Hongzhi Yin

**Abstract**: Recommender systems have become an integral part of online services to help users locate specific information in a sea of data. However, existing studies show that some recommender systems are vulnerable to poisoning attacks, particularly those that involve learning schemes. A poisoning attack is where an adversary injects carefully crafted data into the process of training a model, with the goal of manipulating the system's final recommendations. Based on recent advancements in artificial intelligence, such attacks have gained importance recently. While numerous countermeasures to poisoning attacks have been developed, they have not yet been systematically linked to the properties of the attacks. Consequently, assessing the respective risks and potential success of mitigation strategies is difficult, if not impossible. This survey aims to fill this gap by primarily focusing on poisoning attacks and their countermeasures. This is in contrast to prior surveys that mainly focus on attacks and their detection methods. Through an exhaustive literature review, we provide a novel taxonomy for poisoning attacks, formalise its dimensions, and accordingly organise 30+ attacks described in the literature. Further, we review 40+ countermeasures to detect and/or prevent poisoning attacks, evaluating their effectiveness against specific types of attacks. This comprehensive survey should serve as a point of reference for protecting recommender systems against poisoning attacks. The article concludes with a discussion on open issues in the field and impactful directions for future research. A rich repository of resources associated with poisoning attacks is available at https://github.com/tamlhp/awesome-recsys-poisoning.

摘要: 推荐系统已经成为帮助用户在海量数据中定位特定信息的在线服务的组成部分。然而，现有的研究表明，一些推荐系统容易受到中毒攻击，特别是那些涉及学习方案的系统。中毒攻击是指对手在训练模型的过程中注入精心制作的数据，目的是操纵系统的最终建议。基于人工智能的最新进展，这类攻击最近变得越来越重要。虽然已经制定了许多针对中毒攻击的对策，但它们尚未系统地与攻击的性质联系起来。因此，评估缓解战略的各自风险和潜在成功是困难的，如果不是不可能的话。这项调查旨在通过主要关注中毒攻击及其对策来填补这一空白。这与以往主要关注攻击及其检测方法的调查形成了鲜明对比。通过详尽的文献回顾，我们为中毒攻击提供了一种新的分类，确定了其规模，并相应地组织了文献中描述的30+次攻击。此外，我们还回顾了40多种检测和/或预防中毒攻击的对策，评估了它们对特定类型攻击的有效性。这一全面的调查应该作为保护推荐系统免受中毒攻击的参考点。文章最后对该领域存在的问题进行了讨论，并指出了未来研究的方向。Https://github.com/tamlhp/awesome-recsys-poisoning.上提供了与中毒攻击相关的丰富资源存储库



## **32. Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack**

抗模型倒置攻击的隐私保护人脸识别自适应混合掩蔽策略 cs.CV

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2403.10558v2) [paper-pdf](http://arxiv.org/pdf/2403.10558v2)

**Authors**: Yinggui Wang, Yuanqing Huang, Jianshu Li, Le Yang, Kai Song, Lei Wang

**Abstract**: The utilization of personal sensitive data in training face recognition (FR) models poses significant privacy concerns, as adversaries can employ model inversion attacks (MIA) to infer the original training data. Existing defense methods, such as data augmentation and differential privacy, have been employed to mitigate this issue. However, these methods often fail to strike an optimal balance between privacy and accuracy. To address this limitation, this paper introduces an adaptive hybrid masking algorithm against MIA. Specifically, face images are masked in the frequency domain using an adaptive MixUp strategy. Unlike the traditional MixUp algorithm, which is predominantly used for data augmentation, our modified approach incorporates frequency domain mixing. Previous studies have shown that increasing the number of images mixed in MixUp can enhance privacy preservation but at the expense of reduced face recognition accuracy. To overcome this trade-off, we develop an enhanced adaptive MixUp strategy based on reinforcement learning, which enables us to mix a larger number of images while maintaining satisfactory recognition accuracy. To optimize privacy protection, we propose maximizing the reward function (i.e., the loss function of the FR system) during the training of the strategy network. While the loss function of the FR network is minimized in the phase of training the FR network. The strategy network and the face recognition network can be viewed as antagonistic entities in the training process, ultimately reaching a more balanced trade-off. Experimental results demonstrate that our proposed hybrid masking scheme outperforms existing defense algorithms in terms of privacy preservation and recognition accuracy against MIA.

摘要: 在训练人脸识别(FR)模型中使用个人敏感数据引起了严重的隐私问题，因为攻击者可以使用模型反转攻击(MIA)来推断原始训练数据。现有的防御方法，如数据增强和差异隐私，已经被用来缓解这个问题。然而，这些方法往往无法在隐私和准确性之间取得最佳平衡。针对这一局限性，提出了一种针对MIA的自适应混合掩蔽算法。具体地说，人脸图像在频域中使用自适应混合策略进行掩蔽。与主要用于数据增强的传统混合算法不同，我们的改进方法结合了频域混合。以前的研究表明，增加混合图像的数量可以增强隐私保护，但代价是降低人脸识别的准确性。为了克服这种权衡，我们开发了一种基于强化学习的增强的自适应混合策略，它使我们能够混合更多的图像，同时保持令人满意的识别精度。为了优化隐私保护，我们提出在策略网络的训练过程中最大化奖励函数(即FR系统的损失函数)。在训练FR网络的过程中，使FR网络的损失函数最小。在训练过程中，策略网络和人脸识别网络可以被视为对立的实体，最终达到更平衡的权衡。实验结果表明，本文提出的混合掩蔽方案在隐私保护和MIA识别准确率方面优于已有的防御算法。



## **33. Double Privacy Guard: Robust Traceable Adversarial Watermarking against Face Recognition**

双重隐私保护：针对人脸识别的稳健可追溯对抗水印 cs.CR

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2404.14693v1) [paper-pdf](http://arxiv.org/pdf/2404.14693v1)

**Authors**: Yunming Zhang, Dengpan Ye, Sipeng Shen, Caiyun Xie, Ziyi Liu, Jiacheng Deng, Long Tang

**Abstract**: The wide deployment of Face Recognition (FR) systems poses risks of privacy leakage. One countermeasure to address this issue is adversarial attacks, which deceive malicious FR searches but simultaneously interfere the normal identity verification of trusted authorizers. In this paper, we propose the first Double Privacy Guard (DPG) scheme based on traceable adversarial watermarking. DPG employs a one-time watermark embedding to deceive unauthorized FR models and allows authorizers to perform identity verification by extracting the watermark. Specifically, we propose an information-guided adversarial attack against FR models. The encoder embeds an identity-specific watermark into the deep feature space of the carrier, guiding recognizable features of the image to deviate from the source identity. We further adopt a collaborative meta-optimization strategy compatible with sub-tasks, which regularizes the joint optimization direction of the encoder and decoder. This strategy enhances the representation of universal carrier features, mitigating multi-objective optimization conflicts in watermarking. Experiments confirm that DPG achieves significant attack success rates and traceability accuracy on state-of-the-art FR models, exhibiting remarkable robustness that outperforms the existing privacy protection methods using adversarial attacks and deep watermarking, or simple combinations of the two. Our work potentially opens up new insights into proactive protection for FR privacy.

摘要: 人脸识别(FR)系统的广泛应用带来了隐私泄露的风险。解决这个问题的一种对策是对抗性攻击，它欺骗恶意FR搜索，但同时干扰受信任授权者的正常身份验证。本文提出了第一个基于可追踪对抗水印的双重隐私保护(DPG)方案。DPG采用一次性水印嵌入来欺骗未经授权的FR模型，并允许授权者通过提取水印来进行身份验证。具体地说，我们提出了一种针对FR模型的信息制导的对抗性攻击。编码器将特定于身份的水印嵌入到载体的深层特征空间中，引导图像的可识别特征偏离源身份。进一步采用了和子任务兼容的协作元优化策略，规范了编解码器的联合优化方向。该策略增强了对通用载体特征的表示，缓解了水印中的多目标优化冲突。实验证实，DPG在最先进的FR模型上获得了显著的攻击成功率和可追踪性准确性，表现出显著的稳健性，其性能优于现有的使用对抗性攻击和深度水印的隐私保护方法，或两者的简单组合。我们的工作可能为主动保护FR隐私打开新的洞察力。



## **34. Pseudorandom Permutations from Random Reversible Circuits**

随机可逆电路的伪随机排列 cs.CC

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2404.14648v1) [paper-pdf](http://arxiv.org/pdf/2404.14648v1)

**Authors**: William He, Ryan O'Donnell

**Abstract**: We study pseudorandomness properties of permutations on $\{0,1\}^n$ computed by random circuits made from reversible $3$-bit gates (permutations on $\{0,1\}^3$). Our main result is that a random circuit of depth $n \cdot \tilde{O}(k^2)$, with each layer consisting of $\approx n/3$ random gates in a fixed nearest-neighbor architecture, yields almost $k$-wise independent permutations. The main technical component is showing that the Markov chain on $k$-tuples of $n$-bit strings induced by a single random $3$-bit nearest-neighbor gate has spectral gap at least $1/n \cdot \tilde{O}(k)$. This improves on the original work of Gowers [Gowers96], who showed a gap of $1/\mathrm{poly}(n,k)$ for one random gate (with non-neighboring inputs); and, on subsequent work [HMMR05,BH08] improving the gap to $\Omega(1/n^2k)$ in the same setting.   From the perspective of cryptography, our result can be seen as a particularly simple/practical block cipher construction that gives provable statistical security against attackers with access to $k$~input-output pairs within few rounds. We also show that the Luby--Rackoff construction of pseudorandom permutations from pseudorandom functions can be implemented with reversible circuits. From this, we make progress on the complexity of the Minimum Reversible Circuit Size Problem (MRCSP), showing that block ciphers of fixed polynomial size are computationally secure against arbitrary polynomial-time adversaries, assuming the existence of one-way functions (OWFs).

摘要: 我们研究了由可逆$3$位门($0，1^3$上的置换)构成的随机电路计算的$0，1^n上置换的伪随机性。我们的主要结果是，一个深度为$n\cot\tide{O}(k^2)$的随机电路，每一层由固定最近邻体系结构中的$\约n/3$随机门组成，产生几乎$k$方向的独立排列。主要的技术内容是证明了由单个随机的$3$比特最近邻门产生的$n$比特串的$k$-元组上的马尔可夫链至少有$1/n\cdot\tilde{O}(K)$。这比Gowers[Gowers96]的原始工作有所改进，Gowers[Gowers96]对一个随机门(具有非相邻输入)显示了$1/\mathm{pol}(n，k)$的差距；在随后的工作[HMMR05，BH08]中，在相同设置下将差距改进为$\Omega(1/n^2k)$。从密码学的角度来看，我们的结果可以看作是一种特别简单实用的分组密码构造，它提供了针对在几轮内访问$k$~输入输出对的攻击者的可证明的统计安全性。我们还证明了伪随机函数的伪随机置换的Luby-Rackoff构造可以用可逆电路实现。由此，我们在最小可逆电路大小问题(MRCSP)的复杂性方面取得了进展，表明在假设存在单向函数(OWF)的情况下，固定多项式大小的分组密码在计算上是安全的，可以抵抗任意多项式时间的攻击者。



## **35. RETVec: Resilient and Efficient Text Vectorizer**

RETVec：弹性且高效的文本Vectorizer cs.CL

37th Conference on Neural Information Processing Systems (NeurIPS  2023)

**SubmitDate**: 2024-04-23    [abs](http://arxiv.org/abs/2302.09207v3) [paper-pdf](http://arxiv.org/pdf/2302.09207v3)

**Authors**: Elie Bursztein, Marina Zhang, Owen Vallis, Xinyu Jia, Alexey Kurakin

**Abstract**: This paper describes RETVec, an efficient, resilient, and multilingual text vectorizer designed for neural-based text processing. RETVec combines a novel character encoding with an optional small embedding model to embed words into a 256-dimensional vector space. The RETVec embedding model is pre-trained using pair-wise metric learning to be robust against typos and character-level adversarial attacks. In this paper, we evaluate and compare RETVec to state-of-the-art vectorizers and word embeddings on popular model architectures and datasets. These comparisons demonstrate that RETVec leads to competitive, multilingual models that are significantly more resilient to typos and adversarial text attacks. RETVec is available under the Apache 2 license at https://github.com/google-research/retvec.

摘要: 本文描述了RETVec，这是一种高效、有弹性、多语言的文本载体器，专为基于神经的文本处理而设计。RETVec将新颖的字符编码与可选的小型嵌入模型相结合，将单词嵌入到256维载体空间中。RETVec嵌入模型是使用成对度量学习进行预训练的，以对抗错别字和字符级对抗攻击。在本文中，我们评估并比较了RETVec与流行模型架构和数据集上的最先进的载体器和单词嵌入。这些比较表明，RETVec带来了有竞争力的多语言模型，这些模型对拼写错误和对抗性文本攻击的弹性明显更强。RETVec可在https://github.com/google-research/retvec上使用Apach2许可证。



## **36. Image Hijacks: Adversarial Images can Control Generative Models at Runtime**

图像劫持：对抗图像可以随时控制生成模型 cs.LG

Project page at https://image-hijacks.github.io

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2309.00236v3) [paper-pdf](http://arxiv.org/pdf/2309.00236v3)

**Authors**: Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons

**Abstract**: Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.

摘要: 基础模型针对恶意行为者是否安全？在这项工作中，我们关注的是图像输入到视觉语言模型(VLM)。我们发现了图像劫持，即在推理时控制VLM行为的敌意图像，并介绍了用于训练图像劫持的通用行为匹配算法。从这里，我们得到了提示匹配方法，允许我们使用与我们选择的提示无关的通用现成数据集来训练与任意用户定义的文本提示(例如‘埃菲尔铁塔现在位于罗马’)的行为匹配的劫持者。我们使用行为匹配来为四种类型的攻击制作劫持，迫使VLM生成对手选择的输出，从他们的上下文窗口泄露信息，覆盖他们的安全培训，并相信虚假陈述。我们对基于CLIP和LLAMA-2的最新VLM LLaVA进行了研究，发现所有类型的攻击都达到了80%以上的成功率。此外，我们的攻击是自动化的，只需要很小的图像扰动。



## **37. An Adversarial Approach to Evaluating the Robustness of Event Identification Models**

评估事件识别模型稳健性的对抗方法 eess.SY

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2402.12338v2) [paper-pdf](http://arxiv.org/pdf/2402.12338v2)

**Authors**: Obai Bahwal, Oliver Kosut, Lalitha Sankar

**Abstract**: Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classification model. Thorough experiments on the synthetic South Carolina 500-bus system highlight that a relatively simpler model such as logistic regression is more susceptible to adversarial attacks than gradient boosting.

摘要: 智能机器学习方法正在积极应用于事件检测和识别，从而实现实时的态势感知。然而，这种机器学习算法已被证明容易受到对传入遥测数据的敌意攻击。本文考虑了一种基于物理的模式分解方法来提取事件分类的特征，并重点使用Logistic回归和梯度提升等可解释分类器来区分两种类型的事件：负荷损失和发电损失。然后，将得到的分类器与对抗性算法进行测试，以评估它们的稳健性。在两种设置中测试对抗性攻击：白盒设置，其中攻击者确切地知道分类模型；以及灰盒设置，其中攻击者可以访问来自用于训练分类器的相同网络的历史数据，但不知道分类模型。在合成的南卡罗来纳州500母线系统上进行的彻底实验表明，相对简单的模型，如Logistic回归，比梯度助推更容易受到对抗性攻击。



## **38. Automatic Discovery of Visual Circuits**

视觉回路的自动发现 cs.CV

14 pages, 11 figures

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2404.14349v1) [paper-pdf](http://arxiv.org/pdf/2404.14349v1)

**Authors**: Achyuta Rajaram, Neil Chowdhury, Antonio Torralba, Jacob Andreas, Sarah Schwettmann

**Abstract**: To date, most discoveries of network subcomponents that implement human-interpretable computations in deep vision models have involved close study of single units and large amounts of human labor. We explore scalable methods for extracting the subgraph of a vision model's computational graph that underlies recognition of a specific visual concept. We introduce a new method for identifying these subgraphs: specifying a visual concept using a few examples, and then tracing the interdependence of neuron activations across layers, or their functional connectivity. We find that our approach extracts circuits that causally affect model output, and that editing these circuits can defend large pretrained models from adversarial attacks.

摘要: 迄今为止，大多数在深度视觉模型中实现人类可解释计算的网络子组件的发现都涉及对单个单元和大量人力的密切研究。我们探索提取视觉模型计算图的子图的可扩展方法，该计算图是特定视觉概念识别的基础。我们引入了一种识别这些子图的新方法：使用几个例子指定视觉概念，然后追踪各层神经元激活的相互依赖性，或其功能连接性。我们发现我们的方法提取了对模型输出产生因果影响的电路，并且编辑这些电路可以保护大型预训练模型免受对抗攻击。



## **39. Towards Better Adversarial Purification via Adversarial Denoising Diffusion Training**

通过对抗去噪扩散训练实现更好的对抗净化 cs.CV

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2404.14309v1) [paper-pdf](http://arxiv.org/pdf/2404.14309v1)

**Authors**: Yiming Liu, Kezhao Liu, Yao Xiao, Ziyi Dong, Xiaogang Xu, Pengxu Wei, Liang Lin

**Abstract**: Recently, diffusion-based purification (DBP) has emerged as a promising approach for defending against adversarial attacks. However, previous studies have used questionable methods to evaluate the robustness of DBP models, their explanations of DBP robustness also lack experimental support. We re-examine DBP robustness using precise gradient, and discuss the impact of stochasticity on DBP robustness. To better explain DBP robustness, we assess DBP robustness under a novel attack setting, Deterministic White-box, and pinpoint stochasticity as the main factor in DBP robustness. Our results suggest that DBP models rely on stochasticity to evade the most effective attack direction, rather than directly countering adversarial perturbations. To improve the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT). This technique uses Classifier-Guided Perturbation Optimization (CGPO) to generate adversarial perturbation through guidance from a pre-trained classifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial pertubation into a normal Gaussian distribution. Empirical results show that ADDT improves the robustness of DBP models. Further experiments confirm that ADDT equips DBP models with the ability to directly counter adversarial perturbations.

摘要: 近年来，基于扩散的纯化技术(DBP)已成为一种很有前途的防御敌意攻击的方法。然而，以前的研究使用了有问题的方法来评估DBP模型的稳健性，它们对DBP模型的稳健性的解释也缺乏实验支持。我们使用精确梯度重新检验了DBP稳健性，并讨论了随机性对DBP稳健性的影响。为了更好地解释DBP的稳健性，我们评估了一种新的攻击环境下的DBP稳健性，即确定性白盒，并指出随机性是影响DBP稳健性的主要因素。我们的结果表明，DBP模型依赖于随机性来避开最有效的攻击方向，而不是直接对抗对手的扰动。为了提高DBP模型的稳健性，我们提出了对抗性去噪扩散训练方法。该技术使用分类器引导的扰动优化(CGPO)通过预先训练的分类器的引导来产生对抗性扰动，并使用基于等级的高斯映射(RBGM)将对抗性扰动转换为正态高斯分布。实证结果表明，ADDT提高了DBP模型的稳健性。进一步的实验证实，ADDT使DBP模型具有直接对抗对抗性扰动的能力。



## **40. Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning**

利用强化学习进行黑匣子图像、视频和心电图信号分类的鲁棒性和视觉解释 cs.LG

AAAI Proceedings reference:  https://ojs.aaai.org/index.php/AAAI/article/view/30579

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2403.18985v2) [paper-pdf](http://arxiv.org/pdf/2403.18985v2)

**Authors**: Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Avisek Naug, Sahand Ghorbanpour

**Abstract**: We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.

摘要: 我们提出了一个通用的强化学习（RL）框架，经过优化，用于对来自心电图信号分析（1D）、图像分类（2D）和视频分类（3D）的不同模型类型进行对抗攻击。该框架的重点是识别敏感区域并以最小的失真和各种失真类型引发错误分类。对于所有三种应用，新型RL方法的性能都优于最先进的方法，证明了其效率。我们的RL方法产生了卓越的定位模板，增强了图像分类和心电图分析模型的可解释性。对于心电图分析等应用，我们的平台为临床医生突出显示关键的心电图段，同时确保针对普遍失真的弹性。这个全面的工具旨在通过对抗性培训和各种应用程序和数据类型的透明度来增强弹性。



## **41. Secure compilation of rich smart contracts on poor UTXO blockchains**

在较差的UTXO区块链上安全编写丰富的智能合同 cs.CR

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2305.09545v3) [paper-pdf](http://arxiv.org/pdf/2305.09545v3)

**Authors**: Massimo Bartoletti, Riccardo Marchesin, Roberto Zunino

**Abstract**: Most blockchain platforms from Ethereum onwards render smart contracts as stateful reactive objects that update their state and transfer crypto-assets in response to transactions. A drawback of this design is that when users submit a transaction, they cannot predict in which state it will be executed. This exposes them to transaction-ordering attacks, a widespread class of attacks where adversaries with the power to construct blocks of transactions can extract value from smart contracts (the so-called MEV attacks). The UTXO model is an alternative blockchain design that thwarts these attacks by requiring new transactions to spend past ones: since transactions have unique identifiers, reordering attacks are ineffective. Currently, the blockchains following the UTXO model either provide contracts with limited expressiveness (Bitcoin), or require complex run-time environments (Cardano). We present ILLUM , an Intermediate-Level Language for the UTXO Model. ILLUM can express real-world smart contracts, e.g. those found in Decentralized Finance. We define a compiler from ILLUM to a bare-bone UTXO blockchain with loop-free scripts. Our compilation target only requires minimal extensions to Bitcoin Script: in particular, we exploit covenants, a mechanism for preserving scripts along chains of transactions. We prove the security of our compiler: namely, any attack targeting the compiled contract is also observable at the ILLUM level. Hence, the compiler does not introduce new vulnerabilities that were not already present in the source ILLUM contract. We evaluate the practicality of ILLUM as a compilation target for higher-level languages. To this purpose, we implement a compiler from a contract language inspired by Solidity to ILLUM, and we apply it to a benchmark or real-world smart contracts.

摘要: 从Etherum开始，大多数区块链平台都将智能合约呈现为有状态的反应对象，这些对象更新其状态并传输加密资产以响应交易。这种设计的一个缺点是，当用户提交事务时，他们无法预测该事务将在哪种状态下执行。这使他们面临交易顺序攻击，这是一种广泛存在的攻击类别，在这种攻击中，有能力构建交易块的对手可以从智能合约中提取价值(所谓的MEV攻击)。UTXO模型是一种替代区块链设计，通过要求新交易花费过去的交易来挫败这些攻击：由于交易具有唯一标识符，重新排序攻击是无效的。目前，遵循UTXO模式的区块链要么提供表现力有限的合同(比特币)，要么需要复杂的运行时环境(Cardano)。我们介绍了Illum，一种用于UTXO模型的中级语言。Illum可以表示现实世界中的智能合约，例如在去中心化金融中找到的那些。我们定义了一个从Illum到具有无循环脚本的基本UTXO区块链的编译器。我们的编译目标只需要对比特币脚本进行最小程度的扩展：尤其是，我们利用了契诺，这是一种在交易链上保留脚本的机制。我们证明了我们的编译器的安全性：也就是说，任何针对已编译约定的攻击也可以在Illum级别上观察到。因此，编译器不会引入源Illum协定中尚未存在的新漏洞。我们评估了Illum作为高级语言编译目标的实用性。为此，我们实现了一个编译器，从一种受Solidity启发的契约语言到Illum，并将其应用于基准或现实世界的智能合约。



## **42. Protecting Your LLMs with Information Bottleneck**

通过信息瓶颈保护您的LLC cs.CL

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2404.13968v1) [paper-pdf](http://arxiv.org/pdf/2404.13968v1)

**Authors**: Zichuan Liu, Zefan Wang, Linjie Xu, Jinyu Wang, Lei Song, Tianchun Wang, Chunlin Chen, Wei Cheng, Jiang Bian

**Abstract**: The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content. Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts. To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions. The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer. Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM. Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.

摘要: 大型语言模型的出现给自然语言处理领域带来了革命性的变化，但它们可能会受到攻击，产生有害的内容。尽管努力在道德上调整LLM，但这些往往是脆弱的，可以通过优化或手动对抗性提示通过越狱攻击来绕过。为了解决这个问题，我们引入了信息瓶颈保护器(IBProtector)，这是一种基于信息瓶颈原理的防御机制，我们修改了目标以避免琐碎的解决方案。IBProtector有选择地压缩和干扰提示，由一个轻量级和可训练的提取程序促进，只保留目标LLMS的基本信息，以响应预期的答案。此外，我们还进一步考虑了梯度不可见的情况，以与任何LLM相容。我们的经验评估表明，在不过度影响响应质量或推理速度的情况下，IBProtector在缓解越狱企图方面优于现有的防御方法。它对各种攻击方法和目标LLM的有效性和适应性突显了IBProtector作为一种新型、可转移的防御系统的潜力，无需修改底层模型即可增强LLM的安全性。



## **43. Audio Anti-Spoofing Detection: A Survey**

音频反欺骗检测：调查 cs.SD

submitted to ACM Computing Surveys

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2404.13914v1) [paper-pdf](http://arxiv.org/pdf/2404.13914v1)

**Authors**: Menglu Li, Yasaman Ahmadiadli, Xiao-Ping Zhang

**Abstract**: The availability of smart devices leads to an exponential increase in multimedia content. However, the rapid advancements in deep learning have given rise to sophisticated algorithms capable of manipulating or creating multimedia fake content, known as Deepfake. Audio Deepfakes pose a significant threat by producing highly realistic voices, thus facilitating the spread of misinformation. To address this issue, numerous audio anti-spoofing detection challenges have been organized to foster the development of anti-spoofing countermeasures. This survey paper presents a comprehensive review of every component within the detection pipeline, including algorithm architectures, optimization techniques, application generalizability, evaluation metrics, performance comparisons, available datasets, and open-source availability. For each aspect, we conduct a systematic evaluation of the recent advancements, along with discussions on existing challenges. Additionally, we also explore emerging research topics on audio anti-spoofing, including partial spoofing detection, cross-dataset evaluation, and adversarial attack defence, while proposing some promising research directions for future work. This survey paper not only identifies the current state-of-the-art to establish strong baselines for future experiments but also guides future researchers on a clear path for understanding and enhancing the audio anti-spoofing detection mechanisms.

摘要: 智能设备的出现导致多媒体内容呈指数级增长。然而，深度学习的快速发展催生了能够操纵或创建多媒体假内容的复杂算法，即Deepfac。音频Deepfake会产生高度逼真的声音，从而为错误信息的传播提供便利，从而构成重大威胁。为了解决这个问题，已经组织了许多音频反欺骗检测挑战，以促进反欺骗对策的发展。本文对检测流水线中的每个组件进行了全面的回顾，包括算法体系结构、优化技术、应用程序通用性、评估指标、性能比较、可用的数据集和开放源码可用性。对于每个方面，我们都对最近的进展进行了系统的评估，同时讨论了现有的挑战。此外，我们还探讨了音频反欺骗的新兴研究课题，包括部分欺骗检测、跨数据集评估和对抗性攻击防御，并对未来的工作提出了一些有前景的研究方向。这份调查报告不仅确定了当前的最新水平，为未来的实验建立了坚实的基线，而且也为未来的研究人员提供了一条理解和增强音频反欺骗检测机制的明确途径。



## **44. Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs**

竞争报告：在一致的LLC中寻找通用越狱后门 cs.CL

Competition Report

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2404.14461v1) [paper-pdf](http://arxiv.org/pdf/2404.14461v1)

**Authors**: Javier Rando, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion, Florian Tramèr

**Abstract**: Large language models are aligned to be safe, preventing users from generating harmful content like misinformation or instructions for illegal activities. However, previous work has shown that the alignment process is vulnerable to poisoning attacks. Adversaries can manipulate the safety training data to inject backdoors that act like a universal sudo command: adding the backdoor string to any prompt enables harmful responses from models that, otherwise, behave safely. Our competition, co-located at IEEE SaTML 2024, challenged participants to find universal backdoors in several large language models. This report summarizes the key findings and promising ideas for future research.

摘要: 大型语言模型经过调整以确保安全，防止用户生成错误信息或非法活动指令等有害内容。然而，之前的工作表明，对齐过程很容易受到中毒攻击。对手可以操纵安全训练数据来注入类似于通用sudo命令的后门：将后门字符串添加到任何提示中都会导致模型做出有害响应，否则这些模型会安全地运行。我们的竞赛在IEEE SaTML 2024上举行，挑战参与者在几个大型语言模型中找到通用后门。本报告总结了关键发现和未来研究的有希望的想法。



## **45. Distributional Black-Box Model Inversion Attack with Multi-Agent Reinforcement Learning**

基于多智能体强化学习的分布式黑匣子模型翻转攻击 cs.LG

**SubmitDate**: 2024-04-22    [abs](http://arxiv.org/abs/2404.13860v1) [paper-pdf](http://arxiv.org/pdf/2404.13860v1)

**Authors**: Huan Bao, Kaimin Wei, Yongdong Wu, Jin Qian, Robert H. Deng

**Abstract**: A Model Inversion (MI) attack based on Generative Adversarial Networks (GAN) aims to recover the private training data from complex deep learning models by searching codes in the latent space. However, they merely search a deterministic latent space such that the found latent code is usually suboptimal. In addition, the existing distributional MI schemes assume that an attacker can access the structures and parameters of the target model, which is not always viable in practice. To overcome the above shortcomings, this paper proposes a novel Distributional Black-Box Model Inversion (DBB-MI) attack by constructing the probabilistic latent space for searching the target privacy data. Specifically, DBB-MI does not need the target model parameters or specialized GAN training. Instead, it finds the latent probability distribution by combining the output of the target model with multi-agent reinforcement learning techniques. Then, it randomly chooses latent codes from the latent probability distribution for recovering the private data. As the latent probability distribution closely aligns with the target privacy data in latent space, the recovered data will leak the privacy of training samples of the target model significantly. Abundant experiments conducted on diverse datasets and networks show that the present DBB-MI has better performance than state-of-the-art in attack accuracy, K-nearest neighbor feature distance, and Peak Signal-to-Noise Ratio.

摘要: 基于产生式对抗网络(GAN)的模型反转(MI)攻击旨在通过在潜在空间中搜索码来恢复复杂深度学习模型中的私有训练数据。然而，它们只是搜索确定性的潜在空间，因此发现的潜在代码通常是次优的。此外，现有的分布式MI方案假设攻击者可以访问目标模型的结构和参数，这在实践中并不总是可行的。为了克服上述不足，通过构造搜索目标隐私数据的概率潜在空间，提出了一种新的分布式黑盒模型反转(DBB-MI)攻击。具体地说，DBB-MI不需要目标模型参数或专门的GaN训练。相反，它通过将目标模型的输出与多智能体强化学习技术相结合来寻找潜在概率分布。然后，从潜在概率分布中随机选择潜在代码来恢复私有数据。由于潜在概率分布在潜在空间中与目标隐私数据密切相关，恢复后的数据会严重泄露目标模型训练样本的隐私。在不同的数据集和网络上进行的大量实验表明，本文的DBB-MI在攻击准确率、K近邻特征距离和峰值信噪比方面都优于现有的DBB-MI。



## **46. Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models**

避免扩散模型中概念抑制的概念算法 cs.CV

**SubmitDate**: 2024-04-21    [abs](http://arxiv.org/abs/2404.13706v1) [paper-pdf](http://arxiv.org/pdf/2404.13706v1)

**Authors**: Vitali Petsiuk, Kate Saenko

**Abstract**: Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models.   Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised.   Project page: https://cs-people.bu.edu/vpetsiuk/arc

摘要: 出于伦理和法律方面的考虑，科学界正在积极开发方法，以限制滥用文本到图像的传播模式，在生成的图像中复制受版权保护的、暴力的、露骨的或个人信息。与此同时，研究人员通过扮演对手的角色来测试这些新开发的安全措施，以发现其中的漏洞和后门。我们使用扩散模型的合成属性，允许在单个图像生成中利用多个提示。这一性质允许我们组合不应该受到抑制影响的其他概念，以重建负责目标概念生成的向量，即使不再能够直接计算该向量。我们提供了理论和经验证据，解释了为什么建议的攻击是可能的，并讨论了这些发现对安全模型部署的影响。我们认为，重要的是要考虑所有可能的方法，利用扩散模型生成图像，以供对手使用。我们的工作开启了关于扩散模型中的安全机制的概念、算法和组合推理的含义的讨论。内容建议：本文包含可能被视为冒犯性的讨论和模型生成的内容。建议读者酌情阅读。项目页面：https://cs-people.bu.edu/vpetsiuk/arc



## **47. Large Language Models for Blockchain Security: A Systematic Literature Review**

区块链安全的大型语言模型：系统性文献综述 cs.CR

**SubmitDate**: 2024-04-21    [abs](http://arxiv.org/abs/2403.14280v3) [paper-pdf](http://arxiv.org/pdf/2403.14280v3)

**Authors**: Zheyuan He, Zihao Li, Sen Yang

**Abstract**: Large Language Models (LLMs) have emerged as powerful tools in various domains involving blockchain security (BS). Several recent studies are exploring LLMs applied to BS. However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of LLMs on blockchain security. To fill this gap, we conduct a literature review on LLM4BS.   As the first review of LLM's application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how LLMs contribute to enhancing the security of blockchain systems. Through a thorough examination of scholarly works, we delve into the integration of LLMs into various aspects of blockchain security. We explore the mechanisms through which LLMs can bolster blockchain security, including their applications in smart contract auditing, identity verification, anomaly detection, vulnerable repair, and so on. Furthermore, we critically assess the challenges and limitations associated with leveraging LLMs for blockchain security, considering factors such as scalability, privacy concerns, and adversarial attacks. Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.

摘要: 大型语言模型(LLM)在涉及区块链安全(BS)的各个领域中已成为强大的工具。最近的几项研究正在探索将LLMS应用于BS。然而，对于低成本管理的全部应用范围、影响以及对区块链安全的潜在限制，我们的理解仍然存在差距。为了填补这一空白，我们对LLM4BS进行了文献综述。作为LLM在区块链安全方面应用的首次综述，本研究旨在全面分析现有研究，阐明LLM如何为增强区块链系统的安全性做出贡献。通过对学术著作的深入研究，我们深入研究了LLMS在区块链安全的各个方面的整合。我们探讨了LLMS增强区块链安全的机制，包括它们在智能合同审计、身份验证、异常检测、漏洞修复等方面的应用。此外，考虑到可扩展性、隐私问题和敌意攻击等因素，我们严格评估了利用LLM实现区块链安全所面临的挑战和限制。我们的审查揭示了这种融合所固有的机遇和潜在风险，为研究人员、从业者和政策制定者提供了有价值的见解。



## **48. Attack on Scene Flow using Point Clouds**

使用点云攻击场景流 cs.CV

**SubmitDate**: 2024-04-21    [abs](http://arxiv.org/abs/2404.13621v1) [paper-pdf](http://arxiv.org/pdf/2404.13621v1)

**Authors**: Haniyeh Ehsani Oskouie, Mohammad-Shahram Moin, Shohreh Kasaei

**Abstract**: Deep neural networks have made significant advancements in accurately estimating scene flow using point clouds, which is vital for many applications like video analysis, action recognition, and navigation. Robustness of these techniques, however, remains a concern, particularly in the face of adversarial attacks that have been proven to deceive state-of-the-art deep neural networks in many domains. Surprisingly, the robustness of scene flow networks against such attacks has not been thoroughly investigated. To address this problem, the proposed approach aims to bridge this gap by introducing adversarial white-box attacks specifically tailored for scene flow networks. Experimental results show that the generated adversarial examples obtain up to 33.7 relative degradation in average end-point error on the KITTI and FlyingThings3D datasets. The study also reveals the significant impact that attacks targeting point clouds in only one dimension or color channel have on average end-point error. Analyzing the success and failure of these attacks on the scene flow networks and their 2D optical flow network variants show a higher vulnerability for the optical flow networks.

摘要: 深度神经网络在利用点云准确估计场景流量方面取得了重大进展，这对于视频分析、动作识别和导航等许多应用都是至关重要的。然而，这些技术的健壮性仍然是一个令人担忧的问题，特别是在面对已被证明在许多领域欺骗最先进的深度神经网络的对抗性攻击时。令人惊讶的是，场景流网络对此类攻击的健壮性还没有得到彻底的研究。为了解决这个问题，提出的方法旨在通过引入专门为场景流网络量身定做的对抗性白盒攻击来弥合这一差距。实验结果表明，生成的对抗性实例在Kitti和FlyingThings3D数据集上的平均端点误差相对下降高达33.7。研究还揭示了仅以一维或颜色通道中的点云为目标的攻击对平均端点误差的显著影响。分析这些攻击对场景流网络及其二维光流网络变体的成功和失败，表明光流网络具有更高的脆弱性。



## **49. Robust EEG-based Emotion Recognition Using an Inception and Two-sided Perturbation Model**

使用初始和双边扰动模型的鲁棒性基于脑电波的情绪识别 eess.SP

**SubmitDate**: 2024-04-21    [abs](http://arxiv.org/abs/2404.15373v1) [paper-pdf](http://arxiv.org/pdf/2404.15373v1)

**Authors**: Shadi Sartipi, Mujdat Cetin

**Abstract**: Automated emotion recognition using electroencephalogram (EEG) signals has gained substantial attention. Although deep learning approaches exhibit strong performance, they often suffer from vulnerabilities to various perturbations, like environmental noise and adversarial attacks. In this paper, we propose an Inception feature generator and two-sided perturbation (INC-TSP) approach to enhance emotion recognition in brain-computer interfaces. INC-TSP integrates the Inception module for EEG data analysis and employs two-sided perturbation (TSP) as a defensive mechanism against input perturbations. TSP introduces worst-case perturbations to the model's weights and inputs, reinforcing the model's elasticity against adversarial attacks. The proposed approach addresses the challenge of maintaining accurate emotion recognition in the presence of input uncertainties. We validate INC-TSP in a subject-independent three-class emotion recognition scenario, demonstrating robust performance.

摘要: 使用脑电波（EEG）信号的自动情感识别已引起广泛关注。尽管深度学习方法表现出出色的性能，但它们往往容易受到各种干扰的影响，例如环境噪音和对抗性攻击。在本文中，我们提出了一种初始特征生成器和双边扰动（INC-TBC）方法来增强脑机接口中的情感识别。INC-TPS集成了Incement模块用于脑电数据分析，并采用双边扰动（TBC）作为针对输入扰动的防御机制。TPS向模型的权重和输入引入了最坏情况的扰动，增强了模型对抗对抗攻击的弹性。所提出的方法解决了在存在输入不确定性的情况下保持准确情感识别的挑战。我们在与对象无关的三级情感识别场景中验证了INC-TPS，展示了稳健的性能。



## **50. How to Evaluate Semantic Communications for Images with ViTScore Metric?**

如何使用ViTScore指标评估图像的语义通信？ cs.CV

**SubmitDate**: 2024-04-21    [abs](http://arxiv.org/abs/2309.04891v2) [paper-pdf](http://arxiv.org/pdf/2309.04891v2)

**Authors**: Tingting Zhu, Bo Peng, Jifan Liang, Tingchen Han, Hai Wan, Jingqiao Fu, Junjie Chen

**Abstract**: Semantic communications (SC) have been expected to be a new paradigm shifting to catalyze the next generation communication, whose main concerns shift from accurate bit transmission to effective semantic information exchange in communications. However, the previous and widely-used metrics for images are not applicable to evaluate the image semantic similarity in SC. Classical metrics to measure the similarity between two images usually rely on the pixel level or the structural level, such as the PSNR and the MS-SSIM. Straightforwardly using some tailored metrics based on deep-learning methods in CV community, such as the LPIPS, is infeasible for SC. To tackle this, inspired by BERTScore in NLP community, we propose a novel metric for evaluating image semantic similarity, named Vision Transformer Score (ViTScore). We prove theoretically that ViTScore has 3 important properties, including symmetry, boundedness, and normalization, which make ViTScore convenient and intuitive for image measurement. To evaluate the performance of ViTScore, we compare ViTScore with 3 typical metrics (PSNR, MS-SSIM, and LPIPS) through 4 classes of experiments: (i) correlation with BERTScore through evaluation of image caption downstream CV task, (ii) evaluation in classical image communications, (iii) evaluation in image semantic communication systems, and (iv) evaluation in image semantic communication systems with semantic attack. Experimental results demonstrate that ViTScore is robust and efficient in evaluating the semantic similarity of images. Particularly, ViTScore outperforms the other 3 typical metrics in evaluating the image semantic changes by semantic attack, such as image inverse with Generative Adversarial Networks (GANs). This indicates that ViTScore is an effective performance metric when deployed in SC scenarios.

摘要: 语义通信(SC)被认为是一种新的范式转换，以催化下一代通信，其主要关注点从准确的比特传输转向通信中有效的语义信息交换。然而，以往广泛使用的图像度量方法不适用于SC中的图像语义相似度评价。衡量两幅图像之间相似性的经典度量通常依赖于像素级或结构级，如PSNR和MS-SSIM。对于供应链来说，直接使用一些基于深度学习方法的量身定做的度量方法，如LPIPS，是不可行的。针对这一问题，受自然语言处理领域BERTScore的启发，我们提出了一种新的图像语义相似度评价指标--视觉变换得分(ViTScore)。从理论上证明了ViTScore具有对称性、有界性和归一化三个重要性质，这使得ViTScore能够方便直观地进行图像测量。为了评估ViTScore的性能，我们通过4类实验将ViTScore与3种典型的度量指标(PSNR、MS-SSIM和LPIPS)进行了比较：(I)通过评估图像字幕下行CV任务与BERTScore的相关性；(Ii)在经典图像通信中的评估；(Iii)在图像语义通信系统中的评估；(Iv)在语义攻击下的图像语义通信系统中的评估。实验结果表明，ViTScore在评价图像语义相似度方面具有较强的鲁棒性和较高的效率。特别是，ViTScore在通过语义攻击来评估图像语义变化方面优于其他3种典型的度量标准，例如基于生成性对抗网络的图像逆(GANS)。这表明当部署在SC场景中时，ViTScore是一个有效的性能指标。



