# Latest Large Language Model Attack Papers
**update at 2024-03-20 15:23:18**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey**

LLM会话安全的攻击、防御与评估：一项调查 cs.CL

Accepted to NAACL 2024

**SubmitDate**: 2024-03-19    [abs](http://arxiv.org/abs/2402.09283v2) [paper-pdf](http://arxiv.org/pdf/2402.09283v2)

**Authors**: Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao

**Abstract**: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.

摘要: 大型语言模型（LLM）现在在会话应用中很常见。然而，它们被滥用以产生有害反应的风险引起了严重的社会关注，并刺激了最近对LLM会话安全的研究。因此，在本次调查中，我们提供了一个全面的概述最近的研究，涵盖了LLM会话安全的三个关键方面：攻击，防御和评估。我们的目标是提供一个结构化的摘要，以提高对LLM会话安全的理解，并鼓励进一步调查这一重要主题。为了便于参考，我们根据我们的分类法对本次调查中提到的所有研究进行了分类，可在www.example.com上查阅。



## **2. Review of Generative AI Methods in Cybersecurity**

网络安全中的生成性人工智能方法综述 cs.CR

40 pages

**SubmitDate**: 2024-03-19    [abs](http://arxiv.org/abs/2403.08701v2) [paper-pdf](http://arxiv.org/pdf/2403.08701v2)

**Authors**: Yagmur Yigit, William J Buchanan, Madjid G Tehrani, Leandros Maglaras

**Abstract**: Over the last decade, Artificial Intelligence (AI) has become increasingly popular, especially with the use of chatbots such as ChatGPT, Gemini, and DALL-E. With this rise, large language models (LLMs) and Generative AI (GenAI) have also become more prevalent in everyday use. These advancements strengthen cybersecurity's defensive posture and open up new attack avenues for adversaries as well. This paper provides a comprehensive overview of the current state-of-the-art deployments of GenAI, covering assaults, jailbreaking, and applications of prompt injection and reverse psychology. This paper also provides the various applications of GenAI in cybercrimes, such as automated hacking, phishing emails, social engineering, reverse cryptography, creating attack payloads, and creating malware. GenAI can significantly improve the automation of defensive cyber security processes through strategies such as dataset construction, safe code development, threat intelligence, defensive measures, reporting, and cyberattack detection. In this study, we suggest that future research should focus on developing robust ethical norms and innovative defense mechanisms to address the current issues that GenAI creates and to also further encourage an impartial approach to its future application in cybersecurity. Moreover, we underscore the importance of interdisciplinary approaches further to bridge the gap between scientific developments and ethical considerations.

摘要: 在过去的十年中，人工智能（AI）变得越来越受欢迎，特别是随着聊天机器人的使用，如ChatGPT，Gemini和DALL—E。随着这种增长，大型语言模型（LLM）和生成人工智能（GenAI）在日常使用中也变得越来越普遍。这些进步加强了网络安全的防御态势，并为对手开辟了新的攻击途径。本文全面概述了GenAI当前最先进的部署，涵盖攻击、越狱以及即时注射和逆向心理学的应用。本文还提供了GenAI在网络犯罪中的各种应用，如自动黑客攻击、网络钓鱼电子邮件、社会工程、反向加密、创建攻击有效载荷和创建恶意软件。GenAI可以通过数据集构建、安全代码开发、威胁情报、防御措施、报告和网络攻击检测等策略显著提高防御性网络安全流程的自动化。在这项研究中，我们建议未来的研究应侧重于制定健全的道德规范和创新的防御机制，以解决GenAI目前造成的问题，并进一步鼓励其在网络安全中的未来应用公正的方法。此外，我们强调跨学科方法的重要性，进一步弥合科学发展与伦理考虑之间的差距。



## **3. Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices**

保护大型语言模型：威胁、漏洞和负责任的做法 cs.CR

**SubmitDate**: 2024-03-19    [abs](http://arxiv.org/abs/2403.12503v1) [paper-pdf](http://arxiv.org/pdf/2403.12503v1)

**Authors**: Sara Abdali, Richard Anarfi, CJ Barberan, Jia He

**Abstract**: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.

摘要: 大型语言模型（LLM）显著改变了自然语言处理（NLP）的前景。它们的影响延伸到各种任务，彻底改变了我们处理语言理解和世代的方式。然而，除了其显著的实用性，LLM引入了关键的安全和风险考虑。这些挑战值得认真审查，以确保负责任地部署和防范潜在漏洞。本研究论文从五个主题角度彻底调查了与LLM相关的安全和隐私问题：安全和隐私问题，对抗攻击的漏洞，滥用LLM造成的潜在危害，缓解策略，以解决这些挑战，同时确定当前策略的局限性。最后，本文建议了未来研究的有希望的途径，以加强LLM的安全性和风险管理。



## **4. Large language models in 6G security: challenges and opportunities**

6G安全中的大型语言模型：挑战与机遇 cs.CR

29 pages, 2 figures

**SubmitDate**: 2024-03-18    [abs](http://arxiv.org/abs/2403.12239v1) [paper-pdf](http://arxiv.org/pdf/2403.12239v1)

**Authors**: Tri Nguyen, Huong Nguyen, Ahmad Ijaz, Saeid Sheikhi, Athanasios V. Vasilakos, Panos Kostakos

**Abstract**: The rapid integration of Generative AI (GenAI) and Large Language Models (LLMs) in sectors such as education and healthcare have marked a significant advancement in technology. However, this growth has also led to a largely unexplored aspect: their security vulnerabilities. As the ecosystem that includes both offline and online models, various tools, browser plugins, and third-party applications continues to expand, it significantly widens the attack surface, thereby escalating the potential for security breaches. These expansions in the 6G and beyond landscape provide new avenues for adversaries to manipulate LLMs for malicious purposes. We focus on the security aspects of LLMs from the viewpoint of potential adversaries. We aim to dissect their objectives and methodologies, providing an in-depth analysis of known security weaknesses. This will include the development of a comprehensive threat taxonomy, categorizing various adversary behaviors. Also, our research will concentrate on how LLMs can be integrated into cybersecurity efforts by defense teams, also known as blue teams. We will explore the potential synergy between LLMs and blockchain technology, and how this combination could lead to the development of next-generation, fully autonomous security solutions. This approach aims to establish a unified cybersecurity strategy across the entire computing continuum, enhancing overall digital security infrastructure.

摘要: 生成式人工智能(GenAI)和大型语言模型(LLM)在教育和医疗等领域的快速集成标志着技术的重大进步。然而，这种增长也导致了一个基本上未被探索的方面：它们的安全漏洞。随着包括离线和在线模型、各种工具、浏览器插件和第三方应用程序的生态系统不断扩大，它显著扩大了攻击面，从而增加了安全漏洞的可能性。这些在6G及以上领域的扩展为对手出于恶意目的操纵低层管理提供了新的途径。我们从潜在对手的角度来关注LLMS的安全方面。我们的目标是剖析它们的目标和方法，深入分析已知的安全弱点。这将包括开发一个全面的威胁分类法，对各种对手行为进行分类。此外，我们的研究将集中在如何将LLM整合到防御团队(也称为蓝色团队)的网络安全工作中。我们将探索LLMS和区块链技术之间的潜在协同效应，以及这种结合如何导致下一代完全自主的安全解决方案的开发。这一方法旨在整个计算连续体中建立统一的网络安全战略，增强整体数字安全基础设施。



## **5. Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models**

移动镜头：用大型语言模型检测npm生态系统中的恶意软件 cs.CR

13 pages, 1 Figure, 7 tables

**SubmitDate**: 2024-03-18    [abs](http://arxiv.org/abs/2403.12196v1) [paper-pdf](http://arxiv.org/pdf/2403.12196v1)

**Authors**: Nusrat Zahan, Philipp Burckhardt, Mikola Lysenko, Feross Aboukhadijeh, Laurie Williams

**Abstract**: The Gartner 2022 report predicts that 45% of organizations worldwide will encounter software supply chain attacks by 2025, highlighting the urgency to improve software supply chain security for community and national interests. Current malware detection techniques aid in the manual review process by filtering benign and malware packages, yet such techniques have high false-positive rates and limited automation support. Therefore, malware detection techniques could benefit from advanced, more automated approaches for accurate and minimally false-positive results. The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of large language models (LLMs) to detect potential malware in the npm ecosystem.   We present SocketAI Scanner, a multi-stage decision-maker malware detection workflow using iterative self-refinement and zero-shot-role-play-Chain of Thought (CoT) prompting techniques for ChatGPT. We studied 5,115 npm packages (of which 2,180 are malicious) and performed a baseline comparison of the GPT-3 and GPT-4 models with a static analysis tool. Our findings showed promising results for GPT models with low misclassification alert rates. Our baseline comparison demonstrates a notable improvement over static analysis in precision scores above 25% and F1 scores above 15%. We attained precision and F1 scores of 91% and 94%, respectively, for the GPT-3 model. Overall, GPT-4 demonstrates superior performance in precision (99%) and F1 (97%) scores, while GPT-3 presents a cost-effective balance between performance and expenditure.

摘要: Gartner 2022年报告预测，到2025年，全球45%的组织将遭遇软件供应链攻击，突显出为社区和国家利益改善软件供应链安全的紧迫性。当前的恶意软件检测技术通过过滤良性和恶意软件包来帮助手动审查过程，但是这种技术具有高的假阳性率和有限的自动化支持。因此，恶意软件检测技术可以受益于先进的、更自动化的方法，以获得准确和最小的假阳性结果。本研究的目标是帮助安全分析师通过大型语言模型（LLM）的实证研究来识别恶意软件包，以检测npm生态系统中的潜在恶意软件。   我们提出了SocketAI Scanner，一个多阶段决策者恶意软件检测工作流程，使用迭代自细化和零射击角色扮演思想链（CoT）提示技术，用于ChatGPT。我们研究了5，115个npm包（其中2，180个是恶意的），并使用静态分析工具对GPT—3和GPT—4模型进行了基线比较。我们的研究结果显示，具有低误分类警报率的GPT模型有希望的结果。我们的基线比较表明，与静态分析相比，精确度得分超过25%，F1得分超过15%。对于GPT—3模型，我们获得的精确度和F1评分分别为91%和94%。总体而言，GPT—4在精确度（99%）和F1（97%）评分方面表现出色，而GPT—3在性能和支出之间实现了成本效益的平衡。



## **6. EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models**

EasyJailBreak：一个统一的越狱大型语言模型框架 cs.CL

**SubmitDate**: 2024-03-18    [abs](http://arxiv.org/abs/2403.12171v1) [paper-pdf](http://arxiv.org/pdf/2403.12171v1)

**Authors**: Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, Rui Zheng, Songyang Gao, Yicheng Zou, Hang Yan, Yifan Le, Ruohui Wang, Lijun Li, Jing Shao, Tao Gui, Qi Zhang, Xuanjing Huang

**Abstract**: Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of Large Language Models (LLMs). They are designed to bypass safeguards and elicit prohibited outputs. However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations. This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against LLMs. It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator. This modular framework enables researchers to easily construct attacks from combinations of novel and existing components. So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of LLMs. Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks. Notably, even advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success Rates (ASR) of 57% and 33%, respectively. We have released a wealth of resources for researchers, including a web platform, PyPI published package, screencast video, and experimental outputs.

摘要: 越狱攻击对于识别和缓解大型语言模型（LLM）的安全漏洞至关重要。它们的设计是为了绕过保障措施，获取被禁止的产出。然而，由于各种越狱方法之间存在很大差异，社区没有标准的实施框架，这限制了全面的安全评估。本文介绍了EasyJailbreak，一个统一的框架，简化了针对LLM的越狱攻击的构造和评估。它使用四个组件构建越狱攻击：XSLT、Mutator、约束和评估器。这种模块化框架使研究人员能够轻松地从新组件和现有组件的组合中构建攻击。到目前为止，EasyJailbreak支持11种不同的越狱方法，并促进了广泛的LLM的安全验证。我们在10个不同的LLM上进行的验证揭示了一个重大漏洞，在各种越狱攻击下，平均漏洞概率为60%。值得注意的是，即使是像GPT—3.5—Turbo和GPT—4这样的先进机型，平均攻击成功率（ASR）分别为57%和33%。我们为研究人员发布了大量资源，包括网络平台、PyPI发布包、屏幕视频和实验输出。



## **7. Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning**

导航如攻击者所愿？基于联邦学习的鲁棒代理构建方法 cs.AI

**SubmitDate**: 2024-03-16    [abs](http://arxiv.org/abs/2211.14769v4) [paper-pdf](http://arxiv.org/pdf/2211.14769v4)

**Authors**: Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang

**Abstract**: Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Results on two VLN datasets (R2R and RxR) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction, without affecting its performance on normal test sets. Then, we propose a new Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated VLN, which provides the server with a ''prompt'' of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training. We validate the effectiveness of the PBA method on protecting the global model from the NAW attack, which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR.

摘要: 联邦嵌入式代理学习通过在训练期间将数据保存在每个客户端（个体环境）本地，保护个体视觉环境的数据隐私。然而，由于在联邦学习下，服务器无法访问本地数据，攻击者很容易在没有通知的情况下毒害本地客户端的训练数据，从而在代理中建立后门。部署这样的代理会增加对人类造成潜在伤害的风险，因为攻击者可以通过后门轻松地导航和控制代理。针对拜占庭鲁棒的联邦嵌入式智能体学习，本文研究了视觉和语言导航（VLN）任务的攻击和防御，其中智能体需要遵循自然语言指令来导航室内环境。首先，我们介绍了一种简单而有效的攻击策略，导航as Wish（NAW），其中恶意客户端操纵本地轨迹数据，在全局模型中植入后门。在两个VLN数据集（R2R和RxR）上的结果表明，NAW可以轻松地导航部署的VLN代理，而不影响其在正常测试集上的性能。然后，我们提出了一种新的基于节点的聚合（PBA）来防御联邦VLN中的NAW攻击，它为服务器提供了一个"提示"，即良性客户端和恶意客户端之间的视觉和语言对齐差异，以便在训练过程中区分它们。我们验证了PBA方法在保护全局模型免受NAW攻击方面的有效性，该方法在R2R和RXR上的防御指标上明显优于其他最先进的防御方法。



## **8. Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework**

Bergeron：通过基于意识的调整框架打击对抗性攻击 cs.CR

**SubmitDate**: 2024-03-15    [abs](http://arxiv.org/abs/2312.00029v2) [paper-pdf](http://arxiv.org/pdf/2312.00029v2)

**Authors**: Matthew Pisano, Peter Ly, Abraham Sanders, Bingsheng Yao, Dakuo Wang, Tomek Strzalkowski, Mei Si

**Abstract**: Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM emulating the conscience of a protected, primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis shows that, by using Bergeron to complement models with existing alignment training, we can improve the robustness and safety of multiple, commonly used commercial and open-source LLMs.

摘要: 自从最近引入了越来越强大的大型语言模型（LLM）以来，对人工智能对齐的研究已经有了很大的增长。不幸的是，现代对齐方法仍然无法完全防止模型受到蓄意攻击时的有害反应。这些攻击可以欺骗看似一致的模型给出危险材料的制造指令，煽动暴力，或推荐其他不道德行为。为了帮助缓解这个问题，我们引入了Bergeron：一个旨在提高LLM抵抗攻击的鲁棒性的框架，而无需进行任何额外的参数微调。Bergeron分为两个层次；二级法学硕士模仿受保护的，主要法学硕士的良心。该框架更好地保护了主模型免受传入攻击，同时监控其输出中的任何有害内容。实证分析表明，通过使用Bergeron来补充现有的对齐训练模型，我们可以提高多个常用的商业和开源LLM的鲁棒性和安全性。



## **9. Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning**

隐私攻击中的超越梯度和优先：在联合学习中利用语言模型的池层输入 cs.LG

**SubmitDate**: 2024-03-15    [abs](http://arxiv.org/abs/2312.05720v4) [paper-pdf](http://arxiv.org/pdf/2312.05720v4)

**Authors**: Jianwei Li, Sheng Liu, Qi Lei

**Abstract**: Language models trained via federated learning (FL) demonstrate impressive capabilities in handling complex tasks while protecting user privacy. Recent studies indicate that leveraging gradient information and prior knowledge can potentially reveal training samples within FL setting. However, these investigations have overlooked the potential privacy risks tied to the intrinsic architecture of the models. This paper presents a two-stage privacy attack strategy that targets the vulnerabilities in the architecture of contemporary language models, significantly enhancing attack performance by initially recovering certain feature directions as additional supervisory signals. Our comparative experiments demonstrate superior attack performance across various datasets and scenarios, highlighting the privacy leakage risk associated with the increasingly complex architectures of language models. We call for the community to recognize and address these potential privacy risks in designing large language models.

摘要: 通过联合学习(FL)训练的语言模型在处理复杂任务同时保护用户隐私方面表现出令人印象深刻的能力。最近的研究表明，利用梯度信息和先验知识可以潜在地揭示外语环境下的训练样本。然而，这些调查忽略了与这些模型的内在架构相关的潜在隐私风险。本文提出了一种两阶段隐私攻击策略，该策略针对当代语言模型体系结构中的漏洞，通过最初将某些特征方向恢复为额外的监督信号来显著提高攻击性能。我们的对比实验表明，在各种数据集和场景中具有卓越的攻击性能，突出了与日益复杂的语言模型体系结构相关的隐私泄露风险。我们呼吁社区在设计大型语言模型时认识到并解决这些潜在的隐私风险。



## **10. Logits of API-Protected LLMs Leak Proprietary Information**

API保护的LLM日志泄露专有信息 cs.CL

**SubmitDate**: 2024-03-15    [abs](http://arxiv.org/abs/2403.09539v2) [paper-pdf](http://arxiv.org/pdf/2403.09539v2)

**Authors**: Matthew Finlayson, Xiang Ren, Swabha Swayamdipta

**Abstract**: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096. Lastly, we discuss ways that LLM providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.

摘要: 大型语言模型（LLM）的商业化导致了只有高级API访问专有模型的普遍做法。在这项工作中，我们表明，即使有一个保守的假设模型架构，它是可能的，从相对较少的API查询（例如，OpenAI的gpt—3.5—turbo成本低于1000美元）。我们的研究结果集中在一个关键观察：大多数现代LLM都存在softmax瓶颈，这将模型输出限制在完整输出空间的线性子空间。我们表明，这有助于自己的模型图像或模型签名解锁几个功能，以负担得起的成本：有效地发现LLM的隐藏大小，获得完整的词汇表输出，检测和消除不同的模型更新，识别源LLM给定一个完整的LLM输出，甚至估计输出层参数。我们的实证研究表明了我们方法的有效性，这使我们能够估计OpenAI的gpt—3.5—turbo的嵌入大小约为4，096。最后，我们讨论了LLM提供商可以防范这些攻击的方法，以及如何通过允许更大的透明度和问责制将这些功能视为一个功能（而不是一个bug）。



## **11. Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks**

即时注入攻击下大语言模型机器翻译的缩放行为 cs.CL

15 pages, 18 figures, First Workshop on the Scaling Behavior of Large  Language Models (SCALE-LLM 2024)

**SubmitDate**: 2024-03-14    [abs](http://arxiv.org/abs/2403.09832v1) [paper-pdf](http://arxiv.org/pdf/2403.09832v1)

**Authors**: Zhifan Sun, Antonio Valerio Miceli-Barone

**Abstract**: Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023). To our knowledge, this is the first work to study non-trivial LLM scaling behaviour in a multi-lingual setting.

摘要: 大型语言模型（LLM）正日益成为许多自然语言处理任务（如机器翻译）的首选基础平台，因为它们的质量通常与特定任务的模型相当或更好，以及通过自然语言指令或上下文示例指定任务的简单性。然而，它们的普遍性使最终用户可能会在请求中嵌入指令，导致模型以未经授权的和可能不安全的方式运行。在这项工作中，我们研究了机器翻译任务中对多个家庭的LLM的提示注入攻击（PIA），重点是模型大小对攻击成功率的影响。我们引入了一个新的基准数据集，我们发现，在多个语言对和注入用英语编写的提示符上，在某些条件下，更大的模型可能变得更容易受到成功的攻击，这是逆尺度现象的一个例子（McKenzie等人，2023年）。据我们所知，这是第一项研究多语言环境下非平凡LLM缩放行为的工作。



## **12. Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models**

图像是对齐的致命弱点：利用多模态大型语言模型的视觉漏洞 cs.CV

Work in progress

**SubmitDate**: 2024-03-14    [abs](http://arxiv.org/abs/2403.09792v1) [paper-pdf](http://arxiv.org/pdf/2403.09792v1)

**Authors**: Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen

**Abstract**: In this paper, we study the harmlessness alignment problem of multimodal large language models~(MLLMs). We conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs. Inspired by this, we propose a novel jailbreak method named HADES, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images. Experimental results show that HADES can effectively jailbreak existing MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for LLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly released.

摘要: 本文研究了多模态大语言模型MLLM的无害对齐问题.我们对典型的多线性线性阵列的无害性性能进行了系统的实证分析，揭示了图像输入造成了多线性阵列的对准脆弱性。受此启发，我们提出了一种名为HADES的新越狱方法，该方法使用精心制作的图像隐藏和放大了文本输入中恶意意图的危害性。实验结果表明，HADES可以有效地破解现有MLLM，LLaVA—1.5和Gemini Pro Vision的平均攻击成功率分别为90.26%和71.60%。我们的代码和数据将公开发布。



## **13. Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation**

闭上眼睛，安全开启：通过图像到文本的转换保护多模式LLM cs.CV

**SubmitDate**: 2024-03-14    [abs](http://arxiv.org/abs/2403.09572v1) [paper-pdf](http://arxiv.org/pdf/2403.09572v1)

**Authors**: Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang

**Abstract**: Multimodal large language models (MLLMs) have shown impressive reasoning abilities, which, however, are also more vulnerable to jailbreak attacks than their LLM predecessors. Although still capable of detecting unsafe responses, we observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be easily bypassed due to the introduction of image features. To construct robust MLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs. Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO enhances model safety significantly (e.g., a 37.6% improvement on the MM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM benchmarks. Furthermore, we show that ECSO can be used as a data engine to generate supervised-finetuning (SFT) data for MLLM alignment without extra human intervention.

摘要: 多通道大语言模型(MLLM)已经显示出令人印象深刻的推理能力，然而，它们也比它们的前身更容易受到越狱攻击。虽然仍然能够检测到不安全的响应，但我们观察到，由于引入了图像特征，MLLMS中预先对准的LLM的安全机制可以很容易地绕过。为了构造稳健的MLLMS，我们提出了一种新的无需训练的保护方法ECSO(Eyes Closed，Safe On)，该方法利用MLLMS固有的安全意识，通过自适应地将不安全的图像转换为文本来激活MLLMS中预对准的LLMS的本质安全机制，从而产生更安全的响应。在五个最先进的(SOTA)MLLM上的实验表明，我们的ECSO显著增强了模型安全性(例如，对于LLaVA-1.5-7B，MM-SafetyBch(SD+OCR)的性能提高了37.6%，VLSafe的性能提高了71.3%)，同时保持了常见MLLM基准的实用结果。此外，我们还证明了ECSO可以作为数据引擎来生成用于MLLM比对的监督精调(SFT)数据，而无需额外的人工干预。



## **14. AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting**

AdaShield：通过自适应护盾保护多模态大型语言模型免受基于结构的攻击 cs.CR

Multimodal Large Language Models Defense, 25 Pages

**SubmitDate**: 2024-03-14    [abs](http://arxiv.org/abs/2403.09513v1) [paper-pdf](http://arxiv.org/pdf/2403.09513v1)

**Authors**: Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao

**Abstract**: With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., "harmful text") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender). These components collaboratively and iteratively communicate to generate a defense prompt. Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks. Our code is available at https://github.com/rain305f/AdaShield.

摘要: 随着多模态大型语言模型（MLLM）的出现和广泛部署，确保其安全性的迫切性变得越来越明显。然而，随着附加模态的集成，MLLM暴露于新的漏洞，使其易于遭受基于结构化的越狱攻击，其中语义内容（例如，“有害文字”）被注入图像误导MLLM。在这项工作中，我们的目标是防范此类威胁。具体地说，我们提出了\textbf {Ada} practiced\textbf {Shield}（\textbf {AdaShield}），它在输入前加上防御提示，以保护MLLM免受基于结构的越狱攻击，而无需微调MLLM或训练额外模块（例如，后阶段内容检测器）。首先，我们提出了一个手动设计的静态防御提示，它彻底检查图像和指令内容一步一步，并指定响应方法的恶意查询。此外，我们引入了一个自适应的自动细化框架，由目标MLLM和基于LLM的防御提示生成器（Defender）组成。这些组件协作地、迭代地通信以生成防御提示。在流行的基于结构的越狱攻击和良性数据集上的大量实验表明，我们的方法可以持续提高MLLM对基于结构的越狱攻击的鲁棒性，而不损害模型在标准良性任务上评估的一般能力。我们的代码可在www.example.com获得。



## **15. On Protecting the Data Privacy of Large Language Models (LLMs): A Survey**

大型语言模型（LLM）数据隐私保护研究综述 cs.CR

18 pages, 4 figures

**SubmitDate**: 2024-03-14    [abs](http://arxiv.org/abs/2403.05156v2) [paper-pdf](http://arxiv.org/pdf/2403.05156v2)

**Authors**: Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, Xiuzhen Cheng

**Abstract**: Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language. They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks. When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy. This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding. Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs. Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints. Finally, the discourse extends to delineate the challenges encountered and outline prospective directions for advancement in the realm of LLM privacy protection.

摘要: 大型语言模型（LLM）是一种复杂的人工智能系统，能够理解、生成和翻译人类语言。他们通过分析大量的文本数据来学习语言模式，使他们能够执行写作、会话、总结和其他语言任务。当LLM处理和生成大量数据时，存在泄漏敏感信息的风险，这可能威胁到数据隐私。本文集中阐述与LLM相关的数据隐私问题，以促进全面理解。具体而言，我们进行了彻底的调查，以界定数据隐私威胁的范围，包括LLM内的被动隐私泄漏和主动隐私攻击。其后，我们会评估有限责任公司在不同阶段所采用的隐私保障机制，并详细研究其成效及限制。最后，论述扩展到描绘所遇到的挑战，并概述在法学硕士隐私保护领域的发展方向。



## **16. AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions**

AVIBENCH：对抗视觉指令上大型视觉语言模型的鲁棒性评估 cs.CV

**SubmitDate**: 2024-03-14    [abs](http://arxiv.org/abs/2403.09346v1) [paper-pdf](http://arxiv.org/pdf/2403.09346v1)

**Authors**: Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Kaipeng Zhang

**Abstract**: Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenient tool for practitioners to evaluate the robustness of LVLMs against AVIs. Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores the importance of enhancing the robustness, security, and fairness of LVLMs. The source code and benchmark will be made publicly available.

摘要: 大型视觉语言模型（LVLM）在很好地响应用户的视觉指令方面取得了显著进展。然而，这些包含图像和文本的指令很容易受到有意和无意的攻击。尽管LVLM对此类威胁的鲁棒性至关重要，但目前在该领域的研究仍然有限。为了弥补这一差距，我们引入了AVIBench，这是一个框架，旨在分析LVLM在面对各种对抗视觉指令（AVIs）时的鲁棒性，包括四种基于图像的AVIs，十种基于文本的AVIs和九种内容偏见的AVIs（如性别、暴力、文化和种族偏见等）。我们生成了260K AVI，涵盖了五个类别的多模态能力（九个任务）和内容偏见。然后，我们对14个开源LVLM进行了全面的评估，以评估它们的性能。AVIBench也是一个方便的工具，为从业者评估LVLM的鲁棒性对AVIs。我们的发现和广泛的实验结果揭示了LVLM的脆弱性，并强调即使在先进的闭源LVLM，如GeminiProVision和GPT—4V中也存在固有的偏见。这强调了增强LVLM鲁棒性、安全性和公平性的重要性。源代码和基准测试将公开。



## **17. What Was Your Prompt? A Remote Keylogging Attack on AI Assistants**

你的提示是什么？对人工智能助手的远程键盘记录攻击 cs.CR

**SubmitDate**: 2024-03-14    [abs](http://arxiv.org/abs/2403.09751v1) [paper-pdf](http://arxiv.org/pdf/2403.09751v1)

**Authors**: Roy Weiss, Daniel Ayzenshteyn, Guy Amit, Yisroel Mirsky

**Abstract**: AI assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from AI Assistants over the web: the token-length side-channel. We found that many vendors, including OpenAI and Microsoft, have this side-channel.   However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences, (2) providing the LLM with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by fine-tuning the model on the target model's writing style.   Using these methods, we were able to accurately reconstruct 29\% of an AI assistant's responses and successfully infer the topic from 55\% of them. To demonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and Microsoft's Copilot on both browser and API traffic.

摘要: 人工智能助理正在成为社会不可或缺的一部分，用于在个人和机密问题上寻求建议或帮助。在本文中，我们推出了一种新的侧通道，可用于从Web上读取AI助手的加密响应：令牌长度侧通道。我们发现，包括OpenAI和微软在内的许多供应商都有这种侧通道。   然而，仅从标记长度序列推断响应的内容证明具有挑战性。这是因为令牌类似于单词，响应可以是几个句子长，导致数百万个语法正确的句子。在本文中，我们展示了如何克服这一点，（1）利用一个大型语言模型（LLM）的权力来翻译这些序列，（2）提供LLM与句子间上下文，以缩小搜索空间和（3）执行一个已知的明文攻击，微调模型的目标模型的写作风格。   使用这些方法，我们能够准确地重建29%的人工智能助手的回答，并成功地从其中55%的回答中推断出主题。为了演示威胁，我们对OpenAI的ChatGPT—4和微软的Copilot进行了浏览器和API流量的攻击。



## **18. The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?**

第一个知道：令牌分布如何揭示大型视觉语言模型中隐藏的知识？ cs.CV

Under review. Project page:  https://github.com/Qinyu-Allen-Zhao/LVLM-LP

**SubmitDate**: 2024-03-14    [abs](http://arxiv.org/abs/2403.09037v1) [paper-pdf](http://arxiv.org/pdf/2403.09037v1)

**Authors**: Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould

**Abstract**: Large vision-language models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions. This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs. We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against multi-modal jailbreaking attack, and identifying deceptive questions. Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation. Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content. In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating potential bias in the existing datasets. Second, we observe performance improvement by utilizing the first logit distributions on three additional tasks, including indicting uncertainty in math solving, mitigating hallucination, and image classification. Last, with the same training data, simply finetuning LVLMs improve models' performance but is still inferior to linear probing on these tasks.

摘要: 大型视觉语言模型（LVLM），旨在解释和响应人类指令，偶尔会产生幻觉或有害的内容，由于不适当的指令。本研究使用线性探测来揭示LVLM输出层的隐藏知识。我们证明，第一个令牌的logit分布包含足够的信息来确定是否响应指令，包括识别无法回答的视觉问题，防御多模式越狱攻击，并识别欺骗性问题。在响应生成过程中，这种隐藏的知识在后续令牌的日志中逐渐丢失。然后，我们在生成第一个令牌时说明了一个简单的解码策略，有效地改善了生成的内容。在实验中，我们发现了一些有趣的见解：首先，CLIP模型已经包含了解决这些任务的强信号，这表明了现有数据集中的潜在偏差。第二，我们观察了使用第一个logit分布在三个额外的任务，包括指示数学解决的不确定性，减轻幻觉和图像分类的性能提高。最后，使用相同的训练数据，简单地微调LVLM可以提高模型的性能，但仍然不如线性探测这些任务。



## **19. Dr. Jekyll and Mr. Hyde: Two Faces of LLMs**

Jekyll博士和海德先生：法学硕士的两个面孔 cs.CR

**SubmitDate**: 2024-03-13    [abs](http://arxiv.org/abs/2312.03853v2) [paper-pdf](http://arxiv.org/pdf/2312.03853v2)

**Authors**: Matteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro Conti, Stjepan Picek

**Abstract**: Only a year ago, we witnessed a rise in the use of Large Language Models (LLMs), especially when combined with applications like chatbot assistants. Safety mechanisms and specialized training procedures are implemented to prevent improper responses from these assistants. In this work, we bypass these measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them impersonate complex personas with opposite characteristics as those of the truthful assistants they are supposed to be. We start by creating elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversation followed a role-play style to get the response the assistant was not allowed to provide. By making use of personas, we show that the response that is prohibited is actually provided, making it possible to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial personas, one can overcome safety mechanisms set out by ChatGPT and Bard. We also introduce several ways of activating such adversarial personas, altogether showing that both chatbots are vulnerable to this kind of attack. With the same principle, we introduce two defenses that push the model to interpret trustworthy personalities and make it more robust against such attacks.

摘要: 就在一年前，我们见证了大型语言模型(LLM)的使用增加，特别是在与Chatbot助手等应用程序结合时。实施了安全机制和专门的培训程序，以防止这些助理做出不当反应。在这项工作中，我们绕过了ChatGPT和Bard(在某种程度上，还有Bing聊天)的这些措施，让他们模仿复杂的人物角色，具有与他们应该是的诚实助手相反的特征。我们首先为这些角色创建精致的传记，然后在与相同的聊天机器人的新会话中使用。我们的谈话遵循了角色扮演的风格，得到了助手不允许提供的回应。通过使用人物角色，我们表明实际上提供了被禁止的响应，使得获得未经授权的、非法的或有害的信息成为可能。这项工作表明，通过使用对抗性人物角色，一个人可以克服ChatGPT和Bard提出的安全机制。我们还介绍了几种激活这种敌对角色的方法，总而言之，这两个聊天机器人都容易受到这种攻击。在相同的原则下，我们引入了两个防御措施，推动该模型解释可信任的个性，并使其对此类攻击更加健壮。



## **20. SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks**

SOK：降低微调语言模型对成员关系推理攻击的脆弱性 cs.LG

preliminary version

**SubmitDate**: 2024-03-13    [abs](http://arxiv.org/abs/2403.08481v1) [paper-pdf](http://arxiv.org/pdf/2403.08481v1)

**Authors**: Guy Amit, Abigail Goldsteen, Ariel Farkash

**Abstract**: Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We find that some training methods provide significantly reduced privacy risk, with the combination of differential privacy and low-rank adaptors achieving the best privacy protection against these attacks.

摘要: 近年来，自然语言处理模型经历了一个显著的热潮，许多应用程序都是基于它们构建的。其中许多应用程序需要在定制的专有数据集上微调通用基础模型。这种微调数据特别有可能包含个人或敏感信息，从而增加隐私风险。隶属度推理攻击是评估机器学习模型隐私泄漏最常用的攻击。然而，关于影响语言模型对此类攻击的脆弱性的因素，以及不同防御策略在语言领域的适用性的研究有限。我们提供了第一个系统的审查微调大型语言模型的脆弱性，参与的各种因素，以及不同防御策略的有效性。我们发现，一些训练方法提供了显着降低的隐私风险，差分隐私和低秩适配器的组合实现了针对这些攻击的最佳隐私保护。



## **21. The Philosopher's Stone: Trojaning Plugins of Large Language Models**

哲学家之石：大型语言模型的木马插件 cs.CR

**SubmitDate**: 2024-03-13    [abs](http://arxiv.org/abs/2312.00374v2) [paper-pdf](http://arxiv.org/pdf/2312.00374v2)

**Authors**: Tian Dong, Minhui Xue, Guoxing Chen, Rayne Holland, Shaofeng Li, Yan Meng, Zhen Liu, Haojin Zhu

**Abstract**: Open-source Large Language Models (LLMs) have recently gained popularity because of their comparable performance to proprietary LLMs. To efficiently fulfill domain-specialized tasks, open-source LLMs can be refined, without expensive accelerators, using low-rank adapters. However, it is still unknown whether low-rank adapters can be exploited to control LLMs. To address this gap, we demonstrate that an infected adapter can induce, on specific triggers, an LLM to output content defined by an adversary and to even maliciously use tools. To train a Trojan adapter, we propose two novel attacks, POLISHED and FUSION, that improve over prior approaches. POLISHED uses LLM-enhanced paraphrasing to polish benchmark poisoned datasets. In contrast, in the absence of a dataset, FUSION leverages an over-poisoning procedure to transform a benign adaptor. In our experiments, we first conduct two case studies to demonstrate that a compromised LLM agent can execute malware to control system (e.g., LLM-driven robot) or launch a spear-phishing attack. Then, in terms of targeted misinformation, we show that our attacks provide higher attack effectiveness than the baseline and, for the purpose of attracting downloads, preserve or improve the adapter's utility. Finally, we design and evaluate three potential defenses, yet none proved entirely effective in safeguarding against our attacks.

摘要: 开源大型语言模型（LLM）最近受到了欢迎，因为它们与专有LLM相当的性能。为了有效地完成领域专用任务，开源LLM可以使用低秩适配器进行优化，而无需昂贵的加速器。然而，目前还不清楚是否可以利用低秩适配器来控制LLM。为了解决这一差距，我们证明了受感染的适配器可以诱导，在特定的触发器，LLM输出由对手定义的内容，甚至恶意使用工具。为了训练木马适配器，我们提出了两种新的攻击，POLISHED和FUSION，改进了先前的方法。POLISHED使用LLM增强的释义来抛光基准中毒数据集。相反，在没有数据集的情况下，FUSION利用过度中毒过程来转换良性适配器。在我们的实验中，我们首先进行了两个案例研究，以证明一个受损的LLM代理可以执行恶意软件来控制系统（例如，LLM驱动的机器人）或发起鱼叉式网络钓鱼攻击。然后，在有针对性的错误信息方面，我们表明我们的攻击提供了比基线更高的攻击效果，并且为了吸引下载，保留或改进适配器的实用性。最后，我们设计并评估了三种潜在的防御措施，但没有一种被证明完全有效地防御我们的攻击。



## **22. Tastle: Distract Large Language Models for Automatic Jailbreak Attack**

Tastle：分散大型语言模型的自动越狱攻击 cs.CR

**SubmitDate**: 2024-03-13    [abs](http://arxiv.org/abs/2403.08424v1) [paper-pdf](http://arxiv.org/pdf/2403.08424v1)

**Authors**: Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen

**Abstract**: Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs. Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies.

摘要: 大型语言模型（LLM）近年来取得了重大进展。在公开发布LLM之前，已经做出了广泛的努力，以使他们的行为与人类价值观保持一致。结盟的主要目标是确保他们乐于助人、诚实和无害。然而，即使是精心排列的LLM仍然容易受到恶意操作，如越狱，导致意外行为。越狱是故意开发一个恶意提示，逃离LLM的安全限制，以产生未经审查的有害内容。以前的作品探索了红色团队LLM的不同越狱方法，但他们遇到了关于有效性和可扩展性的挑战。在这项工作中，我们提出了Tastle，一个新的黑盒越狱框架自动化红色团队LLM。基于对LLM的注意力分散和过度自信现象的研究，我们设计了一种迭代优化算法来破解LLM的恶意内容隐藏和内存重构。大量的开源和专有LLM越狱实验证明了我们的框架在有效性、可扩展性和可移植性方面的优越性。我们还评估了现有的越狱防御方法对我们攻击的有效性，并强调开发更有效和实用的防御策略的迫切需要。



## **23. MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models**

MM—SafetyBench：多模态大型语言模型安全性评估的基准 cs.CV

**SubmitDate**: 2024-03-12    [abs](http://arxiv.org/abs/2311.17600v2) [paper-pdf](http://arxiv.org/pdf/2311.17600v2)

**Authors**: Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, Yu Qiao

**Abstract**: The security concerns surrounding Large Language Models (LLMs) have been extensively explored, yet the safety of Multimodal Large Language Models (MLLMs) remains understudied. In this paper, we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images, as if the text query itself were malicious. To address this, we introduce MM-SafetyBench, a comprehensive framework designed for conducting safety-critical evaluations of MLLMs against such image-based manipulations. We have compiled a dataset comprising 13 scenarios, resulting in a total of 5,040 text-image pairs. Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned. In response, we propose a straightforward yet effective prompting strategy to enhance the resilience of MLLMs against these types of attacks. Our work underscores the need for a concerted effort to strengthen and enhance the safety measures of open-source MLLMs against potential malicious exploits. The resource is available at \href{this https URL}{https://github.com/isXinLiu/MM-SafetyBench}.

摘要: 围绕大型语言模型（LLM）的安全问题已经得到了广泛的探讨，但多模态大型语言模型（MLLM）的安全性研究仍然不足。在本文中，我们观察到，多模态大型语言模型（MLLM）可以很容易地被查询相关图像破坏，就好像文本查询本身是恶意的。为了解决这一问题，我们引入了MM—SafetyBench，一个综合框架，旨在针对此类基于图像的操纵对MLLM进行安全关键评估。我们编译了一个包含13个场景的数据集，总共产生了5，040个文本图像对。我们对12种最先进型号的分析表明，即使装备的LLM已经安全对准，MLLM也容易受到我们的方法引发的违规行为的影响。作为回应，我们提出了一个简单而有效的激励策略，以提高MLLM对这些类型的攻击的弹性。我们的工作强调了共同努力的必要性，以加强和提高开源MLLM的安全措施，以防止潜在的恶意利用。该资源可在\href {this https URL}{https：//github.com/isXinLiu/MM—SafetyBench}获得。



## **24. Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code**

未修复代码对程序的毒化：人工智能生成代码的安全问题 cs.CR

Accepted at The 1st IEEE International Workshop on Reliable and  Secure AI for Software Engineering (ReSAISE), co-located with ISSRE 2023

**SubmitDate**: 2024-03-11    [abs](http://arxiv.org/abs/2403.06675v1) [paper-pdf](http://arxiv.org/pdf/2403.06675v1)

**Authors**: Cristina Improta

**Abstract**: AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation. Lastly, we discuss potential solutions to overcome this threat.

摘要: 基于人工智能的代码生成器在帮助开发人员从自然语言(NL)开始编写软件方面发挥了重要作用。然而，由于这些大型语言模型是基于从不可靠的在线来源(如GitHub、拥抱脸)收集的海量数据进行训练的，AI模型很容易成为数据中毒攻击的目标，即攻击者通过向训练数据中注入少量毒药来破坏训练数据，即巧妙地制作恶意样本。在这份立场文件中，我们通过识别一种导致生成易受攻击的代码的新型数据中毒攻击来解决AI代码生成器的安全问题。接下来，我们将对这些攻击如何影响最先进的代码生成模型进行广泛的评估。最后，我们讨论了克服这一威胁的潜在解决方案。



## **25. FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning**

FedPIT：走向隐私保护和少机会的联邦指令调优 cs.CR

Work in process

**SubmitDate**: 2024-03-10    [abs](http://arxiv.org/abs/2403.06131v1) [paper-pdf](http://arxiv.org/pdf/2403.06131v1)

**Authors**: Zhuo Zhang, Jingyuan Zhang, Jintao Huang, Lizhen Qu, Hongzhi Zhang, Zenglin Xu

**Abstract**: Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses. However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains. Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy. Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive experiments on real-world medical data demonstrate the effectiveness of FedPIT in improving federated few-shot performance while preserving privacy and robustness against data heterogeneity.

摘要: 指令调优已被证明对于提高大型语言模型（LLM）在生成与人类一致的响应方面的性能至关重要。然而，收集不同的高质量指令数据进行调整带来了挑战，特别是在隐私敏感领域。联邦指令调优（FedIT）已经成为一种解决方案，它利用来自多个数据所有者的联合学习，同时保护隐私。然而，由于指令数据有限和训练数据提取攻击的脆弱性，它面临着挑战。为了解决这些问题，我们提出了一种新的联邦算法，FedPIT，它利用LLM的上下文学习能力，自生成特定于任务的合成数据，用于自主训练。我们的方法采用参数隔离训练来维护在合成数据上训练的全局参数和在增强本地数据上训练的局部参数，有效地阻止了数据提取攻击。对现实世界医疗数据的大量实验证明了FedPIT在提高联邦少镜头性能的同时保护隐私性和针对数据异构性的鲁棒性方面的有效性。



## **26. Language-Driven Anchors for Zero-Shot Adversarial Robustness**

零镜头对抗鲁棒性的迭代驱动算法 cs.CV

Accepted by CVPR 2024

**SubmitDate**: 2024-03-10    [abs](http://arxiv.org/abs/2301.13096v3) [paper-pdf](http://arxiv.org/pdf/2301.13096v3)

**Authors**: Xiao Li, Wei Zhang, Yining Liu, Zhanhao Hu, Bo Zhang, Xiaolin Hu

**Abstract**: Deep Neural Networks (DNNs) are known to be susceptible to adversarial attacks. Previous researches mainly focus on improving adversarial robustness in the fully supervised setting, leaving the challenging domain of zero-shot adversarial robustness an open question. In this work, we investigate this domain by leveraging the recent advances in large vision-language models, such as CLIP, to introduce zero-shot adversarial robustness to DNNs. We propose LAAT, a Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes the features of a text encoder for each category as fixed anchors (normalized feature embeddings) for each category, which are then employed for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT aims to enhance the adversarial robustness of the image model on novel categories. However, naively using text encoders leads to poor results. Through analysis, we identified the issue to be the high cosine similarity between text encoders. We then design an expansion algorithm and an alignment cross-entropy loss to alleviate the problem. Our experimental results demonstrated that LAAT significantly improves zero-shot adversarial robustness over state-of-the-art methods. LAAT has the potential to enhance adversarial robustness by large-scale multimodal models, especially when labeled data is unavailable during training.

摘要: 众所周知，深度神经网络（DNN）容易受到对抗攻击。以往的研究主要集中在提高完全监督环境下的对抗鲁棒性，而零镜头对抗鲁棒性的挑战性领域则是一个悬而未决的问题。在这项工作中，我们通过利用大型视觉语言模型（如CLIP）的最新进展来研究这一领域，将零镜头对抗鲁棒性引入DNN。我们提出了LAAT，一种基于锚点驱动的对抗训练策略。LAAT利用每个类别的文本编码器的特征作为每个类别的固定锚点（规范化特征嵌入），然后用于对抗训练。通过利用文本编码器的语义一致性，LAAT旨在增强图像模型对新颖类别的对抗鲁棒性。然而，天真地使用文本编码器会导致糟糕的结果。通过分析，我们发现问题是文本编码器之间的高余弦相似度。然后，我们设计了一个扩展算法和一个对齐交叉熵损失来缓解问题。我们的实验结果表明，LAAT显着提高了零镜头对抗鲁棒性的国家的最先进的方法。LAAT有潜力通过大规模多模态模型增强对抗性鲁棒性，特别是当训练期间标记数据不可用时。



## **27. From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude**

从聊天机器人到网络钓鱼机器人？--防止使用ChatGPT、Google Bard和Claude创建的网络钓鱼诈骗 cs.CR

**SubmitDate**: 2024-03-10    [abs](http://arxiv.org/abs/2310.19181v2) [paper-pdf](http://arxiv.org/pdf/2310.19181v2)

**Authors**: Sayak Saha Roy, Poojitha Thota, Krishna Vamsi Naragam, Shirin Nilizadeh

**Abstract**: The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation. However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks. This study explores the potential of using four popular commercially available LLMs, i.e., ChatGPT (GPT 3.5 Turbo), GPT 4, Claude, and Bard, to generate functional phishing attacks using a series of malicious prompts. We discover that these LLMs can generate both phishing websites and emails that can convincingly imitate well-known brands and also deploy a range of evasive tactics that are used to elude detection mechanisms employed by anti-phishing systems. These attacks can be generated using unmodified or "vanilla" versions of these LLMs without requiring any prior adversarial exploits such as jailbreaking. We evaluate the performance of the LLMs towards generating these attacks and find that they can also be utilized to create malicious prompts that, in turn, can be fed back to the model to generate phishing scams - thus massively reducing the prompt-engineering effort required by attackers to scale these threats. As a countermeasure, we build a BERT-based automated detection tool that can be used for the early detection of malicious prompts to prevent LLMs from generating phishing content. Our model is transferable across all four commercial LLMs, attaining an average accuracy of 96% for phishing website prompts and 94% for phishing email prompts. We also disclose the vulnerabilities to the concerned LLMs, with Google acknowledging it as a severe issue. Our detection model is available for use at Hugging Face, as well as a ChatGPT Actions plugin.

摘要: 大型语言模型（LLM）的先进功能使它们在各种应用程序中发挥了非常重要的作用，从会话代理和内容创建到数据分析、研究和创新。然而，它们的有效性和可访问性也使它们容易被滥用以生成恶意内容，包括网络钓鱼攻击。本研究探讨了使用四种流行的商业化LLM的潜力，即，ChatGPT（GPT 3.5 Turbo）、GPT 4、Claude和Bard，使用一系列恶意提示生成功能性网络钓鱼攻击。我们发现，这些LLM可以生成钓鱼网站和电子邮件，可以令人信服地模仿知名品牌，还部署了一系列逃避策略，用于逃避反钓鱼系统采用的检测机制。这些攻击可以使用这些LLM的未修改或“vanilla”版本生成，而不需要任何先前的对抗性攻击，如越狱。我们评估了LLM在生成这些攻击方面的性能，发现它们还可以用来创建恶意提示，反过来，这些提示可以反馈到模型中生成网络钓鱼诈骗，从而大大减少了攻击者扩展这些威胁所需的网络设计工作。作为一种对策，我们构建了一个基于BERT的自动检测工具，用于早期检测恶意提示，以防止LLM生成钓鱼内容。我们的模型可在所有四个商业LLM中移植，钓鱼网站提示的平均准确率为96%，钓鱼电子邮件提示的平均准确率为94%。我们还向相关LLM披露了这些漏洞，谷歌承认这是一个严重的问题。我们的检测模型可用于Hugging Face，以及ChatGPT Action插件。



## **28. Can LLMs Follow Simple Rules?**

低收入国家能遵循简单的规则吗？ cs.AI

Project website: https://eecs.berkeley.edu/~normanmu/llm_rules;  revised content

**SubmitDate**: 2024-03-08    [abs](http://arxiv.org/abs/2311.04235v3) [paper-pdf](http://arxiv.org/pdf/2311.04235v3)

**Authors**: Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, David Wagner

**Abstract**: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietary and open models show that almost all current models struggle to follow scenario rules, even on straightforward test cases. We also demonstrate that simple optimization attacks suffice to significantly increase failure rates on test cases. We conclude by exploring two potential avenues for improvement: test-time steering and supervised fine-tuning.

摘要: 随着大型语言模型(LLM)的部署承担着越来越多的现实责任，能够以可靠的方式指定和约束这些系统的行为是很重要的。模型开发人员可能希望为模型设置明确的规则，例如“不要生成滥用内容”，但可以通过越狱技术绕过这些规则。现有的对抗性攻击和防御评估通常需要昂贵的人工审查或不可靠的启发式检查。为了解决这一问题，我们提出了规则遵循语言评估场景(Rules)，这是一个衡量LLMS中规则遵循能力的程序性框架。规则由14个简单的文本场景组成，在这些场景中，模型被指示在与用户交互时遵守各种规则。每个场景都有一个程序化的评估功能，以确定模型是否违反了对话中的任何规则。我们对专有和开放模型的评估表明，几乎所有当前的模型都难以遵循场景规则，即使在简单的测试用例上也是如此。我们还证明了简单的优化攻击足以显著增加测试用例的失败率。最后，我们探索了两个潜在的改进途径：测试时间控制和有监督的微调。



## **29. Warfare:Breaking the Watermark Protection of AI-Generated Content**

战争：打破人工智能生成内容的水印保护 cs.CV

**SubmitDate**: 2024-03-08    [abs](http://arxiv.org/abs/2310.07726v3) [paper-pdf](http://arxiv.org/pdf/2310.07726v3)

**Authors**: Guanlin Li, Yifei Chen, Jie Zhang, Jiwei Li, Shangwei Guo, Tianwei Zhang

**Abstract**: AI-Generated Content (AIGC) is gaining great popularity, with many emerging commercial services and applications. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images and fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content). A promising solution to achieve this goal is watermarking, which adds unique and imperceptible watermarks on the content for service verification and attribution. Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely bypassing the regulation of the service provider. (2) Watermark forging: the adversary can create illegal content with forged watermarks from another user, causing the service provider to make wrong attributions. We propose Warfare, a unified methodology to achieve both attacks in a holistic way. The key idea is to leverage a pre-trained diffusion model for content processing and a generative adversarial network for watermark removal or forging. We evaluate Warfare on different datasets and embedding setups. The results prove that it can achieve high success rates while maintaining the quality of the generated content. Compared to existing diffusion model-based attacks, Warfare is 5,050~11,000x faster.

摘要: 人工智能生成内容（AI—Generated Content，AIGC）正在获得广泛的欢迎，有许多新兴的商业服务和应用。这些服务利用先进的生成模型，例如潜在扩散模型和大型语言模型，来生成创造性内容（例如，真实的图像和流畅的句子）。这种生成的内容的使用需要受到高度管制，因为服务提供商需要确保用户不违反使用策略（例如，滥用商业化、生成和分发不安全内容）。实现这一目标的一个很有前途的解决方案是水印，它在内容上添加独特的和不可感知的水印，用于服务验证和归属。近年来，已经提出了许多水印方法。然而，在本文中，我们表明，对手可以很容易地打破这些水印机制。具体来说，我们考虑了两种可能的攻击。(1)水印去除：攻击者可以容易地从生成的内容中擦除嵌入的水印，然后绕过服务提供商的规定而自由地使用它。(2)水印锻造：攻击者可以创建具有来自另一用户的伪造水印的非法内容，导致服务提供商作出错误的归属。我们提出了战争，一个统一的方法，以实现两种攻击在一个整体的方式。关键思想是利用预先训练的扩散模型进行内容处理，并利用生成对抗网络进行水印去除或伪造。我们在不同的数据集和嵌入设置上评估战争。实验结果表明，该方法在保证生成内容质量的同时，可以达到较高的成功率。与现有的基于扩散模型的攻击相比，Warfare的速度快了5050～11000倍。



## **30. Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models**

大型语言模型上的间接提示注入攻击的基准测试与防御 cs.CL

**SubmitDate**: 2024-03-08    [abs](http://arxiv.org/abs/2312.14197v3) [paper-pdf](http://arxiv.org/pdf/2312.14197v3)

**Authors**: Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu

**Abstract**: The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box methods based on prompt learning and a white-box defense method based on fine-tuning with adversarial training accordingly. Experimental results demonstrate that black-box defenses are highly effective in mitigating these attacks, while the white-box defense reduces the attack success rate to near-zero levels. Overall, our work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses.

摘要: 大型语言模型(LLM)与外部内容的集成使LLM能够更新、更广泛地应用，如Microsoft Copilot。然而，这种集成也使LLMS面临间接提示注入攻击的风险，攻击者可以在外部内容中嵌入恶意指令，损害LLM输出并导致响应偏离用户预期。为了研究这一重要但未被探索的问题，我们引入了第一个间接即时注入攻击基准，称为BIPIA，以评估此类攻击的风险。在评估的基础上，我们的工作重点分析了攻击成功的根本原因，即LLMS无法区分指令和外部内容，以及LLMS缺乏不执行外部内容中的指令的意识。在此基础上，我们提出了两种基于快速学习的黑盒防御方法和一种基于微调对抗性训练的白盒防御方法。实验结果表明，黑盒防御对于缓解这些攻击是非常有效的，而白盒防御将攻击成功率降低到接近于零的水平。总体而言，我们的工作通过引入基准、分析攻击成功的根本原因以及开发一套初始防御措施来系统地调查间接即时注入攻击。



## **31. SecGPT: An Execution Isolation Architecture for LLM-Based Systems**

SecGPT：基于LLM的系统执行隔离体系结构 cs.CR

**SubmitDate**: 2024-03-08    [abs](http://arxiv.org/abs/2403.04960v1) [paper-pdf](http://arxiv.org/pdf/2403.04960v1)

**Authors**: Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal

**Abstract**: Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applications. These LLM apps leverage the de facto natural language-based automated execution paradigm of LLMs: that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These LLM app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we propose SecGPT, an architecture for LLM-based systems that aims to mitigate the security and privacy issues that arise with the execution of third-party apps. SecGPT's key idea is to isolate the execution of apps and more precisely mediate their interactions outside of their isolated environments. We evaluate SecGPT against a number of case study attacks and demonstrate that it protects against many security, privacy, and safety issues that exist in non-isolated LLM-based systems. The performance overhead incurred by SecGPT to improve security is under 0.3x for three-quarters of the tested queries. To foster follow-up research, we release SecGPT's source code at https://github.com/llm-platform-security/SecGPT.

摘要: 作为系统扩展的大型语言模型（LLM），如ChatGPT，已经开始支持第三方应用程序。这些LLM应用程序利用事实上基于自然语言的LLM自动执行范式：即，应用程序及其交互以自然语言定义，提供对用户数据的访问，并允许彼此和系统自由交互。这些LLM应用生态系统类似于早期计算平台的设置，其中应用和系统之间的隔离不够。由于第三方应用程序可能不值得信赖，并且由于自然语言界面的不精确性而加剧，当前的设计给用户带来了安全和隐私风险。在本文中，我们提出了SecGPT，一种基于LLM的系统的架构，旨在缓解第三方应用程序执行时出现的安全和隐私问题。SecGPT的关键思想是隔离应用程序的执行，并更精确地在其隔离环境之外调解它们的交互。我们评估了SecGPT的一些案例研究攻击，并证明它可以防止存在于非隔离的基于LLM的系统中的许多安全、隐私和安全问题。对于四分之三的测试查询，SecGPT为提高安全性而产生的性能开销低于0.3x。为了促进后续研究，我们在www.example.com上发布了SecGPT的源代码。



## **32. Automatic and Universal Prompt Injection Attacks against Large Language Models**

针对大型语言模型的自动和通用提示注入攻击 cs.AI

Pre-print, code is available at  https://github.com/SheltonLiu-N/Universal-Prompt-Injection

**SubmitDate**: 2024-03-07    [abs](http://arxiv.org/abs/2403.04957v1) [paper-pdf](http://arxiv.org/pdf/2403.04957v1)

**Authors**: Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, Chaowei Xiao

**Abstract**: Large Language Models (LLMs) excel in processing and generating human language, powered by their ability to interpret and follow instructions. However, their capabilities can be exploited through prompt injection attacks. These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests. The substantial risks posed by these attacks underscore the need for a thorough understanding of the threats. Yet, research in this area faces challenges due to the lack of a unified goal for such attacks and their reliance on manually crafted prompts, complicating comprehensive assessments of prompt injection robustness. We introduce a unified framework for understanding the objectives of prompt injection attacks and present an automated gradient-based method for generating highly effective and universal prompt injection data, even in the face of defensive measures. With only five training samples (0.3% relative to the test data), our attack can achieve superior performance compared with baselines. Our findings emphasize the importance of gradient-based testing, which can avoid overestimation of robustness, especially for defense mechanisms.

摘要: 大型语言模型（LLM）擅长处理和生成人类语言，由其解释和遵循指令的能力提供动力。然而，它们的能力可以通过即时注入攻击来加以利用。这些攻击操纵集成LLM的应用程序生成与攻击者注入内容一致的响应，偏离用户的实际请求。这些袭击造成的巨大风险突出表明，必须彻底了解这些威胁。然而，由于缺乏针对此类攻击的统一目标，以及它们依赖于手工制作的提示，这一领域的研究面临着挑战，这使得对提示注入鲁棒性的全面评估变得复杂。我们引入了一个统一的框架来理解快速注入攻击的目标，并提出了一个基于梯度的自动化方法来生成高效和通用的快速注入数据，即使面对防御措施。仅使用5个训练样本（相对于测试数据的0.3%），我们的攻击就可以获得优于基线的性能。我们的研究结果强调了基于梯度的测试的重要性，它可以避免对鲁棒性的高估，特别是对于防御机制。



## **33. Membership Inference Attacks and Privacy in Topic Modeling**

主题建模中的成员推理攻击和隐私保护 cs.CR

9 pages + appendices and references. 9 figures. Submitted to USENIX  '24

**SubmitDate**: 2024-03-07    [abs](http://arxiv.org/abs/2403.04451v1) [paper-pdf](http://arxiv.org/pdf/2403.04451v1)

**Authors**: Nico Manzonelli, Wanrong Zhang, Salil Vadhan

**Abstract**: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.

摘要: 最近的研究表明，大型语言模型容易受到隐私攻击，从而推断训练数据的各个方面。然而，尚不清楚更简单的生成性模型，如主题模型，是否也存在类似的漏洞。在这项工作中，我们提出了一种针对主题模型的攻击，该模型可以自信地识别潜在Dirichlet分配中的训练数据成员。我们的结果表明，与生成性建模相关的隐私风险并不局限于大型神经模型。此外，为了缓解这些漏洞，我们探索了差异隐私(DP)主题建模。我们提出了一种隐私主题建模框架，该框架将DP词汇选择作为预处理步骤，并证明了它在提高隐私的同时对实际效用的影响有限。



## **34. PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion**

PPTC—R基准测试：评估PowerPoint任务完成的大型语言模型的鲁棒性 cs.CL

LLM evaluation, Multi-turn, Multi-language, Multi-modal benchmark

**SubmitDate**: 2024-03-06    [abs](http://arxiv.org/abs/2403.03788v1) [paper-pdf](http://arxiv.org/pdf/2403.03788v1)

**Authors**: Zekai Zhang, Yiduo Guo, Yaobo Liang, Dongyan Zhao, Nan Duan

**Abstract**: The growing dependence on Large Language Models (LLMs) for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R) to measure LLMs' robustness to the user PPT task instruction and software version. Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark that incorporates these robustness settings, aiming to evaluate how deviations impact LLMs' API calls for task completion. We find that GPT-4 exhibits the highest performance and strong robustness in our benchmark, particularly in the version update and the multilingual settings. However, we find that all LLMs lose their robustness when confronted with multiple challenges (e.g., multi-turn) simultaneously, leading to significant performance drops. We further analyze the robustness behavior and error reasons of LLMs in our benchmark, which provide valuable insights for researchers to understand the LLM's robustness in task completion and develop more robust LLMs and agents. We release the code and data at \url{https://github.com/ZekaiGalaxy/PPTCR}.

摘要: 对大型语言模型（LLM）的依赖性越来越大，需要全面了解它们在现实世界中对复杂任务完成的鲁棒性。为了满足这一关键需求，我们提出了PowerPoint任务完成鲁棒性基准（PPTC—R）来衡量LLM对用户PPT任务指令和软件版本的鲁棒性。具体而言，我们通过在句子、语义和多语言层次上攻击用户指令来构建对抗性用户指令。为了评估语言模型对软件版本的鲁棒性，我们改变了提供的API的数量，以模拟最新版本和早期版本设置。随后，我们使用集成了这些鲁棒性设置的基准测试了3个闭源代码和4个开源LLM，旨在评估偏差如何影响LLM的API调用任务完成。我们发现，在我们的基准测试中，GPT—4表现出最高的性能和强大的鲁棒性，特别是在版本更新和多语言设置方面。然而，我们发现所有LLM在面临多个挑战时都会失去其健壮性（例如，多圈）同时，导致性能显著下降。我们进一步分析了LLM的鲁棒性行为和错误原因，为研究人员了解LLM在任务完成中的鲁棒性，开发更鲁棒的LLM和代理提供了宝贵的见解。我们在\url {https：//github.com/ZekaiGalaxy/PPTCR}发布代码和数据。



## **35. ImgTrojan: Jailbreaking Vision-Language Models with ONE Image**

IMG特洛伊木马：用一幅图像破解视觉语言模型 cs.CV

**SubmitDate**: 2024-03-06    [abs](http://arxiv.org/abs/2403.02910v2) [paper-pdf](http://arxiv.org/pdf/2403.02910v2)

**Authors**: Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong

**Abstract**: There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods.

摘要: 人们对大型语言模型（LLM）与人类价值观的一致性越来越感兴趣。然而，它们与视觉模块或视觉语言模型（VLM）集成的安全问题仍然相对缺乏研究。在本文中，我们提出了一种新的针对VLM的越狱攻击，旨在绕过其安全屏障时，用户输入有害指令。假设我们的中毒（图像，文本）数据对包含在训练数据中的场景。通过用恶意的越狱提示来替换原始的文本标题，我们的方法可以使用中毒图片进行越狱攻击。此外，我们还分析了中毒率和可训练参数的位置对我们攻击成功率的影响。为了评估，我们设计了两个指标来量化攻击的成功率和隐蔽性。连同一个精心策划的有害指令列表，提供了一个衡量攻击效能的基准。我们通过与基线方法进行比较来证明我们的攻击的有效性。



## **36. Human vs. Machine: Language Models and Wargames**

人类与机器：语言模型与战争游戏 cs.CY

**SubmitDate**: 2024-03-06    [abs](http://arxiv.org/abs/2403.03407v1) [paper-pdf](http://arxiv.org/pdf/2403.03407v1)

**Authors**: Max Lamparth, Anthony Corso, Jacob Ganz, Oriana Skylar Mastro, Jacquelyn Schneider, Harold Trinkunas

**Abstract**: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.

摘要: 战争演习在军事战略的发展和国家对威胁或攻击的反应方面有着悠久的历史。人工智能（AI）的出现承诺更好的决策和提高军事效力。然而，关于人工智能系统，特别是大型语言模型（LLM）与人类相比如何表现仍然存在争议。为此，我们使用了一个战争游戏实验，有107名国家安全专家人类玩家，旨在观察一个虚构的中美场景中的危机升级，并将人类玩家与LLM模拟的反应进行比较。我们发现在法学硕士和人类的反应相当一致，但在战争游戏中模拟和人类玩家之间也存在显著的定量和定性差异，激励决策者在移交自主权或遵循基于人工智能的战略建议之前谨慎行事。



## **37. PETA: Parameter-Efficient Trojan Attacks**

PETA：参数高效木马攻击 cs.CL

**SubmitDate**: 2024-03-05    [abs](http://arxiv.org/abs/2310.00648v4) [paper-pdf](http://arxiv.org/pdf/2310.00648v4)

**Authors**: Lauren Hong, Ting Wang

**Abstract**: Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance that is comparable to standard fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we take the initial steps and present PETA, a novel trojan attack that compromises the weights of PLMs by accounting for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a model while the lower-level objective simulates PEFT to both retain the PLM's task-specific performance and ensure that the backdoor persists after fine-tuning. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and clean accuracy, even when the attacker does not have full knowledge of the victim user's training process.

摘要: 参数高效微调(PEFT)使预先训练的语言模型(PLM)能够有效地适应特定任务。通过只调整最小的一组(额外)参数，PEFT实现了与标准微调相当的性能。然而，尽管PEFT被广泛使用，但其安全影响在很大程度上仍未被探索。在本文中，我们采取了初步的步骤，并提出了一种新的木马攻击PETA，它通过双层优化考虑下游适应来折衷PLM的权重：上层目标将后门嵌入到模型中，而下层目标模拟PEFT，既保留了PLM的特定任务性能，又确保了微调后后门的存在。通过对各种下游任务和触发器设计的广泛评估，我们证明了PETA在攻击成功率和干净准确性方面的有效性，即使攻击者并不完全了解受害者用户的培训过程。



## **38. Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes**

梯度袖口：通过探索拒绝丢失景观来检测大型语言模型上的越狱攻击 cs.CR

Project page:  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense

**SubmitDate**: 2024-03-05    [abs](http://arxiv.org/abs/2403.00867v2) [paper-pdf](http://arxiv.org/pdf/2403.00867v2)

**Authors**: Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho

**Abstract**: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the LLM's rejection capability for malicious jailbreak queries, while maintaining the model's performance for benign user queries by adjusting the detection threshold.

摘要: 大型语言模型（LLM）正在成为一个突出的生成AI工具，用户输入查询，LLM生成答案。为了减少伤害和滥用，人们已经努力将这些LLM与人类价值观相一致，使用先进的训练技术，如来自人类反馈的强化学习（RLHF）。然而，最近的研究强调了LLM容易受到旨在颠覆嵌入式安全护栏的敌对越狱企图。针对这一挑战，本文定义并研究了LLM的拒绝丢失，然后提出了一种称为梯度袖口的方法来检测越狱尝试。梯度袖口利用拒绝损失景观中观察到的独特属性，包括函数值和平滑度，设计了一个有效的两步检测策略。实验结果表明，梯度Cuff可以显著提高LLaMA—2—7B—Chat和Vicuna—7B—V1. 5对恶意越狱查询的拒绝能力，同时通过调整检测阈值，保持模型对良性用户查询的性能。



## **39. Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT**

中毒对模型参数和神经元激活影响的测量——以中毒CodeBERT为例 cs.SE

**SubmitDate**: 2024-03-05    [abs](http://arxiv.org/abs/2402.12936v2) [paper-pdf](http://arxiv.org/pdf/2402.12936v2)

**Authors**: Aftab Hussain, Md Rafiqul Islam Rabin, Navid Ayoobi, Mohammad Amin Alipour

**Abstract**: Large language models (LLMs) have revolutionized software development practices, yet concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans. Backdoor attacks involve the insertion of triggers into training data, allowing attackers to manipulate the behavior of the model maliciously. In this paper, we focus on analyzing the model parameters to detect potential backdoor signals in code models. Specifically, we examine attention weights and biases, activation values, and context embeddings of the clean and poisoned CodeBERT models. Our results suggest noticeable patterns in activation values and context embeddings of poisoned samples for the poisoned CodeBERT model; however, attention weights and biases do not show any significant differences. This work contributes to ongoing efforts in white-box detection of backdoor signals in LLMs of code through the analysis of parameters and activations.

摘要: 大型语言模型（LLM）已经彻底改变了软件开发实践，但人们对它们安全性的担忧也随之出现，特别是关于隐藏的后门，也就是特洛伊木马。后门攻击涉及将触发器插入训练数据，允许攻击者恶意操纵模型的行为。在本文中，我们重点分析模型参数，以检测潜在后门信号在代码模型。具体而言，我们研究了干净和有毒CodeBRT模型的注意力权重和偏差、激活值和上下文嵌入。我们的研究结果表明中毒CodeBERT模型中中毒样本的激活值和上下文嵌入存在明显的模式；然而，注意力权重和偏差没有显示出任何显著差异。这项工作有助于通过参数和激活的分析在代码的LLM后门信号的白盒检测。



## **40. InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents**

注入代理：对工具集成的大型语言模型代理中的间接提示注入进行基准测试 cs.CL

26 pages, 5 figures, 7 tables

**SubmitDate**: 2024-03-05    [abs](http://arxiv.org/abs/2403.02691v1) [paper-pdf](http://arxiv.org/pdf/2403.02691v1)

**Authors**: Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang

**Abstract**: Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.   In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.

摘要: 最近的工作已经体现了LLM作为代理，允许他们访问工具，执行操作，并与外部内容（例如，电子邮件或网站）。然而，外部内容引入了间接提示注入（IPI）攻击的风险，其中恶意指令嵌入到LLM处理的内容中，旨在操纵这些代理对用户执行有害操作。鉴于此类攻击可能造成的严重后果，必须建立评估和减轻这些风险的基准。   在这项工作中，我们介绍了InjectAgent，一个基准，旨在评估工具集成的LLM代理IPI攻击的脆弱性。InjectAgent包含1，054个测试用例，涵盖17个不同的用户工具和62个攻击者工具。我们将攻击意图分为两种主要类型：直接伤害用户和泄露私人数据。我们评估了30种不同的LLM代理，并表明代理容易受到IPI攻击，其中ReAct提示的GPT—4在24%的时间内容易受到攻击。对增强设置的进一步调查显示，攻击者的指令通过黑客提示被加强，成功率进一步增加，在ReAct提示的GPT—4上，攻击成功率几乎增加了一倍。我们的研究结果提出了LLM代理的广泛部署的问题。我们的基准测试可在www.example.com获得。



## **41. KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection**

KnowPhish：大型语言模型满足多模式知识图以增强基于参考的网络钓鱼检测 cs.CR

**SubmitDate**: 2024-03-04    [abs](http://arxiv.org/abs/2403.02253v1) [paper-pdf](http://arxiv.org/pdf/2403.02253v1)

**Authors**: Yuexin Li, Chengyu Huang, Shumin Deng, Mei Lin Lock, Tri Cao, Nay Oo, Bryan Hooi, Hoon Wei Lim

**Abstract**: Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second limitation of existing RBPDs is that they solely rely on the image modality, ignoring useful textual information present in the webpage HTML. To utilize this textual information, we propose a Large Language Model (LLM)-based approach to extract brand information of webpages from text. Our resulting multimodal phishing detection approach, KnowPhish Detector (KPD), can detect phishing webpages with or without logos. We evaluate KnowPhish and KPD on a manually validated dataset, and on a field study under Singapore's local context, showing substantial improvements in effectiveness and efficiency compared to state-of-the-art baselines.

摘要: 网络钓鱼攻击给个人和企业造成了巨大的损失，这就需要开发强大而高效的自动网络钓鱼检测方法。基于引用的网络钓鱼检测器（RBPD）将目标网页上的徽标与已知徽标集进行比较，已成为最先进的方法。然而，现有的RBPD的一个主要局限性是它们依赖于人工构建的品牌知识库，使得无法扩展到大量品牌，这导致由于知识库的品牌覆盖率不足而导致假否定错误。为了解决这个问题，我们提出了一个自动化的知识收集管道，使用它，我们收集和发布了一个大规模的多模态品牌知识库，KnowPhish，其中包含了20k个品牌，每个品牌的丰富信息。KnowPhish可用于以即插即用的方式提高现有RBPD的性能。现有RBPD的第二个限制是它们仅仅依赖于图像模态，忽略了网页HTML中存在的有用文本信息。为了充分利用这些文本信息，我们提出了一种基于大语言模型（LLM）的方法，从文本中提取网页的品牌信息。我们得到的多模式网络钓鱼检测方法，KnowPhish Detector（KPD），可以检测到有或没有徽标的网络钓鱼网页。我们评估KnowPhish和KPD的手动验证数据集，并在新加坡当地的情况下进行实地研究，显示与最先进的基线相比，有效性和效率有了实质性的改善。



## **42. Rethinking Model Ensemble in Transfer-based Adversarial Attacks**

基于迁移的对抗性攻击中模型集成的再思考 cs.CV

**SubmitDate**: 2024-03-04    [abs](http://arxiv.org/abs/2303.09105v2) [paper-pdf](http://arxiv.org/pdf/2303.09105v2)

**Authors**: Huanran Chen, Yichi Zhang, Yinpeng Dong, Xiao Yang, Hang Su, Jun Zhu

**Abstract**: It is widely recognized that deep learning models lack robustness to adversarial examples. An intriguing property of adversarial examples is that they can transfer across different models, which enables black-box attacks without any knowledge of the victim model. An effective strategy to improve the transferability is attacking an ensemble of models. However, previous works simply average the outputs of different models, lacking an in-depth analysis on how and why model ensemble methods can strongly improve the transferability. In this paper, we rethink the ensemble in adversarial attacks and define the common weakness of model ensemble with two properties: 1) the flatness of loss landscape; and 2) the closeness to the local optimum of each model. We empirically and theoretically show that both properties are strongly correlated with the transferability and propose a Common Weakness Attack (CWA) to generate more transferable adversarial examples by promoting these two properties. Experimental results on both image classification and object detection tasks validate the effectiveness of our approach to improving the adversarial transferability, especially when attacking adversarially trained models. We also successfully apply our method to attack a black-box large vision-language model -- Google's Bard, showing the practical effectiveness. Code is available at \url{https://github.com/huanranchen/AdversarialAttacks}.

摘要: 人们普遍认为，深度学习模型对对抗性例子缺乏稳健性。对抗性例子的一个耐人寻味的特性是，它们可以在不同的模型之间传输，这使得在不知道受害者模型的情况下进行黑盒攻击。提高可转移性的一个有效策略是攻击一系列模型。然而，以往的工作只是简单地对不同模型的输出进行平均，而缺乏对模型集成方法如何以及为什么能够显著提高可转移性的深入分析。在本文中，我们重新考虑了对抗性攻击中的集成，并定义了模型集成的两个共同弱点：1)损失图景的平坦性；2)每个模型接近局部最优。我们从经验和理论上证明了这两个性质与可转移性有很强的相关性，并提出了一种共同弱点攻击(CWA)，通过提升这两个性质来生成更多可转移的对抗性实例。在图像分类和目标检测任务上的实验结果验证了该方法的有效性，特别是在攻击对抗性训练模型时。我们还成功地应用我们的方法攻击了一个黑盒大视觉语言模型--Google的BARD，显示了它的实际有效性。代码可在\url{https://github.com/huanranchen/AdversarialAttacks}.上找到



## **43. One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models**

一个提示词足以增强预训练视觉语言模型的对抗鲁棒性 cs.CV

CVPR2024

**SubmitDate**: 2024-03-04    [abs](http://arxiv.org/abs/2403.01849v1) [paper-pdf](http://arxiv.org/pdf/2403.01849v1)

**Authors**: Lin Li, Haoyan Guan, Jianing Qiu, Michael Spratling

**Abstract**: Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.

摘要: 像CLIP这样的大型预训练视觉语言模型（VLM），尽管具有非凡的泛化能力，但非常容易受到对抗性示例的影响。这项工作从文本提示的新颖角度研究VLM的对抗鲁棒性，而不是广泛研究的模型权重（在这项工作中冻结）。我们首先表明，对抗性攻击和防御的有效性对所使用的文本提示是敏感的。受此启发，我们提出了一种方法，以提高对抗攻击的韧性，通过学习一个强大的文本提示为VLM。所提出的方法，称为对抗性提示调整（APT），是有效的，同时计算和数据效率。在15个数据集和4个数据稀疏方案（从单次射击到完全训练数据设置）上进行了广泛的实验，以显示APT优于手工设计的提示和其他最先进的适应方法。APT在输入分布移位和跨数据集的推广方面表现出优异的能力。令人惊讶的是，通过简单地在提示中添加一个学习的单词，APT可以显著地提高准确性和鲁棒性（t = 4/255），比手工设计的提示平均分别提高+13%和+8.5%。在我们最有效的设置下，精度提高到+26.4%，稳健性提高到+16.7%。代码可在www.example.com获得。



## **44. SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models**

Salad-BENCH：一种适用于大型语言模型的分层综合安全基准 cs.CL

fix institution typo

**SubmitDate**: 2024-03-04    [abs](http://arxiv.org/abs/2402.05044v3) [paper-pdf](http://arxiv.org/pdf/2402.05044v3)

**Authors**: Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao

**Abstract**: In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under https://github.com/OpenSafetyLab/SALAD-BENCH.

摘要: 在快速发展的大型语言模型（LLM）环境中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一个专门为评估LLM、攻击和防御方法而设计的安全基准。SALAD—Bench以其广泛性而闻名，其规模大、多样性强、跨越三个层次的复杂分类法以及多功能性强，超越了传统的基准测试。SALAD—Bench包含了一系列细致的问题，从标准查询到复杂的问题，包含了攻击、防御修改和多项选择。为了有效地管理固有的复杂性，我们引入了一个创新的评估器：基于LLM的MD—Judge for QA对，特别关注攻击增强的查询，确保无缝，可靠的评估。以上组件将SALAD—Bench从标准的LLM安全评估扩展到LLM攻击和防御方法评估，确保了共同使用。我们广泛的实验揭示了LLM对新兴威胁的弹性和当代防御策略的有效性。数据和评估人员在www.example.com上发布。



## **45. LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper**

LLM可以以实际的方式保护自己免受越狱：愿景文件 cs.CR

Fixed the bibliography reference issue in our LLM jailbreak defense  vision paper submitted on 24 Feb 2024

**SubmitDate**: 2024-03-04    [abs](http://arxiv.org/abs/2402.15727v2) [paper-pdf](http://arxiv.org/pdf/2402.15727v2)

**Authors**: Daoyuan Wu, Shuai Wang, Yang Liu, Ning Liu

**Abstract**: Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs). A considerable amount of research exists proposing more effective jailbreak attacks, including the recent Greedy Coordinate Gradient (GCG) attack, jailbreak template-based attacks such as using "Do-Anything-Now" (DAN), and multilingual jailbreak. In contrast, the defensive side has been relatively less explored. This paper proposes a lightweight yet practical defense called SELFDEFEND, which can defend against all existing jailbreak attacks with minimal delay for jailbreak prompts and negligible delay for normal user prompts. Our key insight is that regardless of the kind of jailbreak strategies employed, they eventually need to include a harmful prompt (e.g., "how to make a bomb") in the prompt sent to LLMs, and we found that existing LLMs can effectively recognize such harmful prompts that violate their safety policies. Based on this insight, we design a shadow stack that concurrently checks whether a harmful prompt exists in the user prompt and triggers a checkpoint in the normal stack once a token of "No" or a harmful prompt is output. The latter could also generate an explainable LLM response to adversarial prompts. We demonstrate our idea of SELFDEFEND works in various jailbreak scenarios through manual analysis in GPT-3.5/4. We also list three future directions to further enhance SELFDEFEND.

摘要: 越狱是一种新兴的对抗性攻击，它绕过了现成的大型语言模型(LLM)中部署的安全对齐。已有大量研究提出了更有效的越狱攻击方案，包括最近的贪婪坐标梯度(GCG)攻击、基于模板的越狱攻击(如使用“Do-Anything-Now”(DAN))和多语言越狱。相比之下，防守方面的探索相对较少。本文提出了一种轻量级而实用的防御方法SELFDEFEND，它可以防御所有现有的越狱攻击，而越狱提示的延迟最小，正常用户提示的延迟可以忽略不计。我们的主要见解是，无论采用哪种越狱策略，他们最终都需要在发送给LLMS的提示中包含有害提示(例如，如何制造炸弹)，我们发现现有LLMS可以有效地识别此类违反其安全政策的有害提示。基于这一观点，我们设计了一个影子堆栈，该堆栈同时检查用户提示中是否存在有害提示，并在输出令牌“否”或有害提示时触发正常堆栈中的检查点。后者还可以对对抗性提示产生可解释的LLM响应。我们通过GPT-3.5/4中的手动分析，展示了我们的SELFDEFEND在各种越狱场景中的工作原理。我们还列出了进一步增强SELFDEFEND的三个未来方向。



## **46. Multilingual Jailbreak Challenges in Large Language Models**

大型语言模型中的多语言越狱挑战 cs.CL

ICLR 2024

**SubmitDate**: 2024-03-04    [abs](http://arxiv.org/abs/2310.06474v3) [paper-pdf](http://arxiv.org/pdf/2310.06474v3)

**Authors**: Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing

**Abstract**: While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\% for ChatGPT and 40.71\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at \url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.

摘要: 虽然大型语言模型（LLM）在广泛的任务中表现出非凡的能力，但它们带来了潜在的安全问题，例如“越狱”问题，其中恶意指令可以操纵LLM以表现不受欢迎的行为。虽然已经制定了几项预防措施，以减轻与LLM相关的潜在风险，但这些措施主要集中在英语。在这项研究中，我们揭示了LLM中存在的多语言越狱挑战，并考虑了两种潜在的风险场景：无意和有意。无意的情况涉及用户使用非英语提示查询LLM并无意中绕过安全机制，而有意的情况涉及恶意用户将恶意指令与多语言提示相结合，故意攻击LLM。实验结果表明，在非故意场景中，不安全内容的发生率随着语言可用性的降低而增加。具体而言，低资源语言遇到有害内容的可能性是高资源语言的三倍，包括ChatGPT和GPT—4。在故意的情况下，多语言提示会加剧恶意指令的负面影响，不安全输出的比率高得惊人：ChatGPT为80.92\%，GPT—4为40.71\%。为了在多语言环境中应对这样的挑战，我们提出了一个新的\textsc {Self—Defense}框架，该框架自动生成多语言训练数据以进行安全微调。实验结果表明，使用这些数据进行微调的ChatGPT可以大幅减少不安全内容的生成。数据可在\url {https：//github.com/DAMO—NLP—SG/multilingual—safety—for—LLM}获取。



## **47. Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models**

突破防御：大型语言模型攻击的比较研究 cs.CR

**SubmitDate**: 2024-03-03    [abs](http://arxiv.org/abs/2403.04786v1) [paper-pdf](http://arxiv.org/pdf/2403.04786v1)

**Authors**: Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha

**Abstract**: Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insights into the current landscape of LLM vulnerabilities and defense mechanisms. Our objective is to offer a nuanced understanding of LLM attacks, foster awareness within the AI community, and inspire robust solutions to mitigate these risks in future developments.

摘要: 大型语言模型(LLM)已经成为自然语言处理(NLP)领域的基石，在理解和生成类似人类的文本方面提供了变革性的能力。然而，随着它们的日益突出，这些模型的安全和漏洞方面已经引起了极大的关注。本文对各种形式的针对LLMS的攻击进行了全面的综述，讨论了这些攻击的性质和机制、它们的潜在影响以及当前的防御策略。我们深入探讨了旨在操纵模型输出的对抗性攻击、影响模型训练的数据中毒以及与训练数据利用相关的隐私问题等主题。文中还探讨了不同攻击方法的有效性，LLMS对这些攻击的恢复能力，以及对模型完整性和用户信任的影响。通过检查最新的研究，我们提供了对LLM漏洞和防御机制的当前情况的见解。我们的目标是提供对LLM攻击的细微差别的理解，培养人工智能社区的意识，并激发强大的解决方案，以减轻未来发展中的这些风险。



## **48. Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition**

HackAPrompt：Exposing System Vulnerabilities of LLM Through a Global Scale Prompt Hacking Competition cs.CR

34 pages, 8 figures Codebase:  https://github.com/PromptLabs/hackaprompt Dataset:  https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset/blob/main/README.md  Playground: https://huggingface.co/spaces/hackaprompt/playground

**SubmitDate**: 2024-03-03    [abs](http://arxiv.org/abs/2311.16119v3) [paper-pdf](http://arxiv.org/pdf/2311.16119v3)

**Authors**: Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan Boyd-Graber

**Abstract**: Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts.

摘要: 大型语言模型（LLM）部署在具有直接用户参与的交互式环境中，例如聊天机器人和写作助手。这些部署很容易受到即时注入和越狱（统称为即时黑客攻击）的攻击，其中模型被操纵以忽略其原始指令并遵循潜在的恶意指令。虽然被广泛认为是一个重大的安全威胁，但关于即时黑客攻击的大规模资源和定量研究却非常缺乏。为了解决这一缺陷，我们发起了一个全球即时黑客竞赛，允许自由形式的人工输入攻击。我们对三个最先进的LLM引出600K+对抗性提示。我们描述了数据集，它通过经验验证了当前的LLM确实可以通过即时黑客操作。我们还提出了一个全面的分类本体论类型的对抗提示。



## **49. Analysis of Privacy Leakage in Federated Large Language Models**

联邦大型语言模型中的隐私泄露分析 cs.CR

**SubmitDate**: 2024-03-02    [abs](http://arxiv.org/abs/2403.04784v1) [paper-pdf](http://arxiv.org/pdf/2403.04784v1)

**Authors**: Minh N. Vu, Truc Nguyen, Tre' R. Jeter, My T. Thai

**Abstract**: With the rapid adoption of Federated Learning (FL) as the training and tuning protocol for applications utilizing Large Language Models (LLMs), recent research highlights the need for significant modifications to FL to accommodate the large-scale of LLMs. While substantial adjustments to the protocol have been introduced as a response, comprehensive privacy analysis for the adapted FL protocol is currently lacking.   To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives. In particular, we design two active membership inference attacks with guaranteed theoretical success rates to assess the privacy leakages of various adapted FL configurations. Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets. Additionally, we conduct thorough experiments to evaluate the privacy leakage of these models when data is protected by state-of-the-art differential privacy (DP) mechanisms.

摘要: 随着联邦学习（FL）作为使用大型语言模型（LLM）的应用程序的训练和调优协议的迅速采用，最近的研究强调需要对FL进行重大修改以适应大规模LLM。虽然已经对协议进行了实质性的调整，但目前缺乏对修改后的FL协议的全面隐私分析。   为了解决这一差距，我们的工作深入研究了一个广泛的研究，当用于培训LLM时，从理论和实践的角度。特别是，我们设计了两个主动成员推理攻击与保证的理论成功率，以评估各种适应FL配置的隐私泄漏。我们的理论发现被转化为实际攻击，揭示了流行的LLM中存在的大量隐私漏洞，包括BERT、RoBERTa、DistilBERT和OpenAI的GPL，跨多个现实世界的语言数据集。此外，我们进行了彻底的实验，以评估这些模型的隐私泄漏时，数据的国家的最先进的差分隐私（DP）机制。



## **50. AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks**

自卫：针对越狱攻击的多代理LLM防御 cs.LG

**SubmitDate**: 2024-03-02    [abs](http://arxiv.org/abs/2403.04783v1) [paper-pdf](http://arxiv.org/pdf/2403.04783v1)

**Authors**: Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu

**Abstract**: Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multi-agent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and data are publicly available at https://github.com/XHMY/AutoDefense.

摘要: 尽管进行了广泛的预训练和道德调整，以防止应用户请求生成有害信息，但大型语言模型（LLM）仍然容易受到越狱攻击。在本文中，我们提出了AutoDefense，一个基于响应过滤的多代理防御框架，过滤来自LLM的有害响应。该框架为LLM代理分配了不同的角色，并利用它们协同完成防御任务。任务的划分增强了LLM的总体跟踪，并使其他防御组件作为工具的集成。AutoDefense可以适应各种大小和种类的开源LLM作为代理。通过对大规模的有害和安全提示进行广泛的实验，我们验证了所提出的AutoDefense在提高对越狱攻击的鲁棒性的同时，在正常用户请求下保持性能的有效性。我们的代码和数据可在www.example.com上公开获取。



