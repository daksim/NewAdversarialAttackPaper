# Latest Adversarial Attack Papers
**update at 2024-08-06 10:08:34**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models**

SEAS：大型语言模型的自进化对抗安全优化 cs.CL

**SubmitDate**: 2024-08-05    [abs](http://arxiv.org/abs/2408.02632v1) [paper-pdf](http://arxiv.org/pdf/2408.02632v1)

**Authors**: Muxi Diao, Rumei Li, Shiyang Liu, Guogang Liao, Jingang Wang, Xunliang Cai, Weiran Xu

**Abstract**: As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\mathbf{S}\text{elf-}\mathbf{E}\text{volving }\mathbf{A}\text{dversarial }\mathbf{S}\text{afety }\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduces reliance on manual testing and significantly enhances the security capabilities of LLMs. Our contributions include a novel adversarial framework, a comprehensive safety dataset, and after three iterations, the Target model achieves a security level comparable to GPT-4, while the Red Team model shows a marked increase in attack success rate (ASR) against advanced models.

摘要: 随着大型语言模型在能力和影响力方面的不断进步，确保它们的安全和防止有害输出变得至关重要。解决这些担忧的一个有希望的方法是建立训练模型，为红色团队自动生成对抗性提示。然而，LLMS中不断演变的漏洞的微妙之处挑战了当前对抗性方法的有效性，这些方法难以具体针对和探索这些模型的弱点。为了应对这些挑战，我们引入了$\mathbf{S}\Text{ELF-}\mathbf{E}\Text{volving}\mathbf{A}\Text{dversarial}\mathbf{S}\Text{afty}\mathbf{(SEA)}$优化框架，该框架通过利用模型本身生成的数据来增强安全性。SEA经历了三个迭代阶段：初始化、攻击和对抗性优化，完善了Red Team和Target模型，以提高健壮性和安全性。该框架减少了对手动测试的依赖，显著增强了LLMS的安全能力。我们的贡献包括一个新的对抗性框架，一个全面的安全数据集，经过三次迭代，Target模型达到了与GPT-4相当的安全级别，而Red Team模型显示出相对于高级模型在攻击成功率(ASR)方面的显著提高。



## **2. SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications**

SSAP：一种形状敏感对抗补丁，用于全面破坏自主导航应用中单目深度估计 cs.CV

arXiv admin note: text overlap with arXiv:2303.01351

**SubmitDate**: 2024-08-05    [abs](http://arxiv.org/abs/2403.11515v2) [paper-pdf](http://arxiv.org/pdf/2403.11515v2)

**Authors**: Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Bassem Ouni, Muhammad Shafique

**Abstract**: Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of convolutional neural networks (CNNs) and more recently, Transformers. However, concerns about their susceptibility to adversarial attacks have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing CNN-based depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system's perspective. Notably, our patch is shape-sensitive, meaning it considers the specific shape and scale of the target object, thereby extending its influence beyond immediate proximity. Furthermore, our patch is trained to effectively address different scales and distances from the camera. Experimental results demonstrate that our approach induces a mean depth estimation error surpassing 0.5, impacting up to 99% of the targeted region for CNN-based MDE models. Additionally, we investigate the vulnerability of Transformer-based MDE models to patch-based attacks, revealing that SSAP yields a significant error of 0.59 and exerts substantial influence over 99% of the target region on these models.

摘要: 单眼深度估计(MDE)已经有了显著的进步，主要是通过卷积神经网络(CNN)的集成，以及最近的Transformers。然而，对它们易受对手攻击的担忧已经出现，特别是在自动驾驶和机器人导航等安全关键领域。现有的评估基于CNN的深度预测方法的方法在导致视觉系统全面中断方面做得不够，通常仅限于特定的局部地区。在本文中，我们介绍了形状敏感对抗性补丁(SSAP)，一种新的方法，旨在全面扰乱单眼深度估计(MDE)在自主导航应用中。我们的补丁是为了有选择地以两种不同的方式削弱MDE：通过扭曲估计的距离或通过创造物体从系统的角度消失的错觉。值得注意的是，我们的面片是形状敏感的，这意味着它考虑目标对象的特定形状和比例，从而将其影响扩展到直接邻近之外。此外，我们的补丁经过训练，可以有效地处理不同比例和距离相机的问题。实验结果表明，对于基于CNN的MDE模型，该方法的平均深度估计误差超过0.5，影响高达99%的目标区域。此外，我们研究了基于Transformer的MDE模型对基于补丁的攻击的脆弱性，发现SSAP产生了0.59的显著误差，并且对这些模型上99%的目标区域产生了重大影响。



## **3. APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation**

APARATE：用于自主导航基于CNN的单目深度估计的自适应对抗补丁 cs.CV

**SubmitDate**: 2024-08-05    [abs](http://arxiv.org/abs/2303.01351v3) [paper-pdf](http://arxiv.org/pdf/2303.01351v3)

**Authors**: Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique

**Abstract**: In recent times, monocular depth estimation (MDE) has experienced significant advancements in performance, largely attributed to the integration of innovative architectures, i.e., convolutional neural networks (CNNs) and Transformers. Nevertheless, the susceptibility of these models to adversarial attacks has emerged as a noteworthy concern, especially in domains where safety and security are paramount. This concern holds particular weight for MDE due to its critical role in applications like autonomous driving and robotic navigation, where accurate scene understanding is pivotal. To assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE. However, the existing approaches fall short of inducing a comprehensive and substantially disruptive impact on the vision system. Instead, their influence is partial and confined to specific local areas. These methods lead to erroneous depth predictions only within the overlapping region with the input image, without considering the characteristics of the target object, such as its size, shape, and position. In this paper, we introduce a novel adversarial patch named APARATE. This patch possesses the ability to selectively undermine MDE in two distinct ways: by distorting the estimated distances or by creating the illusion of an object disappearing from the perspective of the autonomous system. Notably, APARATE is designed to be sensitive to the shape and scale of the target object, and its influence extends beyond immediate proximity. APARATE, results in a mean depth estimation error surpassing $0.5$, significantly impacting as much as $99\%$ of the targeted region when applied to CNN-based MDE models. Furthermore, it yields a significant error of $0.34$ and exerts substantial influence over $94\%$ of the target region in the context of Transformer-based MDE.

摘要: 近年来，单目深度估计(MDE)在性能上取得了显著的进步，这在很大程度上归功于卷积神经网络(CNN)和变压器等创新体系结构的集成。然而，这些模型对对抗性攻击的易感性已经成为一个值得关注的问题，特别是在安全和安保至上的领域。这一担忧对MDE来说尤为重要，因为它在自动驾驶和机器人导航等应用中扮演着关键角色，在这些应用中，准确的场景理解至关重要。为了评估基于CNN的深度预测方法的脆弱性，最近的工作试图设计对抗MDE的对抗性补丁。然而，现有的方法不能对视觉系统造成全面和实质性的颠覆性影响。相反，他们的影响是部分的，仅限于特定的当地地区。这些方法只在与输入图像重叠的区域内导致错误的深度预测，而没有考虑目标对象的特征，例如其大小、形状和位置。在本文中，我们介绍了一种新的对抗性补丁APARATE。这个补丁能够以两种不同的方式选择性地削弱MDE：通过扭曲估计的距离或通过从自主系统的角度创造物体消失的错觉。值得注意的是，APARATE被设计为对目标对象的形状和比例敏感，其影响超出了直接接近的范围。APARATE，导致平均深度估计误差超过$0.5$，当应用于基于CNN的MDE模型时，显著影响目标区域的$99\$。此外，在基于变压器的MDE的背景下，它产生了$0.34$的显著误差，并对目标区域的$94\$产生了重大影响。



## **4. Open Sesame! Universal Black Box Jailbreaking of Large Language Models**

芝麻开门！大型语言模型的通用黑匣子越狱 cs.CL

Accepted at SeT-LLM @ ICLR 2024

**SubmitDate**: 2024-08-05    [abs](http://arxiv.org/abs/2309.01446v4) [paper-pdf](http://arxiv.org/pdf/2309.01446v4)

**Authors**: Raz Lapid, Ron Langberg, Moshe Sipper

**Abstract**: Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool for evaluating and enhancing alignment of LLMs with human intent. To our knowledge this is the first automated universal black box jailbreak attack.

摘要: 大型语言模型(LLM)旨在提供有用和安全的响应，它们通常依靠对齐技术来与用户意图和社交指南保持一致。遗憾的是，恶意行为者可能会利用这种对齐，试图出于非预期目的操纵LLM的输出。在本文中，我们介绍了一种新的方法，即在模型结构和参数不可访问的情况下，使用遗传算法(GA)来操作LLM。GA攻击的工作原理是优化一个通用的对抗性提示，当与用户的查询结合在一起时，会扰乱被攻击模型的对齐，导致意外的和潜在的有害输出。我们的新方法通过揭示模型响应偏离预期行为的实例，系统地揭示了模型的局限性和漏洞。通过广泛的实验，我们展示了我们技术的有效性，从而通过提供一种诊断工具来评估和增强LLM与人类意图的一致性，从而为正在进行的关于负责任的人工智能开发的讨论做出贡献。据我们所知，这是第一次自动通用黑匣子越狱攻击。



## **5. On the Robustness of Malware Detectors to Adversarial Samples**

恶意软件检测器对对抗样本的鲁棒性 cs.CR

This is the full version of the paper with the same title to appear  in the proceedings of the 2024 Workshop on Security and Artificial  Intelligence (SECAI 2024)

**SubmitDate**: 2024-08-05    [abs](http://arxiv.org/abs/2408.02310v1) [paper-pdf](http://arxiv.org/pdf/2408.02310v1)

**Authors**: Muhammad Salman, Benjamin Zi Hao Zhao, Hassan Jameel Asghar, Muhammad Ikram, Sidharth Kaushik, Mohamed Ali Kaafar

**Abstract**: Adversarial examples add imperceptible alterations to inputs with the objective to induce misclassification in machine learning models. They have been demonstrated to pose significant challenges in domains like image classification, with results showing that an adversarially perturbed image to evade detection against one classifier is most likely transferable to other classifiers. Adversarial examples have also been studied in malware analysis. Unlike images, program binaries cannot be arbitrarily perturbed without rendering them non-functional. Due to the difficulty of crafting adversarial program binaries, there is no consensus on the transferability of adversarially perturbed programs to different detectors. In this work, we explore the robustness of malware detectors against adversarially perturbed malware. We investigate the transferability of adversarial attacks developed against one detector, against other machine learning-based malware detectors, and code similarity techniques, specifically, locality sensitive hashing-based detectors. Our analysis reveals that adversarial program binaries crafted for one detector are generally less effective against others. We also evaluate an ensemble of detectors and show that they can potentially mitigate the impact of adversarial program binaries. Finally, we demonstrate that substantial program changes made to evade detection may result in the transformation technique being identified, implying that the adversary must make minimal changes to the program binary.

摘要: 对抗性的例子给输入增加了不知不觉的改变，目的是在机器学习模型中导致错误分类。它们已经被证明在像图像分类这样的领域中构成了巨大的挑战，结果表明，被恶意扰动以逃避针对一个分类器的检测的图像最有可能被转移到其他分类器。恶意软件分析中也研究了敌意例子。与图像不同，程序二进制文件不能在不使其不起作用的情况下被任意干扰。由于制作敌意程序二进制文件的难度，对于敌意干扰程序到不同检测器的可转移性，目前还没有达成共识。在这项工作中，我们探索了恶意软件检测器对恶意软件的健壮性。我们研究了针对一个检测器、针对其他基于机器学习的恶意软件检测器以及代码相似性技术，特别是基于位置敏感的散列检测器的恶意攻击的可转移性。我们的分析表明，为一个检测器精心设计的敌意程序二进制文件通常对其他检测器不那么有效。我们还评估了检测器集成，并表明它们可以潜在地缓解对抗性程序二进制文件的影响。最后，我们证明了为逃避检测而进行的大量程序更改可能会导致转换技术被识别，这意味着攻击者必须对程序二进制文件进行最小程度的更改。



## **6. Model Hijacking Attack in Federated Learning**

联邦学习中的劫持攻击模型 cs.CR

**SubmitDate**: 2024-08-04    [abs](http://arxiv.org/abs/2408.02131v1) [paper-pdf](http://arxiv.org/pdf/2408.02131v1)

**Authors**: Zheng Li, Siyuan Wu, Ruichuan Chen, Paarijaat Aditya, Istemi Ekin Akkus, Manohar Vanga, Min Zhang, Hao Li, Yang Zhang

**Abstract**: Machine learning (ML), driven by prominent paradigms such as centralized and federated learning, has made significant progress in various critical applications ranging from autonomous driving to face recognition. However, its remarkable success has been accompanied by various attacks. Recently, the model hijacking attack has shown that ML models can be hijacked to execute tasks different from their original tasks, which increases both accountability and parasitic computational risks. Nevertheless, thus far, this attack has only focused on centralized learning. In this work, we broaden the scope of this attack to the federated learning domain, where multiple clients collaboratively train a global model without sharing their data. Specifically, we present HijackFL, the first-of-its-kind hijacking attack against the global model in federated learning. The adversary aims to force the global model to perform a different task (called hijacking task) from its original task without the server or benign client noticing. To accomplish this, unlike existing methods that use data poisoning to modify the target model's parameters, HijackFL searches for pixel-level perturbations based on their local model (without modifications) to align hijacking samples with the original ones in the feature space. When performing the hijacking task, the adversary applies these cloaks to the hijacking samples, compelling the global model to identify them as original samples and predict them accordingly. We conduct extensive experiments on four benchmark datasets and three popular models. Empirical results demonstrate that its attack performance outperforms baselines. We further investigate the factors that affect its performance and discuss possible defenses to mitigate its impact.

摘要: 在集中式学习和联合学习等重要范式的推动下，机器学习在从自动驾驶到人脸识别的各种关键应用中取得了重大进展。然而，它的显著成功也伴随着各种攻击。最近的模型劫持攻击表明，ML模型可以被劫持来执行与其原始任务不同的任务，这既增加了责任追究，又增加了寄生计算风险。然而，到目前为止，这种攻击只集中在集中学习上。在这项工作中，我们将这种攻击的范围扩大到联合学习领域，在该领域中，多个客户端协作训练全局模型，而不共享他们的数据。具体地说，我们介绍了HijackFL，这是联合学习中针对全球模型的第一次此类劫持攻击。敌手的目的是迫使全局模型执行与其原始任务不同的任务(称为劫持任务)，而不会引起服务器或良性客户端的注意。为此，与现有的使用数据毒化来修改目标模型参数的方法不同，HijackFL基于其本地模型(无需修改)搜索像素级扰动，以将劫持样本与特征空间中的原始样本对齐。在执行劫持任务时，对手将这些伪装应用到劫持样本上，迫使全局模型将它们识别为原始样本并相应地预测它们。我们在四个基准数据集和三个流行的模型上进行了广泛的实验。实验结果表明，该算法的攻击性能优于基线。我们进一步调查了影响其性能的因素，并讨论了可能的防御措施以减轻其影响。



## **7. LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples**

法学硕士谎言：幻觉不是错误，而是对抗性例子的特征 cs.CL

**SubmitDate**: 2024-08-04    [abs](http://arxiv.org/abs/2310.01469v3) [paper-pdf](http://arxiv.org/pdf/2310.01469v3)

**Authors**: Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Yu-Yang Liu, Li Yuan

**Abstract**: Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still cannot completely trust their answers, since LLMs suffer from \textbf{hallucination}\textemdash fabricating non-existent facts, deceiving users with or without their awareness. However, the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that nonsensical prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. Moreover, we provide both theoretical and experimental evidence that transformers can be manipulated to produce specific pre-define tokens by perturbing its input sequence. This phenomenon forces us to revisit that \emph{hallucination may be another view of adversarial examples}, and it shares similar characteristics with conventional adversarial examples as a basic property of LLMs. Therefore, we formalize an automatic hallucination triggering method as the \textit{hallucination attack} in an adversarial way. Finally, we explore the basic properties of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub\footnote{https://github.com/PKU-YuanGroup/Hallucination-Attack}.

摘要: 大型语言模型(LLM)，包括GPT-3.5、骆驼和Palm，似乎知识渊博，能够适应许多任务。然而，我们仍然不能完全信任他们的答案，因为LLMS遭受着捏造不存在的事实、欺骗用户或在他们意识不到的情况下的痛苦。然而，它们存在和普遍存在的原因尚不清楚。在这篇文章中，我们证明了由随机令牌组成的无意义提示也可以诱导LLM做出幻觉反应。此外，我们提供了理论和实验证据，证明可以通过扰动转换器的输入序列来操纵转换器来产生特定的预定义令牌。这一现象迫使我们重新审视幻觉可能是对抗性例子的另一种观点，它与传统对抗性例子具有相似的特征，是LLMS的一个基本性质。因此，我们将一种自动幻觉触发方法形式化为对抗性的幻觉攻击。最后，探讨了被攻击对抗性提示的基本性质，并提出了一种简单有效的防御策略。我们的代码在GitHub\footnote{https://github.com/PKU-YuanGroup/Hallucination-Attack}.上发布



## **8. AdvQDet: Detecting Query-Based Adversarial Attacks with Adversarial Contrastive Prompt Tuning**

AdvQDet：通过对抗对比提示调优检测基于查询的对抗攻击 cs.CV

**SubmitDate**: 2024-08-04    [abs](http://arxiv.org/abs/2408.01978v1) [paper-pdf](http://arxiv.org/pdf/2408.01978v1)

**Authors**: Xin Wang, Kai Chen, Xingjun Ma, Zhineng Chen, Jingjing Chen, Yu-Gang Jiang

**Abstract**: Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks even under a black-box setting where the adversary can only query the model. Particularly, query-based black-box adversarial attacks estimate adversarial gradients based on the returned probability vectors of the target model for a sequence of queries. During this process, the queries made to the target model are intermediate adversarial examples crafted at the previous attack step, which share high similarities in the pixel space. Motivated by this observation, stateful detection methods have been proposed to detect and reject query-based attacks. While demonstrating promising results, these methods either have been evaded by more advanced attacks or suffer from low efficiency in terms of the number of shots (queries) required to detect different attacks. Arguably, the key challenge here is to assign high similarity scores for any two intermediate adversarial examples perturbed from the same clean image. To address this challenge, we propose a novel Adversarial Contrastive Prompt Tuning (ACPT) method to robustly fine-tune the CLIP image encoder to extract similar embeddings for any two intermediate adversarial queries. With ACPT, we further introduce a detection framework AdvQDet that can detect 7 state-of-the-art query-based attacks with $>99\%$ detection rate within 5 shots. We also show that ACPT is robust to 3 types of adaptive attacks. Code is available at https://github.com/xinwong/AdvQDet.

摘要: 即使在对手只能查询模型的黑盒环境下，深度神经网络(DNN)也很容易受到对手的攻击。具体地，基于查询的黑盒对抗性攻击基于针对查询序列返回的目标模型的概率向量来估计对抗性梯度。在这个过程中，对目标模型的查询是在上一次攻击步骤中精心制作的中间对抗性示例，在像素空间上具有很高的相似性。受此启发，状态检测方法被提出来检测和拒绝基于查询的攻击。虽然这些方法显示了令人振奋的结果，但它们要么被更高级的攻击所规避，要么在检测不同攻击所需的击球(查询)数量方面效率低下。可以说，这里的关键挑战是为来自同一干净形象的任何两个中间对抗性例子赋予高相似性分数。为了应对这一挑战，我们提出了一种新的对抗性对比提示调优(ACPT)方法来对剪辑图像编码器进行稳健的微调，以提取针对任意两个中间对抗性查询的相似嵌入。在ACPT的基础上，我们进一步介绍了一个检测框架AdvQDet，该框架可以在5次射击内检测到7种最先进的基于查询的攻击，检测率>99$。我们还证明了ACPT对3种类型的自适应攻击具有很好的鲁棒性。代码可在https://github.com/xinwong/AdvQDet.上找到



## **9. Label Augmentation for Neural Networks Robustness**

神经网络鲁棒性的标签增强 cs.CV

21 pages, 4 figures, Published at 3rd Conference on Lifelong Learning  Agents (CoLLAs), 2024

**SubmitDate**: 2024-08-04    [abs](http://arxiv.org/abs/2408.01977v1) [paper-pdf](http://arxiv.org/pdf/2408.01977v1)

**Authors**: Fatemeh Amerehi, Patrick Healy

**Abstract**: Out-of-distribution generalization can be categorized into two types: common perturbations arising from natural variations in the real world and adversarial perturbations that are intentionally crafted to deceive neural networks. While deep neural networks excel in accuracy under the assumption of identical distributions between training and test data, they often encounter out-of-distribution scenarios resulting in a significant decline in accuracy. Data augmentation methods can effectively enhance robustness against common corruptions, but they typically fall short in improving robustness against adversarial perturbations. In this study, we develop Label Augmentation (LA), which enhances robustness against both common and intentional perturbations and improves uncertainty estimation. Our findings indicate a Clean error rate improvement of up to 23.29% when employing LA in comparisons to the baseline. Additionally, it enhances robustness under common corruptions benchmark by up to 24.23%. When tested against FGSM and PGD attacks, improvements in adversarial robustness are noticeable, with enhancements of up to 53.18% for FGSM and 24.46% for PGD attacks.

摘要: 分布外的泛化可以分为两种类型：由现实世界中的自然变化引起的常见扰动和故意设计来欺骗神经网络的对抗性扰动。虽然深度神经网络在假设训练数据和测试数据之间的分布相同的情况下具有较高的精度，但它们经常遇到分布不均匀的情况，导致精度显著下降。数据增强方法可以有效地增强对常见腐败的稳健性，但它们通常不能提高对对手扰动的稳健性。在这项研究中，我们发展了标签增强(LA)，它增强了对常见和故意扰动的稳健性，并改进了不确定性估计。我们的发现表明，与基准相比，使用LA时，干净错误率提高了23.29%。此外，它在常见腐败基准下的健壮性提高了24.23%。当测试FGSM和PGD攻击时，对手健壮性的改善是显著的，FGSM和PGD攻击的增强分别高达53.18%和24.46%。



## **10. Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph Node Classification**

对异类图节点分类的前K增强强化学习攻击 cs.LG

**SubmitDate**: 2024-08-04    [abs](http://arxiv.org/abs/2408.01964v1) [paper-pdf](http://arxiv.org/pdf/2408.01964v1)

**Authors**: Honglin Gao, Gaoxi Xiao

**Abstract**: Graph Neural Networks (GNNs) have attracted substantial interest due to their exceptional performance on graph-based data. However, their robustness, especially on heterogeneous graphs, remains underexplored, particularly against adversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion black-box attack method for heterogeneous graphs. By integrating reinforcement learning with a Top-K algorithm to reduce the action space, our method efficiently identifies effective attack strategies to disrupt node classification tasks. We validate the effectiveness of HeteroKRLAttack through experiments on multiple heterogeneous graph datasets, showing significant reductions in classification accuracy compared to baseline methods. An ablation study underscores the critical role of the Top-K algorithm in enhancing attack performance. Our findings highlight potential vulnerabilities in current models and provide guidance for future defense strategies against adversarial attacks on heterogeneous graphs.

摘要: 图神经网络(GNN)因其在基于图的数据上的优异性能而吸引了人们的极大兴趣。然而，它们的健壮性，特别是在异质图上的健壮性，仍然没有得到充分的研究，特别是在对抗对手攻击的时候。提出了一种针对异构图的定向规避黑盒攻击方法HeteroKRLAttack。通过将强化学习和Top-K算法相结合来缩减动作空间，该方法可以有效地识别有效的攻击策略来扰乱节点分类任务。我们通过在多个异构图数据集上的实验验证了HeteroKRLAttack的有效性，与基线方法相比，分类精度有了显著的下降。一项烧蚀研究强调了Top-K算法在提高攻击性能方面的关键作用。我们的发现突出了当前模型中的潜在漏洞，并为未来针对异构图上的对抗性攻击的防御策略提供了指导。



## **11. A Survey and Evaluation of Adversarial Attacks for Object Detection**

对象检测的对抗性攻击调查与评估 cs.CV

14 pages

**SubmitDate**: 2024-08-04    [abs](http://arxiv.org/abs/2408.01934v1) [paper-pdf](http://arxiv.org/pdf/2408.01934v1)

**Authors**: Khoi Nguyen Tiet Nguyen, Wenyu Zhang, Kangkang Lu, Yuhuan Wu, Xingjian Zheng, Hui Li Tan, Liangli Zhen

**Abstract**: Deep learning models excel in various computer vision tasks but are susceptible to adversarial examples-subtle perturbations in input data that lead to incorrect predictions. This vulnerability poses significant risks in safety-critical applications such as autonomous vehicles, security surveillance, and aircraft health monitoring. While numerous surveys focus on adversarial attacks in image classification, the literature on such attacks in object detection is limited. This paper offers a comprehensive taxonomy of adversarial attacks specific to object detection, reviews existing adversarial robustness evaluation metrics, and systematically assesses open-source attack methods and model robustness. Key observations are provided to enhance the understanding of attack effectiveness and corresponding countermeasures. Additionally, we identify crucial research challenges to guide future efforts in securing automated object detection systems.

摘要: 深度学习模型在各种计算机视觉任务中表现出色，但容易受到对抗性示例的影响--输入数据中的微妙扰动，导致错误的预测。该漏洞对自动驾驶汽车、安全监控和飞机健康监控等安全关键应用构成重大风险。虽然大量调查关注图像分类中的对抗攻击，但关于对象检测中此类攻击的文献有限。本文提供了针对对象检测的对抗性攻击的全面分类，回顾了现有的对抗性稳健性评估指标，并系统性评估开源攻击方法和模型稳健性。提供了关键观察，以增强对攻击有效性和相应对策的理解。此外，我们还确定了关键的研究挑战，以指导未来保护自动化物体检测系统的工作。



## **12. Sharpness-Aware Cross-Domain Recommendation to Cold-Start Users**

向冷启动用户提供具有敏锐意识的跨域推荐 cs.IR

**SubmitDate**: 2024-08-04    [abs](http://arxiv.org/abs/2408.01931v1) [paper-pdf](http://arxiv.org/pdf/2408.01931v1)

**Authors**: Guohang Zeng, Qian Zhang, Guangquan Zhang, Jie Lu

**Abstract**: Cross-Domain Recommendation (CDR) is a promising paradigm inspired by transfer learning to solve the cold-start problem in recommender systems. Existing state-of-the-art CDR methods train an explicit mapping function to transfer the cold-start users from a data-rich source domain to a target domain. However, a limitation of these methods is that the mapping function is trained on overlapping users across domains, while only a small number of overlapping users are available for training. By visualizing the loss landscape of the existing CDR model, we find that training on a small number of overlapping users causes the model to converge to sharp minima, leading to poor generalization. Based on this observation, we leverage loss-geometry-based machine learning approach and propose a novel CDR method called Sharpness-Aware CDR (SCDR). Our proposed method simultaneously optimizes recommendation loss and loss sharpness, leading to better generalization with theoretical guarantees. Empirical studies on real-world datasets demonstrate that SCDR significantly outperforms the other CDR models for cold-start recommendation tasks, while concurrently enhancing the model's robustness to adversarial attacks.

摘要: 跨域推荐(CDR)是一种受迁移学习启发的解决推荐系统冷启动问题的有效方法。现有最先进的CDR方法训练显式映射函数以将冷启动用户从数据丰富的源域转移到目标域。然而，这些方法的局限性在于映射函数是针对跨域的重叠用户进行训练的，而仅有少量重叠用户可供训练。通过可视化现有CDR模型的损失情况，我们发现，在少量重叠用户上进行训练会导致模型收敛到尖锐的最小值，导致泛化能力较差。基于这一观察，我们利用基于损失几何的机器学习方法，提出了一种新的CDR方法，称为清晰度感知CDR(SCDR)。我们提出的方法同时优化了推荐损失和损失锐度，在理论上保证了更好的泛化能力。在真实数据集上的实证研究表明，SCDR在冷启动推荐任务上显著优于其他CDR模型，同时增强了模型对对手攻击的稳健性。



## **13. State-dependent Filtering of the Ring Model**

环模型的状态相关过滤 q-bio.NC

**SubmitDate**: 2024-08-03    [abs](http://arxiv.org/abs/2408.01817v1) [paper-pdf](http://arxiv.org/pdf/2408.01817v1)

**Authors**: Jing Yan, Yunxuan Feng, Wei Dai, Yaoyu Zhang

**Abstract**: Robustness is a measure of functional reliability of a system against perturbations. To achieve a good and robust performance, a system must filter out external perturbations by its internal priors. These priors are usually distilled in the structure and the states of the system. Biophysical neural network are known to be robust but the exact mechanisms are still elusive. In this paper, we probe how orientation-selective neurons organized on a 1-D ring network respond to perturbations in the hope of gaining some insights on the robustness of visual system in brain. We analyze the steady-state of the rate-based network and prove that the activation state of neurons, rather than their firing rates, determines how the model respond to perturbations. We then identify specific perturbation patterns that induce the largest responses for different configurations of activation states, and find them to be sinusoidal or sinusoidal-like while other patterns are largely attenuated. Similar results are observed in a spiking ring model. Finally, we remap the perturbations in orientation back into the 2-D image space using Gabor functions. The resulted optimal perturbation patterns mirror adversarial attacks in deep learning that exploit the priors of the system. Our results suggest that based on different state configurations, these priors could underlie some of the illusionary experiences as the cost of visual robustness.

摘要: 稳健性是衡量系统抗扰动的功能可靠性的指标。为了获得良好和稳健的性能，系统必须通过其内部先验来过滤外部扰动。这些先验信息通常从系统的结构和状态中提取出来。生物物理神经网络已被认为是稳健的，但确切的机制仍然是难以捉摸的。本文探讨了一维环形网络上的方位选择神经元对扰动的反应，以期对大脑视觉系统的稳健性有一定的了解。我们分析了基于速率的网络的稳态，并证明了神经元的激活状态而不是它们的放电速率决定了模型对扰动的响应。然后，我们确定了对不同的激活态配置产生最大响应的特定微扰模式，并发现它们是正弦或类正弦的，而其他模式在很大程度上被衰减。在钉环模型中也观察到了类似的结果。最后，我们使用Gabor函数将方向上的扰动重新映射到二维图像空间。由此得到的最优扰动模式反映了深度学习中的对抗性攻击，这些攻击利用了系统的先验。我们的结果表明，基于不同的状态配置，这些先验可以作为视觉稳健性的代价来支持一些错觉体验。



## **14. ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features**

ALIF：使用语言特征对黑匣子语音平台进行低成本对抗性音频攻击 cs.CR

Published in the 2024 IEEE Symposium on Security and Privacy (SP)

**SubmitDate**: 2024-08-03    [abs](http://arxiv.org/abs/2408.01808v1) [paper-pdf](http://arxiv.org/pdf/2408.01808v1)

**Authors**: Peng Cheng, Yuwei Wang, Peng Huang, Zhongjie Ba, Xiaodong Lin, Feng Lin, Li Lu, Kui Ren

**Abstract**: Extensive research has revealed that adversarial examples (AE) pose a significant threat to voice-controllable smart devices. Recent studies have proposed black-box adversarial attacks that require only the final transcription from an automatic speech recognition (ASR) system. However, these attacks typically involve many queries to the ASR, resulting in substantial costs. Moreover, AE-based adversarial audio samples are susceptible to ASR updates. In this paper, we identify the root cause of these limitations, namely the inability to construct AE attack samples directly around the decision boundary of deep learning (DL) models. Building on this observation, we propose ALIF, the first black-box adversarial linguistic feature-based attack pipeline. We leverage the reciprocal process of text-to-speech (TTS) and ASR models to generate perturbations in the linguistic embedding space where the decision boundary resides. Based on the ALIF pipeline, we present the ALIF-OTL and ALIF-OTA schemes for launching attacks in both the digital domain and the physical playback environment on four commercial ASRs and voice assistants. Extensive evaluations demonstrate that ALIF-OTL and -OTA significantly improve query efficiency by 97.7% and 73.3%, respectively, while achieving competitive performance compared to existing methods. Notably, ALIF-OTL can generate an attack sample with only one query. Furthermore, our test-of-time experiment validates the robustness of our approach against ASR updates.

摘要: 广泛的研究表明，对抗性例子(AE)对语音控制的智能设备构成了重大威胁。最近的研究提出了黑盒对抗性攻击，只需要自动语音识别(ASR)系统的最终转录。然而，这些攻击通常涉及对ASR的许多查询，导致大量成本。此外，基于声发射的敌意音频样本容易受到ASR更新的影响。在本文中，我们找出了这些局限性的根本原因，即无法直接围绕深度学习模型的决策边界构建AE攻击样本。在此基础上，我们提出了首个基于语言特征的黑盒对抗性攻击流水线ALIF。我们利用文本到语音(TTS)和ASR模型的相互过程在决策边界所在的语言嵌入空间中产生扰动。基于ALIF流水线，我们提出了ALIF-OTL和ALIF-OTA方案，用于在数字域和物理回放环境中对四种商用ASR和语音助手发起攻击。广泛的测试表明，ALIF-OTL和-OTA在取得与现有方法相当的性能的同时，查询效率分别提高了97.7%和73.3%。值得注意的是，ALIF-OTL只需一次查询即可生成攻击样本。此外，我们的时间测试实验验证了我们的方法对ASR更新的健壮性。



## **15. Joint Universal Adversarial Perturbations with Interpretations**

联合普遍对抗性扰动与解释 cs.CR

**SubmitDate**: 2024-08-03    [abs](http://arxiv.org/abs/2408.01715v1) [paper-pdf](http://arxiv.org/pdf/2408.01715v1)

**Authors**: Liang-bo Ning, Zeyu Dai, Wenqi Fan, Jingran Su, Chao Pan, Luning Wang, Qing Li

**Abstract**: Deep neural networks (DNNs) have significantly boosted the performance of many challenging tasks. Despite the great development, DNNs have also exposed their vulnerability. Recent studies have shown that adversaries can manipulate the predictions of DNNs by adding a universal adversarial perturbation (UAP) to benign samples. On the other hand, increasing efforts have been made to help users understand and explain the inner working of DNNs by highlighting the most informative parts (i.e., attribution maps) of samples with respect to their predictions. Moreover, we first empirically find that such attribution maps between benign and adversarial examples have a significant discrepancy, which has the potential to detect universal adversarial perturbations for defending against adversarial attacks. This finding motivates us to further investigate a new research problem: whether there exist universal adversarial perturbations that are able to jointly attack DNNs classifier and its interpretation with malicious desires. It is challenging to give an explicit answer since these two objectives are seemingly conflicting. In this paper, we propose a novel attacking framework to generate joint universal adversarial perturbations (JUAP), which can fool the DNNs model and misguide the inspection from interpreters simultaneously. Comprehensive experiments on various datasets demonstrate the effectiveness of the proposed method JUAP for joint attacks. To the best of our knowledge, this is the first effort to study UAP for jointly attacking both DNNs and interpretations.

摘要: 深度神经网络(DNN)显著提高了许多具有挑战性的任务的性能。尽管有了很大的发展，DNN也暴露了它们的脆弱性。最近的研究表明，攻击者可以通过在良性样本中添加通用对抗扰动(UAP)来操纵DNN的预测。另一方面，已作出更多努力，通过突出样本中与其预测相关的信息量最大的部分(即归属图)，帮助用户理解和解释DNN的内部运作。此外，我们首次经验发现，良性例子和对抗性例子之间的这种属性映射具有显著的差异，这有可能检测到普遍的对抗性扰动，以防御对抗性攻击。这一发现促使我们进一步研究一个新的研究问题：是否存在能够以恶意欲望联合攻击DNNS分类器及其解释的通用对抗性扰动。由于这两个目标似乎是相互冲突的，因此很难给出明确的答案。本文提出了一种新的生成联合通用对抗扰动(JUAP)的攻击框架，该框架可以欺骗DNN模型，同时误导解释器的检查。在不同数据集上的综合实验证明了该方法在联合攻击中的有效性。据我们所知，这是第一次研究UAP联合攻击DNN和解释。



## **16. Downstream Transfer Attack: Adversarial Attacks on Downstream Models with Pre-trained Vision Transformers**

下游传输攻击：使用预训练的视觉变形者对下游模型进行对抗攻击 cs.CV

**SubmitDate**: 2024-08-03    [abs](http://arxiv.org/abs/2408.01705v1) [paper-pdf](http://arxiv.org/pdf/2408.01705v1)

**Authors**: Weijie Zheng, Xingjun Ma, Hanxun Huang, Zuxuan Wu, Yu-Gang Jiang

**Abstract**: With the advancement of vision transformers (ViTs) and self-supervised learning (SSL) techniques, pre-trained large ViTs have become the new foundation models for computer vision applications. However, studies have shown that, like convolutional neural networks (CNNs), ViTs are also susceptible to adversarial attacks, where subtle perturbations in the input can fool the model into making false predictions. This paper studies the transferability of such an adversarial vulnerability from a pre-trained ViT model to downstream tasks. We focus on \emph{sample-wise} transfer attacks and propose a novel attack method termed \emph{Downstream Transfer Attack (DTA)}. For a given test image, DTA leverages a pre-trained ViT model to craft the adversarial example and then applies the adversarial example to attack a fine-tuned version of the model on a downstream dataset. During the attack, DTA identifies and exploits the most vulnerable layers of the pre-trained model guided by a cosine similarity loss to craft highly transferable attacks. Through extensive experiments with pre-trained ViTs by 3 distinct pre-training methods, 3 fine-tuning schemes, and across 10 diverse downstream datasets, we show that DTA achieves an average attack success rate (ASR) exceeding 90\%, surpassing existing methods by a huge margin. When used with adversarial training, the adversarial examples generated by our DTA can significantly improve the model's robustness to different downstream transfer attacks.

摘要: 随着视觉转换器(VITS)和自监督学习(SSL)技术的发展，预训练的大型VITS已成为计算机视觉应用的新的基础模型。然而，研究表明，与卷积神经网络(CNN)一样，VITS也容易受到对手攻击，输入中的细微扰动可能会愚弄模型做出错误预测。本文研究了这种敌意漏洞从预先训练的VIT模型到下游任务的可转移性。针对基于样本的传输攻击，提出了一种新的攻击方法--下行传输攻击(DTA)。对于给定的测试图像，DTA利用预先训练的VIT模型来制作对抗性示例，然后应用该对抗性示例来攻击下游数据集上的模型的微调版本。在攻击过程中，DTA在余弦相似性损失的指导下识别并利用预先训练模型中最易受攻击的层来设计高度可转移的攻击。通过使用3种不同的预训练方法、3种微调方案和10个不同的下游数据集对预训练的VITS进行广泛的实验，我们发现DTA的平均攻击成功率(ASR)超过了90%，远远超过了现有的方法。当与对抗性训练结合使用时，DTA生成的对抗性示例可以显著提高模型对不同下游传输攻击的稳健性。



## **17. Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks**

利用辅助对抗防御网络增强跟踪稳健性 cs.CV

This paper is accepted by ECCV2024

**SubmitDate**: 2024-08-03    [abs](http://arxiv.org/abs/2402.17976v3) [paper-pdf](http://arxiv.org/pdf/2402.17976v3)

**Authors**: Zhewei Wu, Ruilong Yu, Qihe Liu, Shuying Cheng, Shilin Qiu, Shijie Zhou

**Abstract**: Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. However, there is still a lack of research on designing adversarial defense methods for object tracking. To address these issues, we propose an effective auxiliary pre-processing defense network, AADN, which performs defensive transformations on the input images before feeding them into the tracker. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without parameter adjustments. We train AADN using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that AADN maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios. Moreover, when transferring the defense network to heterogeneous trackers, it exhibits reliable transferability. Finally, AADN achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead.

摘要: 视觉目标跟踪中的对抗性攻击通过在图像中引入不可感知的扰动而显著降低了高级跟踪器的性能。然而，目前还缺乏针对目标跟踪设计对抗性防御方法的研究。为了解决这些问题，我们提出了一种有效的辅助预处理防御网络AADN，它在将输入图像送入跟踪器之前对其进行防御性转换。此外，它还可以作为即插即用模块与其他视觉追踪器无缝集成，无需调整参数。我们使用对抗性训练来训练AADN，特别是使用Dua-Loss来生成同时攻击跟踪器的分类和回归分支的对抗性样本。在OTB100、LaSOT和VOT2018基准上进行的大量实验表明，AADN在自适应和非自适应攻击场景中都对对抗性攻击方法保持了良好的防御鲁棒性。此外，当将防御网络转移到不同的跟踪器时，它表现出可靠的转移能力。最后，AADN实现了高达5ms/帧的处理时间，允许与现有的高速跟踪器无缝集成，而不会带来显著的计算开销。



## **18. Trustworthy Machine Learning under Social and Adversarial Data Sources**

社交和对抗数据源下的值得信赖的机器学习 cs.LG

PhD thesis

**SubmitDate**: 2024-08-02    [abs](http://arxiv.org/abs/2408.01596v1) [paper-pdf](http://arxiv.org/pdf/2408.01596v1)

**Authors**: Han Shao

**Abstract**: Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors. These behaviors may have a notable impact on the behavior and performance of machine learning systems. Specifically, during these interactions, data may be generated by strategic individuals, collected by self-interested data collectors, possibly poisoned by adversarial attackers, and used to create predictors, models, and policies satisfying multiple objectives. As a result, the machine learning systems' outputs might degrade, such as the susceptibility of deep neural networks to adversarial examples (Shafahi et al., 2018; Szegedy et al., 2013) and the diminished performance of classic algorithms in the presence of strategic individuals (Ahmadi et al., 2021). Addressing these challenges is imperative for the success of machine learning in societal settings.

摘要: 近年来，机器学习取得了显著的突破。随着机器学习渗透到日常生活的各个方面，个人和组织越来越多地与这些系统交互，表现出广泛的社会和对抗性行为。这些行为可能会对机器学习系统的行为和性能产生显著影响。具体地说，在这些交互过程中，数据可能由战略个人生成，由自私自利的数据收集者收集，可能被敌意攻击者毒化，并用于创建满足多个目标的预测器、模型和策略。因此，机器学习系统的输出可能会下降，例如深度神经网络对敌意例子的敏感性(Shafahi等人，2018年；Szegedy等人，2013年)，以及在战略个人在场的情况下经典算法的性能下降(Ahmadi等人，2021年)。应对这些挑战是机器学习在社会环境中取得成功的当务之急。



## **19. Guardians of Image Quality: Benchmarking Defenses Against Adversarial Attacks on Image Quality Metrics**

图像质量守护者：针对图像质量预设的对抗攻击的基准防御 cs.CV

**SubmitDate**: 2024-08-02    [abs](http://arxiv.org/abs/2408.01541v1) [paper-pdf](http://arxiv.org/pdf/2408.01541v1)

**Authors**: Alexander Gushchin, Khaled Abud, Georgii Bychkov, Ekaterina Shumitskaya, Anna Chistyakova, Sergey Lavrushkin, Bader Rasheed, Kirill Malyshev, Dmitriy Vatolin, Anastasia Antsiferova

**Abstract**: In the field of Image Quality Assessment (IQA), the adversarial robustness of the metrics poses a critical concern. This paper presents a comprehensive benchmarking study of various defense mechanisms in response to the rise in adversarial attacks on IQA. We systematically evaluate 25 defense strategies, including adversarial purification, adversarial training, and certified robustness methods. We applied 14 adversarial attack algorithms of various types in both non-adaptive and adaptive settings and tested these defenses against them. We analyze the differences between defenses and their applicability to IQA tasks, considering that they should preserve IQA scores and image quality. The proposed benchmark aims to guide future developments and accepts submissions of new methods, with the latest results available online: https://videoprocessing.ai/benchmarks/iqa-defenses.html.

摘要: 在图像质量评估（IQA）领域，指标的对抗稳健性构成了一个关键问题。本文对各种防御机制进行了全面的基准研究，以应对IQA对抗性攻击的增加。我们系统地评估了25种防御策略，包括对抗性净化、对抗性训练和认证的稳健性方法。我们在非适应性和适应性环境中应用了14种各种类型的对抗攻击算法，并测试了这些防御措施。我们分析了防御之间的差异及其对IQA任务的适用性，认为它们应该保留IQA分数和图像质量。拟议的基准旨在指导未来的发展并接受新方法的提交，最新结果可在线获取：https://videoprocessing.ai/benchmarks/iqa-defenses.html。



## **20. Assessing Robustness of Machine Learning Models using Covariate Perturbations**

使用协变量扰动评估机器学习模型的稳健性 stat.ML

31 pages, 11 figures, 14 tables

**SubmitDate**: 2024-08-02    [abs](http://arxiv.org/abs/2408.01300v1) [paper-pdf](http://arxiv.org/pdf/2408.01300v1)

**Authors**: Arun Prakash R, Anwesha Bhattacharyya, Joel Vaughan, Vijayan N. Nair

**Abstract**: As machine learning models become increasingly prevalent in critical decision-making models and systems in fields like finance, healthcare, etc., ensuring their robustness against adversarial attacks and changes in the input data is paramount, especially in cases where models potentially overfit. This paper proposes a comprehensive framework for assessing the robustness of machine learning models through covariate perturbation techniques. We explore various perturbation strategies to assess robustness and examine their impact on model predictions, including separate strategies for numeric and non-numeric variables, summaries of perturbations to assess and compare model robustness across different scenarios, and local robustness diagnosis to identify any regions in the data where a model is particularly unstable. Through empirical studies on real world dataset, we demonstrate the effectiveness of our approach in comparing robustness across models, identifying the instabilities in the model, and enhancing model robustness.

摘要: 随着机器学习模型在金融、医疗等领域的关键决策模型和系统中变得越来越普遍，确保其对对手攻击和输入数据更改的稳健性至关重要，特别是在模型可能过度匹配的情况下。本文提出了一个用协变量摄动技术评估机器学习模型稳健性的综合框架。我们探索了各种扰动策略来评估稳健性并检查它们对模型预测的影响，包括针对数值变量和非数值变量的单独策略、用于评估和比较不同场景中的模型稳健性的扰动汇总、以及用于识别数据中模型特别不稳定的任何区域的局部稳健性诊断。通过对真实数据集的实证研究，我们证明了该方法在比较不同模型的稳健性、识别模型中的不稳定性以及增强模型稳健性方面的有效性。



## **21. Revisiting the Robust Alignment of Circuit Breakers**

重新审视断路器的稳健对准 cs.CR

**SubmitDate**: 2024-08-02    [abs](http://arxiv.org/abs/2407.15902v2) [paper-pdf](http://arxiv.org/pdf/2407.15902v2)

**Authors**: Leo Schwinn, Simon Geisler

**Abstract**: Over the past decade, adversarial training has emerged as one of the few reliable methods for enhancing model robustness against adversarial attacks [Szegedy et al., 2014, Madry et al., 2018, Xhonneux et al., 2024], while many alternative approaches have failed to withstand rigorous subsequent evaluations. Recently, an alternative defense mechanism, namely "circuit breakers" [Zou et al., 2024], has shown promising results for aligning LLMs. In this report, we show that the robustness claims of "Improving Alignment and Robustness with Circuit Breakers" against unconstraint continuous attacks in the embedding space of the input tokens may be overestimated [Zou et al., 2024]. Specifically, we demonstrate that by implementing a few simple changes to embedding space attacks [Schwinn et al., 2024a,b], we achieve 100% attack success rate (ASR) against circuit breaker models. Without conducting any further hyperparameter tuning, these adjustments increase the ASR by more than 80% compared to the original evaluation. Code is accessible at: https://github.com/SchwinnL/circuit-breakers-eval

摘要: 在过去的十年中，对抗性训练已经成为少数几种增强模型对对抗性攻击的稳健性的可靠方法之一[Szegedy等人，2014，Madry等人，2018，Xhonneux等人，2024]，而许多替代方法未能经受住严格的后续评估。最近，一种替代的防御机制，即“断路器”[Zou等人，2024]，在对准LLM方面显示了令人振奋的结果。在这份报告中，我们证明了在输入令牌的嵌入空间中使用断路器来提高对非约束连续攻击的稳健性主张可能被高估了[Zou等人，2024]。具体地说，我们证明了通过对嵌入空间攻击[Schwinn等人，2024a，b]进行一些简单的更改，我们对断路器模型实现了100%的攻击成功率(ASR)。在不进行任何进一步的超参数调整的情况下，这些调整使ASR与原始评估相比增加了80%以上。代码可在以下网址访问：https://github.com/SchwinnL/circuit-breakers-eval



## **22. Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition**

使用公理谱重要性分解解释图像模型的全局扰动鲁棒性 cs.AI

Accepted by Transactions on Machine Learning Research (TMLR 2024)

**SubmitDate**: 2024-08-02    [abs](http://arxiv.org/abs/2408.01139v1) [paper-pdf](http://arxiv.org/pdf/2408.01139v1)

**Authors**: Róisín Luo, James McDermott, Colm O'Riordan

**Abstract**: Perturbation robustness evaluates the vulnerabilities of models, arising from a variety of perturbations, such as data corruptions and adversarial attacks. Understanding the mechanisms of perturbation robustness is critical for global interpretability. We present a model-agnostic, global mechanistic interpretability method to interpret the perturbation robustness of image models. This research is motivated by two key aspects. First, previous global interpretability works, in tandem with robustness benchmarks, e.g. mean corruption error (mCE), are not designed to directly interpret the mechanisms of perturbation robustness within image models. Second, we notice that the spectral signal-to-noise ratios (SNR) of perturbed natural images exponentially decay over the frequency. This power-law-like decay implies that: Low-frequency signals are generally more robust than high-frequency signals -- yet high classification accuracy can not be achieved by low-frequency signals alone. By applying Shapley value theory, our method axiomatically quantifies the predictive powers of robust features and non-robust features within an information theory framework. Our method, dubbed as \textbf{I-ASIDE} (\textbf{I}mage \textbf{A}xiomatic \textbf{S}pectral \textbf{I}mportance \textbf{D}ecomposition \textbf{E}xplanation), provides a unique insight into model robustness mechanisms. We conduct extensive experiments over a variety of vision models pre-trained on ImageNet to show that \textbf{I-ASIDE} can not only \textbf{measure} the perturbation robustness but also \textbf{provide interpretations} of its mechanisms.

摘要: 扰动稳健性评估由各种扰动引起的模型的脆弱性，例如数据损坏和对抗性攻击。理解扰动稳健性的机制对于全局可解释性至关重要。我们提出了一种模型不可知的全局机械可解释性方法来解释图像模型的扰动稳健性。这项研究的动机有两个关键方面。首先，以前的全局可解释性与稳健性基准一起工作，例如平均破坏误差(MCE)，不是被设计成直接解释图像模型中的扰动稳健性的机制。其次，我们注意到受扰动的自然图像的光谱信噪比(SNR)随频率呈指数衰减。这种类似幂规律的衰减意味着：低频信号通常比高频信号更健壮--然而，仅靠低频信号不能达到高分类精度。通过应用Shapley值理论，我们的方法在信息论框架内公理地量化了稳健特征和非稳健特征的预测能力。我们的方法称为Textbf{i-side}(Textbf{I}MAGE\Textbf{A}X-Ait\Textbf{S}频谱\Textbf{I}M重要\Textbf{D}分解\Textbf{E}解释)，提供了对模型健壮性机制的独特见解。我们在ImageNet上预先训练的各种视觉模型上进行了大量的实验，结果表明，文本bf{i-side}不仅可以测量扰动的稳健性，而且可以对其机制进行解释。



## **23. On the Perturbed States for Transformed Input-robust Reinforcement Learning**

关于转换输入稳健强化学习的扰动状态 cs.LG

12 pages (Code: https://github.com/tunglm2203/tirl)

**SubmitDate**: 2024-08-02    [abs](http://arxiv.org/abs/2408.00023v2) [paper-pdf](http://arxiv.org/pdf/2408.00023v2)

**Authors**: Tung M. Luu, Haeyong Kang, Tri Ton, Thanh Nguyen, Chang D. Yoo

**Abstract**: Reinforcement Learning (RL) agents demonstrating proficiency in a training environment exhibit vulnerability to adversarial perturbations in input observations during deployment. This underscores the importance of building a robust agent before its real-world deployment. To alleviate the challenging point, prior works focus on developing robust training-based procedures, encompassing efforts to fortify the deep neural network component's robustness or subject the agent to adversarial training against potent attacks. In this work, we propose a novel method referred to as Transformed Input-robust RL (TIRL), which explores another avenue to mitigate the impact of adversaries by employing input transformation-based defenses. Specifically, we introduce two principles for applying transformation-based defenses in learning robust RL agents: (1) autoencoder-styled denoising to reconstruct the original state and (2) bounded transformations (bit-depth reduction and vector quantization (VQ)) to achieve close transformed inputs. The transformations are applied to the state before feeding it into the policy network. Extensive experiments on multiple MuJoCo environments demonstrate that input transformation-based defenses, i.e., VQ, defend against several adversaries in the state observations. The official code is available at https://github.com/tunglm2203/tirl

摘要: 强化学习(RL)代理在训练环境中表现出熟练程度，在部署期间对输入观测中的对抗性扰动表现出脆弱性。这突显了在现实世界部署之前构建一个强大的代理的重要性。为了缓解这一挑战，以前的工作集中于开发基于训练的稳健过程，包括努力加强深度神经网络组件的稳健性或使代理接受针对强大攻击的对抗性训练。在这项工作中，我们提出了一种新的方法，称为转换输入-稳健RL(TirL)，它探索了另一种方法，通过使用基于输入转换的防御来减轻对手的影响。具体地说，我们介绍了在学习健壮的RL代理时应用基于变换的防御的两个原则：(1)自动编码式去噪来重建原始状态；(2)有界变换(比特深度减少和矢量量化(VQ))以获得接近变换的输入。在将国家送入策略网络之前，会将这些转换应用于国家。在多个MuJoCo环境上的大量实验表明，基于输入变换的防御，即VQ，在状态观察中可以防御几个对手。官方代码可在https://github.com/tunglm2203/tirl上获得



## **24. How much should you pay for restaking security?**

您应该支付多少钱来恢复安全？ cs.GT

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00928v1) [paper-pdf](http://arxiv.org/pdf/2408.00928v1)

**Authors**: Tarun Chitra, Mallesh Pai

**Abstract**: Restaking protocols have aggregated billions of dollars of security by utilizing token incentives and payments. A natural question to ask is: How much security do restaked services \emph{really} need to purchase? To answer this question, we expand a model of Durvasula and Roughgarden [DR24] that includes incentives and an expanded threat model consisting of strategic attackers and users. Our model shows that an adversary with a strictly submodular profit combined with strategic node operators who respond to incentives can avoid the large-scale cascading failures of~[DR24]. We utilize our model to construct an approximation algorithm for choosing token-based incentives that achieve a given security level against adversaries who are bounded in the number of services they can simultaneously attack. Our results suggest that incentivized restaking protocols can be secure with proper incentive management.

摘要: 通过利用代币激励和支付，重新赌注协议聚集了数十亿美元的安全性。一个自然会问的问题是：重新设定的服务\{really}需要购买多少安全性？为了回答这个问题，我们扩展了Durvasula和Roughgarden的模型[DR 24]，其中包括激励措施和由战略攻击者和用户组成的扩展威胁模型。我们的模型表明，利润严格亚模化的对手与响应激励的战略节点运营商相结合，可以避免~[DR 24]的大规模连锁故障。我们利用我们的模型来构建一个近似算法，用于选择基于代币的激励，该激励可以针对可同时攻击的服务数量有限的对手实现给定的安全级别。我们的结果表明，通过适当的激励管理，激励重赌注协议可以是安全的。



## **25. Discrete Randomized Smoothing Meets Quantum Computing**

离散随机平滑与量子计算相遇 cs.LG

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00895v1) [paper-pdf](http://arxiv.org/pdf/2408.00895v1)

**Authors**: Tom Wollschläger, Aman Saxena, Nicola Franco, Jeanette Miriam Lorenz, Stephan Günnemann

**Abstract**: Breakthroughs in machine learning (ML) and advances in quantum computing (QC) drive the interdisciplinary field of quantum machine learning to new levels. However, due to the susceptibility of ML models to adversarial attacks, practical use raises safety-critical concerns. Existing Randomized Smoothing (RS) certification methods for classical machine learning models are computationally intensive. In this paper, we propose the combination of QC and the concept of discrete randomized smoothing to speed up the stochastic certification of ML models for discrete data. We show how to encode all the perturbations of the input binary data in superposition and use Quantum Amplitude Estimation (QAE) to obtain a quadratic reduction in the number of calls to the model that are required compared to traditional randomized smoothing techniques. In addition, we propose a new binary threat model to allow for an extensive evaluation of our approach on images, graphs, and text.

摘要: 机器学习（ML）的突破和量子计算（QC）的进步将量子机器学习的跨学科领域推向了新的水平。然而，由于ML模型容易受到对抗攻击，实际使用会引发安全问题。经典机器学习模型的现有随机平滑（RS）认证方法是计算密集型的。本文提出将QC和离散随机平滑概念相结合，以加快离散数据ML模型的随机认证。我们展示了如何将输入二进制数据的所有扰动叠加编码，并使用量子幅度估计（QAE）来获得与传统随机平滑技术相比所需的模型调用次数的二次减少。此外，我们提出了一种新的二元威胁模型，以便对我们的图像、图形和文本方法进行广泛评估。



## **26. Tamper-Resistant Safeguards for Open-Weight LLMs**

开放重量LLM的防篡改保障措施 cs.LG

Website: https://www.tamper-resistant-safeguards.com

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00761v1) [paper-pdf](http://arxiv.org/pdf/2408.00761v1)

**Authors**: Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks, Mantas Mazeika

**Abstract**: Rapid advances in the capabilities of large language models (LLMs) have raised widespread concerns regarding their potential for malicious use. Open-weight LLMs present unique challenges, as existing safeguards lack robustness to tampering attacks that modify model weights. For example, recent works have demonstrated that refusal and unlearning safeguards can be trivially removed with a few steps of fine-tuning. These vulnerabilities necessitate new approaches for enabling the safe release of open-weight LLMs. We develop a method, called TAR, for building tamper-resistant safeguards into open-weight LLMs such that adversaries cannot remove the safeguards even after thousands of steps of fine-tuning. In extensive evaluations and red teaming analyses, we find that our method greatly improves tamper-resistance while preserving benign capabilities. Our results demonstrate that tamper-resistance is a tractable problem, opening up a promising new avenue to improve the safety and security of open-weight LLMs.

摘要: 大型语言模型(LLM)功能的快速发展引起了人们对其潜在恶意使用的广泛关注。开放重量LLM提出了独特的挑战，因为现有的保障措施缺乏对篡改模型权重的篡改攻击的稳健性。例如，最近的研究表明，通过几个步骤的微调，就可以很容易地消除拒绝和遗忘的保障措施。这些漏洞需要新的方法来实现安全释放未加重量的低密度脂蛋白。我们开发了一种名为TAR的方法，用于在开放重量的LLM中构建防篡改保护措施，以便对手即使在数千个步骤的微调之后也无法移除这些保护措施。在广泛的评估和红团队分析中，我们发现我们的方法在保持良性性能的同时大大提高了防篡改能力。我们的结果表明，防篡改是一个容易解决的问题，为提高开重LLMS的安全性开辟了一条很有前途的新途径。



## **27. CERT-ED: Certifiably Robust Text Classification for Edit Distance**

CERT-ED：针对编辑距离的可认证鲁棒文本分类 cs.CL

22 pages, 3 figures, 12 tables. Include 11 pages of appendices

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00728v1) [paper-pdf](http://arxiv.org/pdf/2408.00728v1)

**Authors**: Zhuoqun Huang, Neil G Marchant, Olga Ohrimenko, Benjamin I. P. Rubinstein

**Abstract**: With the growing integration of AI in daily life, ensuring the robustness of systems to inference-time attacks is crucial. Among the approaches for certifying robustness to such adversarial examples, randomized smoothing has emerged as highly promising due to its nature as a wrapper around arbitrary black-box models. Previous work on randomized smoothing in natural language processing has primarily focused on specific subsets of edit distance operations, such as synonym substitution or word insertion, without exploring the certification of all edit operations. In this paper, we adapt Randomized Deletion (Huang et al., 2023) and propose, CERTified Edit Distance defense (CERT-ED) for natural language classification. Through comprehensive experiments, we demonstrate that CERT-ED outperforms the existing Hamming distance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of both accuracy and the cardinality of the certificate. By covering various threat models, including 5 direct and 5 transfer attacks, our method improves empirical robustness in 38 out of 50 settings.

摘要: 随着人工智能在日常生活中的日益融合，确保系统对推理时攻击的健壮性至关重要。在证明对这种对抗性例子的稳健性的方法中，随机平滑因其作为任意黑盒模型的包装器的性质而变得非常有前途。以前关于自然语言处理中的随机化平滑的工作主要集中在编辑距离操作的特定子集上，例如同义词替换或单词插入，而没有探索所有编辑操作的认证。在本文中，我们采用了随机化删除(Huang等人，2023)，并提出了用于自然语言分类的认证编辑距离防御(CERT-ED)。通过综合实验，我们证明了CERT-ED在5个数据集中的4个上优于现有的Hamming距离方法RanMASK(Zeng等人，2023)，无论是在准确率上还是在证书的基数上。通过覆盖各种威胁模型，包括5个直接攻击和5个转移攻击，我们的方法在50种设置中的38种情况下提高了经验稳健性。



## **28. Prover-Verifier Games improve legibility of LLM outputs**

证明者-验证者游戏提高了LLM输出的清晰度 cs.CL

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2407.13692v2) [paper-pdf](http://arxiv.org/pdf/2407.13692v2)

**Authors**: Jan Hendrik Kirchner, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, Yuri Burda

**Abstract**: One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check -- a property we call legibility. We study legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, we propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). Our algorithm iteratively trains small verifiers to predict solution correctness, "helpful" provers to produce correct solutions that the verifier accepts, and "sneaky" provers to produce incorrect solutions that fool the verifier. We find that the helpful prover's accuracy and the verifier's robustness to adversarial attacks increase over the course of training. Furthermore, we show that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover's solutions, and decreases when checking the sneaky prover's solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. Our results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models.

摘要: 增加对大型语言模型(LLM)输出的信心的一种方法是用清晰且易于检查的推理来支持它们--我们称之为易读性。我们在解决小学数学问题的背景下研究了易读性，并表明只为了答案的正确性而优化思维链解决方案会降低它们的易读性。为了减少易读性的损失，我们提出了一种受Anil等人的Prover-Verator游戏启发的训练算法。(2021年)。我们的算法迭代地训练小的验证者来预测解决方案的正确性，“有帮助的”验证者来产生验证者接受的正确的解决方案，而“偷偷摸摸”的验证者产生愚弄验证者的不正确的解决方案。我们发现，随着训练过程的进行，有益证明者的准确率和验证者对敌意攻击的健壮性都有所提高。此外，我们还表明，易读性训练转移到负责验证解决方案正确性的时间受限的人身上。在LLM训练过程中，当检查有用的证明者的解时，人类的准确率提高，而当检查偷偷摸摸的证明者的解时，人类的准确率降低。因此，由小型验证员进行可校验性培训是提高输出清晰度的一种可行的技术。我们的结果表明，针对小验证者的易读性训练是提高大型LLM对人类易读性的实用途径，因此可能有助于超人模型的对齐。



## **29. Defending Jailbreak Attack in VLMs via Cross-modality Information Detector**

通过跨模式信息检测器防御VLM中的越狱攻击 cs.CL

12 pages, 9 figures

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2407.21659v2) [paper-pdf](http://arxiv.org/pdf/2407.21659v2)

**Authors**: Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang

**Abstract**: Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively understand vision information, achieving remarkable performance in many vision-centric tasks. Despite that, recent studies have shown that these models are susceptible to jailbreak attacks, which refer to an exploitative technique where malicious users can break the safety alignment of the target model and generate misleading and harmful answers. This potential threat is caused by both the inherent vulnerabilities of LLM and the larger attack scope introduced by vision input. To enhance the security of VLMs against jailbreak attacks, researchers have developed various defense techniques. However, these methods either require modifications to the model's internal structure or demand significant computational resources during the inference phase. Multimodal information is a double-edged sword. While it increases the risk of attacks, it also provides additional data that can enhance safeguards. Inspired by this, we propose $\underline{\textbf{C}}$ross-modality $\underline{\textbf{I}}$nformation $\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, a plug-and-play jailbreaking detector designed to identify maliciously perturbed image inputs, utilizing the cross-modal similarity between harmful queries and adversarial images. This simple yet effective cross-modality information detector, $\textit{CIDER}$, is independent of the target VLMs and requires less computation cost. Extensive experimental results demonstrate the effectiveness and efficiency of $\textit{CIDER}$, as well as its transferability to both white-box and black-box VLMs.

摘要: 视觉语言模型扩展了视觉语言模型全面理解视觉信息的能力，在许多以视觉为中心的任务中取得了显著的表现。尽管如此，最近的研究表明，这些模型容易受到越狱攻击，越狱攻击指的是一种利用技术，恶意用户可以破坏目标模型的安全对齐，并生成误导性和有害的答案。这种潜在的威胁既是由LLM固有的漏洞造成的，也是由视觉输入引入的更大的攻击范围造成的。为了增强VLMS抵御越狱攻击的安全性，研究人员开发了各种防御技术。然而，这些方法要么需要修改模型的内部结构，要么在推理阶段需要大量的计算资源。多式联运信息是一把双刃剑。虽然它增加了攻击的风险，但它也提供了额外的数据，可以加强安全措施。受此启发，我们提出了一种即插即用的越狱检测器，旨在利用有害查询和敌意图像之间的跨模式相似性来识别恶意干扰的图像输入。这种简单而有效的跨通道信息检测器，$\textit{cider}$，独立于目标VLM，并且需要更少的计算成本。大量的实验结果证明了该算法的有效性和高效性，以及它对白盒和黑盒VLM的可转换性。



## **30. Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion**

针对文本到运动的自主LLM增强对抗攻击 cs.CV

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00352v1) [paper-pdf](http://arxiv.org/pdf/2408.00352v1)

**Authors**: Honglei Miao, Fan Ma, Ruijie Quan, Kun Zhan, Yi Yang

**Abstract**: Human motion generation driven by deep generative models has enabled compelling applications, but the ability of text-to-motion (T2M) models to produce realistic motions from text prompts raises security concerns if exploited maliciously. Despite growing interest in T2M, few methods focus on safeguarding these models against adversarial attacks, with existing work on text-to-image models proving insufficient for the unique motion domain. In the paper, we propose ALERT-Motion, an autonomous framework leveraging large language models (LLMs) to craft targeted adversarial attacks against black-box T2M models. Unlike prior methods modifying prompts through predefined rules, ALERT-Motion uses LLMs' knowledge of human motion to autonomously generate subtle yet powerful adversarial text descriptions. It comprises two key modules: an adaptive dispatching module that constructs an LLM-based agent to iteratively refine and search for adversarial prompts; and a multimodal information contrastive module that extracts semantically relevant motion information to guide the agent's search. Through this LLM-driven approach, ALERT-Motion crafts adversarial prompts querying victim models to produce outputs closely matching targeted motions, while avoiding obvious perturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's superiority over previous methods, achieving higher attack success rates with stealthier adversarial prompts. This pioneering work on T2M adversarial attacks highlights the urgency of developing defensive measures as motion generation technology advances, urging further research into safe and responsible deployment.

摘要: 由深度生成模型驱动的人类运动生成已经实现了令人信服的应用，但文本到运动(T2M)模型从文本提示生成逼真运动的能力如果被恶意利用，会引发安全问题。尽管对T2M的兴趣与日俱增，但很少有方法专注于保护这些模型免受对手攻击，现有的文本到图像模型的工作被证明不足以满足独特的运动域。在本文中，我们提出了ALERT-Motion，这是一个利用大型语言模型(LLM)来针对黑盒T2M模型进行有针对性的对抗性攻击的自主框架。与以前通过预定义规则修改提示的方法不同，ALERT-Motion使用LLMS对人体运动的知识来自主生成微妙但强大的对抗性文本描述。它包括两个关键模块：自适应调度模块和多通道信息对比模块，自适应调度模块构建了一个基于LLM的代理，用于迭代地提炼和搜索对手提示；多通道信息对比模块提取语义相关的运动信息来指导代理的搜索。通过这种LLM驱动的方法，ALERT-Motion恶意提示查询受害者模型以产生与目标运动紧密匹配的输出，同时避免明显的扰动。对流行的T2M模型的评估表明，Alert-Motion比以前的方法更具优势，通过更隐蔽的对手提示实现了更高的攻击成功率。这项关于T2M对抗性攻击的开创性工作突显了随着动作生成技术的进步而开发防御措施的紧迫性，促使对安全和负责任的部署进行进一步研究。



## **31. Securing the Diagnosis of Medical Imaging: An In-depth Analysis of AI-Resistant Attacks**

确保医学成像诊断：深入分析抗人工智能攻击 cs.CR

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00348v1) [paper-pdf](http://arxiv.org/pdf/2408.00348v1)

**Authors**: Angona Biswas, MD Abdullah Al Nasim, Kishor Datta Gupta, Roy George, Abdur Rashid

**Abstract**: Machine learning (ML) is a rapidly developing area of medicine that uses significant resources to apply computer science and statistics to medical issues. ML's proponents laud its capacity to handle vast, complicated, and erratic medical data. It's common knowledge that attackers might cause misclassification by deliberately creating inputs for machine learning classifiers. Research on adversarial examples has been extensively conducted in the field of computer vision applications. Healthcare systems are thought to be highly difficult because of the security and life-or-death considerations they include, and performance accuracy is very important. Recent arguments have suggested that adversarial attacks could be made against medical image analysis (MedIA) technologies because of the accompanying technology infrastructure and powerful financial incentives. Since the diagnosis will be the basis for important decisions, it is essential to assess how strong medical DNN tasks are against adversarial attacks. Simple adversarial attacks have been taken into account in several earlier studies. However, DNNs are susceptible to more risky and realistic attacks. The present paper covers recent proposed adversarial attack strategies against DNNs for medical imaging as well as countermeasures. In this study, we review current techniques for adversarial imaging attacks, detections. It also encompasses various facets of these techniques and offers suggestions for the robustness of neural networks to be improved in the future.

摘要: 机器学习(ML)是一个快速发展的医学领域，它利用大量资源将计算机科学和统计学应用于医学问题。ML的支持者称赞其处理海量、复杂和不稳定的医疗数据的能力。众所周知，攻击者可能会通过故意为机器学习分类器创建输入而导致错误分类。对抗性例子的研究在计算机视觉应用领域得到了广泛的开展。医疗保健系统被认为是非常困难的，因为它们包括安全和生死攸关的考虑，而性能的准确性非常重要。最近的论点表明，可以对医学图像分析(媒体)技术进行对抗性攻击，因为伴随而来的是技术基础设施和强大的财政激励。由于诊断将是重要决策的基础，因此评估医疗DNN任务对抗对手攻击的能力有多强是至关重要的。在早期的几项研究中，简单的对抗性攻击已经被考虑在内。然而，DNN更容易受到风险更高、更现实的攻击。本文讨论了最近提出的针对医学成像领域的DNN的对抗性攻击策略以及对策。在这项研究中，我们回顾了现有的对抗性成像攻击和检测技术。它还涵盖了这些技术的各个方面，并为未来神经网络的健壮性提供了建议。



## **32. MAARS: Multi-Rate Attack-Aware Randomized Scheduling for Securing Real-time Systems**

MAARS：用于保护实时系统的多速率攻击感知随机调度 eess.SY

12 pages including references, Total 10 figures (with 3 having  subfigures). This paper was rejected in RTSS 2024 Conference

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00341v1) [paper-pdf](http://arxiv.org/pdf/2408.00341v1)

**Authors**: Arkaprava Sain, Sunandan Adhikary, Ipsita Koley, Soumyajit Dey

**Abstract**: Modern Cyber-Physical Systems (CPSs) consist of numerous control units interconnected by communication networks. Each control unit executes multiple safety-critical and non-critical tasks in real-time. Most of the safety-critical tasks are executed with a fixed sampling period to ensure deterministic timing behaviour that helps in its safety and performance analysis. However, adversaries can exploit this deterministic behaviour of safety-critical tasks to launch inference-based-based attacks on them. This paper aims to prevent and minimize the possibility of such timing inference or schedule-based attacks to compromise the control units. This is done by switching between strategically chosen execution rates of the safety-critical control tasks such that their performance remains unhampered. Thereafter, we present a novel schedule vulnerability analysis methodology to switch between valid schedules generated for these multiple periodicities of the control tasks in run time. Utilizing these strategies, we introduce a novel Multi-Rate Attack-Aware Randomized Scheduling (MAARS) framework for preemptive fixed-priority schedulers that minimize the success rate of timing-inference-based attacks on safety-critical real-time systems. To our knowledge, this is the first work to propose a schedule randomization method with attack awareness that preserves both the control and scheduling aspects. The efficacy of the framework in terms of attack prevention is finally evaluated on several automotive benchmarks in a Hardware-in-loop (HiL) environment.

摘要: 现代网络物理系统(CPSS)由多个控制单元通过通信网络相互连接而成。每个控制单元实时执行多个安全关键和非关键任务。大多数安全关键任务都以固定的采样周期执行，以确保确定性的定时行为，这有助于其安全和性能分析。然而，攻击者可以利用安全关键任务的这种确定性行为来对它们发起基于推理的攻击。本文的目的是防止和最大限度地减少这种定时推断或基于调度的攻击危害控制单元的可能性。这是通过在战略上选择的安全关键控制任务的执行率之间切换来实现的，从而使它们的性能保持不受阻碍。在此基础上，我们提出了一种新的调度脆弱性分析方法，用于在运行时为控制任务的这些多周期生成的有效调度之间进行切换。利用这些策略，我们提出了一种新的多速率攻击感知随机调度(MAARS)框架，用于抢占式固定优先级调度器，以最大限度地降低基于定时推理的攻击对安全关键实时系统的成功率。据我们所知，这是首次提出一种具有攻击感知的调度随机化方法，该方法同时保留了控制和调度两个方面。最后在硬件在环(HIL)环境下的几个汽车基准上对该框架在攻击预防方面的有效性进行了评估。



## **33. OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial Attack**

Ospel：不可知对抗攻击的最佳传输诱导鲁棒模型 cs.LG

14 pages, 2 figures

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00329v1) [paper-pdf](http://arxiv.org/pdf/2408.00329v1)

**Authors**: Kuo Gai, Sicong Wang, Shihua Zhang

**Abstract**: Deep neural networks (DNNs) are vulnerable to small adversarial perturbations of the inputs, posing a significant challenge to their reliability and robustness. Empirical methods such as adversarial training can defend against particular attacks but remain vulnerable to more powerful attacks. Alternatively, Lipschitz networks provide certified robustness to unseen perturbations but lack sufficient expressive power. To harness the advantages of both approaches, we design a novel two-step Optimal Transport induced Adversarial Defense (OTAD) model that can fit the training data accurately while preserving the local Lipschitz continuity. First, we train a DNN with a regularizer derived from optimal transport theory, yielding a discrete optimal transport map linking data to its features. By leveraging the map's inherent regularity, we interpolate the map by solving the convex integration problem (CIP) to guarantee the local Lipschitz property. OTAD is extensible to diverse architectures of ResNet and Transformer, making it suitable for complex data. For efficient computation, the CIP can be solved through training neural networks. OTAD opens a novel avenue for developing reliable and secure deep learning systems through the regularity of optimal transport maps. Empirical results demonstrate that OTAD can outperform other robust models on diverse datasets.

摘要: 深度神经网络(DNN)容易受到输入的微小对抗性扰动，对其可靠性和健壮性提出了重大挑战。经验方法，如对抗性训练，可以防御特定的攻击，但仍然容易受到更强大的攻击。或者，Lipschitz网络提供了对看不见的扰动的公认的稳健性，但缺乏足够的表达能力。为了利用这两种方法的优点，我们设计了一种新的两步最优传输诱导对抗防御(OTAD)模型，该模型在保持局部Lipschitz连续性的同时，能够准确地拟合训练数据。首先，我们用最优传输理论中的正则化方法训练DNN，得到一个离散的最优传输映射，将数据与其特征联系起来。利用映射的内在正则性，通过求解凸积分问题(CIP)对映射进行内插，以保证局部Lipschitz性质。OTAD可以扩展到不同的ResNet和Transformer架构，适合于复杂的数据。为了提高计算效率，可以通过训练神经网络来求解CIP。OTAD通过最优运输地图的规律性，为开发可靠和安全的深度学习系统开辟了一条新的途径。实验结果表明，在不同的数据集上，OTAD模型的性能优于其他稳健模型。



## **34. ADBM: Adversarial diffusion bridge model for reliable adversarial purification**

ADBM：用于可靠对抗净化的对抗扩散桥模型 cs.LG

20 pages

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00315v1) [paper-pdf](http://arxiv.org/pdf/2408.00315v1)

**Authors**: Xiao Li, Wenxuan Sun, Huanran Chen, Qiongxiu Li, Yining Liu, Yingzhe He, Jie Shi, Xiaolin Hu

**Abstract**: Recently Diffusion-based Purification (DiffPure) has been recognized as an effective defense method against adversarial examples. However, we find DiffPure which directly employs the original pre-trained diffusion models for adversarial purification, to be suboptimal. This is due to an inherent trade-off between noise purification performance and data recovery quality. Additionally, the reliability of existing evaluations for DiffPure is questionable, as they rely on weak adaptive attacks. In this work, we propose a novel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs a reverse bridge from the diffused adversarial data back to its original clean examples, enhancing the purification capabilities of the original diffusion models. Through theoretical analysis and experimental validation across various scenarios, ADBM has proven to be a superior and robust defense mechanism, offering significant promise for practical applications.

摘要: 最近，基于扩散的纯化（DiffPure）被认为是针对对抗性例子的有效防御方法。然而，我们发现直接使用原始预训练的扩散模型进行对抗性纯化的迪夫Pure是次优的。这是由于噪音净化性能和数据恢复质量之间固有的权衡。此外，现有的DistPure评估的可靠性值得怀疑，因为它们依赖于弱适应性攻击。在这项工作中，我们提出了一种新型的对抗扩散桥模型，称为ADBM。ADBM直接构建了从扩散的对抗数据到其原始干净示例的反向桥梁，增强了原始扩散模型的净化能力。通过各种场景的理论分析和实验验证，ADBM已被证明是一种卓越且强大的防御机制，为实际应用提供了巨大的前景。



## **35. Adversarial Text Rewriting for Text-aware Recommender Systems**

文本感知推荐系统的对抗性文本重写 cs.IR

Accepted for publication at: 33rd ACM International Conference on  Information and Knowledge Management (CIKM 2024). Code and data at:  https://github.com/sejoonoh/ATR

**SubmitDate**: 2024-08-01    [abs](http://arxiv.org/abs/2408.00312v1) [paper-pdf](http://arxiv.org/pdf/2408.00312v1)

**Authors**: Sejoon Oh, Gaurav Verma, Srijan Kumar

**Abstract**: Text-aware recommender systems incorporate rich textual features, such as titles and descriptions, to generate item recommendations for users. The use of textual features helps mitigate cold-start problems, and thus, such recommender systems have attracted increased attention. However, we argue that the dependency on item descriptions makes the recommender system vulnerable to manipulation by adversarial sellers on e-commerce platforms. In this paper, we explore the possibility of such manipulation by proposing a new text rewriting framework to attack text-aware recommender systems. We show that the rewriting attack can be exploited by sellers to unfairly uprank their products, even though the adversarially rewritten descriptions are perceived as realistic by human evaluators. Methodologically, we investigate two different variations to carry out text rewriting attacks: (1) two-phase fine-tuning for greater attack performance, and (2) in-context learning for higher text rewriting quality. Experiments spanning 3 different datasets and 4 existing approaches demonstrate that recommender systems exhibit vulnerability against the proposed text rewriting attack. Our work adds to the existing literature around the robustness of recommender systems, while highlighting a new dimension of vulnerability in the age of large-scale automated text generation.

摘要: 文本感知推荐系统结合了丰富的文本特征，如标题和描述，以生成针对用户的项目推荐。文本特征的使用有助于缓解冷启动问题，因此，这样的推荐系统吸引了越来越多的关注。然而，我们认为，对商品描述的依赖使得推荐系统容易受到电子商务平台上敌对卖家的操纵。在本文中，我们通过提出一种新的文本重写框架来攻击文本感知推荐系统，从而探索了这种操纵的可能性。我们表明，重写攻击可以被卖家利用来不公平地升级他们的产品，即使人工评估者认为敌意重写的描述是真实的。在方法论上，我们研究了两种不同的文本重写攻击方法：(1)两阶段微调，以获得更好的攻击性能；(2)上下文学习，以获得更高的文本重写质量。在3个不同的数据集和4个现有方法上的实验表明，推荐系统对所提出的文本重写攻击表现出脆弱性。我们的工作增加了关于推荐系统健壮性的现有文献，同时强调了大规模自动文本生成时代的脆弱性的一个新维度。



## **36. Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks**

隐藏的毒药：机器遗忘导致伪装中毒攻击 cs.LG

**SubmitDate**: 2024-07-31    [abs](http://arxiv.org/abs/2212.10717v2) [paper-pdf](http://arxiv.org/pdf/2212.10717v2)

**Authors**: Jimmy Z. Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, Ayush Sekhari

**Abstract**: We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset.

摘要: 我们引入了伪装数据中毒攻击，这是一种新的攻击载体，出现在机器取消学习和其他可能引发模型再训练的环境中。对手首先在训练数据集中添加一些精心设计的点，以便对模型预测的影响最小。对手随后触发一个请求，以删除所引入点的子集，此时攻击将被释放，模型的预测将受到负面影响。特别是，我们考虑对包括CIFAR-10、Imagenette和Imagewoof在内的数据集进行干净标签定向攻击（目标是导致模型错误分类特定测试点）。这种攻击是通过构建伪装数据点来掩盖有毒数据集的影响来实现的。



## **37. Vera Verto: Multimodal Hijacking Attack**

Vera Verto：多模式劫持攻击 cs.CR

**SubmitDate**: 2024-07-31    [abs](http://arxiv.org/abs/2408.00129v1) [paper-pdf](http://arxiv.org/pdf/2408.00129v1)

**Authors**: Minxing Zhang, Ahmed Salem, Michael Backes, Yang Zhang

**Abstract**: The increasing cost of training machine learning (ML) models has led to the inclusion of new parties to the training pipeline, such as users who contribute training data and companies that provide computing resources. This involvement of such new parties in the ML training process has introduced new attack surfaces for an adversary to exploit. A recent attack in this domain is the model hijacking attack, whereby an adversary hijacks a victim model to implement their own -- possibly malicious -- hijacking tasks. However, the scope of the model hijacking attack is so far limited to the homogeneous-modality tasks. In this paper, we transform the model hijacking attack into a more general multimodal setting, where the hijacking and original tasks are performed on data of different modalities. Specifically, we focus on the setting where an adversary implements a natural language processing (NLP) hijacking task into an image classification model. To mount the attack, we propose a novel encoder-decoder based framework, namely the Blender, which relies on advanced image and language models. Experimental results show that our modal hijacking attack achieves strong performances in different settings. For instance, our attack achieves 94%, 94%, and 95% attack success rate when using the Sogou news dataset to hijack STL10, CIFAR-10, and MNIST classifiers.

摘要: 培训机器学习(ML)模型的成本不断增加，导致培训渠道中纳入了新的参与方，如提供培训数据的用户和提供计算资源的公司。这些新人在ML训练过程中的参与为对手带来了新的攻击面来利用。该领域最近的一种攻击是模型劫持攻击，借此对手劫持受害者模型来执行他们自己的--可能是恶意的--劫持任务。然而，到目前为止，模型劫持攻击的范围仅限于同质通道任务。在本文中，我们将劫持攻击模型转化为更一般的多通道环境，其中劫持和原始任务是在不同通道的数据上执行的。具体地说，我们关注的是敌手将自然语言处理(NLP)劫持任务实现到图像分类模型中的设置。为了发动攻击，我们提出了一种新的基于编解码器的框架，即Blender，它依赖于先进的图像和语言模型。实验结果表明，我们的模式劫持攻击在不同的设置下都取得了很好的性能。例如，当使用搜狗新闻数据集劫持STL10、CIFAR-10和MNIST分类器时，我们的攻击获得了94%、94%和95%的攻击成功率。



## **38. TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods**

TAROT：使用策略优化方法的面向任务的作者混淆 cs.CL

**SubmitDate**: 2024-07-31    [abs](http://arxiv.org/abs/2407.21630v1) [paper-pdf](http://arxiv.org/pdf/2407.21630v1)

**Authors**: Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi

**Abstract**: Authorship obfuscation aims to disguise the identity of an author within a text by altering the writing style, vocabulary, syntax, and other linguistic features associated with the text author. This alteration needs to balance privacy and utility. While strong obfuscation techniques can effectively hide the author's identity, they often degrade the quality and usefulness of the text for its intended purpose. Conversely, maintaining high utility tends to provide insufficient privacy, making it easier for an adversary to de-anonymize the author. Thus, achieving an optimal trade-off between these two conflicting objectives is crucial. In this paper, we propose TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization, a new unsupervised authorship obfuscation method whose goal is to optimize the privacy-utility trade-off by regenerating the entire text considering its downstream utility. Our approach leverages policy optimization as a fine-tuning paradigm over small language models in order to rewrite texts by preserving author identity and downstream task utility. We show that our approach largely reduce the accuracy of attackers while preserving utility. We make our code and models publicly available.

摘要: 作者身份混淆旨在通过改变与文本作者相关的写作风格、词汇、句法和其他语言特征来掩盖作者在文本中的身份。这一改变需要平衡隐私和效用。虽然强大的混淆技术可以有效地隐藏作者的身份，但它们往往会降低文本的质量和对预期目的的有用性。相反，保持高实用性往往会提供不充分的隐私，使对手更容易解除作者的匿名。因此，在这两个相互冲突的目标之间实现最佳权衡至关重要。在本文中，我们提出了一种新的无监督作者身份混淆方法--TAROT：基于策略优化的面向任务的作者身份混淆方法，其目标是通过重新生成考虑下游效用的整个文本来优化隐私和效用之间的权衡。我们的方法利用策略优化作为小语言模型上的微调范式，以便通过保留作者身份和下游任务效用来重写文本。我们表明，我们的方法在很大程度上降低了攻击者的准确性，同时保持了实用性。我们公开我们的代码和模型。



## **39. DeepBaR: Fault Backdoor Attack on Deep Neural Network Layers**

DeepBaR：深度神经网络层的故障后门攻击 cs.LG

**SubmitDate**: 2024-07-30    [abs](http://arxiv.org/abs/2407.21220v1) [paper-pdf](http://arxiv.org/pdf/2407.21220v1)

**Authors**: C. A. Martínez-Mejía, J. Solano, J. Breier, D. Bucko, X. Hou

**Abstract**: Machine Learning using neural networks has received prominent attention recently because of its success in solving a wide variety of computational tasks, in particular in the field of computer vision. However, several works have drawn attention to potential security risks involved with the training and implementation of such networks. In this work, we introduce DeepBaR, a novel approach that implants backdoors on neural networks by faulting their behavior at training, especially during fine-tuning. Our technique aims to generate adversarial samples by optimizing a custom loss function that mimics the implanted backdoors while adding an almost non-visible trigger in the image. We attack three popular convolutional neural network architectures and show that DeepBaR attacks have a success rate of up to 98.30\%. Furthermore, DeepBaR does not significantly affect the accuracy of the attacked networks after deployment when non-malicious inputs are given. Remarkably, DeepBaR allows attackers to choose an input that looks similar to a given class, from a human perspective, but that will be classified as belonging to an arbitrary target class.

摘要: 基于神经网络的机器学习由于能够成功地解决各种计算任务，特别是在计算机视觉领域，近年来受到了广泛的关注。然而，有几项工作提请注意此类网络的培训和实施所涉及的潜在安全风险。在这项工作中，我们引入了DeepBaR，这是一种新的方法，通过在训练时特别是在微调期间对神经网络的行为进行错误检查来在神经网络上植入后门。我们的技术旨在通过优化自定义损失函数来生成敌意样本，该函数模仿植入的后门，同时在图像中添加几乎不可见的触发器。我们对三种流行的卷积神经网络结构进行了攻击，结果表明DeepBaR攻击的成功率高达98.30%。此外，在给出非恶意输入的情况下，DeepBaR不会显著影响部署后被攻击网络的准确性。值得注意的是，DeepBaR允许攻击者选择从人类角度看与给定类相似的输入，但这将被归类为属于任意目标类。



## **40. AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning**

实践中的人工智能安全：增强多模式图像字幕中的对抗鲁棒性 cs.CV

Accepted into KDD 2024 workshop on Ethical AI

**SubmitDate**: 2024-07-30    [abs](http://arxiv.org/abs/2407.21174v1) [paper-pdf](http://arxiv.org/pdf/2407.21174v1)

**Authors**: Maisha Binte Rashid, Pablo Rivas

**Abstract**: Multimodal machine learning models that combine visual and textual data are increasingly being deployed in critical applications, raising significant safety and security concerns due to their vulnerability to adversarial attacks. This paper presents an effective strategy to enhance the robustness of multimodal image captioning models against such attacks. By leveraging the Fast Gradient Sign Method (FGSM) to generate adversarial examples and incorporating adversarial training techniques, we demonstrate improved model robustness on two benchmark datasets: Flickr8k and COCO. Our findings indicate that selectively training only the text decoder of the multimodal architecture shows performance comparable to full adversarial training while offering increased computational efficiency. This targeted approach suggests a balance between robustness and training costs, facilitating the ethical deployment of multimodal AI systems across various domains.

摘要: 结合视觉和文本数据的多模式机器学习模型越来越多地被部署在关键应用程序中，由于它们容易受到对抗性攻击，从而引发了严重的安全和安保问题。本文提出了一种有效的策略来增强多模式图像字幕模型对此类攻击的鲁棒性。通过利用快速梯度符号法（FGSM）生成对抗性示例并结合对抗性训练技术，我们在两个基准数据集：Flickr 8k和COCO上展示了改进的模型稳健性。我们的研究结果表明，选择性地仅训练多模式架构的文本解码器显示出与完全对抗训练相当的性能，同时提供更高的计算效率。这种有针对性的方法表明了稳健性和训练成本之间的平衡，促进了多模式人工智能系统在各个领域的道德部署。



## **41. Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks**

人工智能生成图像检测中的漏洞：对抗性攻击的挑战 cs.CV

**SubmitDate**: 2024-07-30    [abs](http://arxiv.org/abs/2407.20836v1) [paper-pdf](http://arxiv.org/pdf/2407.20836v1)

**Authors**: Yunfeng Diao, Naixin Zhai, Changtao Miao, Xun Yang, Meng Wang

**Abstract**: Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of these AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. For the task of AIGI detection, we propose a new attack containing two main parts. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous models, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as frequency-based post-train Bayesian attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario.

摘要: 最近在图像合成方面的进步，特别是随着GaN和扩散模型的出现，放大了公众对虚假信息传播的担忧。为了解决这些问题，人们已经提出了许多人工智能生成的图像(AIGI)检测器，并在识别虚假图像方面取得了良好的性能。然而，对这些AIGI检测器的对抗健壮性仍然缺乏系统的了解。本文研究了白盒和黑盒环境下最新的AIGI检测器抵抗敌意攻击的脆弱性，这是迄今为止很少被研究的。针对AIGI检测任务，我们提出了一种包含两个主要部分的新攻击。首先，受真伪图像在频域存在明显差异的启发，在频域下加入扰动，使图像偏离其原有的频率分布。其次，我们探索了代理模型的完全后验分布，以进一步缩小不同模型之间的差距，例如跨CNN和VITS传输对抗性实例。这是通过引入一种新颖的后训练贝叶斯策略来实现的，该策略将单个代理转变为贝叶斯策略，能够使用一个预先训练的代理来模拟不同的受害者模型，而不需要重新训练。我们将我们的方法命名为基于频率的训练后贝叶斯攻击，或称FPBA。通过FPBA，我们证明了敌意攻击是对AIGI检测器的真正威胁，因为FPBA可以跨模型、生成器、防御方法提供成功的黑盒攻击，甚至可以逃避交叉生成器检测，这是现实世界中的一个关键检测场景。



## **42. Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence**

利用敌对少数派影响攻击合作多智能体强化学习 cs.LG

**SubmitDate**: 2024-07-30    [abs](http://arxiv.org/abs/2302.03322v3) [paper-pdf](http://arxiv.org/pdf/2302.03322v3)

**Authors**: Simin Li, Jun Guo, Jingqiao Xiu, Yuwei Zheng, Pu Feng, Xin Yu, Aishan Liu, Yaodong Yang, Bo An, Wenjun Wu, Xianglong Liu

**Abstract**: This study probes the vulnerabilities of cooperative multi-agent reinforcement learning (c-MARL) under adversarial attacks, a critical determinant of c-MARL's worst-case performance prior to real-world implementation. Current observation-based attacks, constrained by white-box assumptions, overlook c-MARL's complex multi-agent interactions and cooperative objectives, resulting in impractical and limited attack capabilities. To address these shortcomes, we propose Adversarial Minority Influence (AMI), a practical and strong for c-MARL. AMI is a practical black-box attack and can be launched without knowing victim parameters. AMI is also strong by considering the complex multi-agent interaction and the cooperative goal of agents, enabling a single adversarial agent to unilaterally misleads majority victims to form targeted worst-case cooperation. This mirrors minority influence phenomena in social psychology. To achieve maximum deviation in victim policies under complex agent-wise interactions, our unilateral attack aims to characterize and maximize the impact of the adversary on the victims. This is achieved by adapting a unilateral agent-wise relation metric derived from mutual information, thereby mitigating the adverse effects of victim influence on the adversary. To lead the victims into a jointly detrimental scenario, our targeted attack deceives victims into a long-term, cooperatively harmful situation by guiding each victim towards a specific target, determined through a trial-and-error process executed by a reinforcement learning agent. Through AMI, we achieve the first successful attack against real-world robot swarms and effectively fool agents in simulated environments into collectively worst-case scenarios, including Starcraft II and Multi-agent Mujoco. The source code and demonstrations can be found at: https://github.com/DIG-Beihang/AMI.

摘要: 该研究探讨了协作多智能体强化学习(c-Marl)在对抗攻击下的脆弱性，这是c-Marl在现实世界实现之前最差情况性能的关键决定因素。目前的基于观测的攻击受白盒假设的约束，忽略了c-Marl复杂的多智能体交互和合作目标，导致攻击能力不切实际和有限。针对这些不足，我们提出了一种实用而强大的c-Marl算法--对抗性少数影响算法。AMI是一种实用的黑盒攻击，可以在不知道受害者参数的情况下启动。通过考虑复杂的多智能体相互作用和智能体的合作目标，使单一对抗智能体能够单方面误导大多数受害者形成有针对性的最坏情况合作，AMI也很强大。这反映了社会心理学中的小众影响现象。为了在复杂的智能体相互作用下实现受害者政策的最大偏差，我们的单边攻击旨在刻画和最大化对手对受害者的影响。这是通过采用来自互信息的单边代理关系度量来实现的，从而减轻了受害者影响对对手的不利影响。为了将受害者引导到共同有害的情景中，我们的有针对性的攻击通过引导每个受害者指向特定的目标，将受害者欺骗到长期的、合作有害的情况中，该特定目标是通过由强化学习代理执行的反复试验过程确定的。通过AMI，我们实现了对真实世界机器人群的第一次成功攻击，并有效地将模拟环境中的代理愚弄到了集体最坏的情况下，包括星际争霸II和多代理Mujoco。源代码和演示可在以下网址找到：https://github.com/DIG-Beihang/AMI.



## **43. Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks**

可转移对抗攻击的预算驱动对比学习 cs.CV

Accepted to ECCV 2024, Project Page: https://PDCL-Attack.github.io

**SubmitDate**: 2024-07-30    [abs](http://arxiv.org/abs/2407.20657v1) [paper-pdf](http://arxiv.org/pdf/2407.20657v1)

**Authors**: Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon

**Abstract**: Recent vision-language foundation models, such as CLIP, have demonstrated superior capabilities in learning representations that can be transferable across diverse range of downstream tasks and domains. With the emergence of such powerful models, it has become crucial to effectively leverage their capabilities in tackling challenging vision tasks. On the other hand, only a few works have focused on devising adversarial examples that transfer well to both unknown domains and model architectures. In this paper, we propose a novel transfer attack method called PDCL-Attack, which leverages the CLIP model to enhance the transferability of adversarial perturbations generated by a generative model-based attack framework. Specifically, we formulate an effective prompt-driven feature guidance by harnessing the semantic representation power of text, particularly from the ground-truth class labels of input images. To the best of our knowledge, we are the first to introduce prompt learning to enhance the transferable generative attacks. Extensive experiments conducted across various cross-domain and cross-model settings empirically validate our approach, demonstrating its superiority over state-of-the-art methods.

摘要: 最近的视觉语言基础模型，如CLIP，在学习表征方面表现出了优越的能力，这些表征可以跨不同范围的下游任务和领域转移。随着如此强大的模型的出现，有效地利用它们的能力来处理具有挑战性的愿景任务变得至关重要。另一方面，只有少数工作专注于设计能够很好地移植到未知领域和模型体系结构的对抗性例子。本文提出了一种新的转移攻击方法PDCL-Attack，该方法利用CLIP模型来增强基于产生式模型的攻击框架产生的敌意扰动的可转移性。具体地说，我们通过利用文本的语义表征能力，特别是从输入图像的基本事实类标签来制定有效的提示驱动的特征指导。据我们所知，我们是第一个引入快速学习来增强可转移的生成性攻击的。在各种跨域和跨模型环境下进行的广泛实验验证了我们的方法，证明了它比最先进的方法更优越。



## **44. FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks**

FACL攻击：可转移对抗攻击的频率感知对比学习 cs.CV

Accepted to AAAI 2024, Project Page: https://FACL-Attack.github.io

**SubmitDate**: 2024-07-30    [abs](http://arxiv.org/abs/2407.20653v1) [paper-pdf](http://arxiv.org/pdf/2407.20653v1)

**Authors**: Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon

**Abstract**: Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to generate adversarial examples that are robust in both cross-domain and cross-model settings. With that goal in mind, we propose two modules that are only employed during the training phase: a Frequency-Aware Domain Randomization (FADR) module to randomize domain-variant low- and high-range frequency components and a Frequency-Augmented Contrastive Learning (FACL) module to effectively separate domain-invariant mid-frequency features of clean and perturbed image. We demonstrate strong transferability of our generated adversarial perturbations through extensive cross-domain and cross-model experiments, while keeping the inference time complexity.

摘要: 众所周知，由于对抗性例子的固有可转移性，深度神经网络容易受到安全风险的影响。尽管最近基于模型的生成性攻击取得了成功，表现出很强的可转移性，但在目标域和模型体系结构都未知的现实世界严格的黑盒环境中，设计有效的攻击策略仍然是一个挑战。在本文中，我们试图探索一种在频域中进行特征对比的方法，以生成在跨域和跨模型设置下都具有健壮性的对抗性示例。考虑到这一目标，我们提出了两个仅在训练阶段使用的模块：频率感知域随机化(FADR)模块，用于随机化域变化的低频和高频分量；以及频率增强对比学习(FACL)模块，用于有效分离干净和扰动图像的域不变中频特征。通过广泛的跨域和跨模型实验，我们证明了我们生成的对抗性扰动具有很强的可转移性，同时保持了推理的时间复杂性。



## **45. Robust Federated Learning for Wireless Networks: A Demonstration with Channel Estimation**

无线网络的鲁棒联邦学习：信道估计的演示 cs.LG

Submitted to IEEE GLOBECOM 2024

**SubmitDate**: 2024-07-30    [abs](http://arxiv.org/abs/2404.03088v2) [paper-pdf](http://arxiv.org/pdf/2404.03088v2)

**Authors**: Zexin Fang, Bin Han, Hans D. Schotten

**Abstract**: Federated learning (FL) offers a privacy-preserving collaborative approach for training models in wireless networks, with channel estimation emerging as a promising application. Despite extensive studies on FL-empowered channel estimation, the security concerns associated with FL require meticulous attention. In a scenario where small base stations (SBSs) serve as local models trained on cached data, and a macro base station (MBS) functions as the global model setting, an attacker can exploit the vulnerability of FL, launching attacks with various adversarial attacks or deployment tactics. In this paper, we analyze such vulnerabilities, corresponding solutions were brought forth, and validated through simulation.

摘要: 联合学习（FL）为无线网络中的训练模型提供了一种保护隐私的协作方法，而信道估计正在成为一个有前途的应用。尽管对FL授权的信道估计进行了广泛的研究，但与FL相关的安全问题需要仔细关注。在小型基站（SBS）充当在缓存数据上训练的本地模型，宏基站（MBS）充当全局模型设置的场景中，攻击者可以利用FL的漏洞，通过各种对抗性攻击或部署策略发起攻击。本文对此类漏洞进行了分析，提出了相应的解决方案，并通过仿真进行了验证。



## **46. From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection Models against Adversarial Attacks**

从ML到LLM：评估网络钓鱼网页检测模型对抗对抗攻击的稳健性 cs.CR

**SubmitDate**: 2024-07-29    [abs](http://arxiv.org/abs/2407.20361v1) [paper-pdf](http://arxiv.org/pdf/2407.20361v1)

**Authors**: Aditya Kulkarni, Vivek Balachandran, Dinil Mon Divakaran, Tamal Das

**Abstract**: Phishing attacks attempt to deceive users into stealing sensitive information, posing a significant cybersecurity threat. Advances in machine learning (ML) and deep learning (DL) have led to the development of numerous phishing webpage detection solutions, but these models remain vulnerable to adversarial attacks. Evaluating their robustness against adversarial phishing webpages is essential. Existing tools contain datasets of pre-designed phishing webpages for a limited number of brands, and lack diversity in phishing features.   To address these challenges, we develop PhishOracle, a tool that generates adversarial phishing webpages by embedding diverse phishing features into legitimate webpages. We evaluate the robustness of two existing models, Stack model and Phishpedia, in classifying PhishOracle-generated adversarial phishing webpages. Additionally, we study a commercial large language model, Gemini Pro Vision, in the context of adversarial attacks. We conduct a user study to determine whether PhishOracle-generated adversarial phishing webpages deceive users. Our findings reveal that many PhishOracle-generated phishing webpages evade current phishing webpage detection models and deceive users, but Gemini Pro Vision is robust to the attack. We also develop the PhishOracle web app, allowing users to input a legitimate URL, select relevant phishing features and generate a corresponding phishing webpage. All resources are publicly available on GitHub.

摘要: 网络钓鱼攻击试图欺骗用户窃取敏感信息，构成重大的网络安全威胁。机器学习(ML)和深度学习(DL)的进步导致了许多钓鱼网页检测解决方案的发展，但这些模型仍然容易受到对手攻击。评估它们对敌意网络钓鱼网页的健壮性是至关重要的。现有工具包含为有限数量的品牌预先设计的钓鱼网页的数据集，并且在钓鱼功能方面缺乏多样性。为了应对这些挑战，我们开发了PhishOracle，这是一个通过在合法网页中嵌入不同的钓鱼功能来生成敌意钓鱼网页的工具。我们评估了现有的两种模型Stack模型和Phishpedia模型对PhishOracle生成的敌意钓鱼网页进行分类的稳健性。此外，我们研究了一个商业大型语言模型，Gemini Pro Vision，在对抗性攻击的背景下。我们进行了一项用户研究，以确定PhishOracle生成的敌意钓鱼网页是否欺骗了用户。我们的研究结果显示，许多PhishOracle生成的钓鱼网页逃避了当前的钓鱼网页检测模型并欺骗用户，但Gemini Pro Vision对攻击具有健壮性。我们还开发了PhishOracle Web应用程序，允许用户输入合法的URL，选择相关的网络钓鱼功能并生成相应的网络钓鱼网页。所有资源都在GitHub上公开提供。



## **47. Prompt Leakage effect and defense strategies for multi-turn LLM interactions**

多圈LLM相互作用的即时泄漏效应和防御策略 cs.CR

**SubmitDate**: 2024-07-29    [abs](http://arxiv.org/abs/2404.16251v3) [paper-pdf](http://arxiv.org/pdf/2404.16251v3)

**Authors**: Divyansh Agarwal, Alexander R. Fabbri, Ben Risher, Philippe Laban, Shafiq Joty, Chien-Sheng Wu

**Abstract**: Prompt leakage poses a compelling security and privacy threat in LLM applications. Leakage of system prompts may compromise intellectual property, and act as adversarial reconnaissance for an attacker. A systematic evaluation of prompt leakage threats and mitigation strategies is lacking, especially for multi-turn LLM interactions. In this paper, we systematically investigate LLM vulnerabilities against prompt leakage for 10 closed- and open-source LLMs, across four domains. We design a unique threat model which leverages the LLM sycophancy effect and elevates the average attack success rate (ASR) from 17.7% to 86.2% in a multi-turn setting. Our standardized setup further allows dissecting leakage of specific prompt contents such as task instructions and knowledge documents. We measure the mitigation effect of 7 black-box defense strategies, along with finetuning an open-source model to defend against leakage attempts. We present different combination of defenses against our threat model, including a cost analysis. Our study highlights key takeaways for building secure LLM applications and provides directions for research in multi-turn LLM interactions

摘要: 即时泄漏在LLM应用程序中构成了令人信服的安全和隐私威胁。泄露系统提示可能会危及知识产权，并充当攻击者的对抗性侦察。缺乏对即时泄漏威胁和缓解策略的系统评估，特别是对于多回合的LLM相互作用。在这篇文章中，我们系统地研究了10个封闭和开放源代码的LLM在四个域中针对即时泄漏的LLM漏洞。我们设计了一种独特的威胁模型，该模型利用了LLM的奉承效应，在多回合环境下将平均攻击成功率从17.7%提高到86.2%。我们的标准化设置还允许剖析特定提示内容的泄漏，如任务说明和知识文档。我们测量了7种黑盒防御策略的缓解效果，并微调了一个开源模型来防御泄漏企图。针对我们的威胁模型，我们提出了不同的防御组合，包括成本分析。我们的研究强调了构建安全的LLM应用程序的关键要点，并为多轮LLM交互的研究提供了方向



## **48. DDAP: Dual-Domain Anti-Personalization against Text-to-Image Diffusion Models**

DDAP：针对文本到图像扩散模型的双域反个性化 cs.CV

Accepted by IJCB 2024

**SubmitDate**: 2024-07-29    [abs](http://arxiv.org/abs/2407.20141v1) [paper-pdf](http://arxiv.org/pdf/2407.20141v1)

**Authors**: Jing Yang, Runping Xi, Yingxin Lai, Xun Lin, Zitong Yu

**Abstract**: Diffusion-based personalized visual content generation technologies have achieved significant breakthroughs, allowing for the creation of specific objects by just learning from a few reference photos. However, when misused to fabricate fake news or unsettling content targeting individuals, these technologies could cause considerable societal harm. To address this problem, current methods generate adversarial samples by adversarially maximizing the training loss, thereby disrupting the output of any personalized generation model trained with these samples. However, the existing methods fail to achieve effective defense and maintain stealthiness, as they overlook the intrinsic properties of diffusion models. In this paper, we introduce a novel Dual-Domain Anti-Personalization framework (DDAP). Specifically, we have developed Spatial Perturbation Learning (SPL) by exploiting the fixed and perturbation-sensitive nature of the image encoder in personalized generation. Subsequently, we have designed a Frequency Perturbation Learning (FPL) method that utilizes the characteristics of diffusion models in the frequency domain. The SPL disrupts the overall texture of the generated images, while the FPL focuses on image details. By alternating between these two methods, we construct the DDAP framework, effectively harnessing the strengths of both domains. To further enhance the visual quality of the adversarial samples, we design a localization module to accurately capture attentive areas while ensuring the effectiveness of the attack and avoiding unnecessary disturbances in the background. Extensive experiments on facial benchmarks have shown that the proposed DDAP enhances the disruption of personalized generation models while also maintaining high quality in adversarial samples, making it more effective in protecting privacy in practical applications.

摘要: 基于扩散的个性化视觉内容生成技术取得了重大突破，只需从几张参考照片中学习，就可以创建特定的对象。然而，当这些技术被滥用来编造假新闻或针对个人的令人不安的内容时，可能会造成相当大的社会危害。为了解决这个问题，当前的方法通过对抗性地最大化训练损失来生成对抗性样本，从而扰乱用这些样本训练的任何个性化生成模型的输出。然而，现有的方法忽略了扩散模型的内在特性，无法实现有效的防御和保持隐蔽性。提出了一种新型的双域反个性化框架(DDAP)。具体地说，我们通过在个性化生成中利用图像编码器的固定和对扰动敏感的性质来发展空间扰动学习(SPL)。随后，我们设计了一种利用频域扩散模型特性的频率微扰学习(FPL)方法。SPL破坏了生成图像的整体纹理，而FPL则专注于图像细节。通过在这两种方法之间交替使用，我们构建了DDAP框架，有效地利用了这两个领域的优势。为了进一步提高对手样本的视觉质量，我们设计了定位模块，在保证攻击有效性的同时，准确地捕捉到关注区域，避免了背景中不必要的干扰。在人脸基准上的广泛实验表明，所提出的DDAP增强了个性化生成模型的颠覆性，同时保持了对手样本的高质量，使其在实际应用中更有效地保护隐私。



## **49. Adversarial Robustness in RGB-Skeleton Action Recognition: Leveraging Attention Modality Reweighter**

RGB骨架动作识别中的对抗鲁棒性：利用注意力情态重新加权 cs.CV

Accepted by IJCB 2024

**SubmitDate**: 2024-07-29    [abs](http://arxiv.org/abs/2407.19981v1) [paper-pdf](http://arxiv.org/pdf/2407.19981v1)

**Authors**: Chao Liu, Xin Liu, Zitong Yu, Yonghong Hou, Huanjing Yue, Jingyu Yang

**Abstract**: Deep neural networks (DNNs) have been applied in many computer vision tasks and achieved state-of-the-art (SOTA) performance. However, misclassification will occur when DNNs predict adversarial examples which are created by adding human-imperceptible adversarial noise to natural examples. This limits the application of DNN in security-critical fields. In order to enhance the robustness of models, previous research has primarily focused on the unimodal domain, such as image recognition and video understanding. Although multi-modal learning has achieved advanced performance in various tasks, such as action recognition, research on the robustness of RGB-skeleton action recognition models is scarce. In this paper, we systematically investigate how to improve the robustness of RGB-skeleton action recognition models. We initially conducted empirical analysis on the robustness of different modalities and observed that the skeleton modality is more robust than the RGB modality. Motivated by this observation, we propose the \formatword{A}ttention-based \formatword{M}odality \formatword{R}eweighter (\formatword{AMR}), which utilizes an attention layer to re-weight the two modalities, enabling the model to learn more robust features. Our AMR is plug-and-play, allowing easy integration with multimodal models. To demonstrate the effectiveness of AMR, we conducted extensive experiments on various datasets. For example, compared to the SOTA methods, AMR exhibits a 43.77\% improvement against PGD20 attacks on the NTU-RGB+D 60 dataset. Furthermore, it effectively balances the differences in robustness between different modalities.

摘要: 深度神经网络(DNN)已被应用于许多计算机视觉任务中，并取得了最先进的性能(SOTA)。然而，当DNN预测通过在自然实例中添加人类不可察觉的对抗性噪声而产生的对抗性实例时，就会发生误分类。这限制了DNN在安全关键领域的应用。为了增强模型的稳健性，以往的研究主要集中在单峰领域，如图像识别和视频理解。尽管多通道学习在动作识别等各种任务中取得了较好的性能，但对RGB骨架动作识别模型的稳健性研究还很少。本文系统地研究了如何提高RGB-骨架动作识别模型的健壮性。我们最初对不同通道的稳健性进行了实证分析，观察到骨架通道比RGB通道更健壮。基于这一观察结果，我们提出了基于格式词{A}时延的格式词{M}奇异性\格式词{R}权重器(\Formatword{AMR})，它利用关注层对两个通道进行重新加权，使模型能够学习更健壮的特征。我们的AMR是即插即用的，允许与多模式模型轻松集成。为了验证AMR的有效性，我们在不同的数据集上进行了广泛的实验。例如，与SOTA方法相比，AMR在NTU-RGB+D60数据集上对PGD20攻击的性能提高了43.77%。此外，它有效地平衡了不同模式之间在稳健性方面的差异。



## **50. Quasi-Framelets: Robust Graph Neural Networks via Adaptive Framelet Convolution**

准框架集：通过自适应框架集卷积的鲁棒图神经网络 cs.LG

**SubmitDate**: 2024-07-29    [abs](http://arxiv.org/abs/2201.04728v2) [paper-pdf](http://arxiv.org/pdf/2201.04728v2)

**Authors**: Mengxi Yang, Dai Shi, Xuebin Zheng, Jie Yin, Junbin Gao

**Abstract**: This paper aims to provide a novel design of a multiscale framelet convolution for spectral graph neural networks (GNNs). While current spectral methods excel in various graph learning tasks, they often lack the flexibility to adapt to noisy, incomplete, or perturbed graph signals, making them fragile in such conditions. Our newly proposed framelet convolution addresses these limitations by decomposing graph data into low-pass and high-pass spectra through a finely-tuned multiscale approach. Our approach directly designs filtering functions within the spectral domain, allowing for precise control over the spectral components. The proposed design excels in filtering out unwanted spectral information and significantly reduces the adverse effects of noisy graph signals. Our approach not only enhances the robustness of GNNs but also preserves crucial graph features and structures. Through extensive experiments on diverse, real-world graph datasets, we demonstrate that our framelet convolution achieves superior performance in node classification tasks. It exhibits remarkable resilience to noisy data and adversarial attacks, highlighting its potential as a robust solution for real-world graph applications. This advancement opens new avenues for more adaptive and reliable spectral GNN architectures.

摘要: 本文旨在为谱图神经网络提供一种新的多尺度框架卷积设计。虽然目前的谱方法在各种图形学习任务中表现出色，但它们往往缺乏适应噪声、不完整或扰动的图形信号的灵活性，这使得它们在这些条件下很脆弱。我们最新提出的框架卷积通过精细调整的多尺度方法将图形数据分解成低通和高通光谱，从而解决了这些限制。我们的方法直接在谱域内设计滤波函数，允许对谱分量进行精确控制。所提出的设计在滤除不需要的光谱信息方面表现出色，并显著降低了噪声图形信号的不利影响。我们的方法不仅增强了GNN的健壮性，而且保留了关键的图特征和结构。通过在不同的真实图形数据集上的广泛实验，我们证明了我们的框架集卷积在节点分类任务中取得了优异的性能。它表现出了对噪声数据和对手攻击的非凡弹性，突出了它作为现实世界图形应用程序的健壮解决方案的潜力。这一进展为更适应和更可靠的频谱GNN体系结构开辟了新的途径。



