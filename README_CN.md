# Latest Adversarial Attack Papers
**update at 2022-09-08 06:31:26**

翻译来自 https://cloud.tencent.com/document/product/551/15619

## **1. Bag of Tricks for FGSM Adversarial Training**

用于FGSM对抗性训练的技巧包 cs.CV

**SubmitDate**: 2022-09-06    [paper-pdf](http://arxiv.org/pdf/2209.02684v1)

**Authors**: Zichao Li, Li Liu, Zeyu Wang, Yuyin Zhou, Cihang Xie

**Abstracts**: Adversarial training (AT) with samples generated by Fast Gradient Sign Method (FGSM), also known as FGSM-AT, is a computationally simple method to train robust networks. However, during its training procedure, an unstable mode of "catastrophic overfitting" has been identified in arXiv:2001.03994 [cs.LG], where the robust accuracy abruptly drops to zero within a single training step. Existing methods use gradient regularizers or random initialization tricks to attenuate this issue, whereas they either take high computational cost or lead to lower robust accuracy. In this work, we provide the first study, which thoroughly examines a collection of tricks from three perspectives: Data Initialization, Network Structure, and Optimization, to overcome the catastrophic overfitting in FGSM-AT.   Surprisingly, we find that simple tricks, i.e., a) masking partial pixels (even without randomness), b) setting a large convolution stride and smooth activation functions, or c) regularizing the weights of the first convolutional layer, can effectively tackle the overfitting issue. Extensive results on a range of network architectures validate the effectiveness of each proposed trick, and the combinations of tricks are also investigated. For example, trained with PreActResNet-18 on CIFAR-10, our method attains 49.8% accuracy against PGD-50 attacker and 46.4% accuracy against AutoAttack, demonstrating that pure FGSM-AT is capable of enabling robust learners. The code and models are publicly available at https://github.com/UCSC-VLAA/Bag-of-Tricks-for-FGSM-AT.

摘要: 用快速梯度符号法(FGSM)生成样本的对抗性训练(AT)，也称为FGSM-AT，是一种计算简单的稳健网络训练方法。然而，在其训练过程中，在ARXIV：2001.03994[cs.LG]中发现了一种不稳定的“灾难性过拟合”模式，其中鲁棒精度在单一训练步骤内突然降至零。现有的方法使用梯度正则化或随机初始化技巧来减弱这一问题，但它们要么计算量大，要么导致鲁棒精度较低。在这项工作中，我们提供了第一项研究，从数据初始化、网络结构和优化三个角度深入研究了一系列技巧，以克服FGSM-AT中灾难性的过拟合。令人惊讶的是，我们发现，简单的技巧，即a)掩蔽部分像素(即使没有随机性)，b)设置大的卷积步长和平滑的激活函数，或c)正则化第一卷积层的权重，可以有效地解决过拟合问题。在一系列网络体系结构上的广泛结果验证了所提出的每种技巧的有效性，并对各种技巧的组合进行了研究。例如，在CIFAR-10上用PreActResNet-18进行训练，我们的方法对PGD-50攻击者的准确率达到49.8%，对AutoAttack的准确率达到46.4%，表明纯FGSM-AT能够支持健壮的学习者。代码和模型可在https://github.com/UCSC-VLAA/Bag-of-Tricks-for-FGSM-AT.上公开获取



## **2. Improving the Accuracy and Robustness of CNNs Using a Deep CCA Neural Data Regularizer**

利用深度CCA神经数据规则器提高CNN的精度和鲁棒性 cs.CV

**SubmitDate**: 2022-09-06    [paper-pdf](http://arxiv.org/pdf/2209.02582v1)

**Authors**: Cassidy Pirlot, Richard C. Gerum, Cory Efird, Joel Zylberberg, Alona Fyshe

**Abstracts**: As convolutional neural networks (CNNs) become more accurate at object recognition, their representations become more similar to the primate visual system. This finding has inspired us and other researchers to ask if the implication also runs the other way: If CNN representations become more brain-like, does the network become more accurate? Previous attempts to address this question showed very modest gains in accuracy, owing in part to limitations of the regularization method. To overcome these limitations, we developed a new neural data regularizer for CNNs that uses Deep Canonical Correlation Analysis (DCCA) to optimize the resemblance of the CNN's image representations to that of the monkey visual cortex. Using this new neural data regularizer, we see much larger performance gains in both classification accuracy and within-super-class accuracy, as compared to the previous state-of-the-art neural data regularizers. These networks are also more robust to adversarial attacks than their unregularized counterparts. Together, these results confirm that neural data regularization can push CNN performance higher, and introduces a new method that obtains a larger performance boost.

摘要: 随着卷积神经网络(CNN)在物体识别方面变得更加准确，它们的表示变得更加类似于灵长类视觉系统。这一发现激发了我们和其他研究人员的疑问：如果CNN的表现变得更像大脑，网络是否会变得更准确？以前解决这一问题的尝试显示，由于正规化方法的局限性，在准确性方面取得了很小的进步。为了克服这些限制，我们开发了一种新的CNN神经数据正则化方法，它使用深度典型相关分析(DCCA)来优化CNN图像表示与猴子视觉皮质的相似性。使用这种新的神经数据正则化器，我们看到与以前最先进的神经数据正则化器相比，在分类精度和超类内精度方面都有更大的性能收益。与非正规网络相比，这些网络对对手攻击的抵抗力也更强。综上所述，这些结果证实了神经数据正则化可以提高CNN的性能，并介绍了一种新的方法，获得了更大的性能提升。



## **3. Instance Attack:An Explanation-based Vulnerability Analysis Framework Against DNNs for Malware Detection**

实例攻击：一种基于解释的DNN漏洞分析框架 cs.CR

**SubmitDate**: 2022-09-06    [paper-pdf](http://arxiv.org/pdf/2209.02453v1)

**Authors**: Sun RuiJin, Guo ShiZe, Guo JinHong, Xing ChangYou, Yang LuMing, Guo Xi, Pan ZhiSong

**Abstracts**: Deep neural networks (DNNs) are increasingly being applied in malware detection and their robustness has been widely debated. Traditionally an adversarial example generation scheme relies on either detailed model information (gradient-based methods) or lots of samples to train a surrogate model, neither of which are available in most scenarios.   We propose the notion of the instance-based attack. Our scheme is interpretable and can work in a black-box environment. Given a specific binary example and a malware classifier, we use the data augmentation strategies to produce enough data from which we can train a simple interpretable model. We explain the detection model by displaying the weight of different parts of the specific binary. By analyzing the explanations, we found that the data subsections play an important role in Windows PE malware detection. We proposed a new function preserving transformation algorithm that can be applied to data subsections. By employing the binary-diversification techniques that we proposed, we eliminated the influence of the most weighted part to generate adversarial examples. Our algorithm can fool the DNNs in certain cases with a success rate of nearly 100\%. Our method outperforms the state-of-the-art method . The most important aspect is that our method operates in black-box settings and the results can be validated with domain knowledge. Our analysis model can assist people in improving the robustness of malware detectors.

摘要: 深度神经网络(DNN)在恶意软件检测中的应用越来越广泛，其健壮性一直备受争议。传统的对抗性示例生成方案依赖于详细的模型信息(基于梯度的方法)或大量样本来训练代理模型，这两种方法在大多数情况下都不可用。我们提出了基于实例攻击的概念。我们的方案是可解释的，可以在黑盒环境中工作。给出一个特定的二进制例子和一个恶意软件分类器，我们使用数据扩充策略来产生足够的数据，从中我们可以训练一个简单的可解释模型。我们通过显示特定二进制的不同部分的权重来解释检测模型。通过分析解释，我们发现数据子部分在Windows PE恶意软件检测中起着重要作用。提出了一种新的适用于数据细分的保函数变换算法。通过采用我们提出的二元多样化技术，我们消除了权重最大的部分对生成对抗性例子的影响。在某些情况下，我们的算法可以欺骗DNN，成功率接近100%。我们的方法比最先进的方法性能更好。最重要的是，我们的方法是在黑盒环境下运行的，并且结果可以用领域知识来验证。我们的分析模型可以帮助人们提高恶意软件检测器的健壮性。



## **4. MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World**

MACAB：真实世界中自然触发目标检测的模型不可知性Clean-Annotation后门 cs.CV

**SubmitDate**: 2022-09-06    [paper-pdf](http://arxiv.org/pdf/2209.02339v1)

**Authors**: Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Nepal Surya, Derek Abbott

**Abstracts**: Object detection is the foundation of various critical computer-vision tasks such as segmentation, object tracking, and event detection. To train an object detector with satisfactory accuracy, a large amount of data is required. However, due to the intensive workforce involved with annotating large datasets, such a data curation task is often outsourced to a third party or relied on volunteers. This work reveals severe vulnerabilities of such data curation pipeline. We propose MACAB that crafts clean-annotated images to stealthily implant the backdoor into the object detectors trained on them even when the data curator can manually audit the images. We observe that the backdoor effect of both misclassification and the cloaking are robustly achieved in the wild when the backdoor is activated with inconspicuously natural physical triggers. Backdooring non-classification object detection with clean-annotation is challenging compared to backdooring existing image classification tasks with clean-label, owing to the complexity of having multiple objects within each frame, including victim and non-victim objects. The efficacy of the MACAB is ensured by constructively i abusing the image-scaling function used by the deep learning framework, ii incorporating the proposed adversarial clean image replica technique, and iii combining poison data selection criteria given constrained attacking budget. Extensive experiments demonstrate that MACAB exhibits more than 90% attack success rate under various real-world scenes. This includes both cloaking and misclassification backdoor effect even restricted with a small attack budget. The poisoned samples cannot be effectively identified by state-of-the-art detection techniques.The comprehensive video demo is at https://youtu.be/MA7L_LpXkp4, which is based on a poison rate of 0.14% for YOLOv4 cloaking backdoor and Faster R-CNN misclassification backdoor.

摘要: 目标检测是各种关键的计算机视觉任务的基础，如分割、目标跟踪和事件检测。为了以令人满意的精度训练目标检测器，需要大量的数据。然而，由于注释大型数据集所涉及的密集劳动力，这样的数据管理任务通常被外包给第三方或依赖于志愿者。这项工作揭示了这种数据管理管道的严重漏洞。我们建议MACAB制作经过干净注释的图像，以便在数据管理员可以手动审计图像的情况下，将后门秘密植入对其训练的对象检测器。我们观察到，当后门被不明显的自然物理触发激活时，错误分类和伪装的后门效应在野外都得到了很好的实现。由于每帧中包含多个对象(包括受害者和非受害者对象)的复杂性，与使用干净标签回溯现有图像分类任务相比，使用干净注释来回溯非分类对象检测是具有挑战性的。MACAB的有效性是通过建设性地I滥用深度学习框架使用的图像缩放函数，II结合所提出的对抗性干净图像复制技术，III在给定有限攻击预算的情况下结合毒物数据选择标准来确保的。大量实验表明，MACAB在各种真实场景下的攻击成功率均在90%以上。这包括隐形和错误分类的后门效应，即使在攻击预算很小的情况下也是如此。最先进的检测技术无法有效地识别有毒样本。全面的视频演示在https://youtu.be/MA7L_LpXkp4，上进行，这是基于YOLOv4伪装后门的毒用率为0.14%和更快的R-CNN错误分类后门。



## **5. A Survey of Machine Unlearning**

机器遗忘研究综述 cs.LG

**SubmitDate**: 2022-09-06    [paper-pdf](http://arxiv.org/pdf/2209.02299v1)

**Authors**: Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, Quoc Viet Hung Nguyen

**Abstracts**: Computer systems hold a large amount of personal data over decades. On the one hand, such data abundance allows breakthroughs in artificial intelligence (AI), especially machine learning (ML) models. On the other hand, it can threaten the privacy of users and weaken the trust between humans and AI. Recent regulations require that private information about a user can be removed from computer systems in general and from ML models in particular upon request (e.g. the "right to be forgotten"). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often "remember" the old data. Existing adversarial attacks proved that we can learn private membership or attributes of the training data from the trained models. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to solve the problem completely due to the lack of common frameworks and resources. In this survey paper, we seek to provide a thorough investigation of machine unlearning in its definitions, scenarios, mechanisms, and applications. Specifically, as a categorical collection of state-of-the-art research, we hope to provide a broad reference for those seeking a primer on machine unlearning and its various formulations, design requirements, removal requests, algorithms, and uses in a variety of ML applications. Furthermore, we hope to outline key findings and trends in the paradigm as well as highlight new areas of research that have yet to see the application of machine unlearning, but could nonetheless benefit immensely. We hope this survey provides a valuable reference for ML researchers as well as those seeking to innovate privacy technologies. Our resources are at https://github.com/tamlhp/awesome-machine-unlearning.

摘要: 几十年来，计算机系统保存着大量的个人数据。一方面，这样的数据丰富使人工智能(AI)，特别是机器学习(ML)模型取得了突破。另一方面，它会威胁用户的隐私，削弱人类与AI之间的信任。最近的法规要求，一般情况下，可以从计算机系统中删除关于用户的私人信息，特别是在请求时可以从ML模型中删除用户的私人信息(例如，“被遗忘权”)。虽然从后端数据库中删除数据应该很简单，但在人工智能环境中这是不够的，因为ML模型经常“记住”旧数据。现有的对抗性攻击证明，我们可以从训练好的模型中学习训练数据的私人成员或属性。这种现象呼唤一种新的范式，即机器遗忘，以使ML模型忘记特定的数据。事实证明，由于缺乏通用的框架和资源，最近关于机器遗忘的研究并不能完全解决这个问题。在这篇调查论文中，我们试图对机器遗忘的定义、场景、机制和应用进行全面的调查。具体地说，作为最新研究的分类集合，我们希望为那些寻求机器遗忘及其各种公式、设计要求、移除请求、算法和在各种ML应用中使用的入门知识的人提供广泛的参考。此外，我们希望概述该范式中的主要发现和趋势，并强调尚未看到机器遗忘应用的新研究领域，但仍可能受益匪浅。我们希望这项调查为ML研究人员以及那些寻求创新隐私技术的人提供有价值的参考。我们的资源在https://github.com/tamlhp/awesome-machine-unlearning.



## **6. White-Box Adversarial Policies in Deep Reinforcement Learning**

深度强化学习中的白盒对抗策略 cs.AI

Code is available at  https://github.com/thestephencasper/white_box_rarl

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2209.02167v1)

**Authors**: Stephen Casper, Dylan Hadfield-Menell, Gabriel Kreiman

**Abstracts**: Adversarial examples against AI systems pose both risks via malicious attacks and opportunities for improving robustness via adversarial training. In multiagent settings, adversarial policies can be developed by training an adversarial agent to minimize a victim agent's rewards. Prior work has studied black-box attacks where the adversary only sees the state observations and effectively treats the victim as any other part of the environment. In this work, we experiment with white-box adversarial policies to study whether an agent's internal state can offer useful information for other agents. We make three contributions. First, we introduce white-box adversarial policies in which an attacker can observe a victim's internal state at each timestep. Second, we demonstrate that white-box access to a victim makes for better attacks in two-agent environments, resulting in both faster initial learning and higher asymptotic performance against the victim. Third, we show that training against white-box adversarial policies can be used to make learners in single-agent environments more robust to domain shifts.

摘要: 针对人工智能系统的对抗性例子既有通过恶意攻击带来的风险，也有通过对抗性培训提高稳健性的机会。在多代理设置中，可以通过训练对抗代理来制定对抗策略，以将受害者代理的回报降至最低。以前的工作已经研究了黑盒攻击，在这种攻击中，对手只看到状态观察，并有效地将受害者视为环境的任何其他部分。在这项工作中，我们实验了白盒对抗策略，以研究一个代理的内部状态是否能为其他代理提供有用的信息。我们有三点贡献。首先，我们引入了白盒对抗性策略，其中攻击者可以在每个时间步观察受害者的内部状态。其次，我们证明了在双代理环境中，对受害者的白盒访问有助于更好的攻击，从而导致更快的初始学习和更高的针对受害者的渐近性能。第三，我们证明了针对白盒对抗策略的训练可以用来使单代理环境中的学习者对域转移更健壮。



## **7. Reinforcement learning-based optimised control for tracking of nonlinear systems with adversarial attacks**

基于强化学习的对抗性非线性系统跟踪优化控制 eess.SY

Submitted for The 10th RSI International Conference on Robotics and  Mechatronics (ICRoM 2022)

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2209.02165v1)

**Authors**: Farshad Rahimi, Sepideh Ziaei

**Abstracts**: This paper introduces a reinforcement learning-based tracking control approach for a class of nonlinear systems using neural networks. In this approach, adversarial attacks were considered both in the actuator and on the outputs. This approach incorporates a simultaneous tracking and optimization process. It is necessary to be able to solve the Hamilton-Jacobi-Bellman equation (HJB) in order to obtain optimal control input, but this is difficult due to the strong nonlinearity terms in the equation. In order to find the solution to the HJB equation, we used a reinforcement learning approach. In this online adaptive learning approach, three neural networks are simultaneously adapted: the critic neural network, the actor neural network, and the adversary neural network. Ultimately, simulation results are presented to demonstrate the effectiveness of the introduced method on a manipulator.

摘要: 针对一类神经网络非线性系统，提出了一种基于强化学习的跟踪控制方法。在这种方法中，在执行器和输出端都考虑了对抗性攻击。这种方法结合了同步跟踪和优化过程。为了获得最优控制输入，必须能解Hamilton-Jacobi-Bellman方程(HJB)，但由于方程中的强非线性项，这是很困难的。为了找到HJB方程的解，我们使用了强化学习方法。在这种在线自适应学习方法中，同时自适应了三个神经网络：批评者神经网络、行动者神经网络和对手神经网络。最后，以机械手为例，给出了仿真结果，验证了该方法的有效性。



## **8. On the Anonymity of Peer-To-Peer Network Anonymity Schemes Used by Cryptocurrencies**

加密货币使用的对等网络匿名方案的匿名性研究 cs.CR

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2201.11860v3)

**Authors**: Piyush Kumar Sharma, Devashish Gosain, Claudia Diaz

**Abstracts**: Cryptocurrency systems can be subject to deanonimization attacks by exploiting the network-level communication on their peer-to-peer network. Adversaries who control a set of colluding node(s) within the peer-to-peer network can observe transactions being exchanged and infer the parties involved. Thus, various network anonymity schemes have been proposed to mitigate this problem, with some solutions providing theoretical anonymity guarantees.   In this work, we model such peer-to-peer network anonymity solutions and evaluate their anonymity guarantees. To do so, we propose a novel framework that uses Bayesian inference to obtain the probability distributions linking transactions to their possible originators. We characterize transaction anonymity with those distributions, using entropy as metric of adversarial uncertainty on the originator's identity. In particular, we model Dandelion, Dandelion++ and Lightning Network. We study different configurations and demonstrate that none of them offers acceptable anonymity to their users. For instance, our analysis reveals that in the widely deployed Lightning Network, with 1% strategically chosen colluding nodes the adversary can uniquely determine the originator for about 50% of the total transactions in the network. In Dandelion, an adversary that controls 15% of the nodes has on average uncertainty among only 8 possible originators. Moreover, we observe that due to the way Dandelion and Dandelion++ are designed, increasing the network size does not correspond to an increase in the anonymity set of potential originators. Alarmingly, our longitudinal analysis of Lightning Network reveals rather an inverse trend -- with the growth of the network the overall anonymity decreases.

摘要: 通过利用其对等网络上的网络级通信，加密货币系统可能会受到反匿名化攻击。在对等网络中控制一组串通节点的敌手可以观察正在交换的交易并推断所涉及的各方。因此，各种网络匿名方案被提出来缓解这一问题，一些解决方案提供了理论上的匿名性保证。在这项工作中，我们对这种对等网络匿名解决方案进行建模，并评估它们的匿名性保证。为此，我们提出了一个新的框架，它使用贝叶斯推理来获得将事务链接到可能的发起者的概率分布。我们使用这些分布来表征交易匿名性，使用熵作为对发起者身份的敌意不确定性的度量。特别是，我们对蒲公英、蒲公英++和闪电网络进行了建模。我们研究了不同的配置，并证明它们都不能为用户提供可接受的匿名性。例如，我们的分析表明，在广泛部署的闪电网络中，通过1%的策略选择合谋节点，对手可以唯一地确定网络中约50%的总交易的发起者。在蒲公英中，一个控制了15%节点的对手平均只有8个可能的发起者中存在不确定性。此外，我们观察到，由于蒲公英和蒲公英++的设计方式，增加网络规模并不对应于潜在发起者匿名性集合的增加。令人担忧的是，我们对Lightning Network的纵向分析揭示了一个相反的趋势--随着网络的增长，总体匿名性下降。



## **9. Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples**

通过手工制作的对抗性例子评估预先训练的语言模型的敏感性 cs.CL

10 pages, 1 figure, 3 tables

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2209.02128v1)

**Authors**: Hezekiah J. Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del Castillo Iglesias, Ron Heichman, Ramesh Darwishi

**Abstracts**: Recent advances in the development of large language models have resulted in public access to state-of-the-art pre-trained language models (PLMs), including Generative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT). However, evaluations of PLMs, in practice, have shown their susceptibility to adversarial attacks during the training and fine-tuning stages of development. Such attacks can result in erroneous outputs, model-generated hate speech, and the exposure of users' sensitive information. While existing research has focused on adversarial attacks during either the training or the fine-tuning of PLMs, there is a deficit of information on attacks made between these two development phases. In this work, we highlight a major security vulnerability in the public release of GPT-3 and further investigate this vulnerability in other state-of-the-art PLMs. We restrict our work to pre-trained models that have not undergone fine-tuning. Further, we underscore token distance-minimized perturbations as an effective adversarial approach, bypassing both supervised and unsupervised quality measures. Following this approach, we observe a significant decrease in text classification quality when evaluating for semantic similarity.

摘要: 在开发大型语言模型方面的最新进展导致公众能够访问最先进的预训练语言模型(PLM)，包括生成性预训练转换器3(GPT-3)和来自转换器的双向编码器表示(BERT)。然而，在实践中，对PLM的评估表明，它们在训练和发展的微调阶段容易受到对抗性攻击。此类攻击可能导致错误输出、模型生成的仇恨言论以及用户敏感信息的暴露。虽然现有的研究集中在PLM的训练或微调期间的对抗性攻击，但关于这两个开发阶段之间的攻击的信息不足。在这项工作中，我们突出了公开发布的GPT-3中的一个主要安全漏洞，并进一步调查了其他最先进的PLM中的此漏洞。我们将我们的工作限制在未经微调的预先训练的模型上。此外，我们强调令牌距离最小化扰动是一种有效的对抗性方法，绕过了监督和非监督质量度量。按照这种方法，我们观察到在评估语义相似性时文本分类质量显著下降。



## **10. PatchZero: Defending against Adversarial Patch Attacks by Detecting and Zeroing the Patch**

PatchZero：通过检测和归零补丁来防御敌意补丁攻击 cs.CV

Accepted to WACV 2023

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2207.01795v3)

**Authors**: Ke Xu, Yao Xiao, Zhaoheng Zheng, Kaijie Cai, Ram Nevatia

**Abstracts**: Adversarial patch attacks mislead neural networks by injecting adversarial pixels within a local region. Patch attacks can be highly effective in a variety of tasks and physically realizable via attachment (e.g. a sticker) to the real-world objects. Despite the diversity in attack patterns, adversarial patches tend to be highly textured and different in appearance from natural images. We exploit this property and present PatchZero, a general defense pipeline against white-box adversarial patches without retraining the downstream classifier or detector. Specifically, our defense detects adversaries at the pixel-level and "zeros out" the patch region by repainting with mean pixel values. We further design a two-stage adversarial training scheme to defend against the stronger adaptive attacks. PatchZero achieves SOTA defense performance on the image classification (ImageNet, RESISC45), object detection (PASCAL VOC), and video classification (UCF101) tasks with little degradation in benign performance. In addition, PatchZero transfers to different patch shapes and attack types.

摘要: 对抗性补丁攻击通过在局部区域内注入对抗性像素来误导神经网络。补丁攻击可以在各种任务中非常有效，并且可以通过附着(例如贴纸)到真实世界的对象来物理实现。尽管攻击模式多种多样，但敌方补丁往往纹理丰富，外观与自然图像不同。我们利用这一特性，提出了PatchZero，一种针对白盒恶意补丁的通用防御管道，而不需要重新训练下游的分类器或检测器。具体地说，我们的防御在像素级检测对手，并通过使用平均像素值重新绘制来对补丁区域进行“清零”。我们进一步设计了一种两阶段对抗性训练方案，以抵御更强的适应性攻击。PatchZero在图像分类(ImageNet，RESISC45)、目标检测(Pascal VOC)和视频分类(UCF101)任务上实现了SOTA防御性能，性能良好，性能几乎没有下降。此外，PatchZero还可以转换为不同的补丁形状和攻击类型。



## **11. Adversarial Detection: Attacking Object Detection in Real Time**

对抗性检测：攻击目标的实时检测 cs.AI

7 pages, 10 figures

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2209.01962v1)

**Authors**: Han Wu, Syed Yunas, Sareh Rowlands, Wenjie Ruan, Johan Wahlstrom

**Abstracts**: Intelligent robots hinge on accurate object detection models to perceive the environment. Advances in deep learning security unveil that object detection models are vulnerable to adversarial attacks. However, prior research primarily focuses on attacking static images or offline videos. It is still unclear if such attacks could jeopardize real-world robotic applications in dynamic environments. There is still a gap between theoretical discoveries and real-world applications. We bridge the gap by proposing the first real-time online attack against object detection models. We devised three attacks that fabricate bounding boxes for nonexistent objects at desired locations.

摘要: 智能机器人依赖于准确的目标检测模型来感知环境。深度学习安全方面的进展揭示了目标检测模型容易受到对手攻击。然而，以往的研究主要集中在攻击静态图像或离线视频上。目前尚不清楚此类攻击是否会危及动态环境中真实世界的机器人应用。在理论发现和现实应用之间仍有差距。我们通过提出第一个针对目标检测的实时在线攻击模型来弥补这一差距。我们设计了三种攻击，在所需位置为不存在的对象制造边界框。



## **12. Jamming Modulation: An Active Anti-Jamming Scheme**

干扰调制：一种主动抗干扰性方案 cs.IT

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2209.01943v1)

**Authors**: Jianhui Ma, Qiang Li, Zilong Liu, Linsong Du, Hongyang Chen, Nirwan Ansari

**Abstracts**: Providing quality communications under adversarial electronic attacks, e.g., broadband jamming attacks, is a challenging task. Unlike state-of-the-art approaches which treat jamming signals as destructive interference, this paper presents a novel active anti-jamming (AAJ) scheme for a jammed channel to enhance the communication quality between a transmitter node (TN) and receiver node (RN), where the TN actively exploits the jamming signal as a carrier to send messages. Specifically, the TN is equipped with a programmable-gain amplifier, which is capable of re-modulating the jamming signals for jamming modulation. Considering four typical jamming types, we derive both the bit error rates (BER) and the corresponding optimal detection thresholds of the AAJ scheme. The asymptotic performances of the AAJ scheme are discussed under the high jamming-to-noise ratio (JNR) and sampling rate cases. Our analysis shows that there exists a BER floor for sufficiently large JNR. Simulation results indicate that the proposed AAJ scheme allows the TN to communicate with the RN reliably even under extremely strong and/or broadband jamming. Additionally, we investigate the channel capacity of the proposed AAJ scheme and show that the channel capacity of the AAJ scheme outperforms that of the direct transmission when the JNR is relatively high.

摘要: 在敌意电子攻击(如宽带干扰攻击)下提供高质量的通信是一项具有挑战性的任务。不同于现有的将干扰信号视为破坏性干扰的方法，提出了一种新的针对干扰信道的有源抗扰(AAJ)方案，以提高发送节点(TN)和接收节点(RN)之间的通信质量，其中TN主动利用干扰信号作为发送消息的载波。具体地说，TN配备了可编程增益放大器，能够对干扰信号进行重新调制以进行干扰调制。考虑到四种典型的干扰类型，我们推导了AAJ方案的误比特率和相应的最优检测门限。讨论了AAJ方案在高信噪比和高采样率情况下的渐近性能。我们的分析表明，对于足够大的Jnr，存在一个误码率下限。仿真结果表明，所提出的AAJ方案允许TN在极强和/或宽带干扰下与RN可靠地通信。此外，我们还对所提出的AAJ方案的信道容量进行了研究，结果表明，当JNR相对较高时，AAJ方案的信道容量优于直接传输。



## **13. Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation**

基于重整化影响估计的训练集攻击目标识别 cs.LG

Accepted at CCS'2022 -- Extended version including the supplementary  material

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2201.10055v2)

**Authors**: Zayd Hammoudeh, Daniel Lowd

**Abstracts**: Targeted training-set attacks inject malicious instances into the training set to cause a trained model to mislabel one or more specific test instances. This work proposes the task of target identification, which determines whether a specific test instance is the target of a training-set attack. Target identification can be combined with adversarial-instance identification to find (and remove) the attack instances, mitigating the attack with minimal impact on other predictions. Rather than focusing on a single attack method or data modality, we build on influence estimation, which quantifies each training instance's contribution to a model's prediction. We show that existing influence estimators' poor practical performance often derives from their over-reliance on training instances and iterations with large losses. Our renormalized influence estimators fix this weakness; they far outperform the original estimators at identifying influential groups of training examples in both adversarial and non-adversarial settings, even finding up to 100% of adversarial training instances with no clean-data false positives. Target identification then simplifies to detecting test instances with anomalous influence values. We demonstrate our method's effectiveness on backdoor and poisoning attacks across various data domains, including text, vision, and speech, as well as against a gray-box, adaptive attacker that specifically optimizes the adversarial instances to evade our method. Our source code is available at https://github.com/ZaydH/target_identification.

摘要: 有针对性的训练集攻击将恶意实例注入训练集，以导致训练模型错误标记一个或多个特定测试实例。本文提出了目标识别的任务，即确定特定的测试实例是否是训练集攻击的目标。目标识别可以与对抗性实例识别相结合来发现(和删除)攻击实例，从而在对其他预测影响最小的情况下减轻攻击。我们不是专注于单一的攻击方法或数据模式，而是建立在影响估计的基础上，该估计量化了每个训练实例对模型预测的贡献。我们指出，现有影响估值器的实际性能较差往往源于它们过度依赖训练实例和具有较大损失的迭代。我们的重整化影响估计器修复了这一弱点；在识别对抗性和非对抗性设置中有影响力的训练样本组方面，它们远远优于原始估计器，甚至可以发现高达100%的对抗性训练实例，而没有干净的数据误报。然后，目标识别简化为检测具有异常影响值的测试实例。我们展示了我们的方法在各种数据领域的后门和中毒攻击中的有效性，包括文本、视觉和语音，以及针对灰盒、自适应攻击者的攻击，该攻击者专门优化敌对实例来逃避我们的方法。我们的源代码可以在https://github.com/ZaydH/target_identification.上找到



## **14. "Is your explanation stable?": A Robustness Evaluation Framework for Feature Attribution**

“你的解释稳定吗？”：特征归因的稳健性评估框架 cs.AI

Accepted by ACM CCS 2022

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2209.01782v1)

**Authors**: Yuyou Gan, Yuhao Mao, Xuhong Zhang, Shouling Ji, Yuwen Pu, Meng Han, Jianwei Yin, Ting Wang

**Abstracts**: Understanding the decision process of neural networks is hard. One vital method for explanation is to attribute its decision to pivotal features. Although many algorithms are proposed, most of them solely improve the faithfulness to the model. However, the real environment contains many random noises, which may leads to great fluctuations in the explanations. More seriously, recent works show that explanation algorithms are vulnerable to adversarial attacks. All of these make the explanation hard to trust in real scenarios.   To bridge this gap, we propose a model-agnostic method \emph{Median Test for Feature Attribution} (MeTFA) to quantify the uncertainty and increase the stability of explanation algorithms with theoretical guarantees. MeTFA has the following two functions: (1) examine whether one feature is significantly important or unimportant and generate a MeTFA-significant map to visualize the results; (2) compute the confidence interval of a feature attribution score and generate a MeTFA-smoothed map to increase the stability of the explanation. Experiments show that MeTFA improves the visual quality of explanations and significantly reduces the instability while maintaining the faithfulness. To quantitatively evaluate the faithfulness of an explanation under different noise settings, we further propose several robust faithfulness metrics. Experiment results show that the MeTFA-smoothed explanation can significantly increase the robust faithfulness. In addition, we use two scenarios to show MeTFA's potential in the applications. First, when applied to the SOTA explanation method to locate context bias for semantic segmentation models, MeTFA-significant explanations use far smaller regions to maintain 99\%+ faithfulness. Second, when tested with different explanation-oriented attacks, MeTFA can help defend vanilla, as well as adaptive, adversarial attacks against explanations.

摘要: 理解神经网络的决策过程是很困难的。解释的一个重要方法是将其决定归因于关键特征。虽然有很多算法被提出，但大多数算法都只是提高了对模型的忠诚度。然而，现实环境中包含了许多随机噪声，这可能会导致解释中的巨大波动。更严重的是，最近的研究表明，解释算法容易受到敌意攻击。所有这些都使得这种解释在真实情况下很难令人信任。为了弥补这一差距，我们提出了一种模型不可知的方法\emph(特征属性的中位数测试)来量化不确定性并在理论上保证解释算法的稳定性。MeTFA具有以下两个功能：(1)检查一个特征是否显著重要，并生成MeTFA显著图以可视化结果；(2)计算特征属性得分的置信度，并生成MeTFA平滑的图，以增加解释的稳定性。实验表明，MeTFA在保持解释真实性的同时，提高了解释的视觉质量，显著降低了解释的不稳定性。为了定量评估不同噪声环境下解释的真实性，我们进一步提出了几种稳健的忠诚度度量。实验结果表明，经MeTFA平滑后的解释能够显著提高系统的稳健忠实性。此外，我们还通过两个场景展示了MeTFA在应用中的潜力。首先，当应用于SOTA解释方法来定位语义分割模型的语境偏差时，MeTFA显著解释使用小得多的区域来保持99+的忠实性。其次，当使用不同的面向解释的攻击进行测试时，MeTFA可以帮助防御普通攻击，以及针对解释的适应性对抗性攻击。



## **15. An Adaptive Black-box Defense against Trojan Attacks (TrojDef)**

木马攻击的自适应黑盒防御(TrojDef) cs.CR

**SubmitDate**: 2022-09-05    [paper-pdf](http://arxiv.org/pdf/2209.01721v1)

**Authors**: Guanxiong Liu, Abdallah Khreishah, Fatima Sharadgah, Issa Khalil

**Abstracts**: Trojan backdoor is a poisoning attack against Neural Network (NN) classifiers in which adversaries try to exploit the (highly desirable) model reuse property to implant Trojans into model parameters for backdoor breaches through a poisoned training process. Most of the proposed defenses against Trojan attacks assume a white-box setup, in which the defender either has access to the inner state of NN or is able to run back-propagation through it. In this work, we propose a more practical black-box defense, dubbed TrojDef, which can only run forward-pass of the NN. TrojDef tries to identify and filter out Trojan inputs (i.e., inputs augmented with the Trojan trigger) by monitoring the changes in the prediction confidence when the input is repeatedly perturbed by random noise. We derive a function based on the prediction outputs which is called the prediction confidence bound to decide whether the input example is Trojan or not. The intuition is that Trojan inputs are more stable as the misclassification only depends on the trigger, while benign inputs will suffer when augmented with noise due to the perturbation of the classification features.   Through mathematical analysis, we show that if the attacker is perfect in injecting the backdoor, the Trojan infected model will be trained to learn the appropriate prediction confidence bound, which is used to distinguish Trojan and benign inputs under arbitrary perturbations. However, because the attacker might not be perfect in injecting the backdoor, we introduce a nonlinear transform to the prediction confidence bound to improve the detection accuracy in practical settings. Extensive empirical evaluations show that TrojDef significantly outperforms the-state-of-the-art defenses and is highly stable under different settings, even when the classifier architecture, the training process, or the hyper-parameters change.

摘要: 特洛伊木马后门是一种针对神经网络(NN)分类器的中毒攻击，攻击者试图利用(非常理想的)模型重用属性，通过有毒的训练过程将特洛伊木马植入到后门入侵的模型参数中。大多数针对特洛伊木马攻击的防御建议采用白盒设置，在白盒设置中，防御者要么可以访问NN的内部状态，要么能够通过它进行反向传播。在这项工作中，我们提出了一种更实用的黑盒防御方法，称为TrojDef，它只能运行神经网络的前传。当输入被随机噪声反复扰动时，TrojDef试图通过监视预测置信度的变化来识别和过滤特洛伊木马输入(即，使用特洛伊木马触发器增强的输入)。我们根据预测输出推导出一个函数，称为预测置信限，用来判断输入示例是否为木马。直觉是，木马输入更稳定，因为误分类只取决于触发器，而良性输入将由于分类特征的扰动而在噪声中增强时受到影响。通过数学分析，我们证明了如果攻击者是完美的后门注入，木马感染模型将被训练来学习适当的预测置信界，用于区分任意扰动下的木马和良性输入。然而，由于攻击者在注入后门时可能并不完美，我们引入了对预测置信限的非线性变换，以提高实际环境中的检测精度。大量的实验评估表明，TrojDef的性能显著优于最先进的防御措施，并且在不同的设置下具有高度的稳定性，即使在分类器结构、训练过程或超参数发生变化的情况下也是如此。



## **16. Autonomous Cross Domain Adaptation under Extreme Label Scarcity**

极端标签稀缺下的自主跨域自适应 cs.LG

**SubmitDate**: 2022-09-04    [paper-pdf](http://arxiv.org/pdf/2209.01548v1)

**Authors**: Weiwei Weng, Mahardhika Pratama, Choiru Za'in, Marcus De Carvalho, Rakaraddi Appan, Andri Ashfahani, Edward Yapp Kien Yee

**Abstracts**: A cross domain multistream classification is a challenging problem calling for fast domain adaptations to handle different but related streams in never-ending and rapidly changing environments. Notwithstanding that existing multistream classifiers assume no labelled samples in the target stream, they still incur expensive labelling cost since they require fully labelled samples of the source stream. This paper aims to attack the problem of extreme label shortage in the cross domain multistream classification problems where only very few labelled samples of the source stream are provided before process runs. Our solution, namely Learning Streaming Process from Partial Ground Truth (LEOPARD), is built upon a flexible deep clustering network where its hidden nodes, layers and clusters are added and removed dynamically in respect to varying data distributions. A deep clustering strategy is underpinned by a simultaneous feature learning and clustering technique leading to clustering-friendly latent spaces. A domain adaptation strategy relies on the adversarial domain adaptation technique where a feature extractor is trained to fool a domain classifier classifying source and target streams. Our numerical study demonstrates the efficacy of LEOPARD where it delivers improved performances compared to prominent algorithms in 15 of 24 cases. Source codes of LEOPARD are shared in \url{https://github.com/wengweng001/LEOPARD.git} to enable further study.

摘要: 跨域多流分类是一个具有挑战性的问题，需要快速的域自适应来处理永无止境和快速变化的环境中不同但相关的流。尽管现有的多数据流分类器假定目标流中没有标记的样本，但它们仍然招致昂贵的标记成本，因为它们需要源流的完全标记的样本。本文旨在解决跨域多数据流分类问题中的极端标签短缺问题，即在处理运行之前只提供极少的源流的标签样本。我们的解决方案，即从部分地面真相学习流媒体过程(LEOPARD)，建立在一个灵活的深度聚类网络之上，其中隐藏的节点、层和簇根据不同的数据分布动态地添加和删除。深度聚类策略由同时的特征学习和聚类技术支持，从而产生对聚类友好的潜在空间。域自适应策略依赖于对抗性域自适应技术，其中特征提取者被训练来愚弄对源和目标流进行分类的域分类器。我们的数值研究证明了Leopard的有效性，与24个案例中的15个案例相比，它提供了比著名算法更好的性能。Leopard的源代码在\url{https://github.com/wengweng001/LEOPARD.git}上共享，以便进一步研究。



## **17. Are Attribute Inference Attacks Just Imputation?**

属性推理攻击仅仅是归罪吗？ cs.CR

13 (main body) + 4 (references and appendix) pages. To appear in  CCS'22

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2209.01292v1)

**Authors**: Bargav Jayaraman, David Evans

**Abstracts**: Models can expose sensitive information about their training data. In an attribute inference attack, an adversary has partial knowledge of some training records and access to a model trained on those records, and infers the unknown values of a sensitive feature of those records. We study a fine-grained variant of attribute inference we call \emph{sensitive value inference}, where the adversary's goal is to identify with high confidence some records from a candidate set where the unknown attribute has a particular sensitive value. We explicitly compare attribute inference with data imputation that captures the training distribution statistics, under various assumptions about the training data available to the adversary. Our main conclusions are: (1) previous attribute inference methods do not reveal more about the training data from the model than can be inferred by an adversary without access to the trained model, but with the same knowledge of the underlying distribution as needed to train the attribute inference attack; (2) black-box attribute inference attacks rarely learn anything that cannot be learned without the model; but (3) white-box attacks, which we introduce and evaluate in the paper, can reliably identify some records with the sensitive value attribute that would not be predicted without having access to the model. Furthermore, we show that proposed defenses such as differentially private training and removing vulnerable records from training do not mitigate this privacy risk. The code for our experiments is available at \url{https://github.com/bargavj/EvaluatingDPML}.

摘要: 模型可能会暴露有关其训练数据的敏感信息。在属性推理攻击中，敌手对一些训练记录具有部分知识，并访问在这些记录上训练的模型，并推断这些记录的敏感特征的未知值。我们研究了属性推理的一种细粒度变体，称为\emph(敏感值推理)，其中对手的目标是高置信度地从未知属性具有特定敏感值的候选集合中识别一些记录。在关于对手可用的训练数据的各种假设下，我们显式地将属性推理与捕获训练分布统计的数据补偿进行比较。我们的主要结论是：(1)以前的属性推理方法并不能揭示更多关于模型训练数据的信息，而不是对手无法访问训练的模型所能推断的数据，而是具有与训练属性推理攻击所需的相同的底层分布知识；(2)黑盒属性推理攻击很少学习没有模型无法学习的任何东西；但是(3)我们在论文中引入和评估的白盒攻击可以可靠地识别一些具有敏感值属性的记录，这些记录在没有访问模型的情况下是无法预测的。此外，我们表明，建议的防御措施，如区分隐私培训和从培训中删除易受攻击的记录，并不能减轻这种隐私风险。我们实验的代码可以在\url{https://github.com/bargavj/EvaluatingDPML}.上找到



## **18. Semi-supervised Conditional GAN for Simultaneous Generation and Detection of Phishing URLs: A Game theoretic Perspective**

基于博弈论的半监督条件遗传算法同时生成和检测钓鱼URL cs.CR

Accepted to ICMLA 2022

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2108.01852v2)

**Authors**: Sharif Amit Kamran, Shamik Sengupta, Alireza Tavakkoli

**Abstracts**: Spear Phishing is a type of cyber-attack where the attacker sends hyperlinks through email on well-researched targets. The objective is to obtain sensitive information by imitating oneself as a trustworthy website. In recent times, deep learning has become the standard for defending against such attacks. However, these architectures were designed with only defense in mind. Moreover, the attacker's perspective and motivation are absent while creating such models. To address this, we need a game-theoretic approach to understand the perspective of the attacker (Hacker) and the defender (Phishing URL detector). We propose a Conditional Generative Adversarial Network with novel training strategy for real-time phishing URL detection. Additionally, we train our architecture in a semi-supervised manner to distinguish between adversarial and real examples, along with detecting malicious and benign URLs. We also design two games between the attacker and defender in training and deployment settings by utilizing the game-theoretic perspective. Our experiments confirm that the proposed architecture surpasses recent state-of-the-art architectures for phishing URLs detection.

摘要: 鱼叉式网络钓鱼是一种网络攻击，攻击者通过电子邮件向经过充分研究的目标发送超链接。其目标是通过将自己伪装成一个值得信赖的网站来获取敏感信息。最近，深度学习已成为防御此类攻击的标准。然而，这些架构在设计时只考虑到了防御。此外，在创建这样的模型时，攻击者的视角和动机是缺失的。为了解决这个问题，我们需要一个博弈论的方法来理解攻击者(黑客)和防御者(网络钓鱼URL检测器)的角度。提出了一种具有新颖训练策略的条件生成对抗网络，用于实时网络钓鱼URL检测。此外，我们以半监督的方式训练我们的体系结构，以区分敌意和真实的示例，以及检测恶意和良性URL。我们还利用博弈论的观点设计了攻防双方在训练和部署环境下的两场比赛。我们的实验证实，该体系结构在网络钓鱼URL检测方面优于目前最先进的体系结构。



## **19. Bayesian Pseudo Labels: Expectation Maximization for Robust and Efficient Semi-Supervised Segmentation**

贝叶斯伪标签：稳健有效的半监督分割的期望最大化 cs.CV

MICCAI 2022 (Early accept, Student Travel Award)

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2208.04435v2)

**Authors**: Mou-Cheng Xu, Yukun Zhou, Chen Jin, Marius de Groot, Daniel C. Alexander, Neil P. Oxtoby, Yipeng Hu, Joseph Jacob

**Abstracts**: This paper concerns pseudo labelling in segmentation. Our contribution is fourfold. Firstly, we present a new formulation of pseudo-labelling as an Expectation-Maximization (EM) algorithm for clear statistical interpretation. Secondly, we propose a semi-supervised medical image segmentation method purely based on the original pseudo labelling, namely SegPL. We demonstrate SegPL is a competitive approach against state-of-the-art consistency regularisation based methods on semi-supervised segmentation on a 2D multi-class MRI brain tumour segmentation task and a 3D binary CT lung vessel segmentation task. The simplicity of SegPL allows less computational cost comparing to prior methods. Thirdly, we demonstrate that the effectiveness of SegPL may originate from its robustness against out-of-distribution noises and adversarial attacks. Lastly, under the EM framework, we introduce a probabilistic generalisation of SegPL via variational inference, which learns a dynamic threshold for pseudo labelling during the training. We show that SegPL with variational inference can perform uncertainty estimation on par with the gold-standard method Deep Ensemble.

摘要: 本文研究的是分割中的伪标注问题。我们的贡献是四倍的。首先，我们提出了一种新的伪标记公式，作为一种用于清晰统计解释的期望最大化(EM)算法。其次，提出了一种完全基于原始伪标记的半监督医学图像分割方法--SegPL。在2D多类MRI脑肿瘤分割任务和3D二值CT肺血管分割任务中，我们证明了SegPL是一种与最先进的基于一致性正则化的半监督分割方法相竞争的方法。与以前的方法相比，SegPL的简单性允许更少的计算成本。第三，我们证明了SegPL的有效性可能源于它对分布外噪声和对手攻击的健壮性。最后，在EM框架下，我们通过变分推理对SegPL进行概率推广，在训练过程中学习伪标签的动态阈值。我们证明了带变分推理的SegPL方法可以与金标准方法深层集成一样进行不确定度估计。



## **20. Subject Membership Inference Attacks in Federated Learning**

联合学习中的主体成员推理攻击 cs.LG

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2206.03317v2)

**Authors**: Anshuman Suri, Pallika Kanani, Virendra J. Marathe, Daniel W. Peterson

**Abstracts**: Privacy attacks on Machine Learning (ML) models often focus on inferring the existence of particular data points in the training data. However, what the adversary really wants to know is if a particular \emph{individual}'s (\emph{subject}'s) data was included during training. In such scenarios, the adversary is more likely to have access to the distribution of a particular subject, than actual records. Furthermore, in settings like cross-silo Federated Learning (FL), a subject's data can be embodied by multiple data records that are spread across multiple organizations. Nearly all of the existing private FL literature is dedicated to studying privacy at two granularities -- item-level (individual data records), and user-level (participating user in the federation), neither of which apply to data subjects in cross-silo FL. This insight motivates us to shift our attention from the privacy of data records to the privacy of \emph{data subjects}, also known as subject-level privacy. We propose two black-box attacks for \emph{subject membership inference}, of which one assumes access to a model after each training round. Using these attacks, we estimate subject membership inference risk on real-world data for single-party models as well as FL scenarios. We find our attacks to be extremely potent, even without access to exact training records, and using the knowledge of membership for a handful of subjects. To better understand the various factors that may influence subject privacy risk in cross-silo FL settings, we systematically generate several hundred synthetic federation configurations, varying properties of the data, model design and training, and the federation itself. Finally, we investigate the effectiveness of Differential Privacy in mitigating this threat.

摘要: 针对机器学习(ML)模型的隐私攻击通常集中在推断训练数据中特定数据点的存在。然而，对手真正想知道的是，在训练过程中是否包括了特定的\emph{个人}的数据。在这种情况下，对手更有可能访问特定主题的分布，而不是实际记录。此外，在像跨竖井联合学习(FL)这样的环境中，受试者的数据可以通过分布在多个组织中的多个数据记录来体现。几乎所有现有的私有FL文献都致力于在两个粒度上研究隐私--项级(个人数据记录)和用户级(参与联盟的用户)，这两者都不适用于跨竖井FL中的数据主体。这种洞察力促使我们将注意力从数据记录的隐私转移到数据主题的隐私，也称为主题级别隐私。我们提出了两种针对主题成员推理的黑盒攻击，其中一种假设在每一轮训练后都可以访问模型。使用这些攻击，我们在单方模型和FL场景的真实世界数据上估计了主体成员关系推断风险。我们发现我们的攻击非常强大，即使没有获得确切的训练记录，并使用少数科目的成员知识。为了更好地了解在跨竖井FL设置中可能影响主体隐私风险的各种因素，我们系统地生成了数百个合成联邦配置、数据的不同属性、模型设计和训练以及联邦本身。最后，我们研究了差分隐私在缓解这一威胁方面的有效性。



## **21. Group Property Inference Attacks Against Graph Neural Networks**

针对图神经网络的群属性推理攻击 cs.LG

Full version of the ACM CCS'22 paper

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2209.01100v1)

**Authors**: Xiuling Wang, Wendy Hui Wang

**Abstracts**: With the fast adoption of machine learning (ML) techniques, sharing of ML models is becoming popular. However, ML models are vulnerable to privacy attacks that leak information about the training data. In this work, we focus on a particular type of privacy attacks named property inference attack (PIA) which infers the sensitive properties of the training data through the access to the target ML model. In particular, we consider Graph Neural Networks (GNNs) as the target model, and distribution of particular groups of nodes and links in the training graph as the target property. While the existing work has investigated PIAs that target at graph-level properties, no prior works have studied the inference of node and link properties at group level yet.   In this work, we perform the first systematic study of group property inference attacks (GPIA) against GNNs. First, we consider a taxonomy of threat models under both black-box and white-box settings with various types of adversary knowledge, and design six different attacks for these settings. We evaluate the effectiveness of these attacks through extensive experiments on three representative GNN models and three real-world graphs. Our results demonstrate the effectiveness of these attacks whose accuracy outperforms the baseline approaches. Second, we analyze the underlying factors that contribute to GPIA's success, and show that the target model trained on the graphs with or without the target property represents some dissimilarity in model parameters and/or model outputs, which enables the adversary to infer the existence of the property. Further, we design a set of defense mechanisms against the GPIA attacks, and demonstrate that these mechanisms can reduce attack accuracy effectively with small loss on GNN model accuracy.

摘要: 随着机器学习(ML)技术的快速采用，ML模型的共享变得越来越流行。然而，ML模型容易受到隐私攻击，从而泄露有关训练数据的信息。在这项工作中，我们重点研究了一种特殊类型的隐私攻击，称为属性推理攻击(PIA)，它通过访问目标ML模型来推断训练数据的敏感属性。特别地，我们将图神经网络(GNN)作为目标模型，将训练图中特定节点组和链路组的分布作为目标属性。虽然现有的工作已经研究了针对图级属性的PIA，但还没有先前的工作研究在组级别上的节点和链接属性的推理。在本工作中，我们首次系统地研究了针对GNN的群属性推理攻击(GPIA)。首先，我们考虑了黑盒和白盒两种不同类型对手知识的威胁模型的分类，并针对这些设置设计了六种不同的攻击。我们通过在三个具有代表性的GNN模型和三个真实世界图上的大量实验来评估这些攻击的有效性。我们的结果证明了这些攻击的有效性，它们的准确率超过了基线方法。其次，我们分析了导致GPIA成功的潜在因素，并证明了在有或没有目标属性的图上训练的目标模型表示了模型参数和/或模型输出的一些不同，这使得对手能够推断该属性的存在。进一步，我们设计了一套针对GPIA攻击的防御机制，并证明了这些机制可以在较小的GNN模型精度损失的情况下有效地降低攻击精度。



## **22. Scalable Adversarial Attack Algorithms on Influence Maximization**

基于影响力最大化的可扩展敌意攻击算法 cs.SI

11 pages, 2 figures

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2209.00892v1)

**Authors**: Lichao Sun, Xiaobin Rui, Wei Chen

**Abstracts**: In this paper, we study the adversarial attacks on influence maximization under dynamic influence propagation models in social networks. In particular, given a known seed set S, the problem is to minimize the influence spread from S by deleting a limited number of nodes and edges. This problem reflects many application scenarios, such as blocking virus (e.g. COVID-19) propagation in social networks by quarantine and vaccination, blocking rumor spread by freezing fake accounts, or attacking competitor's influence by incentivizing some users to ignore the information from the competitor. In this paper, under the linear threshold model, we adapt the reverse influence sampling approach and provide efficient algorithms of sampling valid reverse reachable paths to solve the problem.

摘要: 本文研究了社会网络中动态影响传播模型下影响最大化的对抗性攻击。特别地，给定一个已知的种子集S，问题是通过删除有限数量的节点和边来最小化从S传播的影响。这个问题反映了很多应用场景，比如通过隔离和接种疫苗来阻止病毒(例如新冠肺炎)在社交网络中的传播，通过冻结虚假账号来阻止谣言传播，或者通过激励一些用户忽略竞争对手的信息来攻击竞争对手的影响力。本文在线性门限模型下，采用反向影响抽样方法，给出了有效反向可达路径抽样的有效算法。



## **23. Adversarial Color Film: Effective Physical-World Attack to DNNs**

对抗性彩色电影：对DNN的有效物理世界攻击 cs.CV

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2209.02430v1)

**Authors**: Chengyin Hu, Weiwen Shi

**Abstracts**: It is well known that the performance of deep neural networks (DNNs) is susceptible to subtle interference. So far, camera-based physical adversarial attacks haven't gotten much attention, but it is the vacancy of physical attack. In this paper, we propose a simple and efficient camera-based physical attack called Adversarial Color Film (AdvCF), which manipulates the physical parameters of color film to perform attacks. Carefully designed experiments show the effectiveness of the proposed method in both digital and physical environments. In addition, experimental results show that the adversarial samples generated by AdvCF have excellent performance in attack transferability, which enables AdvCF effective black-box attacks. At the same time, we give the guidance of defense against AdvCF by means of adversarial training. Finally, we look into AdvCF's threat to future vision-based systems and propose some promising mentality for camera-based physical attacks.

摘要: 众所周知，深度神经网络(DNN)的性能容易受到细微干扰的影响。到目前为止，基于摄像机的身体对抗攻击还没有得到太多的关注，但它是身体攻击的空白。本文提出了一种简单而有效的基于摄像机的物理攻击方法，称为对抗性彩色胶片攻击(AdvCF)，它通过操纵彩色胶片的物理参数来进行攻击。精心设计的实验表明，该方法在数字和物理环境中都是有效的。此外，实验结果表明，由AdvCF生成的对抗性样本具有良好的攻击可转移性，使得AdvCF能够有效地进行黑盒攻击。同时，通过对抗性训练的方式，指导对Advcf的防御。最后，我们展望了AdvCF对未来基于视觉的系统的威胁，并对基于摄像机的物理攻击提出了一些有前景的思路。



## **24. Impact of Scaled Image on Robustness of Deep Neural Networks**

尺度图像对深度神经网络稳健性的影响 cs.CV

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2209.02132v1)

**Authors**: Chengyin Hu, Weiwen Shi

**Abstracts**: Deep neural networks (DNNs) have been widely used in computer vision tasks like image classification, object detection and segmentation. Whereas recent studies have shown their vulnerability to manual digital perturbations or distortion in the input images. The accuracy of the networks is remarkably influenced by the data distribution of their training dataset. Scaling the raw images creates out-of-distribution data, which makes it a possible adversarial attack to fool the networks. In this work, we propose a Scaling-distortion dataset ImageNet-CS by Scaling a subset of the ImageNet Challenge dataset by different multiples. The aim of our work is to study the impact of scaled images on the performance of advanced DNNs. We perform experiments on several state-of-the-art deep neural network architectures on the proposed ImageNet-CS, and the results show a significant positive correlation between scaling size and accuracy decline. Moreover, based on ResNet50 architecture, we demonstrate some tests on the performance of recent proposed robust training techniques and strategies like Augmix, Revisiting and Normalizer Free on our proposed ImageNet-CS. Experiment results have shown that these robust training techniques can improve networks' robustness to scaling transformation.

摘要: 深度神经网络在图像分类、目标检测和分割等计算机视觉任务中有着广泛的应用。然而，最近的研究表明，它们在输入图像中容易受到人工数字干扰或失真的影响。训练数据集的数据分布对网络的精度有很大影响。对原始图像进行缩放会产生分布不均的数据，这使得它可能成为愚弄网络的敌意攻击。在这项工作中，我们通过对ImageNet挑战数据集的子集进行不同倍数的缩放，提出了一个缩放失真数据集ImageNet-CS。我们工作的目的是研究缩放图像对高级DNN性能的影响。我们在提出的ImageNet-CS上对几种最先进的深度神经网络结构进行了实验，结果表明，尺度大小与准确率下降呈显著正相关。此外，基于ResNet50体系结构，我们在我们提出的ImageNet-CS上对最近提出的健壮训练技术和策略，如AugMix、Revising和Normal izer Free的性能进行了测试。实验结果表明，这些稳健的训练技术可以提高网络对尺度变换的鲁棒性。



## **25. Reliable Representations Make A Stronger Defender: Unsupervised Structure Refinement for Robust GNN**

可靠的表示使防御者更强大：健壮GNN的无监督结构求精 cs.LG

Accepted in KDD2022

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2207.00012v2)

**Authors**: Kuan Li, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, Qing He

**Abstracts**: Benefiting from the message passing mechanism, Graph Neural Networks (GNNs) have been successful on flourish tasks over graph data. However, recent studies have shown that attackers can catastrophically degrade the performance of GNNs by maliciously modifying the graph structure. A straightforward solution to remedy this issue is to model the edge weights by learning a metric function between pairwise representations of two end nodes, which attempts to assign low weights to adversarial edges. The existing methods use either raw features or representations learned by supervised GNNs to model the edge weights. However, both strategies are faced with some immediate problems: raw features cannot represent various properties of nodes (e.g., structure information), and representations learned by supervised GNN may suffer from the poor performance of the classifier on the poisoned graph. We need representations that carry both feature information and as mush correct structure information as possible and are insensitive to structural perturbations. To this end, we propose an unsupervised pipeline, named STABLE, to optimize the graph structure. Finally, we input the well-refined graph into a downstream classifier. For this part, we design an advanced GCN that significantly enhances the robustness of vanilla GCN without increasing the time complexity. Extensive experiments on four real-world graph benchmarks demonstrate that STABLE outperforms the state-of-the-art methods and successfully defends against various attacks.

摘要: 得益于消息传递机制，图神经网络(GNN)已经成功地处理了大量的图数据任务。然而，最近的研究表明，攻击者可以通过恶意修改图结构来灾难性地降低GNN的性能。解决这一问题的一个直接解决方案是通过学习两个末端节点的成对表示之间的度量函数来对边权重进行建模，该度量函数试图为对抗性边分配较低的权重。现有的方法要么使用原始特征，要么使用由监督GNN学习的表示来对边权重进行建模。然而，这两种策略都面临着一些迫在眉睫的问题：原始特征不能表示节点的各种属性(例如结构信息)，而有监督GNN学习的表示可能会受到有毒图上分类器性能较差的影响。我们需要既携带特征信息又尽可能正确的结构信息并对结构扰动不敏感的表示法。为此，我们提出了一种名为STRATE的无监督流水线来优化图的结构。最后，我们将精化后的图输入到下游分类器中。对于这一部分，我们设计了一种改进的GCN，它在不增加时间复杂度的情况下显著增强了普通GCN的健壮性。在四个真实图形基准上的大量实验表明，STRATE的性能优于最先进的方法，并成功地防御了各种攻击。



## **26. Universal Fourier Attack for Time Series**

时间序列的通用傅里叶攻击 cs.CR

**SubmitDate**: 2022-09-02    [paper-pdf](http://arxiv.org/pdf/2209.00757v1)

**Authors**: Elizabeth Coda, Brad Clymer, Chance DeSmet, Yijing Watkins, Michael Girard

**Abstracts**: A wide variety of adversarial attacks have been proposed and explored using image and audio data. These attacks are notoriously easy to generate digitally when the attacker can directly manipulate the input to a model, but are much more difficult to implement in the real-world. In this paper we present a universal, time invariant attack for general time series data such that the attack has a frequency spectrum primarily composed of the frequencies present in the original data. The universality of the attack makes it fast and easy to implement as no computation is required to add it to an input, while time invariance is useful for real-world deployment. Additionally, the frequency constraint ensures the attack can withstand filtering. We demonstrate the effectiveness of the attack in two different domains, speech recognition and unintended radiated emission, and show that the attack is robust against common transform-and-compare defense pipelines.

摘要: 已经提出并探索了使用图像和音频数据的各种对抗性攻击。当攻击者可以直接操作模型的输入时，这些攻击以数字方式生成是出了名的容易，但在现实世界中实现起来要困难得多。在本文中，我们提出了一种针对一般时间序列数据的通用、时不变攻击，使得该攻击具有主要由原始数据中存在的频率组成的频谱。该攻击的普遍性使其易于快速实现，因为不需要计算即可将其添加到输入，而时间不变性对于真实世界的部署很有用。此外，频率限制确保了攻击能够经受住过滤。我们在语音识别和意外辐射两个不同的领域证明了该攻击的有效性，并证明了该攻击对常见的变换和比较防御流水线具有健壮性。



## **27. Adversarial for Social Privacy: A Poisoning Strategy to Degrade User Identity Linkage**

社交隐私的对抗性：降低用户身份链接的中毒策略 cs.SI

**SubmitDate**: 2022-09-01    [paper-pdf](http://arxiv.org/pdf/2209.00269v1)

**Authors**: Jiangli Shao, Yongqing Wang, Boshen Shi, Hao Gao, Huawei Shen, Xueqi Cheng

**Abstracts**: Privacy issues on social networks have been extensively discussed in recent years. The user identity linkage (UIL) task, aiming at finding corresponding users across different social networks, would be a threat to privacy if unethically applied. The sensitive user information might be detected through connected identities. A promising and novel solution to this issue is to design an adversarial strategy to degrade the matching performance of UIL models. However, most existing adversarial attacks on graphs are designed for models working in a single network, while UIL is a cross-network learning task. Meanwhile, privacy protection against UIL works unilaterally in real-world scenarios, i.e., the service provider can only add perturbations to its own network to protect its users from being linked. To tackle these challenges, this paper proposes a novel adversarial attack strategy that poisons one target network to prevent its nodes from being linked to other networks by UIL algorithms. Specifically, we reformalize the UIL problem in the perspective of kernelized topology consistency and convert the attack objective to maximizing the structural changes within the target network before and after attacks. A novel graph kernel is then defined with Earth mover's distance (EMD) on the edge-embedding space. In terms of efficiency, a fast attack strategy is proposed by greedy searching and replacing EMD with its lower bound. Results on three real-world datasets indicate that the proposed attacks can best fool a wide range of UIL models and reach a balance between attack effectiveness and imperceptibility.

摘要: 近年来，社交网络上的隐私问题得到了广泛的讨论。用户身份链接(UIL)任务旨在跨不同的社交网络找到对应的用户，如果应用不道德，将对隐私构成威胁。敏感的用户信息可以通过连接的身份被检测到。解决这一问题的一个有前途的新方案是设计一种对抗性策略来降低UIL模型的匹配性能。然而，现有的大多数对抗性图攻击都是针对工作在单一网络中的模型而设计的，而UIL是一个跨网络的学习任务。同时，针对UIL的隐私保护在现实世界场景中单方面起作用，即服务提供商只能在其自己的网络中添加扰动，以保护其用户不被链接。为了应对这些挑战，本文提出了一种新颖的对抗性攻击策略，通过毒化一个目标网络来防止其节点通过UIL算法链接到其他网络。具体地说，我们从核化拓扑一致性的角度对UIL问题进行了改造，将攻击目标转化为最大化攻击前后目标网络内部的结构变化。然后在边嵌入空间上定义了一种新的图核：地球移动距离(EMD)。在效率方面，提出了一种贪婪搜索并用EMD下界代替EMD的快速攻击策略。在三个真实数据集上的结果表明，所提出的攻击最好地欺骗了广泛的UIL模型，并在攻击有效性和不可感知性之间取得了平衡。



## **28. FDB: Fraud Dataset Benchmark**

FDB：欺诈数据集基准 cs.LG

**SubmitDate**: 2022-08-31    [paper-pdf](http://arxiv.org/pdf/2208.14417v2)

**Authors**: Prince Grover, Zheng Li, Jianbo Liu, Jakub Zablocki, Hao Zhou, Julia Xu, Anqi Cheng

**Abstracts**: Standardized datasets and benchmarks have spurred innovations in computer vision, natural language processing, multi-modal and tabular settings. We note that, as compared to other well researched fields fraud detection has numerous differences. The differences include a high class imbalance, diverse feature types, frequently changing fraud patterns, and adversarial nature of the problem. Due to these differences, the modeling approaches that are designed for other classification tasks may not work well for the fraud detection. We introduce Fraud Dataset Benchmark (FDB), a compilation of publicly available datasets catered to fraud detection. FDB comprises variety of fraud related tasks, ranging from identifying fraudulent card-not-present transactions, detecting bot attacks, classifying malicious URLs, predicting risk of loan to content moderation. The Python based library from FDB provides consistent API for data loading with standardized training and testing splits. For reference, we also provide baseline evaluations of different modeling approaches on FDB. Considering the increasing popularity of Automated Machine Learning (AutoML) for various research and business problems, we used AutoML frameworks for our baseline evaluations. For fraud prevention, the organizations that operate with limited resources and lack ML expertise often hire a team of investigators, use blocklists and manual rules, all of which are inefficient and do not scale well. Such organizations can benefit from AutoML solutions that are easy to deploy in production and pass the bar of fraud prevention requirements. We hope that FDB helps in the development of customized fraud detection techniques catered to different fraud modus operandi (MOs) as well as in the improvement of AutoML systems that can work well for all datasets in the benchmark.

摘要: 标准化的数据集和基准促进了计算机视觉、自然语言处理、多模式和表格设置方面的创新。我们注意到，与其他经过充分研究的领域相比，欺诈检测有许多不同之处。不同之处包括高级别不平衡、多样化的特征类型、频繁变化的欺诈模式以及问题的对抗性。由于这些差异，为其他分类任务设计的建模方法可能不能很好地用于欺诈检测。我们介绍了欺诈数据集基准(FDB)，这是一种面向欺诈检测的公开可用的数据集的汇编。FDB包括各种与欺诈相关的任务，从识别欺诈性的卡不存在交易、检测机器人攻击、对恶意URL进行分类、预测贷款风险到内容审核。FDB的基于Python的库通过标准化的训练和测试拆分为数据加载提供了一致的API。作为参考，我们还提供了对FDB的不同建模方法的基线评估。考虑到自动化机器学习(AutoML)在各种研究和商业问题上越来越受欢迎，我们使用AutoML框架进行基线评估。为了防止欺诈，那些资源有限、缺乏ML专业知识的组织往往会雇佣一支调查团队，使用阻止名单和手动规则，这些都效率低下，伸缩性不好。这样的组织可以受益于AutoML解决方案，这些解决方案易于在生产中部署，并通过了防欺诈要求的门槛。我们希望FDB有助于开发迎合不同欺诈手法(MO)的定制欺诈检测技术，并有助于改进AutoML系统，使其能够很好地适用于基准中的所有数据集。



## **29. Wiggle: Physical Challenge-Response Verification of Vehicle Platooning**

摆动：车辆排队的物理挑战-响应验证 cs.CR

10 pages, 13 figures

**SubmitDate**: 2022-08-31    [paper-pdf](http://arxiv.org/pdf/2209.00080v1)

**Authors**: Connor Dickey, Christopher Smith, Quentin Johnson, Jingcheng Li, Ziqi Xu, Loukas Lazos, Ming Li

**Abstracts**: Autonomous vehicle platooning promises many benefits such as fuel efficiency, road safety, reduced traffic congestion, and passenger comfort. Platooning vehicles travel in a single file, in close distance, and at the same velocity. The platoon formation is autonomously maintained by a Cooperative Adaptive Cruise Control (CACC) system which relies on sensory data and vehicle-to-vehicle (V2V) communications. In fact, V2V messages play a critical role in shortening the platooning distance while maintaining safety. Whereas V2V message integrity and source authentication can be verified via cryptographic methods, establishing the truthfulness of the message contents is a much harder task.   This work establishes a physical access control mechanism to restrict V2V messages to platooning members. Specifically, we aim at tying the digital identity of a candidate requesting to join a platoon to its physical trajectory relative to the platoon. We propose the {\em Wiggle} protocol that employs a physical challenge-response exchange to prove that a candidate requesting to be admitted into a platoon actually follows it. The protocol name is inspired by the random longitudinal movements that the candidate is challenged to execute. {\em Wiggle} prevents any remote adversary from joining the platoon and injecting fake CACC messages. Compared to prior works, {\em Wiggle} is resistant to pre-recording attacks and can verify that the candidate is directly behind the verifier at the same lane.

摘要: 自动驾驶汽车排成一排承诺了许多好处，如燃油效率、道路安全、减少交通拥堵和乘客舒适度。排成一排的车辆排成一列，近距离，以相同的速度行进。编队由协作自适应巡航控制(CACC)系统自主维护，该系统依赖于传感器数据和车对车(V2V)通信。事实上，V2V消息在缩短排兵距离的同时保持安全起着至关重要的作用。虽然V2V消息完整性和来源验证可以通过加密方法进行验证，但建立消息内容的真实性则是一项困难得多的任务。这项工作建立了一种物理访问控制机制，以限制V2V消息发送给排成一排的成员。具体地说，我们的目标是将申请加入排的候选人的数字身份与其相对于排的物理轨迹捆绑在一起。我们提出了{\em Wigger}协议，该协议使用物理挑战-响应交换来证明请求进入排的候选人实际上遵循了该协议。协议名称的灵感来自候选人被挑战执行的随机纵向运动。{\em Wigger}阻止任何远程对手加入排并注入虚假的CACC消息。与以往的工作相比，{\em Wigger}能够抵抗预录攻击，并可以验证候选人是否在同一车道上的验证器正后方。



## **30. Membership Inference Attacks by Exploiting Loss Trajectory**

利用丢失轨迹的成员关系推理攻击 cs.CR

Accepted by CCS 2022

**SubmitDate**: 2022-08-31    [paper-pdf](http://arxiv.org/pdf/2208.14933v1)

**Authors**: Yiyong Liu, Zhengyu Zhao, Michael Backes, Yang Zhang

**Abstracts**: Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called \system, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely \emph{distilled loss trajectory}, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6$\times$ higher true-positive rate at a low false-positive rate of 0.1\% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.

摘要: 机器学习模型容易受到成员推理攻击，在这种攻击中，对手的目标是预测特定样本是否包含在目标模型的训练数据集中。现有的攻击方法通常只利用给定目标模型的输出信息(主要是损失)。因此，在成员样本和非成员样本都产生类似小损失的实际情况下，这些方法自然无法区分它们。针对这一局限性，本文提出了一种新的攻击方法--系统攻击方法，该方法可以利用目标模型整个训练过程中的成员信息来提高攻击性能。为了在普通黑盒环境下发动攻击，我们利用知识蒸馏，将隶属度信息表示为一系列中间模型在不同蒸馏时期的损失，即\emph(蒸馏损失轨迹)，以及给定目标模型的损失。在不同的数据集和模型架构上的实验结果表明，该攻击在不同的度量方面具有很大的优势。例如，在CINIC-10上，我们的攻击在0.1的低误检率下获得了至少6倍以上的真阳性率。进一步的分析表明，我们的攻击在更严格的情况下总体上是有效的。



## **31. Be Your Own Neighborhood: Detecting Adversarial Example by the Neighborhood Relations Built on Self-Supervised Learning**

做自己的邻里：通过建立在自我监督学习基础上的邻里关系发现敌对榜样 cs.LG

co-first author

**SubmitDate**: 2022-08-31    [paper-pdf](http://arxiv.org/pdf/2209.00005v1)

**Authors**: Zhiyuan He, Yijun Yang, Pin-Yu Chen, Qiang Xu, Tsung-Yi Ho

**Abstracts**: Deep Neural Networks (DNNs) have achieved excellent performance in various fields. However, DNNs' vulnerability to Adversarial Examples (AE) hinders their deployments to safety-critical applications. This paper presents a novel AE detection framework, named BEYOND, for trustworthy predictions. BEYOND performs the detection by distinguishing the AE's abnormal relation with its augmented versions, i.e. neighbors, from two prospects: representation similarity and label consistency. An off-the-shelf Self-Supervised Learning (SSL) model is used to extract the representation and predict the label for its highly informative representation capacity compared to supervised learning models. For clean samples, their representations and predictions are closely consistent with their neighbors, whereas those of AEs differ greatly. Furthermore, we explain this observation and show that by leveraging this discrepancy BEYOND can effectively detect AEs. We develop a rigorous justification for the effectiveness of BEYOND. Furthermore, as a plug-and-play model, BEYOND can easily cooperate with the Adversarial Trained Classifier (ATC), achieving the state-of-the-art (SOTA) robustness accuracy. Experimental results show that BEYOND outperforms baselines by a large margin, especially under adaptive attacks. Empowered by the robust relation net built on SSL, we found that BEYOND outperforms baselines in terms of both detection ability and speed. Our code will be publicly available.

摘要: 深度神经网络(DNN)在各个领域都取得了优异的性能。然而，DNN对敌意示例(AE)的脆弱性阻碍了它们在安全关键应用中的部署。本文提出了一种用于可信预测的新型声发射检测框架Beyond。Beyond通过从表示相似性和标签一致性两个方面区分AE与其扩展版本(即邻居)的异常关系来执行检测。与监督学习模型相比，现有的自监督学习模型被用来提取表示和预测标签，因为它具有高度信息量的表示能力。对于清洁样品，它们的表述和预测与它们的邻居非常一致，而AEs的表述和预测差别很大。此外，我们解释了这一观察结果，并表明通过利用这种差异，Beyond可以有效地检测到AEs。我们为Beyond的有效性制定了严格的理由。此外，作为一种即插即用的模型，Beyond可以很容易地与对手训练分类器(ATC)合作，实现最先进的(SOTA)稳健性精度。实验结果表明，Beyond的性能明显优于Baseline，特别是在自适应攻击下。在建立在SSL上的健壮关系网络的支持下，我们发现Beyond在检测能力和速度方面都优于Baseline。我们的代码将公开可用。



## **32. Vulnerability of Distributed Inverter VAR Control in PV Distributed Energy System**

分布式逆变器无功控制在光伏分布式能源系统中的脆弱性 eess.SY

**SubmitDate**: 2022-08-31    [paper-pdf](http://arxiv.org/pdf/2208.14672v1)

**Authors**: Bo Tu, Wen-Tai Li, Chau Yuen

**Abstracts**: This work studies the potential vulnerability of distributed control schemes in smart grids. To this end, we consider an optimal inverter VAR control problem within a PV-integrated distribution network. First, we formulate the centralized optimization problem considering the reactive power priority and further reformulate the problem into a distributed framework by an accelerated proximal projection method. The inverter controller can curtail the PV output of each user by clamping the reactive power. To illustrate the studied distributed control scheme that may be vulnerable due to the two-hop information communication pattern, we present a heuristic attack injecting false data during the information exchange. Then we analyze the attack impact on the update procedure of critical parameters. A case study with an eight-node test feeder demonstrates that adversaries can violate the constraints of distributed control scheme without being detected through simple attacks such as the proposed attack.

摘要: 本文研究了智能电网中分布式控制方案的潜在脆弱性。为此，我们考虑了光伏集成配电网中逆变器的最优无功控制问题。首先，对考虑无功优先的集中优化问题进行了形式化描述，并利用加速近邻投影法将问题转化为分布式框架。逆变器控制器可以通过钳位无功功率来减少每个用户的光伏输出。为了说明所研究的分布式控制方案可能由于两跳信息通信模式而易受攻击，我们提出了一种在信息交换过程中注入虚假数据的启发式攻击。然后分析了攻击对关键参数更新过程的影响。一个具有8个节点的测试馈线的案例研究表明，攻击者可以违反分布式控制方案的约束，而不会被简单的攻击(如提出的攻击)检测到。



## **33. Unrestricted Adversarial Samples Based on Non-semantic Feature Clusters Substitution**

基于非语义特征簇替换的无限制对抗性样本 cs.CV

**SubmitDate**: 2022-08-31    [paper-pdf](http://arxiv.org/pdf/2209.02406v1)

**Authors**: MingWei Zhou, Xiaobing Pei

**Abstracts**: Most current methods generate adversarial examples with the $L_p$ norm specification. As a result, many defense methods utilize this property to eliminate the impact of such attacking algorithms. In this paper,we instead introduce "unrestricted" perturbations that create adversarial samples by using spurious relations which were learned by model training. Specifically, we find feature clusters in non-semantic features that are strongly correlated with model judgment results, and treat them as spurious relations learned by the model. Then we create adversarial samples by using them to replace the corresponding feature clusters in the target image. Experimental evaluations show that in both black-box and white-box situations. Our adversarial examples do not change the semantics of images, while still being effective at fooling an adversarially trained DNN image classifier.

摘要: 目前的大多数方法都使用$L_p$范数规范生成对抗性实例。因此，许多防御方法利用这一特性来消除此类攻击算法的影响。在本文中，我们改为引入“无限制”扰动，通过使用通过模型训练学习的伪关系来创建对抗性样本。具体地说，我们在非语义特征中发现与模型判断结果强相关的特征簇，并将它们视为模型学习的伪关系。然后利用对抗性样本替换目标图像中相应的特征簇来生成对抗性样本。实验评估表明，在黑盒和白盒两种情况下都是如此。我们的对抗性例子不会改变图像的语义，同时仍然可以有效地愚弄经过对抗性训练的DNN图像分类器。



## **34. A Black-Box Attack on Optical Character Recognition Systems**

一种针对光学字符识别系统的黑盒攻击 cs.CV

11 Pages, CVMI-2022

**SubmitDate**: 2022-08-30    [paper-pdf](http://arxiv.org/pdf/2208.14302v1)

**Authors**: Samet Bayram, Kenneth Barner

**Abstracts**: Adversarial machine learning is an emerging area showing the vulnerability of deep learning models. Exploring attack methods to challenge state of the art artificial intelligence (A.I.) models is an area of critical concern. The reliability and robustness of such A.I. models are one of the major concerns with an increasing number of effective adversarial attack methods. Classification tasks are a major vulnerable area for adversarial attacks. The majority of attack strategies are developed for colored or gray-scaled images. Consequently, adversarial attacks on binary image recognition systems have not been sufficiently studied. Binary images are simple two possible pixel-valued signals with a single channel. The simplicity of binary images has a significant advantage compared to colored and gray scaled images, namely computation efficiency. Moreover, most optical character recognition systems (O.C.R.s), such as handwritten character recognition, plate number identification, and bank check recognition systems, use binary images or binarization in their processing steps. In this paper, we propose a simple yet efficient attack method, Efficient Combinatorial Black-box Adversarial Attack, on binary image classifiers. We validate the efficiency of the attack technique on two different data sets and three classification networks, demonstrating its performance. Furthermore, we compare our proposed method with state-of-the-art methods regarding advantages and disadvantages as well as applicability.

摘要: 对抗性机器学习是一个显示深度学习模型脆弱性的新兴领域。探索攻击方法，挑战最先进的人工智能(A.I.)模特是一个备受关注的领域。随着越来越多的有效对抗性攻击方法的出现，这种人工智能模型的可靠性和健壮性是人们主要关注的问题之一。分类任务是敌对攻击的一个主要易受攻击的领域。大多数攻击策略都是针对彩色或灰度图像开发的。因此，对二值图像识别系统的敌意攻击还没有得到充分的研究。二进制图像是简单的具有单通道的两个可能的像素值信号。与彩色和灰度图像相比，二值图像的简单性具有显著的优势，即计算效率。此外，大多数光学字符识别系统(O.C.R.)，如手写字符识别、车牌号码识别和银行支票识别系统，在其处理步骤中使用二进制图像或二值化。本文针对二值图像分类器提出了一种简单而有效的攻击方法--高效组合黑盒对抗攻击。我们在两个不同的数据集和三个分类网络上验证了该攻击技术的有效性，并展示了其性能。此外，我们还比较了我们提出的方法和最新的方法的优缺点以及适用性。



## **35. Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers**

利用反向触发器解决基于后门的深度神经网络水印的封装攻击 cs.CR

**SubmitDate**: 2022-08-30    [paper-pdf](http://arxiv.org/pdf/2208.14127v1)

**Authors**: Fangqi Li, Shilin Wang, Yun Zhu

**Abstracts**: Backdoor-based watermarking schemes were proposed to protect the intellectual property of artificial intelligence models, especially deep neural networks, under the black-box setting. Compared with ordinary backdoors, backdoor-based watermarks need to digitally incorporate the owner's identity, which fact adds extra requirements to the trigger generation and verification programs. Moreover, these concerns produce additional security risks after the watermarking scheme has been published for as a forensics tool or the owner's evidence has been eavesdropped on. This paper proposes the capsulation attack, an efficient method that can invalidate most established backdoor-based watermarking schemes without sacrificing the pirated model's functionality. By encapsulating the deep neural network with a rule-based or Bayes filter, an adversary can block ownership probing and reject the ownership verification. We propose a metric, CAScore, to measure a backdoor-based watermarking scheme's security against the capsulation attack. This paper also proposes a new backdoor-based deep neural network watermarking scheme that is secure against the capsulation attack by reversing the encoding process and randomizing the exposure of triggers.

摘要: 针对黑盒环境下人工智能模型，特别是深度神经网络的知识产权保护问题，提出了基于后门的数字水印方案。与普通的后门相比，基于后门的水印需要数字地结合所有者的身份，这实际上增加了对触发生成和验证程序的额外要求。此外，在水印方案作为取证工具发布或所有者的证据被窃听后，这些担忧会产生额外的安全风险。本文提出了一种有效的方法--封装攻击，它可以在不牺牲盗版模型功能的情况下使大多数已建立的基于后门的水印方案失效。通过使用基于规则的或贝叶斯过滤器封装深度神经网络，攻击者可以阻止所有权探测并拒绝所有权验证。我们提出了一个度量标准CAScore来衡量基于后门的水印方案抵抗封装攻击的安全性。本文还提出了一种新的基于后门的深度神经网络水印方案，该方案通过颠倒编码过程和随机化触发器的暴露来保证水印对封装攻击的安全性。



## **36. Adversarial Scratches: Deployable Attacks to CNN Classifiers**

对抗性抓痕：对CNN分类器的可部署攻击 cs.LG

This work is published at Pattern Recognition (Elsevier). This paper  stems from 'Scratch that! An Evolution-based Adversarial Attack against  Neural Networks' for which an arXiv preprint is available at  arXiv:1912.02316. Further studies led to a complete overhaul of the work,  resulting in this paper

**SubmitDate**: 2022-08-30    [paper-pdf](http://arxiv.org/pdf/2204.09397v2)

**Authors**: Loris Giulivi, Malhar Jere, Loris Rossi, Farinaz Koushanfar, Gabriela Ciocarlie, Briland Hitaj, Giacomo Boracchi

**Abstracts**: A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model's input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage B\'ezier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that, often, our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer queries and modifying very few pixels.

摘要: 越来越多的研究表明，深度神经网络很容易受到敌意例子的影响。这些采用的形式是应用于模型输入的小扰动，从而导致不正确的预测。不幸的是，大多数文献关注的是应用于数字图像的视觉上不可察觉的扰动，而根据设计，数字图像通常不可能被部署到物理目标上。我们提出了对抗性划痕：一种新颖的L0黑盒攻击，它采用图像划痕的形式，并且比其他最先进的攻击具有更大的可部署性。对抗性划痕利用B‘ezier曲线来减少搜索空间的维度，并可能将攻击限制在特定位置。我们在几个场景中测试了对抗性划痕，包括公开可用的API和交通标志图像。结果表明，我们的攻击通常比其他可部署的最先进方法获得更高的愚骗率，同时需要的查询和修改的像素也非常少。



## **37. Adversarial Examples for Good: Adversarial Examples Guided Imbalanced Learning**

好的对抗性例子：指导不平衡学习的对抗性例子 cs.LG

Appeared in ICIP 2022

**SubmitDate**: 2022-08-30    [paper-pdf](http://arxiv.org/pdf/2201.12356v2)

**Authors**: Jie Zhang, Lei Zhang, Gang Li, Chao Wu

**Abstracts**: Adversarial examples are inputs for machine learning models that have been designed by attackers to cause the model to make mistakes. In this paper, we demonstrate that adversarial examples can also be utilized for good to improve the performance of imbalanced learning. We provide a new perspective on how to deal with imbalanced data: adjust the biased decision boundary by training with Guiding Adversarial Examples (GAEs). Our method can effectively increase the accuracy of minority classes while sacrificing little accuracy on majority classes. We empirically show, on several benchmark datasets, our proposed method is comparable to the state-of-the-art method. To our best knowledge, we are the first to deal with imbalanced learning with adversarial examples.

摘要: 对抗性例子是攻击者为使模型出错而设计的机器学习模型的输入。在本文中，我们证明了对抗性例子也可以被用来改善不平衡学习的性能。我们为如何处理不平衡的数据提供了一个新的视角：通过用指导性对手实例(GAE)进行训练来调整有偏的决策边界。我们的方法可以有效地提高少数类的准确率，而对多数类的准确率损失很小。我们在几个基准数据集上的实验表明，我们提出的方法与最先进的方法是相当的。据我们所知，我们是第一个用对抗性例子来处理不平衡学习的人。



## **38. Reducing Certified Regression to Certified Classification**

将认证回归归结为认证分类 cs.LG

**SubmitDate**: 2022-08-29    [paper-pdf](http://arxiv.org/pdf/2208.13904v1)

**Authors**: Zayd Hammoudeh, Daniel Lowd

**Abstracts**: Adversarial training instances can severely distort a model's behavior. This work investigates certified regression defenses, which provide guaranteed limits on how much a regressor's prediction may change under a training-set attack. Our key insight is that certified regression reduces to certified classification when using median as a model's primary decision function. Coupling our reduction with existing certified classifiers, we propose six new provably-robust regressors. To the extent of our knowledge, this is the first work that certifies the robustness of individual regression predictions without any assumptions about the data distribution and model architecture. We also show that existing state-of-the-art certified classifiers often make overly-pessimistic assumptions that can degrade their provable guarantees. We introduce a tighter analysis of model robustness, which in many cases results in significantly improved certified guarantees. Lastly, we empirically demonstrate our approaches' effectiveness on both regression and classification data, where the accuracy of up to 50% of test predictions can be guaranteed under 1% training-set corruption and up to 30% of predictions under 4% corruption. Our source code is available at https://github.com/ZaydH/certified-regression.

摘要: 对抗性训练实例会严重扭曲模型的行为。这项工作调查了经过认证的回归防御，它为回归者的预测在训练集攻击下可能改变多少提供了保证限制。我们的主要见解是，当使用中值作为模型的主要决策函数时，认证回归简化为认证分类。结合我们的约简和现有的认证分类器，我们提出了六个新的可证明稳健的回归。就我们所知，这是第一个在没有任何关于数据分布和模型体系结构的假设的情况下证明个体回归预测的稳健性的工作。我们还表明，现有的最先进的认证分类器经常做出过于悲观的假设，这可能会降低他们的可证明保证。我们引入了对模型稳健性的更严格的分析，这在许多情况下导致了显著改进的认证保证。最后，我们在回归和分类数据上验证了我们的方法的有效性，其中在1%的训练集损坏情况下可以保证高达50%的测试预测的准确性，在4%的损坏情况下可以保证高达30%的预测准确率。我们的源代码可以在https://github.com/ZaydH/certified-regression.上找到



## **39. Reinforcement Learning for Hardware Security: Opportunities, Developments, and Challenges**

硬件安全强化学习：机遇、发展和挑战 cs.CR

To Appear in 2022 19th International SoC Conference (ISOCC 2022),  October 2022

**SubmitDate**: 2022-08-29    [paper-pdf](http://arxiv.org/pdf/2208.13885v1)

**Authors**: Satwik Patnaik, Vasudev Gohil, Hao Guo, Jeyavijayan, Rajendran

**Abstracts**: Reinforcement learning (RL) is a machine learning paradigm where an autonomous agent learns to make an optimal sequence of decisions by interacting with the underlying environment. The promise demonstrated by RL-guided workflows in unraveling electronic design automation problems has encouraged hardware security researchers to utilize autonomous RL agents in solving domain-specific problems. From the perspective of hardware security, such autonomous agents are appealing as they can generate optimal actions in an unknown adversarial environment. On the other hand, the continued globalization of the integrated circuit supply chain has forced chip fabrication to off-shore, untrustworthy entities, leading to increased concerns about the security of the hardware. Furthermore, the unknown adversarial environment and increasing design complexity make it challenging for defenders to detect subtle modifications made by attackers (a.k.a. hardware Trojans). In this brief, we outline the development of RL agents in detecting hardware Trojans, one of the most challenging hardware security problems. Additionally, we outline potential opportunities and enlist the challenges of applying RL to solve hardware security problems.

摘要: 强化学习(RL)是一种机器学习范式，其中自主代理通过与底层环境交互来学习做出最优决策序列。RL引导的工作流在解决电子设计自动化问题方面所展示的前景鼓励了硬件安全研究人员利用自主RL代理来解决特定领域的问题。从硬件安全的角度来看，这样的自治代理很有吸引力，因为它们可以在未知的对抗性环境中产生最优动作。另一方面，集成电路供应链的持续全球化迫使芯片制造转移到离岸、不可信赖的实体，导致对硬件安全的担忧增加。此外，未知的对手环境和日益增加的设计复杂性使得防御者很难检测到攻击者所做的微妙修改(又名。硬件特洛伊木马)。在本文中，我们概述了RL代理在检测硬件特洛伊木马方面的发展，硬件特洛伊木马是最具挑战性的硬件安全问题之一。此外，我们还概述了应用RL解决硬件安全问题的潜在机遇和挑战。



## **40. Towards Adversarial Purification using Denoising AutoEncoders**

使用去噪自动编码器进行对抗性净化 cs.LG

Submitted to AAAI 2023

**SubmitDate**: 2022-08-29    [paper-pdf](http://arxiv.org/pdf/2208.13838v1)

**Authors**: Dvij Kalaria, Aritra Hazra, Partha Pratim Chakrabarti

**Abstracts**: With the rapid advancement and increased use of deep learning models in image identification, security becomes a major concern to their deployment in safety-critical systems. Since the accuracy and robustness of deep learning models are primarily attributed from the purity of the training samples, therefore the deep learning architectures are often susceptible to adversarial attacks. Adversarial attacks are often obtained by making subtle perturbations to normal images, which are mostly imperceptible to humans, but can seriously confuse the state-of-the-art machine learning models. We propose a framework, named APuDAE, leveraging Denoising AutoEncoders (DAEs) to purify these samples by using them in an adaptive way and thus improve the classification accuracy of the target classifier networks that have been attacked. We also show how using DAEs adaptively instead of using them directly, improves classification accuracy further and is more robust to the possibility of designing adaptive attacks to fool them. We demonstrate our results over MNIST, CIFAR-10, ImageNet dataset and show how our framework (APuDAE) provides comparable and in most cases better performance to the baseline methods in purifying adversaries. We also design adaptive attack specifically designed to attack our purifying model and demonstrate how our defense is robust to that.

摘要: 随着深度学习模型在图像识别中的快速发展和越来越多的使用，安全性成为它们在安全关键系统中部署的主要考虑因素。由于深度学习模型的准确性和稳健性主要取决于训练样本的纯度，因此深度学习结构往往容易受到敌意攻击。对抗性攻击通常是通过对正常图像进行微妙的扰动来获得的，这通常是人类无法察觉的，但会严重混淆最先进的机器学习模型。我们提出了一个名为APuDAE的框架，利用去噪自动编码器(DAE)自适应地使用这些样本来净化这些样本，从而提高了受到攻击的目标分类器网络的分类精度。我们还展示了如何自适应地使用DAE而不是直接使用DAE，进一步提高了分类精度，并对设计自适应攻击来愚弄它们的可能性具有更强的鲁棒性。我们在MNIST、CIFAR-10、ImageNet数据集上展示了我们的结果，并展示了我们的框架(APuDAE)如何在净化对手方面提供与基线方法相当且在大多数情况下更好的性能。我们还设计了专门为攻击我们的净化模型而设计的自适应攻击，并展示了我们的防御是如何健壮的。



## **41. Demystifying Arch-hints for Model Extraction: An Attack in Unified Memory System**

揭开模型提取拱门的神秘面纱：统一存储系统中的攻击 cs.CR

**SubmitDate**: 2022-08-29    [paper-pdf](http://arxiv.org/pdf/2208.13720v1)

**Authors**: Zhendong Wang, Xiaoming Zeng, Xulong Tang, Danfeng Zhang, Xing Hu, Yang Hu

**Abstracts**: The deep neural network (DNN) models are deemed confidential due to their unique value in expensive training efforts, privacy-sensitive training data, and proprietary network characteristics. Consequently, the model value raises incentive for adversary to steal the model for profits, such as the representative model extraction attack. Emerging attack can leverage timing-sensitive architecture-level events (i.e., Arch-hints) disclosed in hardware platforms to extract DNN model layer information accurately. In this paper, we take the first step to uncover the root cause of such Arch-hints and summarize the principles to identify them. We then apply these principles to emerging Unified Memory (UM) management system and identify three new Arch-hints caused by UM's unique data movement patterns. We then develop a new extraction attack, UMProbe. We also create the first DNN benchmark suite in UM and utilize the benchmark suite to evaluate UMProbe. Our evaluation shows that UMProbe can extract the layer sequence with an accuracy of 95% for almost all victim test models, which thus calls for more attention to the DNN security in UM system.

摘要: 深度神经网络(DNN)模型被认为是机密的，因为它们在昂贵的训练工作、隐私敏感的训练数据和专有的网络特征方面具有独特的价值。因此，模型值增加了攻击者窃取模型以获取利润的动机，例如典型的模型提取攻击。新出现的攻击可以利用硬件平台中披露的计时敏感的架构级事件(即Arch-hints)来准确提取DNN模型层信息。在这篇文章中，我们首先要找出这些暗示语产生的根本原因，并总结出识别这些暗示语的原则。然后，我们将这些原则应用到新兴的统一内存(UM)管理系统中，并确定了由UM独特的数据移动模式引起的三个新的拱形提示。然后，我们开发了一种新的提取攻击UMProbe。我们还创建了UM中的第一个DNN基准测试套件，并利用该基准测试套件对UMProbe进行了评估。我们的评估表明，UMProbe对几乎所有的受害者测试模型都能以95%的准确率提取层序列，这就要求我们更多地关注UM系统中的DNN安全性。



## **42. Understanding the Limits of Poisoning Attacks in Episodic Reinforcement Learning**

在情景强化学习中理解中毒攻击的限度 cs.LG

Accepted at International Joint Conferences on Artificial  Intelligence (IJCAI) 2022

**SubmitDate**: 2022-08-29    [paper-pdf](http://arxiv.org/pdf/2208.13663v1)

**Authors**: Anshuka Rangi, Haifeng Xu, Long Tran-Thanh, Massimo Franceschetti

**Abstracts**: To understand the security threats to reinforcement learning (RL) algorithms, this paper studies poisoning attacks to manipulate \emph{any} order-optimal learning algorithm towards a targeted policy in episodic RL and examines the potential damage of two natural types of poisoning attacks, i.e., the manipulation of \emph{reward} and \emph{action}. We discover that the effect of attacks crucially depend on whether the rewards are bounded or unbounded. In bounded reward settings, we show that only reward manipulation or only action manipulation cannot guarantee a successful attack. However, by combining reward and action manipulation, the adversary can manipulate any order-optimal learning algorithm to follow any targeted policy with $\tilde{\Theta}(\sqrt{T})$ total attack cost, which is order-optimal, without any knowledge of the underlying MDP. In contrast, in unbounded reward settings, we show that reward manipulation attacks are sufficient for an adversary to successfully manipulate any order-optimal learning algorithm to follow any targeted policy using $\tilde{O}(\sqrt{T})$ amount of contamination. Our results reveal useful insights about what can or cannot be achieved by poisoning attacks, and are set to spur more works on the design of robust RL algorithms.

摘要: 为了了解强化学习(RL)算法面临的安全威胁，研究了剧情强化学习(RL)中针对目标策略操纵任意阶优化学习算法的中毒攻击，并考察了两种自然类型的中毒攻击，即操纵EMPH{REWART}和EMPH{ACTION}的潜在危害.我们发现，攻击的效果关键取决于回报是有界的还是无界的。在有界奖赏设置下，我们证明了仅有奖赏操纵或仅有动作操纵不能保证攻击成功。然而，通过结合奖励和动作操纵，攻击者可以操纵任何顺序最优的学习算法来遵循任何具有$tide{\theta}(\Sqrt{T})$攻击总代价的目标策略，这是顺序最优的，而不需要知道潜在的MDP。相反，在无界奖赏环境下，我们证明了奖赏操纵攻击足以使敌手成功操纵任何阶数最优学习算法来遵循任何目标策略，并且使用$tide{O}(\sqrt{T})$污染量。我们的结果揭示了关于中毒攻击可以或不能实现什么的有用见解，并将刺激更多关于健壮RL算法设计的工作。



## **43. HAT4RD: Hierarchical Adversarial Training for Rumor Detection on Social Media**

HAT4RD：针对社交媒体谣言检测的分级对抗性训练 cs.CL

**SubmitDate**: 2022-08-29    [paper-pdf](http://arxiv.org/pdf/2110.00425v2)

**Authors**: Shiwen Ni, Jiawen Li, Hung-Yu Kao

**Abstracts**: With the development of social media, social communication has changed. While this facilitates people's communication and access to information, it also provides an ideal platform for spreading rumors. In normal or critical situations, rumors will affect people's judgment and even endanger social security. However, natural language is high-dimensional and sparse, and the same rumor may be expressed in hundreds of ways on social media. As such, the robustness and generalization of the current rumor detection model are put into question. We proposed a novel \textbf{h}ierarchical \textbf{a}dversarial \textbf{t}raining method for \textbf{r}umor \textbf{d}etection (HAT4RD) on social media. Specifically, HAT4RD is based on gradient ascent by adding adversarial perturbations to the embedding layers of post-level and event-level modules to deceive the detector. At the same time, the detector uses stochastic gradient descent to minimize the adversarial risk to learn a more robust model. In this way, the post-level and event-level sample spaces are enhanced, and we have verified the robustness of our model under a variety of adversarial attacks. Moreover, visual experiments indicate that the proposed model drifts into an area with a flat loss landscape, leading to better generalization. We evaluate our proposed method on three public rumors datasets from two commonly used social platforms (Twitter and Weibo). Experiment results demonstrate that our model achieves better results than state-of-the-art methods.

摘要: 随着社交媒体的发展，社交传播也发生了变化。这在方便人们交流和获取信息的同时，也为谣言传播提供了一个理想的平台。在正常或危急的情况下，谣言会影响人们的判断，甚至危害社会治安。然而，自然语言是高维和稀疏的，同样的谣言可能会在社交媒体上以数百种方式表达。因此，当前谣言检测模型的健壮性和泛化能力受到了质疑。针对社交媒体上的文本bf{r}和文本bf{d}评论，我们提出了一种新的层次式文本bf{a}分布式文本bf{t}挖掘方法(HAT4RD)。具体地说，HAT4RD是基于梯度上升的，通过在后期和事件级模块的嵌入层添加对抗性扰动来欺骗检测器。同时，检测器使用随机梯度下降来最小化对抗风险，以学习更健壮的模型。通过这种方式，增强了后级和事件级样本空间，验证了该模型在各种对抗性攻击下的健壮性。此外，视觉实验表明，所提出的模型漂移到损失平坦的区域，具有更好的泛化能力。我们在两个常用社交平台(Twitter和微博)的三个公开谣言数据集上对我们提出的方法进行了评估。实验结果表明，我们的模型取得了比现有方法更好的结果。



## **44. Towards Both Accurate and Robust Neural Networks without Extra Data**

无额外数据的精确和稳健的神经网络 cs.CV

**SubmitDate**: 2022-08-29    [paper-pdf](http://arxiv.org/pdf/2103.13124v2)

**Authors**: Faqiang Liu, Rong Zhao

**Abstracts**: Deep neural networks have achieved remarkable performance in various applications but are extremely vulnerable to adversarial perturbation. The most representative and promising methods that can enhance model robustness, such as adversarial training and its variants, substantially degrade model accuracy on benign samples, limiting practical utility. Although incorporating extra training data can alleviate the trade-off to a certain extent, it remains unsolved to achieve both robustness and accuracy under limited training data. Here, we demonstrate the feasibility of overcoming the trade-off, by developing an adversarial feature stacking (AFS) model, which combines multiple independent feature extractors with varied levels of robustness and accuracy. Theoretical analysis is further conducted, and general principles for the selection of basic feature extractors are provided. We evaluate the AFS model on CIFAR-10 and CIFAR-100 datasets with strong adaptive attack methods, significantly advancing the state-of-the-art in terms of the trade-off. The AFS model achieves a benign accuracy improvement of ~6% on CIFAR-10 and ~10% on CIFAR-100 with comparable or even stronger robustness than the state-of-the-art adversarial training methods.

摘要: 深度神经网络在各种应用中取得了显著的性能，但极易受到对抗性扰动的影响。最具代表性和最有前景的增强模型稳健性的方法，如对抗性训练及其变种，大大降低了良性样本的模型精度，限制了实际应用。虽然加入额外的训练数据可以在一定程度上缓解这一权衡，但在有限的训练数据下达到稳健性和准确性的问题仍然没有解决。在这里，我们通过开发一种对抗性特征堆栈(AFS)模型来证明克服这种权衡的可行性，该模型结合了多个具有不同水平的稳健性和准确性的独立特征提取器。在此基础上进行了理论分析，给出了基本特征提取算子选择的一般原则。我们在CIFAR-10和CIFAR-100数据集上使用强自适应攻击方法对AFS模型进行了评估，在权衡方面显著提高了最新水平。AFS模型在CIFAR-10和CIFAR-100上的准确率分别提高了~6%和~10%，与最先进的对抗性训练方法相比，具有相当甚至更强的鲁棒性。



## **45. Tricking the Hashing Trick: A Tight Lower Bound on the Robustness of CountSketch to Adaptive Inputs**

欺骗散列技巧：CountSketch对自适应输入的稳健性的紧致下界 cs.DS

**SubmitDate**: 2022-08-28    [paper-pdf](http://arxiv.org/pdf/2207.00956v2)

**Authors**: Edith Cohen, Jelani Nelson, Tamás Sarlós, Uri Stemmer

**Abstracts**: CountSketch and Feature Hashing (the "hashing trick") are popular randomized dimensionality reduction methods that support recovery of $\ell_2$-heavy hitters (keys $i$ where $v_i^2 > \epsilon \|\boldsymbol{v}\|_2^2$) and approximate inner products. When the inputs are {\em not adaptive} (do not depend on prior outputs), classic estimators applied to a sketch of size $O(\ell/\epsilon)$ are accurate for a number of queries that is exponential in $\ell$. When inputs are adaptive, however, an adversarial input can be constructed after $O(\ell)$ queries with the classic estimator and the best known robust estimator only supports $\tilde{O}(\ell^2)$ queries. In this work we show that this quadratic dependence is in a sense inherent: We design an attack that after $O(\ell^2)$ queries produces an adversarial input vector whose sketch is highly biased. Our attack uses "natural" non-adaptive inputs (only the final adversarial input is chosen adaptively) and universally applies with any correct estimator, including one that is unknown to the attacker. In that, we expose inherent vulnerability of this fundamental method.

摘要: CountSketch和Feature Hash(散列技巧)是流行的随机降维方法，支持恢复$\ell_2$-重打击者(key$i$where$v_i^2>\epsilon\boldsign{v}\_2^2$)和近似内积。当输入是{\em不自适应的}(不依赖于先前的输出)时，应用于大小为$O(\ell/\epsilon)$的草图的经典估计器对于许多以$\ell$为指数的查询是准确的。然而，当输入是自适应的时，可以用经典估计在$O(\ell)$查询之后构造敌意输入，而最著名的稳健估计只支持$T{O}(\ell^2)$查询。在这项工作中，我们证明了这种二次依赖在某种意义上是固有的：我们设计了一个攻击，在$O(\ell^2)$查询后产生一个高度有偏的敌对输入向量。我们的攻击使用“自然的”非自适应输入(只有最终的对抗性输入是自适应选择的)，并且普遍适用于任何正确的估计器，包括攻击者未知的估计器。在这一点上，我们暴露了这一基本方法的固有弱点。



## **46. Categorical composable cryptography: extended version**

范畴可合成密码学：扩展版本 cs.CR

Extended version of arXiv:2105.05949 which appeared in FoSSaCS 2022

**SubmitDate**: 2022-08-28    [paper-pdf](http://arxiv.org/pdf/2208.13232v1)

**Authors**: Anne Broadbent, Martti Karvonen

**Abstracts**: We formalize the simulation paradigm of cryptography in terms of category theory and show that protocols secure against abstract attacks form a symmetric monoidal category, thus giving an abstract model of composable security definitions in cryptography. Our model is able to incorporate computational security, set-up assumptions and various attack models such as colluding or independently acting subsets of adversaries in a modular, flexible fashion. W We conclude by using string diagrams to rederive the security of the one-time pad and no-go results concerning the limits of bipartite and tripartite cryptography, ruling out e.g., composable commitments and broadcasting. On the way, we exhibit two categorical constructions of resource theories that might be of independent interest: one capturing resources shared among multiple parties and one capturing resource conversions that succeed asymptotically.

摘要: 我们用范畴理论形式化了密码学的模拟范型，证明了对抽象攻击安全的协议形成了对称的么半范畴，从而给出了密码学中可组合安全定义的抽象模型。我们的模型能够以模块化、灵活的方式结合计算安全性、设置假设和各种攻击模型，例如串通或独立行动的对手子集。W我们最后使用字符串图重新推导了关于两方和三方密码术限制的一次一密和不进行结果的安全性，排除了例如可组合承诺和广播。在此过程中，我们展示了两种可能独立感兴趣的资源理论范畴结构：一种是捕获多方共享的资源，另一种是捕获渐近成功的资源转换。



## **47. Categorical composable cryptography**

范畴可合成密码学 cs.CR

Updated to match the proceedings version

**SubmitDate**: 2022-08-28    [paper-pdf](http://arxiv.org/pdf/2105.05949v3)

**Authors**: Anne Broadbent, Martti Karvonen

**Abstracts**: We formalize the simulation paradigm of cryptography in terms of category theory and show that protocols secure against abstract attacks form a symmetric monoidal category, thus giving an abstract model of composable security definitions in cryptography. Our model is able to incorporate computational security, set-up assumptions and various attack models such as colluding or independently acting subsets of adversaries in a modular, flexible fashion. We conclude by using string diagrams to rederive the security of the one-time pad and no-go results concerning the limits of bipartite and tripartite cryptography, ruling out e.g., composable commitments and broadcasting.

摘要: 我们用范畴理论形式化了密码学的模拟范型，证明了对抽象攻击安全的协议形成了对称的么半范畴，从而给出了密码学中可组合安全定义的抽象模型。我们的模型能够以模块化、灵活的方式结合计算安全性、设置假设和各种攻击模型，例如串通或独立行动的对手子集。最后，我们使用字符串图重新推导出关于两方和三方密码术限制的一次性密码本和禁止结果的安全性，排除了例如可组合承诺和广播。



## **48. Self-Supervised Adversarial Example Detection by Disentangled Representation**

基于解缠表示的自监督敌意范例检测 cs.CV

to appear in TrustCom 2022

**SubmitDate**: 2022-08-28    [paper-pdf](http://arxiv.org/pdf/2105.03689v4)

**Authors**: Zhaoxi Zhang, Leo Yu Zhang, Xufei Zheng, Jinyu Tian, Jiantao Zhou

**Abstracts**: Deep learning models are known to be vulnerable to adversarial examples that are elaborately designed for malicious purposes and are imperceptible to the human perceptual system. Autoencoder, when trained solely over benign examples, has been widely used for (self-supervised) adversarial detection based on the assumption that adversarial examples yield larger reconstruction errors. However, because lacking adversarial examples in its training and the too strong generalization ability of autoencoder, this assumption does not always hold true in practice. To alleviate this problem, we explore how to detect adversarial examples with disentangled label/semantic features under the autoencoder structure. Specifically, we propose Disentangled Representation-based Reconstruction (DRR). In DRR, we train an autoencoder over both correctly paired label/semantic features and incorrectly paired label/semantic features to reconstruct benign and counterexamples. This mimics the behavior of adversarial examples and can reduce the unnecessary generalization ability of autoencoder. We compare our method with the state-of-the-art self-supervised detection methods under different adversarial attacks and different victim models, and it exhibits better performance in various metrics (area under the ROC curve, true positive rate, and true negative rate) for most attack settings. Though DRR is initially designed for visual tasks only, we demonstrate that it can be easily extended for natural language tasks as well. Notably, different from other autoencoder-based detectors, our method can provide resistance to the adaptive adversary.

摘要: 众所周知，深度学习模型容易受到敌意例子的攻击，这些例子是为恶意目的精心设计的，人类感知系统无法察觉。当仅对良性样本进行训练时，自动编码器已被广泛用于(自监督)敌意检测，其基础是假设对抗性样本产生更大的重建误差。然而，由于在训练中缺乏对抗性的例子，而且自动编码器的泛化能力太强，这一假设在实践中并不总是成立的。为了缓解这一问题，我们探索了如何在自动编码器结构下检测具有分离的标签/语义特征的对抗性示例。具体地说，我们提出了基于解缠表示的重建算法(DRR)。在DRR中，我们训练一个自动编码器，包括正确配对的标签/语义特征和错误配对的标签/语义特征，以重建良性和反例。这模仿了对抗性例子的行为，并且可以降低自动编码器不必要的泛化能力。在不同的对手攻击和不同的受害者模型下，我们的方法与最新的自监督检测方法进行了比较，在大多数攻击环境下，它在各种指标(ROC曲线下面积、真正确率和真负率)上都表现出了更好的性能。虽然DRR最初是为视觉任务设计的，但我们演示了它也可以很容易地扩展到自然语言任务。值得注意的是，与其他基于自动编码器的检测器不同，我们的方法可以提供对自适应对手的抵抗。



## **49. Cross-domain Cross-architecture Black-box Attacks on Fine-tuned Models with Transferred Evolutionary Strategies**

基于转移进化策略的精调模型跨域跨体系结构黑盒攻击 cs.LG

To appear in CIKM 2022

**SubmitDate**: 2022-08-28    [paper-pdf](http://arxiv.org/pdf/2208.13182v1)

**Authors**: Yinghua Zhang, Yangqiu Song, Kun Bai, Qiang Yang

**Abstracts**: Fine-tuning can be vulnerable to adversarial attacks. Existing works about black-box attacks on fine-tuned models (BAFT) are limited by strong assumptions. To fill the gap, we propose two novel BAFT settings, cross-domain and cross-domain cross-architecture BAFT, which only assume that (1) the target model for attacking is a fine-tuned model, and (2) the source domain data is known and accessible. To successfully attack fine-tuned models under both settings, we propose to first train an adversarial generator against the source model, which adopts an encoder-decoder architecture and maps a clean input to an adversarial example. Then we search in the low-dimensional latent space produced by the encoder of the adversarial generator. The search is conducted under the guidance of the surrogate gradient obtained from the source model. Experimental results on different domains and different network architectures demonstrate that the proposed attack method can effectively and efficiently attack the fine-tuned models.

摘要: 微调很容易受到对手的攻击。现有的关于精调模型(BAFT)上的黑盒攻击的工作都受到强假设的限制。为了填补这一空白，我们提出了两种新的BAFT设置，跨域和跨域跨架构BAFT，它们只假设(1)攻击的目标模型是微调的模型，(2)源域数据是已知的和可访问的。为了在这两种情况下成功攻击微调模型，我们建议首先针对源模型训练敌意生成器，该模型采用编解码器架构，并将干净的输入映射到对抗性示例。然后在对抗性生成器的编码器产生的低维潜在空间中进行搜索。搜索是在从源模型获得的代理梯度的指导下进行的。在不同域和不同网络体系结构上的实验结果表明，该攻击方法能够有效地攻击微调模型。



## **50. Cyberattacks on Energy Infrastructures: Modern War Weapons**

对能源基础设施的网络攻击：现代战争武器 cs.CR

**SubmitDate**: 2022-08-28    [paper-pdf](http://arxiv.org/pdf/2208.14225v1)

**Authors**: Tawfiq M. Aljohani

**Abstracts**: Recent high-profile cyberattacks on energy infrastructures, such as the security breach of the Colonial Pipeline in 2021 and attacks that have disrupted Ukraine's power grid from the mid-2010s till date, have pushed cybersecurity as a top priority. As political tensions have escalated in Europe this year, concerns about critical infrastructure security have increased. Operators in the industrial sector face new cybersecurity threats that increase the risk of disruptions in services, property damages, and environmental harm. Amid rising geopolitical tensions, industrial companies, with their network-connected systems, are now considered major targets for adversaries to advance political, social, or military agendas. Moreover, the recent Russian-Ukrainian conflict has set the alarm worldwide about the danger of targeting energy grids via cyberattacks. Attack methodologies, techniques, and procedures used successfully to hack energy grids in Ukraine can be used elsewhere. This work aims to present a thorough analysis of the cybersecurity of the energy infrastructure amid the increased rise of cyberwars. The article navigates through the recent history of energy-related cyberattacks and their reasoning, discusses the grid's vulnerability, and makes a precautionary argument for securing the grids against them.

摘要: 最近针对能源基础设施的高调网络攻击，例如2021年殖民管道的安全漏洞，以及从2010年代中期到目前为止扰乱乌克兰电网的攻击，已将网络安全推上了头等大事。随着今年欧洲政治紧张局势的升级，人们对关键基础设施安全的担忧有所增加。工业部门的运营商面临新的网络安全威胁，增加了服务中断、财产损失和环境损害的风险。在日益加剧的地缘政治紧张局势中，工业公司凭借其联网系统，现在被认为是对手推进政治、社会或军事议程的主要目标。此外，最近的俄罗斯和乌克兰的冲突在全球范围内敲响了警钟，提醒人们通过网络攻击将能源电网作为目标的危险。成功入侵乌克兰能源电网的攻击方法、技术和程序也可以在其他地方使用。这项工作旨在对网络战争日益加剧的情况下能源基础设施的网络安全进行彻底的分析。这篇文章介绍了与能源有关的网络攻击的近期历史及其原因，讨论了电网的脆弱性，并提出了保护电网免受攻击的预防性论点。



